{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Sim RL Training\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Environment\n",
    "from osim.env import L2RunEnv as ENV # rename environment to be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.processors import WhiteningNormalizerProcessor\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class\n",
    "Reference: https://github.com/keras-rl/keras-rl/blob/master/examples/ddpg_mujoco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,env):\n",
    "        nb_actions = env.action_space.shape[0]\n",
    "        \n",
    "        self.env = env\n",
    "        self.actor = self.build_actor(env)\n",
    "        self.critic, action_input = self.build_critic(env)\n",
    "        self.loss = self.build_loss()\n",
    "\n",
    "        self.memory = SequentialMemory(limit=100000, window_length=1)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.1)\n",
    "        self.agent = DDPGAgent(   nb_actions=nb_actions, actor=self.actor, \n",
    "                                  critic=self.critic, critic_action_input=action_input,\n",
    "                                  memory=self.memory, nb_steps_warmup_critic=1000, \n",
    "                                  nb_steps_warmup_actor=1000,\n",
    "                                  random_process=self.random_process, \n",
    "                                  gamma=.99, target_model_update=1e-3,\n",
    "                                  processor=WhiteningNormalizerProcessor()  )\n",
    "        self.agent.compile([Adam(lr=1e-4), Adam(lr=1e-3)], metrics=self.loss)\n",
    "\n",
    "    def build_loss(self):\n",
    "        return ['mse']\n",
    "\n",
    "    def build_actor(self,env):\n",
    "        nb_actions = env.action_space.shape[0]\n",
    "        actor = Sequential()\n",
    "        actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "        actor.add(Dense(400))\n",
    "        actor.add(Activation('relu'))\n",
    "        actor.add(Dense(300))\n",
    "        actor.add(Activation('relu'))\n",
    "        actor.add(Dense(nb_actions,\n",
    "                        activation='tanh',\n",
    "                        kernel_constraint=  keras.constraints.min_max_norm(\n",
    "                                            min_value=0,\n",
    "                                            max_value=nb_actions,\n",
    "                                            axis=1) ) )\n",
    "        actor.summary()\n",
    "\n",
    "        inD = Input(shape=(1,) + env.observation_space.shape)\n",
    "        out = actor(inD)\n",
    "\n",
    "        return Model(inD,out)\n",
    "\n",
    "    def build_critic(self,env):\n",
    "        nb_actions = env.action_space.shape[0]\n",
    "        action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "        observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "        flattened_observation = Flatten()(observation_input)\n",
    "        x = Dense(400)(flattened_observation)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Concatenate()([x, action_input])\n",
    "        x = Dense(300)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('linear')(x)\n",
    "\n",
    "        critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "        critic.summary()\n",
    "\n",
    "        return critic, action_input\n",
    "    \n",
    "    def fit(self, **kwargs):\n",
    "        return self.agent.fit(self.env,**kwargs)\n",
    "    \n",
    "    def test(self, **kwargs):\n",
    "        return self.agent.test(self.env,**kwargs)\n",
    "    \n",
    "    def save_weights(self,filename='ddpg_{}_weights.h5f'):\n",
    "        self.agent.save_weights(filename.format(\"opensim\"), overwrite=True)\n",
    "        \n",
    "    def load_weights(self,filename='ddpg_{}_weights.h5f'):\n",
    "        self.agent.load_weights(filename.format(\"opensim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TrainEnv(ENV):\n",
    "    pass\n",
    "# TODO: define virtual assistant forces on agent\n",
    "# TODO: define search through easier environments\n",
    "# TODO: make environment harder once the agent has trained for challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Simulation\n",
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = TrainEnv(visualize=False)\n",
    "observation = env.reset( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               16800     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 18)                5418      \n",
      "=================================================================\n",
      "Total params: 142,518\n",
      "Trainable params: 142,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  (None, 1, 41)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 41)           0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 400)          16800       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 400)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 418)          0           activation_3[0][0]               \n",
      "                                                                 action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 300)          125700      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 300)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            301         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1)            0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,801\n",
      "Trainable params: 142,801\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.load_weights( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration: 0\n",
      "Training for 2000 steps ...\n",
      "  195/2000: episode: 1, duration: 5.798s, episode steps: 195, steps per second: 34, episode reward: -0.801, mean reward: -0.004 [-0.020, 0.004], mean action: 0.004 [-1.023, 0.857], mean observation: 0.073 [-4.036, 5.242], loss: --, mean_squared_error: --, mean_q: --\n",
      "  381/2000: episode: 2, duration: 4.646s, episode steps: 186, steps per second: 40, episode reward: -0.789, mean reward: -0.004 [-0.020, 0.005], mean action: 0.046 [-0.945, 1.067], mean observation: 0.065 [-5.320, 6.739], loss: --, mean_squared_error: --, mean_q: --\n",
      "  563/2000: episode: 3, duration: 4.741s, episode steps: 182, steps per second: 38, episode reward: 0.560, mean reward: 0.003 [-0.003, 0.011], mean action: 0.050 [-0.843, 0.993], mean observation: 0.135 [-5.071, 6.784], loss: --, mean_squared_error: --, mean_q: --\n",
      "  732/2000: episode: 4, duration: 4.466s, episode steps: 169, steps per second: 38, episode reward: 0.536, mean reward: 0.003 [-0.002, 0.010], mean action: 0.029 [-0.951, 1.155], mean observation: 0.137 [-4.417, 6.143], loss: --, mean_squared_error: --, mean_q: --\n",
      "  902/2000: episode: 5, duration: 4.453s, episode steps: 170, steps per second: 38, episode reward: 0.542, mean reward: 0.003 [-0.001, 0.011], mean action: 0.035 [-1.000, 1.044], mean observation: 0.137 [-4.008, 5.728], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1101/2000: episode: 6, duration: 9.510s, episode steps: 199, steps per second: 21, episode reward: -0.656, mean reward: -0.003 [-0.020, 0.004], mean action: 0.150 [-1.223, 1.147], mean observation: 0.064 [-12.922, 9.801], loss: 0.021893, mean_squared_error: 0.043786, mean_q: 0.120716\n",
      " 1236/2000: episode: 7, duration: 10.700s, episode steps: 135, steps per second: 13, episode reward: -0.815, mean reward: -0.006 [-0.016, 0.006], mean action: 0.404 [-1.198, 1.231], mean observation: 0.058 [-16.648, 11.220], loss: 0.004292, mean_squared_error: 0.008585, mean_q: 0.141282\n",
      " 1323/2000: episode: 8, duration: 24.046s, episode steps: 87, steps per second: 4, episode reward: 0.465, mean reward: 0.005 [-0.001, 0.012], mean action: 0.256 [-1.115, 1.181], mean observation: 0.189 [-13.052, 17.197], loss: 0.018967, mean_squared_error: 0.037934, mean_q: 0.190528\n",
      " 1471/2000: episode: 9, duration: 8.444s, episode steps: 148, steps per second: 18, episode reward: -0.757, mean reward: -0.005 [-0.018, 0.009], mean action: 0.011 [-1.133, 1.239], mean observation: 0.032 [-17.127, 20.011], loss: 0.003077, mean_squared_error: 0.006153, mean_q: 0.230466\n",
      " 1657/2000: episode: 10, duration: 8.289s, episode steps: 186, steps per second: 22, episode reward: -0.706, mean reward: -0.004 [-0.019, 0.009], mean action: 0.058 [-1.274, 1.228], mean observation: 0.049 [-16.611, 11.664], loss: 0.002434, mean_squared_error: 0.004869, mean_q: 0.258952\n",
      " 1756/2000: episode: 11, duration: 7.556s, episode steps: 99, steps per second: 13, episode reward: -0.428, mean reward: -0.004 [-0.022, 0.003], mean action: -0.118 [-1.160, 1.149], mean observation: 0.006 [-12.281, 13.668], loss: 0.002118, mean_squared_error: 0.004235, mean_q: 0.285388\n",
      " 1833/2000: episode: 12, duration: 8.147s, episode steps: 77, steps per second: 9, episode reward: -0.516, mean reward: -0.007 [-0.018, 0.008], mean action: 0.022 [-1.120, 1.229], mean observation: -0.051 [-16.303, 13.651], loss: 0.003020, mean_squared_error: 0.006041, mean_q: 0.293694\n",
      " 1914/2000: episode: 13, duration: 9.312s, episode steps: 81, steps per second: 9, episode reward: -0.443, mean reward: -0.005 [-0.018, 0.009], mean action: 0.093 [-1.130, 1.186], mean observation: 0.032 [-31.660, 15.249], loss: 0.002453, mean_squared_error: 0.004907, mean_q: 0.311154\n",
      " 1939/2000: episode: 14, duration: 4.827s, episode steps: 25, steps per second: 5, episode reward: -0.011, mean reward: -0.000 [-0.002, 0.003], mean action: -0.014 [-1.084, 1.088], mean observation: -0.152 [-21.013, 9.517], loss: 0.004197, mean_squared_error: 0.008394, mean_q: 0.305331\n",
      " 1967/2000: episode: 15, duration: 4.820s, episode steps: 28, steps per second: 6, episode reward: 0.024, mean reward: 0.001 [-0.001, 0.006], mean action: 0.081 [-1.094, 1.081], mean observation: -0.116 [-24.251, 10.752], loss: 0.001959, mean_squared_error: 0.003919, mean_q: 0.322944\n",
      "done, took 122.746 seconds\n",
      "\n",
      "\n",
      "iteration: 1\n",
      "Training for 2000 steps ...\n",
      "   43/2000: episode: 1, duration: 4.955s, episode steps: 43, steps per second: 9, episode reward: 0.188, mean reward: 0.004 [0.001, 0.007], mean action: 0.038 [-1.099, 1.081], mean observation: -0.076 [-19.516, 10.122], loss: --, mean_squared_error: --, mean_q: --\n",
      "   85/2000: episode: 2, duration: 4.599s, episode steps: 42, steps per second: 9, episode reward: 0.179, mean reward: 0.004 [0.001, 0.007], mean action: 0.043 [-1.087, 1.087], mean observation: -0.077 [-19.728, 10.237], loss: --, mean_squared_error: --, mean_q: --\n",
      "  128/2000: episode: 3, duration: 4.616s, episode steps: 43, steps per second: 9, episode reward: 0.186, mean reward: 0.004 [0.001, 0.007], mean action: 0.014 [-1.146, 1.046], mean observation: -0.074 [-19.237, 10.290], loss: --, mean_squared_error: --, mean_q: --\n",
      "  170/2000: episode: 4, duration: 4.529s, episode steps: 42, steps per second: 9, episode reward: 0.174, mean reward: 0.004 [0.001, 0.006], mean action: 0.036 [-1.094, 1.049], mean observation: -0.076 [-19.651, 10.193], loss: --, mean_squared_error: --, mean_q: --\n",
      "  208/2000: episode: 5, duration: 3.926s, episode steps: 38, steps per second: 10, episode reward: 0.157, mean reward: 0.004 [0.001, 0.008], mean action: 0.045 [-1.109, 1.114], mean observation: -0.096 [-19.570, 10.308], loss: --, mean_squared_error: --, mean_q: --\n",
      "  246/2000: episode: 6, duration: 3.772s, episode steps: 38, steps per second: 10, episode reward: 0.168, mean reward: 0.004 [0.001, 0.008], mean action: 0.043 [-1.046, 1.102], mean observation: -0.102 [-18.520, 10.255], loss: --, mean_squared_error: --, mean_q: --\n",
      "  283/2000: episode: 7, duration: 3.521s, episode steps: 37, steps per second: 11, episode reward: 0.149, mean reward: 0.004 [0.001, 0.008], mean action: 0.035 [-1.085, 1.141], mean observation: -0.111 [-19.285, 10.267], loss: --, mean_squared_error: --, mean_q: --\n",
      "  325/2000: episode: 8, duration: 4.344s, episode steps: 42, steps per second: 10, episode reward: 0.173, mean reward: 0.004 [0.001, 0.006], mean action: 0.030 [-1.128, 1.168], mean observation: -0.076 [-19.433, 10.309], loss: --, mean_squared_error: --, mean_q: --\n",
      "  367/2000: episode: 9, duration: 4.373s, episode steps: 42, steps per second: 10, episode reward: 0.166, mean reward: 0.004 [0.001, 0.006], mean action: 0.043 [-1.082, 1.133], mean observation: -0.083 [-19.309, 10.355], loss: --, mean_squared_error: --, mean_q: --\n",
      "  409/2000: episode: 10, duration: 4.463s, episode steps: 42, steps per second: 9, episode reward: 0.179, mean reward: 0.004 [0.001, 0.007], mean action: 0.050 [-1.085, 1.170], mean observation: -0.078 [-19.749, 10.220], loss: --, mean_squared_error: --, mean_q: --\n",
      "  450/2000: episode: 11, duration: 4.387s, episode steps: 41, steps per second: 9, episode reward: 0.170, mean reward: 0.004 [0.001, 0.006], mean action: 0.041 [-1.070, 1.054], mean observation: -0.085 [-19.469, 10.309], loss: --, mean_squared_error: --, mean_q: --\n",
      "  491/2000: episode: 12, duration: 4.277s, episode steps: 41, steps per second: 10, episode reward: 0.175, mean reward: 0.004 [0.001, 0.007], mean action: 0.038 [-1.056, 1.111], mean observation: -0.086 [-19.532, 10.333], loss: --, mean_squared_error: --, mean_q: --\n",
      "  533/2000: episode: 13, duration: 4.515s, episode steps: 42, steps per second: 9, episode reward: 0.179, mean reward: 0.004 [0.001, 0.007], mean action: 0.034 [-1.134, 1.109], mean observation: -0.077 [-19.755, 10.189], loss: --, mean_squared_error: --, mean_q: --\n",
      "  575/2000: episode: 14, duration: 4.446s, episode steps: 42, steps per second: 9, episode reward: 0.171, mean reward: 0.004 [0.001, 0.006], mean action: 0.042 [-1.074, 1.097], mean observation: -0.077 [-19.625, 10.339], loss: --, mean_squared_error: --, mean_q: --\n",
      "  617/2000: episode: 15, duration: 4.561s, episode steps: 42, steps per second: 9, episode reward: 0.175, mean reward: 0.004 [0.001, 0.007], mean action: 0.035 [-1.093, 1.052], mean observation: -0.079 [-19.602, 10.269], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  660/2000: episode: 16, duration: 4.745s, episode steps: 43, steps per second: 9, episode reward: 0.174, mean reward: 0.004 [0.001, 0.007], mean action: 0.039 [-1.119, 1.145], mean observation: -0.074 [-19.306, 10.080], loss: --, mean_squared_error: --, mean_q: --\n",
      "  702/2000: episode: 17, duration: 4.391s, episode steps: 42, steps per second: 10, episode reward: 0.167, mean reward: 0.004 [0.001, 0.006], mean action: 0.031 [-1.068, 1.106], mean observation: -0.080 [-19.472, 10.284], loss: --, mean_squared_error: --, mean_q: --\n",
      "  744/2000: episode: 18, duration: 4.439s, episode steps: 42, steps per second: 9, episode reward: 0.172, mean reward: 0.004 [0.001, 0.006], mean action: 0.031 [-1.102, 1.088], mean observation: -0.079 [-19.139, 10.211], loss: --, mean_squared_error: --, mean_q: --\n",
      "  784/2000: episode: 19, duration: 4.227s, episode steps: 40, steps per second: 9, episode reward: 0.161, mean reward: 0.004 [0.001, 0.007], mean action: 0.046 [-1.065, 1.067], mean observation: -0.090 [-19.677, 10.278], loss: --, mean_squared_error: --, mean_q: --\n",
      "  826/2000: episode: 20, duration: 4.252s, episode steps: 42, steps per second: 10, episode reward: 0.165, mean reward: 0.004 [0.001, 0.006], mean action: 0.054 [-1.082, 1.112], mean observation: -0.077 [-19.400, 10.192], loss: --, mean_squared_error: --, mean_q: --\n",
      "  869/2000: episode: 21, duration: 4.773s, episode steps: 43, steps per second: 9, episode reward: 0.179, mean reward: 0.004 [0.001, 0.006], mean action: 0.032 [-1.118, 1.088], mean observation: -0.074 [-19.554, 10.178], loss: --, mean_squared_error: --, mean_q: --\n",
      "  911/2000: episode: 22, duration: 4.461s, episode steps: 42, steps per second: 9, episode reward: 0.166, mean reward: 0.004 [0.001, 0.006], mean action: 0.043 [-1.082, 1.133], mean observation: -0.077 [-19.530, 10.360], loss: --, mean_squared_error: --, mean_q: --\n",
      "  953/2000: episode: 23, duration: 4.321s, episode steps: 42, steps per second: 10, episode reward: 0.173, mean reward: 0.004 [0.001, 0.006], mean action: 0.037 [-1.115, 1.115], mean observation: -0.076 [-19.518, 10.167], loss: --, mean_squared_error: --, mean_q: --\n",
      "  992/2000: episode: 24, duration: 4.377s, episode steps: 39, steps per second: 9, episode reward: 0.168, mean reward: 0.004 [0.001, 0.008], mean action: 0.036 [-1.087, 1.071], mean observation: -0.098 [-19.818, 10.187], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1037/2000: episode: 25, duration: 5.627s, episode steps: 45, steps per second: 8, episode reward: 0.206, mean reward: 0.005 [0.001, 0.007], mean action: 0.010 [-1.122, 1.071], mean observation: -0.044 [-19.421, 10.329], loss: 0.005011, mean_squared_error: 0.010022, mean_q: 0.308737\n",
      " 1093/2000: episode: 26, duration: 7.703s, episode steps: 56, steps per second: 7, episode reward: 0.266, mean reward: 0.005 [0.001, 0.010], mean action: -0.068 [-1.178, 1.107], mean observation: 0.015 [-35.861, 10.352], loss: 0.003409, mean_squared_error: 0.006817, mean_q: 0.312467\n",
      " 1258/2000: episode: 27, duration: 15.637s, episode steps: 165, steps per second: 11, episode reward: -0.445, mean reward: -0.003 [-0.011, 0.009], mean action: 0.130 [-1.188, 1.180], mean observation: 0.032 [-13.002, 14.535], loss: 0.003213, mean_squared_error: 0.006426, mean_q: 0.327411\n",
      " 1355/2000: episode: 28, duration: 14.408s, episode steps: 97, steps per second: 7, episode reward: -0.541, mean reward: -0.006 [-0.019, 0.009], mean action: 0.078 [-1.167, 1.153], mean observation: -0.004 [-17.869, 17.257], loss: 0.002467, mean_squared_error: 0.004934, mean_q: 0.349024\n",
      " 1461/2000: episode: 29, duration: 9.512s, episode steps: 106, steps per second: 11, episode reward: -0.575, mean reward: -0.005 [-0.022, 0.009], mean action: 0.113 [-1.215, 1.210], mean observation: 0.018 [-11.940, 16.729], loss: 0.001720, mean_squared_error: 0.003440, mean_q: 0.354732\n",
      " 1533/2000: episode: 30, duration: 21.047s, episode steps: 72, steps per second: 3, episode reward: -0.402, mean reward: -0.006 [-0.012, 0.003], mean action: -0.030 [-1.138, 1.166], mean observation: -0.009 [-14.714, 13.124], loss: 0.002051, mean_squared_error: 0.004102, mean_q: 0.367915\n",
      " 1645/2000: episode: 31, duration: 15.986s, episode steps: 112, steps per second: 7, episode reward: -0.412, mean reward: -0.004 [-0.012, 0.003], mean action: -0.063 [-1.127, 1.126], mean observation: -0.030 [-15.034, 10.770], loss: 0.002121, mean_squared_error: 0.004242, mean_q: 0.374233\n",
      " 1718/2000: episode: 32, duration: 9.284s, episode steps: 73, steps per second: 8, episode reward: -0.441, mean reward: -0.006 [-0.014, 0.003], mean action: -0.088 [-1.271, 1.132], mean observation: -0.028 [-10.565, 11.013], loss: 0.001068, mean_squared_error: 0.002137, mean_q: 0.376417\n",
      " 1805/2000: episode: 33, duration: 12.501s, episode steps: 87, steps per second: 7, episode reward: -0.511, mean reward: -0.006 [-0.013, 0.003], mean action: -0.059 [-1.296, 1.135], mean observation: -0.014 [-12.943, 14.065], loss: 0.002432, mean_squared_error: 0.004865, mean_q: 0.384445\n",
      " 1951/2000: episode: 34, duration: 19.732s, episode steps: 146, steps per second: 7, episode reward: -1.009, mean reward: -0.007 [-0.020, 0.005], mean action: 0.019 [-1.214, 1.140], mean observation: 0.037 [-18.016, 16.064], loss: 0.000897, mean_squared_error: 0.001794, mean_q: 0.392397\n",
      "done, took 239.624 seconds\n",
      "\n",
      "\n",
      "iteration: 2\n",
      "Training for 2000 steps ...\n",
      "  127/2000: episode: 1, duration: 15.384s, episode steps: 127, steps per second: 8, episode reward: -0.857, mean reward: -0.007 [-0.023, 0.011], mean action: 0.055 [-1.192, 1.155], mean observation: 0.042 [-42.685, 18.080], loss: --, mean_squared_error: --, mean_q: --\n",
      "  254/2000: episode: 2, duration: 13.360s, episode steps: 127, steps per second: 10, episode reward: -0.859, mean reward: -0.007 [-0.021, 0.011], mean action: 0.089 [-1.161, 1.204], mean observation: 0.041 [-24.691, 18.053], loss: --, mean_squared_error: --, mean_q: --\n",
      "  379/2000: episode: 3, duration: 11.843s, episode steps: 125, steps per second: 11, episode reward: -0.848, mean reward: -0.007 [-0.019, 0.011], mean action: 0.117 [-1.138, 1.182], mean observation: 0.034 [-24.794, 17.968], loss: --, mean_squared_error: --, mean_q: --\n",
      "  507/2000: episode: 4, duration: 14.932s, episode steps: 128, steps per second: 9, episode reward: -0.877, mean reward: -0.007 [-0.025, 0.011], mean action: 0.102 [-1.068, 1.228], mean observation: 0.038 [-42.589, 18.002], loss: --, mean_squared_error: --, mean_q: --\n",
      "  636/2000: episode: 5, duration: 14.114s, episode steps: 129, steps per second: 9, episode reward: -0.913, mean reward: -0.007 [-0.025, 0.011], mean action: 0.083 [-1.188, 1.157], mean observation: 0.041 [-24.588, 18.013], loss: --, mean_squared_error: --, mean_q: --\n",
      "  764/2000: episode: 6, duration: 14.908s, episode steps: 128, steps per second: 9, episode reward: -0.883, mean reward: -0.007 [-0.024, 0.011], mean action: 0.090 [-1.184, 1.144], mean observation: 0.043 [-25.125, 18.187], loss: --, mean_squared_error: --, mean_q: --\n",
      "  893/2000: episode: 7, duration: 12.795s, episode steps: 129, steps per second: 10, episode reward: -0.887, mean reward: -0.007 [-0.023, 0.011], mean action: 0.107 [-1.102, 1.176], mean observation: 0.039 [-24.541, 17.938], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1021/2000: episode: 8, duration: 13.507s, episode steps: 128, steps per second: 9, episode reward: -0.871, mean reward: -0.007 [-0.021, 0.011], mean action: 0.091 [-1.199, 1.147], mean observation: 0.035 [-24.935, 18.097], loss: 0.000843, mean_squared_error: 0.001686, mean_q: 0.405351\n",
      " 1108/2000: episode: 9, duration: 9.380s, episode steps: 87, steps per second: 9, episode reward: 0.067, mean reward: 0.001 [-0.005, 0.010], mean action: 0.002 [-1.181, 1.152], mean observation: -0.012 [-38.351, 18.406], loss: 0.001187, mean_squared_error: 0.002374, mean_q: 0.389741\n",
      " 1196/2000: episode: 10, duration: 17.189s, episode steps: 88, steps per second: 5, episode reward: 0.029, mean reward: 0.000 [-0.004, 0.006], mean action: -0.046 [-1.134, 1.147], mean observation: 0.020 [-30.835, 16.380], loss: 0.001051, mean_squared_error: 0.002102, mean_q: 0.394167\n",
      " 1309/2000: episode: 11, duration: 16.655s, episode steps: 113, steps per second: 7, episode reward: -0.536, mean reward: -0.005 [-0.016, 0.005], mean action: 0.063 [-1.163, 1.156], mean observation: 0.052 [-24.395, 17.965], loss: 0.001477, mean_squared_error: 0.002954, mean_q: 0.403046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1433/2000: episode: 12, duration: 6.753s, episode steps: 124, steps per second: 18, episode reward: -0.926, mean reward: -0.007 [-0.022, 0.005], mean action: 0.119 [-1.120, 1.210], mean observation: -0.021 [-15.630, 15.024], loss: 0.000746, mean_squared_error: 0.001493, mean_q: 0.407273\n",
      " 1560/2000: episode: 13, duration: 9.036s, episode steps: 127, steps per second: 14, episode reward: -0.779, mean reward: -0.006 [-0.019, 0.008], mean action: 0.166 [-1.154, 1.179], mean observation: -0.019 [-12.694, 11.112], loss: 0.000885, mean_squared_error: 0.001769, mean_q: 0.411115\n",
      " 1684/2000: episode: 14, duration: 6.166s, episode steps: 124, steps per second: 20, episode reward: -0.799, mean reward: -0.006 [-0.020, 0.008], mean action: 0.133 [-1.128, 1.254], mean observation: 0.010 [-29.551, 11.779], loss: 0.001544, mean_squared_error: 0.003088, mean_q: 0.418433\n",
      " 1819/2000: episode: 15, duration: 7.333s, episode steps: 135, steps per second: 18, episode reward: -0.801, mean reward: -0.006 [-0.019, 0.010], mean action: 0.217 [-1.242, 1.206], mean observation: 0.023 [-16.657, 11.045], loss: 0.001047, mean_squared_error: 0.002094, mean_q: 0.420644\n",
      " 1957/2000: episode: 16, duration: 8.140s, episode steps: 138, steps per second: 17, episode reward: -0.718, mean reward: -0.005 [-0.017, 0.012], mean action: 0.123 [-1.339, 1.132], mean observation: 0.022 [-25.061, 15.171], loss: 0.000323, mean_squared_error: 0.000646, mean_q: 0.423681\n",
      "done, took 194.436 seconds\n",
      "\n",
      "\n",
      "iteration: 3\n",
      "Training for 2000 steps ...\n",
      "  134/2000: episode: 1, duration: 5.812s, episode steps: 134, steps per second: 23, episode reward: -0.759, mean reward: -0.006 [-0.018, 0.011], mean action: 0.161 [-1.235, 1.218], mean observation: 0.014 [-13.399, 14.618], loss: --, mean_squared_error: --, mean_q: --\n",
      "  269/2000: episode: 2, duration: 6.008s, episode steps: 135, steps per second: 22, episode reward: -0.760, mean reward: -0.006 [-0.018, 0.011], mean action: 0.144 [-1.240, 1.130], mean observation: 0.019 [-11.021, 14.532], loss: --, mean_squared_error: --, mean_q: --\n",
      "  402/2000: episode: 3, duration: 5.712s, episode steps: 133, steps per second: 23, episode reward: -0.718, mean reward: -0.005 [-0.017, 0.011], mean action: 0.182 [-1.215, 1.241], mean observation: 0.019 [-11.049, 14.205], loss: --, mean_squared_error: --, mean_q: --\n",
      "  537/2000: episode: 4, duration: 5.849s, episode steps: 135, steps per second: 23, episode reward: -0.755, mean reward: -0.006 [-0.018, 0.011], mean action: 0.150 [-1.195, 1.076], mean observation: 0.021 [-11.444, 14.800], loss: --, mean_squared_error: --, mean_q: --\n",
      "  672/2000: episode: 5, duration: 5.759s, episode steps: 135, steps per second: 23, episode reward: -0.732, mean reward: -0.005 [-0.017, 0.012], mean action: 0.184 [-1.128, 1.205], mean observation: 0.019 [-11.362, 14.919], loss: --, mean_squared_error: --, mean_q: --\n",
      "  806/2000: episode: 6, duration: 5.806s, episode steps: 134, steps per second: 23, episode reward: -0.738, mean reward: -0.006 [-0.017, 0.011], mean action: 0.170 [-1.221, 1.121], mean observation: 0.019 [-11.849, 14.657], loss: --, mean_squared_error: --, mean_q: --\n",
      "  941/2000: episode: 7, duration: 5.866s, episode steps: 135, steps per second: 23, episode reward: -0.723, mean reward: -0.005 [-0.017, 0.012], mean action: 0.140 [-1.253, 1.103], mean observation: 0.023 [-11.802, 14.898], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1075/2000: episode: 8, duration: 6.705s, episode steps: 134, steps per second: 20, episode reward: -0.683, mean reward: -0.005 [-0.016, 0.011], mean action: 0.138 [-1.207, 1.176], mean observation: 0.029 [-10.583, 14.435], loss: 0.001286, mean_squared_error: 0.002572, mean_q: 0.415180\n",
      " 1220/2000: episode: 9, duration: 9.267s, episode steps: 145, steps per second: 16, episode reward: -0.535, mean reward: -0.004 [-0.013, 0.009], mean action: 0.040 [-1.310, 1.235], mean observation: 0.033 [-44.362, 17.777], loss: 0.000641, mean_squared_error: 0.001281, mean_q: 0.418968\n",
      " 1350/2000: episode: 10, duration: 7.917s, episode steps: 130, steps per second: 16, episode reward: -0.568, mean reward: -0.004 [-0.013, 0.008], mean action: -0.029 [-1.273, 1.158], mean observation: 0.023 [-17.612, 14.849], loss: 0.000633, mean_squared_error: 0.001266, mean_q: 0.421587\n",
      " 1491/2000: episode: 11, duration: 12.392s, episode steps: 141, steps per second: 11, episode reward: -0.745, mean reward: -0.005 [-0.020, 0.007], mean action: -0.021 [-1.148, 1.192], mean observation: 0.004 [-12.223, 16.973], loss: 0.000835, mean_squared_error: 0.001670, mean_q: 0.426192\n",
      " 1617/2000: episode: 12, duration: 10.736s, episode steps: 126, steps per second: 12, episode reward: -0.577, mean reward: -0.005 [-0.020, 0.009], mean action: 0.132 [-1.147, 1.266], mean observation: 0.004 [-19.610, 16.716], loss: 0.000722, mean_squared_error: 0.001444, mean_q: 0.428256\n",
      " 1745/2000: episode: 13, duration: 24.038s, episode steps: 128, steps per second: 5, episode reward: -0.755, mean reward: -0.006 [-0.018, 0.008], mean action: 0.092 [-1.467, 1.139], mean observation: 0.011 [-23.377, 15.469], loss: 0.001088, mean_squared_error: 0.002177, mean_q: 0.428032\n",
      " 1867/2000: episode: 14, duration: 23.473s, episode steps: 122, steps per second: 5, episode reward: -0.405, mean reward: -0.003 [-0.014, 0.009], mean action: 0.166 [-1.161, 1.214], mean observation: 0.062 [-24.961, 16.252], loss: 0.000543, mean_squared_error: 0.001086, mean_q: 0.431810\n",
      " 1944/2000: episode: 15, duration: 10.822s, episode steps: 77, steps per second: 7, episode reward: 0.130, mean reward: 0.002 [-0.003, 0.009], mean action: 0.158 [-1.088, 1.218], mean observation: -0.047 [-33.348, 18.414], loss: 0.000492, mean_squared_error: 0.000984, mean_q: 0.432788\n",
      "done, took 151.767 seconds\n",
      "\n",
      "\n",
      "iteration: 4\n",
      "Training for 2000 steps ...\n",
      "   83/2000: episode: 1, duration: 12.086s, episode steps: 83, steps per second: 7, episode reward: 0.060, mean reward: 0.001 [-0.004, 0.009], mean action: 0.131 [-1.117, 1.212], mean observation: -0.062 [-30.846, 17.652], loss: --, mean_squared_error: --, mean_q: --\n",
      "  158/2000: episode: 2, duration: 8.099s, episode steps: 75, steps per second: 9, episode reward: 0.108, mean reward: 0.001 [-0.004, 0.009], mean action: 0.125 [-1.082, 1.167], mean observation: -0.008 [-20.016, 17.366], loss: --, mean_squared_error: --, mean_q: --\n",
      "  227/2000: episode: 3, duration: 11.405s, episode steps: 69, steps per second: 6, episode reward: 0.145, mean reward: 0.002 [-0.003, 0.009], mean action: 0.094 [-1.117, 1.109], mean observation: 0.008 [-19.106, 17.562], loss: --, mean_squared_error: --, mean_q: --\n",
      "  307/2000: episode: 4, duration: 12.410s, episode steps: 80, steps per second: 6, episode reward: 0.072, mean reward: 0.001 [-0.004, 0.008], mean action: 0.130 [-1.124, 1.124], mean observation: -0.056 [-20.749, 17.347], loss: --, mean_squared_error: --, mean_q: --\n",
      "  377/2000: episode: 5, duration: 13.614s, episode steps: 70, steps per second: 5, episode reward: 0.141, mean reward: 0.002 [-0.003, 0.009], mean action: 0.097 [-1.216, 1.202], mean observation: 0.012 [-19.259, 17.571], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 6, duration: 11.270s, episode steps: 82, steps per second: 7, episode reward: 0.073, mean reward: 0.001 [-0.004, 0.009], mean action: 0.116 [-1.167, 1.142], mean observation: -0.050 [-29.799, 17.521], loss: --, mean_squared_error: --, mean_q: --\n",
      "  528/2000: episode: 7, duration: 12.084s, episode steps: 69, steps per second: 6, episode reward: 0.149, mean reward: 0.002 [-0.003, 0.009], mean action: 0.106 [-1.143, 1.154], mean observation: 0.002 [-19.629, 17.282], loss: --, mean_squared_error: --, mean_q: --\n",
      "  597/2000: episode: 8, duration: 13.019s, episode steps: 69, steps per second: 5, episode reward: 0.147, mean reward: 0.002 [-0.003, 0.009], mean action: 0.091 [-1.118, 1.072], mean observation: 0.005 [-19.664, 17.186], loss: --, mean_squared_error: --, mean_q: --\n",
      "  665/2000: episode: 9, duration: 13.377s, episode steps: 68, steps per second: 5, episode reward: 0.146, mean reward: 0.002 [-0.003, 0.009], mean action: 0.113 [-1.130, 1.155], mean observation: 0.006 [-18.386, 17.789], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 10, duration: 11.220s, episode steps: 82, steps per second: 7, episode reward: 0.069, mean reward: 0.001 [-0.004, 0.009], mean action: 0.145 [-1.111, 1.297], mean observation: -0.042 [-29.695, 17.317], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  822/2000: episode: 11, duration: 8.563s, episode steps: 75, steps per second: 9, episode reward: 0.110, mean reward: 0.001 [-0.004, 0.009], mean action: 0.125 [-1.098, 1.155], mean observation: -0.003 [-19.890, 17.429], loss: --, mean_squared_error: --, mean_q: --\n",
      "  890/2000: episode: 12, duration: 12.107s, episode steps: 68, steps per second: 6, episode reward: 0.144, mean reward: 0.002 [-0.003, 0.009], mean action: 0.064 [-1.214, 1.119], mean observation: 0.001 [-20.189, 17.549], loss: --, mean_squared_error: --, mean_q: --\n",
      "  958/2000: episode: 13, duration: 13.114s, episode steps: 68, steps per second: 5, episode reward: 0.143, mean reward: 0.002 [-0.003, 0.009], mean action: 0.106 [-1.144, 1.235], mean observation: 0.004 [-19.343, 17.863], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1041/2000: episode: 14, duration: 13.366s, episode steps: 83, steps per second: 6, episode reward: 0.062, mean reward: 0.001 [-0.004, 0.009], mean action: 0.111 [-1.140, 1.135], mean observation: -0.040 [-27.082, 17.564], loss: 0.000296, mean_squared_error: 0.000592, mean_q: 0.447091\n",
      " 1152/2000: episode: 15, duration: 9.486s, episode steps: 111, steps per second: 12, episode reward: -0.582, mean reward: -0.005 [-0.021, 0.010], mean action: 0.082 [-1.240, 1.257], mean observation: -0.002 [-20.680, 15.335], loss: 0.000721, mean_squared_error: 0.001442, mean_q: 0.444889\n",
      " 1247/2000: episode: 16, duration: 10.058s, episode steps: 95, steps per second: 9, episode reward: -0.056, mean reward: -0.001 [-0.006, 0.010], mean action: 0.058 [-1.141, 1.093], mean observation: 0.023 [-21.456, 16.842], loss: 0.001335, mean_squared_error: 0.002670, mean_q: 0.446605\n",
      " 1323/2000: episode: 17, duration: 11.251s, episode steps: 76, steps per second: 7, episode reward: 0.060, mean reward: 0.001 [-0.006, 0.009], mean action: 0.047 [-1.128, 1.127], mean observation: -0.015 [-13.426, 13.714], loss: 0.000242, mean_squared_error: 0.000485, mean_q: 0.451473\n",
      " 1397/2000: episode: 18, duration: 13.883s, episode steps: 74, steps per second: 5, episode reward: 0.084, mean reward: 0.001 [-0.005, 0.010], mean action: -0.006 [-1.123, 1.170], mean observation: -0.022 [-21.950, 16.099], loss: 0.000237, mean_squared_error: 0.000473, mean_q: 0.449061\n",
      " 1468/2000: episode: 19, duration: 12.581s, episode steps: 71, steps per second: 6, episode reward: 0.116, mean reward: 0.002 [-0.004, 0.010], mean action: -0.012 [-1.156, 1.212], mean observation: -0.019 [-27.933, 12.831], loss: 0.000750, mean_squared_error: 0.001500, mean_q: 0.459860\n",
      " 1589/2000: episode: 20, duration: 9.558s, episode steps: 121, steps per second: 13, episode reward: -0.811, mean reward: -0.007 [-0.022, 0.005], mean action: -0.030 [-1.199, 1.168], mean observation: -0.037 [-26.118, 13.903], loss: 0.001324, mean_squared_error: 0.002649, mean_q: 0.456700\n",
      " 1662/2000: episode: 21, duration: 11.777s, episode steps: 73, steps per second: 6, episode reward: 0.100, mean reward: 0.001 [-0.004, 0.007], mean action: 0.016 [-1.166, 1.171], mean observation: -0.015 [-25.527, 15.581], loss: 0.000425, mean_squared_error: 0.000850, mean_q: 0.451425\n",
      " 1740/2000: episode: 22, duration: 12.204s, episode steps: 78, steps per second: 6, episode reward: 0.029, mean reward: 0.000 [-0.009, 0.010], mean action: 0.023 [-1.129, 1.151], mean observation: 0.062 [-30.027, 17.704], loss: 0.001684, mean_squared_error: 0.003367, mean_q: 0.457250\n",
      " 1819/2000: episode: 23, duration: 22.846s, episode steps: 79, steps per second: 3, episode reward: 0.027, mean reward: 0.000 [-0.008, 0.013], mean action: 0.052 [-1.168, 1.166], mean observation: 0.049 [-27.213, 18.080], loss: 0.001080, mean_squared_error: 0.002160, mean_q: 0.452680\n",
      " 1939/2000: episode: 24, duration: 7.134s, episode steps: 120, steps per second: 17, episode reward: -0.655, mean reward: -0.005 [-0.015, 0.003], mean action: -0.004 [-1.205, 1.293], mean observation: 0.018 [-34.804, 18.168], loss: 0.000623, mean_squared_error: 0.001246, mean_q: 0.458080\n",
      "done, took 290.917 seconds\n",
      "\n",
      "\n",
      "iteration: 5\n",
      "Training for 2000 steps ...\n",
      "  127/2000: episode: 1, duration: 20.288s, episode steps: 127, steps per second: 6, episode reward: -0.080, mean reward: -0.001 [-0.010, 0.013], mean action: 0.044 [-1.148, 1.280], mean observation: 0.044 [-36.994, 17.665], loss: --, mean_squared_error: --, mean_q: --\n",
      "  213/2000: episode: 2, duration: 6.279s, episode steps: 86, steps per second: 14, episode reward: -0.496, mean reward: -0.006 [-0.017, 0.004], mean action: 0.144 [-1.145, 1.150], mean observation: 0.016 [-26.764, 18.001], loss: --, mean_squared_error: --, mean_q: --\n",
      "  344/2000: episode: 3, duration: 23.549s, episode steps: 131, steps per second: 6, episode reward: -0.192, mean reward: -0.001 [-0.008, 0.009], mean action: 0.036 [-1.180, 1.272], mean observation: 0.060 [-29.953, 18.055], loss: --, mean_squared_error: --, mean_q: --\n",
      "  424/2000: episode: 4, duration: 6.339s, episode steps: 80, steps per second: 13, episode reward: -0.455, mean reward: -0.006 [-0.018, 0.004], mean action: 0.130 [-1.158, 1.137], mean observation: 0.014 [-26.866, 17.876], loss: --, mean_squared_error: --, mean_q: --\n",
      "  550/2000: episode: 5, duration: 21.763s, episode steps: 126, steps per second: 6, episode reward: -0.151, mean reward: -0.001 [-0.009, 0.011], mean action: 0.024 [-1.151, 1.207], mean observation: 0.069 [-26.504, 17.912], loss: --, mean_squared_error: --, mean_q: --\n",
      "  675/2000: episode: 6, duration: 25.027s, episode steps: 125, steps per second: 5, episode reward: -0.136, mean reward: -0.001 [-0.009, 0.011], mean action: 0.044 [-1.160, 1.215], mean observation: 0.069 [-26.330, 17.697], loss: --, mean_squared_error: --, mean_q: --\n",
      "  803/2000: episode: 7, duration: 21.709s, episode steps: 128, steps per second: 6, episode reward: -0.172, mean reward: -0.001 [-0.008, 0.010], mean action: 0.019 [-1.127, 1.212], mean observation: 0.060 [-26.875, 17.941], loss: --, mean_squared_error: --, mean_q: --\n",
      "  936/2000: episode: 8, duration: 24.030s, episode steps: 133, steps per second: 6, episode reward: -0.227, mean reward: -0.002 [-0.010, 0.010], mean action: 0.024 [-1.275, 1.286], mean observation: 0.064 [-26.743, 17.981], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1064/2000: episode: 9, duration: 21.464s, episode steps: 128, steps per second: 6, episode reward: -0.141, mean reward: -0.001 [-0.009, 0.010], mean action: 0.031 [-1.160, 1.131], mean observation: 0.068 [-36.638, 19.206], loss: 0.000232, mean_squared_error: 0.000465, mean_q: 0.457062\n",
      " 1172/2000: episode: 10, duration: 8.128s, episode steps: 108, steps per second: 13, episode reward: -0.414, mean reward: -0.004 [-0.015, 0.005], mean action: 0.030 [-1.162, 1.141], mean observation: 0.035 [-22.752, 16.553], loss: 0.001069, mean_squared_error: 0.002137, mean_q: 0.464806\n",
      " 1293/2000: episode: 11, duration: 7.931s, episode steps: 121, steps per second: 15, episode reward: -0.726, mean reward: -0.006 [-0.026, 0.008], mean action: 0.007 [-1.180, 1.152], mean observation: 0.043 [-25.457, 17.740], loss: 0.000563, mean_squared_error: 0.001127, mean_q: 0.459056\n",
      " 1382/2000: episode: 12, duration: 9.662s, episode steps: 89, steps per second: 9, episode reward: -0.032, mean reward: -0.000 [-0.011, 0.008], mean action: 0.076 [-1.081, 1.150], mean observation: 0.010 [-21.473, 20.690], loss: 0.000174, mean_squared_error: 0.000348, mean_q: 0.463885\n",
      " 1504/2000: episode: 13, duration: 6.439s, episode steps: 122, steps per second: 19, episode reward: -0.777, mean reward: -0.006 [-0.023, 0.007], mean action: 0.011 [-1.354, 1.135], mean observation: 0.016 [-9.267, 17.346], loss: 0.000525, mean_squared_error: 0.001049, mean_q: 0.463973\n",
      " 1621/2000: episode: 14, duration: 6.267s, episode steps: 117, steps per second: 19, episode reward: -0.765, mean reward: -0.007 [-0.023, 0.008], mean action: 0.053 [-1.105, 1.238], mean observation: 0.006 [-16.072, 17.709], loss: 0.000503, mean_squared_error: 0.001006, mean_q: 0.462496\n",
      " 1747/2000: episode: 15, duration: 6.866s, episode steps: 126, steps per second: 18, episode reward: -0.844, mean reward: -0.007 [-0.022, 0.005], mean action: 0.051 [-1.172, 1.182], mean observation: 0.000 [-21.324, 11.619], loss: 0.000469, mean_squared_error: 0.000938, mean_q: 0.467529\n",
      " 1815/2000: episode: 16, duration: 10.709s, episode steps: 68, steps per second: 6, episode reward: 0.128, mean reward: 0.002 [-0.003, 0.007], mean action: 0.027 [-1.136, 1.098], mean observation: 0.017 [-16.920, 11.723], loss: 0.000204, mean_squared_error: 0.000407, mean_q: 0.466854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1949/2000: episode: 17, duration: 7.937s, episode steps: 134, steps per second: 17, episode reward: -0.742, mean reward: -0.006 [-0.020, 0.007], mean action: -0.040 [-1.117, 1.098], mean observation: 0.020 [-32.519, 15.308], loss: 0.000152, mean_squared_error: 0.000303, mean_q: 0.467680\n",
      "done, took 237.125 seconds\n",
      "\n",
      "\n",
      "iteration: 6\n",
      "Training for 2000 steps ...\n",
      "   94/2000: episode: 1, duration: 7.968s, episode steps: 94, steps per second: 12, episode reward: 0.272, mean reward: 0.003 [-0.001, 0.009], mean action: 0.073 [-1.144, 1.242], mean observation: 0.018 [-13.086, 17.499], loss: --, mean_squared_error: --, mean_q: --\n",
      "  185/2000: episode: 2, duration: 6.643s, episode steps: 91, steps per second: 14, episode reward: 0.283, mean reward: 0.003 [-0.001, 0.009], mean action: 0.088 [-1.077, 1.181], mean observation: 0.019 [-15.033, 17.262], loss: --, mean_squared_error: --, mean_q: --\n",
      "  284/2000: episode: 3, duration: 7.350s, episode steps: 99, steps per second: 13, episode reward: 0.224, mean reward: 0.002 [-0.001, 0.009], mean action: 0.040 [-1.302, 1.145], mean observation: 0.016 [-10.489, 16.205], loss: --, mean_squared_error: --, mean_q: --\n",
      "  379/2000: episode: 4, duration: 7.509s, episode steps: 95, steps per second: 13, episode reward: 0.275, mean reward: 0.003 [-0.000, 0.009], mean action: 0.061 [-1.175, 1.125], mean observation: 0.021 [-13.884, 17.053], loss: --, mean_squared_error: --, mean_q: --\n",
      "  470/2000: episode: 5, duration: 7.100s, episode steps: 91, steps per second: 13, episode reward: 0.260, mean reward: 0.003 [-0.001, 0.009], mean action: 0.054 [-1.219, 1.100], mean observation: 0.011 [-17.151, 16.206], loss: --, mean_squared_error: --, mean_q: --\n",
      "  568/2000: episode: 6, duration: 7.545s, episode steps: 98, steps per second: 13, episode reward: 0.270, mean reward: 0.003 [-0.001, 0.009], mean action: 0.068 [-1.220, 1.177], mean observation: 0.023 [-12.152, 16.626], loss: --, mean_squared_error: --, mean_q: --\n",
      "  662/2000: episode: 7, duration: 7.892s, episode steps: 94, steps per second: 12, episode reward: 0.277, mean reward: 0.003 [-0.001, 0.009], mean action: 0.070 [-1.232, 1.113], mean observation: 0.017 [-20.113, 16.483], loss: --, mean_squared_error: --, mean_q: --\n",
      "  757/2000: episode: 8, duration: 7.096s, episode steps: 95, steps per second: 13, episode reward: 0.274, mean reward: 0.003 [-0.001, 0.009], mean action: 0.080 [-1.128, 1.133], mean observation: 0.022 [-13.991, 15.807], loss: --, mean_squared_error: --, mean_q: --\n",
      "  853/2000: episode: 9, duration: 7.404s, episode steps: 96, steps per second: 13, episode reward: 0.259, mean reward: 0.003 [-0.001, 0.009], mean action: 0.063 [-1.203, 1.190], mean observation: 0.017 [-17.410, 16.719], loss: --, mean_squared_error: --, mean_q: --\n",
      "  942/2000: episode: 10, duration: 8.131s, episode steps: 89, steps per second: 11, episode reward: 0.265, mean reward: 0.003 [-0.001, 0.009], mean action: 0.088 [-1.166, 1.206], mean observation: 0.012 [-13.524, 16.184], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1039/2000: episode: 11, duration: 8.036s, episode steps: 97, steps per second: 12, episode reward: 0.276, mean reward: 0.003 [-0.001, 0.009], mean action: 0.084 [-1.139, 1.235], mean observation: 0.023 [-11.473, 15.993], loss: 0.001100, mean_squared_error: 0.002200, mean_q: 0.477274\n",
      " 1152/2000: episode: 12, duration: 7.589s, episode steps: 113, steps per second: 15, episode reward: 0.370, mean reward: 0.003 [-0.003, 0.010], mean action: 0.103 [-1.228, 1.132], mean observation: 0.093 [-10.752, 16.580], loss: 0.000561, mean_squared_error: 0.001122, mean_q: 0.489207\n",
      " 1264/2000: episode: 13, duration: 7.250s, episode steps: 112, steps per second: 15, episode reward: 0.442, mean reward: 0.004 [-0.001, 0.010], mean action: 0.047 [-1.184, 1.335], mean observation: 0.098 [-9.168, 18.880], loss: 0.000342, mean_squared_error: 0.000683, mean_q: 0.490389\n",
      " 1404/2000: episode: 14, duration: 5.708s, episode steps: 140, steps per second: 25, episode reward: 0.536, mean reward: 0.004 [-0.001, 0.010], mean action: 0.107 [-1.190, 1.180], mean observation: 0.139 [-9.980, 17.734], loss: 0.000215, mean_squared_error: 0.000431, mean_q: 0.494444\n",
      " 1537/2000: episode: 15, duration: 5.566s, episode steps: 133, steps per second: 24, episode reward: 0.522, mean reward: 0.004 [-0.002, 0.010], mean action: 0.167 [-1.186, 1.190], mean observation: 0.136 [-9.274, 15.334], loss: 0.000408, mean_squared_error: 0.000816, mean_q: 0.510195\n",
      " 1658/2000: episode: 16, duration: 5.097s, episode steps: 121, steps per second: 24, episode reward: 0.524, mean reward: 0.004 [-0.002, 0.010], mean action: 0.167 [-1.201, 1.254], mean observation: 0.138 [-9.401, 18.809], loss: 0.000196, mean_squared_error: 0.000392, mean_q: 0.515291\n",
      " 1763/2000: episode: 17, duration: 4.660s, episode steps: 105, steps per second: 23, episode reward: 0.474, mean reward: 0.005 [-0.001, 0.011], mean action: 0.261 [-1.133, 1.167], mean observation: 0.133 [-9.642, 17.848], loss: 0.000132, mean_squared_error: 0.000265, mean_q: 0.510045\n",
      " 1860/2000: episode: 18, duration: 4.318s, episode steps: 97, steps per second: 22, episode reward: 0.472, mean reward: 0.005 [-0.001, 0.011], mean action: 0.239 [-1.082, 1.157], mean observation: 0.133 [-9.683, 17.985], loss: 0.000671, mean_squared_error: 0.001342, mean_q: 0.523336\n",
      " 1956/2000: episode: 19, duration: 4.208s, episode steps: 96, steps per second: 23, episode reward: 0.458, mean reward: 0.005 [-0.001, 0.012], mean action: 0.232 [-1.180, 1.149], mean observation: 0.131 [-9.702, 18.209], loss: 0.000416, mean_squared_error: 0.000832, mean_q: 0.526859\n",
      "done, took 129.247 seconds\n",
      "\n",
      "\n",
      "iteration: 7\n",
      "Training for 2000 steps ...\n",
      "   90/2000: episode: 1, duration: 3.220s, episode steps: 90, steps per second: 28, episode reward: 0.464, mean reward: 0.005 [-0.001, 0.013], mean action: 0.290 [-1.157, 1.214], mean observation: 0.129 [-9.671, 18.350], loss: --, mean_squared_error: --, mean_q: --\n",
      "  179/2000: episode: 2, duration: 3.149s, episode steps: 89, steps per second: 28, episode reward: 0.458, mean reward: 0.005 [-0.001, 0.013], mean action: 0.284 [-1.147, 1.203], mean observation: 0.128 [-9.789, 18.022], loss: --, mean_squared_error: --, mean_q: --\n",
      "  270/2000: episode: 3, duration: 3.255s, episode steps: 91, steps per second: 28, episode reward: 0.462, mean reward: 0.005 [-0.001, 0.012], mean action: 0.288 [-1.154, 1.184], mean observation: 0.128 [-9.665, 18.405], loss: --, mean_squared_error: --, mean_q: --\n",
      "  361/2000: episode: 4, duration: 3.243s, episode steps: 91, steps per second: 28, episode reward: 0.461, mean reward: 0.005 [-0.001, 0.013], mean action: 0.322 [-1.133, 1.281], mean observation: 0.128 [-9.809, 18.327], loss: --, mean_squared_error: --, mean_q: --\n",
      "  451/2000: episode: 5, duration: 3.182s, episode steps: 90, steps per second: 28, episode reward: 0.461, mean reward: 0.005 [-0.001, 0.013], mean action: 0.300 [-1.068, 1.244], mean observation: 0.128 [-9.805, 18.414], loss: --, mean_squared_error: --, mean_q: --\n",
      "  544/2000: episode: 6, duration: 3.283s, episode steps: 93, steps per second: 28, episode reward: 0.472, mean reward: 0.005 [-0.001, 0.012], mean action: 0.288 [-1.124, 1.134], mean observation: 0.128 [-9.650, 18.421], loss: --, mean_squared_error: --, mean_q: --\n",
      "  635/2000: episode: 7, duration: 3.274s, episode steps: 91, steps per second: 28, episode reward: 0.471, mean reward: 0.005 [-0.001, 0.013], mean action: 0.279 [-1.124, 1.280], mean observation: 0.128 [-9.747, 18.293], loss: --, mean_squared_error: --, mean_q: --\n",
      "  727/2000: episode: 8, duration: 3.309s, episode steps: 92, steps per second: 28, episode reward: 0.468, mean reward: 0.005 [-0.001, 0.012], mean action: 0.259 [-1.125, 1.166], mean observation: 0.127 [-9.725, 18.295], loss: --, mean_squared_error: --, mean_q: --\n",
      "  817/2000: episode: 9, duration: 3.243s, episode steps: 90, steps per second: 28, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.013], mean action: 0.285 [-1.132, 1.168], mean observation: 0.128 [-9.737, 18.116], loss: --, mean_squared_error: --, mean_q: --\n",
      "  909/2000: episode: 10, duration: 3.325s, episode steps: 92, steps per second: 28, episode reward: 0.471, mean reward: 0.005 [-0.001, 0.013], mean action: 0.288 [-1.142, 1.201], mean observation: 0.128 [-9.702, 18.384], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/2000: episode: 11, duration: 3.260s, episode steps: 91, steps per second: 28, episode reward: 0.473, mean reward: 0.005 [-0.001, 0.013], mean action: 0.282 [-1.095, 1.117], mean observation: 0.128 [-9.738, 18.260], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1091/2000: episode: 12, duration: 4.077s, episode steps: 91, steps per second: 22, episode reward: 0.456, mean reward: 0.005 [-0.001, 0.013], mean action: 0.309 [-1.164, 1.211], mean observation: 0.128 [-9.660, 18.004], loss: 0.001412, mean_squared_error: 0.002824, mean_q: 0.573506\n",
      " 1171/2000: episode: 13, duration: 3.679s, episode steps: 80, steps per second: 22, episode reward: 0.421, mean reward: 0.005 [-0.002, 0.014], mean action: 0.272 [-1.168, 1.127], mean observation: 0.126 [-9.713, 18.031], loss: 0.000778, mean_squared_error: 0.001556, mean_q: 0.585379\n",
      " 1261/2000: episode: 14, duration: 4.328s, episode steps: 90, steps per second: 21, episode reward: 0.448, mean reward: 0.005 [-0.001, 0.013], mean action: 0.258 [-1.192, 1.167], mean observation: 0.128 [-9.391, 14.583], loss: 0.000890, mean_squared_error: 0.001781, mean_q: 0.571526\n",
      " 1356/2000: episode: 15, duration: 4.038s, episode steps: 95, steps per second: 24, episode reward: 0.486, mean reward: 0.005 [0.000, 0.012], mean action: 0.330 [-1.169, 1.148], mean observation: 0.130 [-9.458, 14.530], loss: 0.000877, mean_squared_error: 0.001754, mean_q: 0.581511\n",
      " 1445/2000: episode: 16, duration: 3.873s, episode steps: 89, steps per second: 23, episode reward: 0.467, mean reward: 0.005 [0.000, 0.013], mean action: 0.317 [-1.097, 1.287], mean observation: 0.127 [-9.533, 18.148], loss: 0.000630, mean_squared_error: 0.001260, mean_q: 0.578814\n",
      " 1536/2000: episode: 17, duration: 4.174s, episode steps: 91, steps per second: 22, episode reward: 0.444, mean reward: 0.005 [-0.001, 0.013], mean action: 0.218 [-1.161, 1.154], mean observation: 0.125 [-9.563, 14.670], loss: 0.000811, mean_squared_error: 0.001622, mean_q: 0.574347\n",
      " 1692/2000: episode: 18, duration: 9.454s, episode steps: 156, steps per second: 17, episode reward: 0.680, mean reward: 0.004 [-0.001, 0.013], mean action: 0.427 [-1.120, 1.166], mean observation: 0.129 [-9.335, 15.610], loss: 0.000720, mean_squared_error: 0.001441, mean_q: 0.588256\n",
      " 1835/2000: episode: 19, duration: 7.789s, episode steps: 143, steps per second: 18, episode reward: -0.715, mean reward: -0.005 [-0.018, 0.012], mean action: 0.175 [-1.172, 1.197], mean observation: 0.034 [-27.490, 12.896], loss: 0.000636, mean_squared_error: 0.001271, mean_q: 0.586005\n",
      "done, took 84.828 seconds\n",
      "\n",
      "\n",
      "iteration: 8\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 4.854s, episode steps: 105, steps per second: 22, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.013], mean action: 0.378 [-1.136, 1.169], mean observation: 0.117 [-9.417, 17.914], loss: --, mean_squared_error: --, mean_q: --\n",
      "  209/2000: episode: 2, duration: 4.824s, episode steps: 104, steps per second: 22, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.013], mean action: 0.390 [-1.135, 1.235], mean observation: 0.117 [-9.440, 18.051], loss: --, mean_squared_error: --, mean_q: --\n",
      "  315/2000: episode: 3, duration: 4.909s, episode steps: 106, steps per second: 22, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.368 [-1.223, 1.229], mean observation: 0.118 [-9.401, 17.794], loss: --, mean_squared_error: --, mean_q: --\n",
      "  420/2000: episode: 4, duration: 4.906s, episode steps: 105, steps per second: 21, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.013], mean action: 0.377 [-1.124, 1.226], mean observation: 0.118 [-9.450, 17.892], loss: --, mean_squared_error: --, mean_q: --\n",
      "  524/2000: episode: 5, duration: 4.827s, episode steps: 104, steps per second: 22, episode reward: 0.621, mean reward: 0.006 [-0.001, 0.013], mean action: 0.383 [-1.195, 1.164], mean observation: 0.117 [-9.557, 17.812], loss: --, mean_squared_error: --, mean_q: --\n",
      "  629/2000: episode: 6, duration: 4.882s, episode steps: 105, steps per second: 22, episode reward: 0.619, mean reward: 0.006 [-0.001, 0.013], mean action: 0.380 [-1.184, 1.293], mean observation: 0.118 [-9.434, 17.935], loss: --, mean_squared_error: --, mean_q: --\n",
      "  734/2000: episode: 7, duration: 4.909s, episode steps: 105, steps per second: 21, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.013], mean action: 0.407 [-1.071, 1.191], mean observation: 0.117 [-9.429, 17.812], loss: --, mean_squared_error: --, mean_q: --\n",
      "  840/2000: episode: 8, duration: 4.900s, episode steps: 106, steps per second: 22, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.013], mean action: 0.384 [-1.228, 1.277], mean observation: 0.117 [-9.440, 17.875], loss: --, mean_squared_error: --, mean_q: --\n",
      "  945/2000: episode: 9, duration: 4.822s, episode steps: 105, steps per second: 22, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.013], mean action: 0.405 [-1.133, 1.156], mean observation: 0.118 [-9.427, 17.864], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1049/2000: episode: 10, duration: 5.290s, episode steps: 104, steps per second: 20, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.013], mean action: 0.388 [-1.161, 1.180], mean observation: 0.118 [-9.483, 18.012], loss: 0.001152, mean_squared_error: 0.002305, mean_q: 0.603487\n",
      " 1152/2000: episode: 11, duration: 6.450s, episode steps: 103, steps per second: 16, episode reward: 0.601, mean reward: 0.006 [-0.001, 0.014], mean action: 0.469 [-1.086, 1.227], mean observation: 0.115 [-25.455, 17.773], loss: 0.000621, mean_squared_error: 0.001242, mean_q: 0.619468\n",
      " 1264/2000: episode: 12, duration: 6.679s, episode steps: 112, steps per second: 17, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.013], mean action: 0.400 [-1.217, 1.268], mean observation: 0.123 [-12.616, 16.643], loss: 0.000474, mean_squared_error: 0.000948, mean_q: 0.606151\n",
      " 1360/2000: episode: 13, duration: 4.896s, episode steps: 96, steps per second: 20, episode reward: 0.649, mean reward: 0.007 [-0.002, 0.013], mean action: 0.387 [-1.161, 1.190], mean observation: 0.119 [-9.391, 19.602], loss: 0.000784, mean_squared_error: 0.001568, mean_q: 0.607192\n",
      " 1458/2000: episode: 14, duration: 6.029s, episode steps: 98, steps per second: 16, episode reward: 0.595, mean reward: 0.006 [-0.002, 0.013], mean action: 0.363 [-1.111, 1.212], mean observation: 0.120 [-9.256, 19.385], loss: 0.000539, mean_squared_error: 0.001079, mean_q: 0.607925\n",
      " 1534/2000: episode: 15, duration: 3.470s, episode steps: 76, steps per second: 22, episode reward: 0.424, mean reward: 0.006 [-0.001, 0.014], mean action: 0.310 [-1.100, 1.163], mean observation: 0.120 [-9.263, 19.720], loss: 0.000513, mean_squared_error: 0.001025, mean_q: 0.604283\n",
      " 1612/2000: episode: 16, duration: 3.485s, episode steps: 78, steps per second: 22, episode reward: 0.439, mean reward: 0.006 [-0.000, 0.014], mean action: 0.320 [-1.103, 1.133], mean observation: 0.121 [-9.019, 19.559], loss: 0.000387, mean_squared_error: 0.000774, mean_q: 0.614988\n",
      " 1688/2000: episode: 17, duration: 3.399s, episode steps: 76, steps per second: 22, episode reward: 0.425, mean reward: 0.006 [-0.001, 0.014], mean action: 0.294 [-1.102, 1.132], mean observation: 0.120 [-9.235, 19.674], loss: 0.000571, mean_squared_error: 0.001142, mean_q: 0.621570\n",
      " 1764/2000: episode: 18, duration: 3.589s, episode steps: 76, steps per second: 21, episode reward: 0.411, mean reward: 0.005 [-0.001, 0.014], mean action: 0.242 [-1.161, 1.070], mean observation: 0.122 [-9.395, 19.677], loss: 0.000343, mean_squared_error: 0.000687, mean_q: 0.616942\n",
      " 1841/2000: episode: 19, duration: 3.511s, episode steps: 77, steps per second: 22, episode reward: 0.420, mean reward: 0.005 [-0.001, 0.014], mean action: 0.232 [-1.123, 1.103], mean observation: 0.122 [-9.425, 19.645], loss: 0.000445, mean_squared_error: 0.000891, mean_q: 0.614136\n",
      " 1918/2000: episode: 20, duration: 3.558s, episode steps: 77, steps per second: 22, episode reward: 0.420, mean reward: 0.005 [-0.002, 0.014], mean action: 0.237 [-1.190, 1.195], mean observation: 0.122 [-9.388, 19.611], loss: 0.000470, mean_squared_error: 0.000939, mean_q: 0.619614\n",
      " 1996/2000: episode: 21, duration: 3.734s, episode steps: 78, steps per second: 21, episode reward: 0.416, mean reward: 0.005 [-0.002, 0.014], mean action: 0.225 [-1.084, 1.216], mean observation: 0.123 [-9.331, 19.649], loss: 0.000330, mean_squared_error: 0.000660, mean_q: 0.605981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 98.276 seconds\n",
      "\n",
      "\n",
      "iteration: 9\n",
      "Training for 2000 steps ...\n",
      "   77/2000: episode: 1, duration: 2.844s, episode steps: 77, steps per second: 27, episode reward: 0.420, mean reward: 0.005 [-0.002, 0.013], mean action: 0.242 [-1.134, 1.151], mean observation: 0.124 [-9.458, 20.074], loss: --, mean_squared_error: --, mean_q: --\n",
      "  156/2000: episode: 2, duration: 2.909s, episode steps: 79, steps per second: 27, episode reward: 0.436, mean reward: 0.006 [-0.002, 0.013], mean action: 0.220 [-1.142, 1.122], mean observation: 0.123 [-9.366, 20.045], loss: --, mean_squared_error: --, mean_q: --\n",
      "  233/2000: episode: 3, duration: 2.769s, episode steps: 77, steps per second: 28, episode reward: 0.415, mean reward: 0.005 [-0.002, 0.013], mean action: 0.233 [-1.162, 1.160], mean observation: 0.124 [-9.418, 19.983], loss: --, mean_squared_error: --, mean_q: --\n",
      "  312/2000: episode: 4, duration: 2.872s, episode steps: 79, steps per second: 28, episode reward: 0.433, mean reward: 0.005 [-0.002, 0.013], mean action: 0.228 [-1.097, 1.128], mean observation: 0.124 [-9.228, 19.987], loss: --, mean_squared_error: --, mean_q: --\n",
      "  388/2000: episode: 5, duration: 2.741s, episode steps: 76, steps per second: 28, episode reward: 0.410, mean reward: 0.005 [-0.002, 0.013], mean action: 0.234 [-1.191, 1.122], mean observation: 0.125 [-9.383, 20.054], loss: --, mean_squared_error: --, mean_q: --\n",
      "  466/2000: episode: 6, duration: 2.758s, episode steps: 78, steps per second: 28, episode reward: 0.421, mean reward: 0.005 [-0.002, 0.013], mean action: 0.229 [-1.280, 1.175], mean observation: 0.125 [-9.171, 20.303], loss: --, mean_squared_error: --, mean_q: --\n",
      "  546/2000: episode: 7, duration: 2.941s, episode steps: 80, steps per second: 27, episode reward: 0.437, mean reward: 0.005 [-0.002, 0.013], mean action: 0.212 [-1.129, 1.104], mean observation: 0.123 [-9.196, 19.856], loss: --, mean_squared_error: --, mean_q: --\n",
      "  624/2000: episode: 8, duration: 2.838s, episode steps: 78, steps per second: 27, episode reward: 0.427, mean reward: 0.005 [-0.002, 0.013], mean action: 0.228 [-1.104, 1.168], mean observation: 0.123 [-9.247, 20.169], loss: --, mean_squared_error: --, mean_q: --\n",
      "  703/2000: episode: 9, duration: 2.847s, episode steps: 79, steps per second: 28, episode reward: 0.430, mean reward: 0.005 [-0.002, 0.013], mean action: 0.245 [-1.123, 1.251], mean observation: 0.125 [-9.324, 19.885], loss: --, mean_squared_error: --, mean_q: --\n",
      "  780/2000: episode: 10, duration: 2.783s, episode steps: 77, steps per second: 28, episode reward: 0.424, mean reward: 0.006 [-0.002, 0.013], mean action: 0.227 [-1.071, 1.158], mean observation: 0.123 [-9.248, 20.067], loss: --, mean_squared_error: --, mean_q: --\n",
      "  857/2000: episode: 11, duration: 2.757s, episode steps: 77, steps per second: 28, episode reward: 0.417, mean reward: 0.005 [-0.002, 0.013], mean action: 0.221 [-1.150, 1.184], mean observation: 0.124 [-9.322, 19.917], loss: --, mean_squared_error: --, mean_q: --\n",
      "  935/2000: episode: 12, duration: 2.815s, episode steps: 78, steps per second: 28, episode reward: 0.424, mean reward: 0.005 [-0.002, 0.013], mean action: 0.232 [-1.084, 1.115], mean observation: 0.124 [-9.233, 20.126], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1015/2000: episode: 13, duration: 3.075s, episode steps: 80, steps per second: 26, episode reward: 0.439, mean reward: 0.005 [-0.002, 0.013], mean action: 0.226 [-1.090, 1.164], mean observation: 0.124 [-9.192, 20.048], loss: 0.000166, mean_squared_error: 0.000333, mean_q: 0.659530\n",
      " 1093/2000: episode: 14, duration: 3.521s, episode steps: 78, steps per second: 22, episode reward: 0.422, mean reward: 0.005 [-0.002, 0.013], mean action: 0.218 [-1.085, 1.087], mean observation: 0.123 [-9.407, 19.905], loss: 0.000526, mean_squared_error: 0.001053, mean_q: 0.647229\n",
      " 1171/2000: episode: 15, duration: 3.605s, episode steps: 78, steps per second: 22, episode reward: 0.428, mean reward: 0.005 [-0.002, 0.014], mean action: 0.213 [-1.113, 1.127], mean observation: 0.122 [-9.494, 19.373], loss: 0.000493, mean_squared_error: 0.000986, mean_q: 0.648697\n",
      " 1248/2000: episode: 16, duration: 3.511s, episode steps: 77, steps per second: 22, episode reward: 0.418, mean reward: 0.005 [-0.001, 0.014], mean action: 0.223 [-1.162, 1.172], mean observation: 0.123 [-9.373, 19.685], loss: 0.000659, mean_squared_error: 0.001318, mean_q: 0.651826\n",
      " 1325/2000: episode: 17, duration: 3.450s, episode steps: 77, steps per second: 22, episode reward: 0.416, mean reward: 0.005 [-0.002, 0.014], mean action: 0.234 [-1.080, 1.181], mean observation: 0.124 [-9.363, 19.490], loss: 0.000550, mean_squared_error: 0.001101, mean_q: 0.653431\n",
      " 1403/2000: episode: 18, duration: 3.550s, episode steps: 78, steps per second: 22, episode reward: 0.423, mean reward: 0.005 [-0.002, 0.014], mean action: 0.221 [-1.185, 1.132], mean observation: 0.124 [-9.439, 19.654], loss: 0.000304, mean_squared_error: 0.000609, mean_q: 0.653109\n",
      " 1478/2000: episode: 19, duration: 3.301s, episode steps: 75, steps per second: 23, episode reward: 0.402, mean reward: 0.005 [-0.002, 0.013], mean action: 0.213 [-1.139, 1.119], mean observation: 0.125 [-9.142, 19.598], loss: 0.000274, mean_squared_error: 0.000547, mean_q: 0.643352\n",
      " 1557/2000: episode: 20, duration: 3.599s, episode steps: 79, steps per second: 22, episode reward: 0.432, mean reward: 0.005 [-0.001, 0.013], mean action: 0.256 [-1.175, 1.149], mean observation: 0.125 [-9.090, 19.548], loss: 0.000273, mean_squared_error: 0.000545, mean_q: 0.649814\n",
      " 1636/2000: episode: 21, duration: 3.562s, episode steps: 79, steps per second: 22, episode reward: 0.435, mean reward: 0.006 [-0.001, 0.013], mean action: 0.410 [-1.110, 1.179], mean observation: 0.123 [-9.002, 19.726], loss: 0.000215, mean_squared_error: 0.000429, mean_q: 0.662498\n",
      " 1716/2000: episode: 22, duration: 3.489s, episode steps: 80, steps per second: 23, episode reward: 0.443, mean reward: 0.006 [-0.000, 0.013], mean action: 0.423 [-1.059, 1.152], mean observation: 0.124 [-9.238, 19.678], loss: 0.000126, mean_squared_error: 0.000252, mean_q: 0.652048\n",
      " 1793/2000: episode: 23, duration: 3.286s, episode steps: 77, steps per second: 23, episode reward: 0.426, mean reward: 0.006 [-0.000, 0.013], mean action: 0.413 [-1.086, 1.254], mean observation: 0.123 [-9.121, 19.518], loss: 0.000280, mean_squared_error: 0.000560, mean_q: 0.662117\n",
      " 1872/2000: episode: 24, duration: 3.387s, episode steps: 79, steps per second: 23, episode reward: 0.439, mean reward: 0.006 [-0.000, 0.013], mean action: 0.405 [-1.092, 1.212], mean observation: 0.123 [-9.143, 19.640], loss: 0.000285, mean_squared_error: 0.000571, mean_q: 0.664885\n",
      " 1951/2000: episode: 25, duration: 3.450s, episode steps: 79, steps per second: 23, episode reward: 0.441, mean reward: 0.006 [-0.001, 0.013], mean action: 0.410 [-1.114, 1.125], mean observation: 0.122 [-9.226, 19.401], loss: 0.000205, mean_squared_error: 0.000410, mean_q: 0.657687\n",
      "done, took 80.818 seconds\n",
      "\n",
      "\n",
      "iteration: 10\n",
      "Training for 2000 steps ...\n",
      "   78/2000: episode: 1, duration: 2.559s, episode steps: 78, steps per second: 30, episode reward: 0.436, mean reward: 0.006 [-0.000, 0.013], mean action: 0.413 [-1.077, 1.193], mean observation: 0.124 [-9.221, 19.397], loss: --, mean_squared_error: --, mean_q: --\n",
      "  158/2000: episode: 2, duration: 2.738s, episode steps: 80, steps per second: 29, episode reward: 0.447, mean reward: 0.006 [-0.000, 0.013], mean action: 0.395 [-1.216, 1.173], mean observation: 0.123 [-9.291, 19.195], loss: --, mean_squared_error: --, mean_q: --\n",
      "  237/2000: episode: 3, duration: 2.634s, episode steps: 79, steps per second: 30, episode reward: 0.438, mean reward: 0.006 [-0.000, 0.013], mean action: 0.413 [-1.068, 1.202], mean observation: 0.124 [-9.244, 19.411], loss: --, mean_squared_error: --, mean_q: --\n",
      "  317/2000: episode: 4, duration: 2.717s, episode steps: 80, steps per second: 29, episode reward: 0.441, mean reward: 0.006 [-0.000, 0.013], mean action: 0.422 [-1.131, 1.167], mean observation: 0.124 [-9.227, 19.247], loss: --, mean_squared_error: --, mean_q: --\n",
      "  397/2000: episode: 5, duration: 2.698s, episode steps: 80, steps per second: 30, episode reward: 0.442, mean reward: 0.006 [-0.000, 0.013], mean action: 0.414 [-1.127, 1.167], mean observation: 0.124 [-9.235, 19.099], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  477/2000: episode: 6, duration: 2.714s, episode steps: 80, steps per second: 29, episode reward: 0.447, mean reward: 0.006 [-0.000, 0.013], mean action: 0.398 [-1.187, 1.179], mean observation: 0.123 [-9.195, 19.156], loss: --, mean_squared_error: --, mean_q: --\n",
      "  555/2000: episode: 7, duration: 2.547s, episode steps: 78, steps per second: 31, episode reward: 0.432, mean reward: 0.006 [-0.000, 0.013], mean action: 0.400 [-1.120, 1.186], mean observation: 0.124 [-9.225, 19.311], loss: --, mean_squared_error: --, mean_q: --\n",
      "  635/2000: episode: 8, duration: 2.701s, episode steps: 80, steps per second: 30, episode reward: 0.444, mean reward: 0.006 [-0.000, 0.013], mean action: 0.412 [-1.242, 1.224], mean observation: 0.124 [-9.309, 19.426], loss: --, mean_squared_error: --, mean_q: --\n",
      "  715/2000: episode: 9, duration: 2.609s, episode steps: 80, steps per second: 31, episode reward: 0.444, mean reward: 0.006 [-0.000, 0.013], mean action: 0.399 [-1.110, 1.120], mean observation: 0.124 [-9.249, 19.417], loss: --, mean_squared_error: --, mean_q: --\n",
      "  795/2000: episode: 10, duration: 2.728s, episode steps: 80, steps per second: 29, episode reward: 0.442, mean reward: 0.006 [-0.000, 0.013], mean action: 0.411 [-1.135, 1.208], mean observation: 0.124 [-9.237, 19.073], loss: --, mean_squared_error: --, mean_q: --\n",
      "  874/2000: episode: 11, duration: 2.664s, episode steps: 79, steps per second: 30, episode reward: 0.435, mean reward: 0.006 [-0.000, 0.013], mean action: 0.420 [-1.149, 1.239], mean observation: 0.124 [-9.221, 19.324], loss: --, mean_squared_error: --, mean_q: --\n",
      "  953/2000: episode: 12, duration: 2.631s, episode steps: 79, steps per second: 30, episode reward: 0.437, mean reward: 0.006 [-0.000, 0.013], mean action: 0.413 [-1.079, 1.152], mean observation: 0.124 [-9.219, 19.289], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1031/2000: episode: 13, duration: 2.836s, episode steps: 78, steps per second: 28, episode reward: 0.429, mean reward: 0.005 [-0.000, 0.013], mean action: 0.401 [-1.057, 1.124], mean observation: 0.124 [-9.302, 19.195], loss: 0.000251, mean_squared_error: 0.000502, mean_q: 0.668769\n",
      " 1111/2000: episode: 14, duration: 3.327s, episode steps: 80, steps per second: 24, episode reward: 0.441, mean reward: 0.006 [-0.000, 0.013], mean action: 0.404 [-1.090, 1.100], mean observation: 0.126 [-9.279, 19.243], loss: 0.000293, mean_squared_error: 0.000587, mean_q: 0.661623\n",
      " 1191/2000: episode: 15, duration: 3.443s, episode steps: 80, steps per second: 23, episode reward: 0.438, mean reward: 0.005 [-0.000, 0.013], mean action: 0.399 [-1.191, 1.105], mean observation: 0.125 [-9.284, 18.979], loss: 0.000358, mean_squared_error: 0.000716, mean_q: 0.677467\n",
      " 1270/2000: episode: 16, duration: 3.385s, episode steps: 79, steps per second: 23, episode reward: 0.441, mean reward: 0.006 [-0.000, 0.013], mean action: 0.407 [-1.103, 1.136], mean observation: 0.124 [-9.252, 19.093], loss: 0.000248, mean_squared_error: 0.000495, mean_q: 0.680841\n",
      " 1349/2000: episode: 17, duration: 3.356s, episode steps: 79, steps per second: 24, episode reward: 0.437, mean reward: 0.006 [-0.000, 0.013], mean action: 0.425 [-1.089, 1.182], mean observation: 0.125 [-9.541, 18.968], loss: 0.000195, mean_squared_error: 0.000390, mean_q: 0.667398\n",
      " 1435/2000: episode: 18, duration: 3.791s, episode steps: 86, steps per second: 23, episode reward: 0.468, mean reward: 0.005 [0.000, 0.013], mean action: 0.406 [-1.131, 1.168], mean observation: 0.128 [-17.756, 16.867], loss: 0.000302, mean_squared_error: 0.000604, mean_q: 0.684961\n",
      " 1517/2000: episode: 19, duration: 3.550s, episode steps: 82, steps per second: 23, episode reward: 0.450, mean reward: 0.005 [-0.000, 0.014], mean action: 0.421 [-1.094, 1.184], mean observation: 0.129 [-15.414, 17.399], loss: 0.000216, mean_squared_error: 0.000433, mean_q: 0.673096\n",
      " 1602/2000: episode: 20, duration: 3.819s, episode steps: 85, steps per second: 22, episode reward: 0.475, mean reward: 0.006 [-0.000, 0.014], mean action: 0.355 [-1.160, 1.140], mean observation: 0.127 [-16.994, 17.549], loss: 0.000235, mean_squared_error: 0.000469, mean_q: 0.684819\n",
      " 1687/2000: episode: 21, duration: 3.799s, episode steps: 85, steps per second: 22, episode reward: 0.476, mean reward: 0.006 [-0.000, 0.014], mean action: 0.290 [-1.088, 1.220], mean observation: 0.127 [-18.306, 17.598], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.676521\n",
      " 1771/2000: episode: 22, duration: 3.740s, episode steps: 84, steps per second: 22, episode reward: 0.474, mean reward: 0.006 [-0.000, 0.014], mean action: 0.292 [-1.123, 1.106], mean observation: 0.126 [-17.764, 17.894], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.674652\n",
      " 1853/2000: episode: 23, duration: 3.580s, episode steps: 82, steps per second: 23, episode reward: 0.464, mean reward: 0.006 [-0.000, 0.014], mean action: 0.282 [-1.228, 1.148], mean observation: 0.124 [-18.290, 17.517], loss: 0.000104, mean_squared_error: 0.000209, mean_q: 0.683481\n",
      " 1937/2000: episode: 24, duration: 3.698s, episode steps: 84, steps per second: 23, episode reward: 0.467, mean reward: 0.006 [-0.000, 0.014], mean action: 0.303 [-1.136, 1.221], mean observation: 0.127 [-17.026, 17.716], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.697029\n",
      "done, took 77.047 seconds\n",
      "\n",
      "\n",
      "iteration: 11\n",
      "Training for 2000 steps ...\n",
      "   82/2000: episode: 1, duration: 2.730s, episode steps: 82, steps per second: 30, episode reward: 0.465, mean reward: 0.006 [-0.000, 0.014], mean action: 0.303 [-1.159, 1.156], mean observation: 0.126 [-15.750, 18.008], loss: --, mean_squared_error: --, mean_q: --\n",
      "  165/2000: episode: 2, duration: 2.832s, episode steps: 83, steps per second: 29, episode reward: 0.464, mean reward: 0.006 [-0.000, 0.014], mean action: 0.322 [-1.084, 1.141], mean observation: 0.126 [-15.866, 17.698], loss: --, mean_squared_error: --, mean_q: --\n",
      "  248/2000: episode: 3, duration: 2.756s, episode steps: 83, steps per second: 30, episode reward: 0.462, mean reward: 0.006 [-0.000, 0.014], mean action: 0.300 [-1.095, 1.183], mean observation: 0.127 [-15.838, 17.713], loss: --, mean_squared_error: --, mean_q: --\n",
      "  331/2000: episode: 4, duration: 2.826s, episode steps: 83, steps per second: 29, episode reward: 0.468, mean reward: 0.006 [-0.000, 0.014], mean action: 0.284 [-1.199, 1.138], mean observation: 0.126 [-14.635, 17.620], loss: --, mean_squared_error: --, mean_q: --\n",
      "  415/2000: episode: 5, duration: 2.845s, episode steps: 84, steps per second: 30, episode reward: 0.471, mean reward: 0.006 [-0.000, 0.014], mean action: 0.270 [-1.146, 1.100], mean observation: 0.127 [-15.800, 17.376], loss: --, mean_squared_error: --, mean_q: --\n",
      "  496/2000: episode: 6, duration: 2.696s, episode steps: 81, steps per second: 30, episode reward: 0.454, mean reward: 0.006 [-0.000, 0.014], mean action: 0.293 [-1.124, 1.206], mean observation: 0.126 [-16.127, 17.794], loss: --, mean_squared_error: --, mean_q: --\n",
      "  579/2000: episode: 7, duration: 2.781s, episode steps: 83, steps per second: 30, episode reward: 0.469, mean reward: 0.006 [-0.000, 0.014], mean action: 0.308 [-1.191, 1.237], mean observation: 0.126 [-16.092, 17.780], loss: --, mean_squared_error: --, mean_q: --\n",
      "  663/2000: episode: 8, duration: 2.833s, episode steps: 84, steps per second: 30, episode reward: 0.469, mean reward: 0.006 [-0.000, 0.014], mean action: 0.294 [-1.087, 1.199], mean observation: 0.126 [-15.962, 17.613], loss: --, mean_squared_error: --, mean_q: --\n",
      "  745/2000: episode: 9, duration: 2.756s, episode steps: 82, steps per second: 30, episode reward: 0.463, mean reward: 0.006 [-0.000, 0.014], mean action: 0.276 [-1.131, 1.110], mean observation: 0.125 [-15.285, 17.852], loss: --, mean_squared_error: --, mean_q: --\n",
      "  827/2000: episode: 10, duration: 2.789s, episode steps: 82, steps per second: 29, episode reward: 0.450, mean reward: 0.005 [-0.000, 0.014], mean action: 0.278 [-1.193, 1.165], mean observation: 0.127 [-15.842, 17.744], loss: --, mean_squared_error: --, mean_q: --\n",
      "  907/2000: episode: 11, duration: 2.616s, episode steps: 80, steps per second: 31, episode reward: 0.442, mean reward: 0.006 [-0.000, 0.014], mean action: 0.279 [-1.141, 1.132], mean observation: 0.127 [-16.045, 17.704], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  990/2000: episode: 12, duration: 2.880s, episode steps: 83, steps per second: 29, episode reward: 0.473, mean reward: 0.006 [-0.000, 0.014], mean action: 0.293 [-1.119, 1.151], mean observation: 0.124 [-16.184, 17.632], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1072/2000: episode: 13, duration: 3.449s, episode steps: 82, steps per second: 24, episode reward: 0.459, mean reward: 0.006 [-0.000, 0.014], mean action: 0.299 [-1.139, 1.168], mean observation: 0.125 [-15.733, 17.484], loss: 0.000201, mean_squared_error: 0.000403, mean_q: 0.708665\n",
      " 1154/2000: episode: 14, duration: 3.601s, episode steps: 82, steps per second: 23, episode reward: 0.458, mean reward: 0.006 [-0.000, 0.014], mean action: 0.305 [-1.173, 1.216], mean observation: 0.125 [-17.183, 17.703], loss: 0.000214, mean_squared_error: 0.000427, mean_q: 0.703556\n",
      " 1234/2000: episode: 15, duration: 3.407s, episode steps: 80, steps per second: 23, episode reward: 0.444, mean reward: 0.006 [-0.000, 0.014], mean action: 0.296 [-1.241, 1.239], mean observation: 0.126 [-15.803, 17.718], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.693624\n",
      " 1315/2000: episode: 16, duration: 3.592s, episode steps: 81, steps per second: 23, episode reward: 0.455, mean reward: 0.006 [-0.000, 0.014], mean action: 0.290 [-1.099, 1.130], mean observation: 0.126 [-17.765, 17.538], loss: 0.000115, mean_squared_error: 0.000230, mean_q: 0.713434\n",
      " 1396/2000: episode: 17, duration: 3.606s, episode steps: 81, steps per second: 22, episode reward: 0.444, mean reward: 0.005 [-0.000, 0.014], mean action: 0.363 [-1.092, 1.196], mean observation: 0.127 [-17.737, 17.385], loss: 0.000175, mean_squared_error: 0.000350, mean_q: 0.701973\n",
      " 1477/2000: episode: 18, duration: 3.416s, episode steps: 81, steps per second: 24, episode reward: 0.446, mean reward: 0.006 [-0.000, 0.014], mean action: 0.420 [-1.055, 1.197], mean observation: 0.126 [-14.332, 17.668], loss: 0.000073, mean_squared_error: 0.000147, mean_q: 0.705909\n",
      " 1559/2000: episode: 19, duration: 3.553s, episode steps: 82, steps per second: 23, episode reward: 0.452, mean reward: 0.006 [-0.000, 0.014], mean action: 0.418 [-1.103, 1.267], mean observation: 0.127 [-14.022, 17.810], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.698025\n",
      " 1641/2000: episode: 20, duration: 3.577s, episode steps: 82, steps per second: 23, episode reward: 0.452, mean reward: 0.006 [-0.000, 0.014], mean action: 0.422 [-1.128, 1.172], mean observation: 0.127 [-16.008, 17.448], loss: 0.000091, mean_squared_error: 0.000182, mean_q: 0.702379\n",
      " 1718/2000: episode: 21, duration: 3.475s, episode steps: 77, steps per second: 22, episode reward: 0.440, mean reward: 0.006 [-0.001, 0.014], mean action: 0.293 [-1.125, 1.185], mean observation: 0.124 [-12.962, 17.815], loss: 0.000165, mean_squared_error: 0.000330, mean_q: 0.699136\n",
      " 1802/2000: episode: 22, duration: 3.705s, episode steps: 84, steps per second: 23, episode reward: 0.479, mean reward: 0.006 [-0.000, 0.014], mean action: 0.267 [-1.124, 1.120], mean observation: 0.126 [-10.031, 17.493], loss: 0.000166, mean_squared_error: 0.000332, mean_q: 0.719420\n",
      " 1876/2000: episode: 23, duration: 3.215s, episode steps: 74, steps per second: 23, episode reward: 0.437, mean reward: 0.006 [-0.001, 0.014], mean action: 0.182 [-1.136, 1.070], mean observation: 0.124 [-15.452, 17.614], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.693704\n",
      " 1951/2000: episode: 24, duration: 3.286s, episode steps: 75, steps per second: 23, episode reward: 0.431, mean reward: 0.006 [-0.001, 0.014], mean action: 0.235 [-1.109, 1.146], mean observation: 0.124 [-9.926, 17.774], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.695520\n",
      "done, took 77.626 seconds\n",
      "\n",
      "\n",
      "iteration: 12\n",
      "Training for 2000 steps ...\n",
      "   80/2000: episode: 1, duration: 2.815s, episode steps: 80, steps per second: 28, episode reward: 0.452, mean reward: 0.006 [-0.001, 0.014], mean action: 0.359 [-1.149, 1.110], mean observation: 0.127 [-11.888, 17.870], loss: --, mean_squared_error: --, mean_q: --\n",
      "  159/2000: episode: 2, duration: 2.798s, episode steps: 79, steps per second: 28, episode reward: 0.448, mean reward: 0.006 [-0.001, 0.014], mean action: 0.369 [-1.104, 1.213], mean observation: 0.126 [-12.089, 17.819], loss: --, mean_squared_error: --, mean_q: --\n",
      "  239/2000: episode: 3, duration: 2.870s, episode steps: 80, steps per second: 28, episode reward: 0.450, mean reward: 0.006 [-0.001, 0.014], mean action: 0.380 [-1.108, 1.141], mean observation: 0.125 [-11.934, 17.912], loss: --, mean_squared_error: --, mean_q: --\n",
      "  319/2000: episode: 4, duration: 2.816s, episode steps: 80, steps per second: 28, episode reward: 0.450, mean reward: 0.006 [-0.001, 0.014], mean action: 0.375 [-1.165, 1.247], mean observation: 0.126 [-12.137, 17.853], loss: --, mean_squared_error: --, mean_q: --\n",
      "  398/2000: episode: 5, duration: 2.818s, episode steps: 79, steps per second: 28, episode reward: 0.442, mean reward: 0.006 [-0.001, 0.014], mean action: 0.362 [-1.177, 1.156], mean observation: 0.127 [-12.404, 17.837], loss: --, mean_squared_error: --, mean_q: --\n",
      "  478/2000: episode: 6, duration: 2.834s, episode steps: 80, steps per second: 28, episode reward: 0.448, mean reward: 0.006 [-0.001, 0.014], mean action: 0.374 [-1.173, 1.181], mean observation: 0.127 [-12.037, 17.847], loss: --, mean_squared_error: --, mean_q: --\n",
      "  558/2000: episode: 7, duration: 2.813s, episode steps: 80, steps per second: 28, episode reward: 0.449, mean reward: 0.006 [-0.001, 0.014], mean action: 0.392 [-1.095, 1.311], mean observation: 0.126 [-11.748, 17.804], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 8, duration: 2.737s, episode steps: 79, steps per second: 29, episode reward: 0.442, mean reward: 0.006 [-0.001, 0.014], mean action: 0.359 [-1.125, 1.229], mean observation: 0.127 [-12.310, 17.701], loss: --, mean_squared_error: --, mean_q: --\n",
      "  717/2000: episode: 9, duration: 2.810s, episode steps: 80, steps per second: 28, episode reward: 0.449, mean reward: 0.006 [-0.000, 0.014], mean action: 0.359 [-1.095, 1.098], mean observation: 0.127 [-11.984, 17.848], loss: --, mean_squared_error: --, mean_q: --\n",
      "  797/2000: episode: 10, duration: 2.897s, episode steps: 80, steps per second: 28, episode reward: 0.453, mean reward: 0.006 [-0.001, 0.014], mean action: 0.394 [-1.075, 1.162], mean observation: 0.125 [-11.762, 17.804], loss: --, mean_squared_error: --, mean_q: --\n",
      "  878/2000: episode: 11, duration: 2.906s, episode steps: 81, steps per second: 28, episode reward: 0.452, mean reward: 0.006 [-0.001, 0.014], mean action: 0.373 [-1.084, 1.159], mean observation: 0.127 [-12.169, 17.744], loss: --, mean_squared_error: --, mean_q: --\n",
      "  960/2000: episode: 12, duration: 2.993s, episode steps: 82, steps per second: 27, episode reward: 0.467, mean reward: 0.006 [-0.001, 0.014], mean action: 0.363 [-1.114, 1.183], mean observation: 0.125 [-12.070, 17.753], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1040/2000: episode: 13, duration: 3.213s, episode steps: 80, steps per second: 25, episode reward: 0.453, mean reward: 0.006 [-0.000, 0.014], mean action: 0.380 [-1.127, 1.230], mean observation: 0.126 [-11.643, 17.918], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.704446\n",
      " 1114/2000: episode: 14, duration: 3.286s, episode steps: 74, steps per second: 23, episode reward: 0.444, mean reward: 0.006 [-0.001, 0.014], mean action: 0.373 [-1.204, 1.147], mean observation: 0.126 [-15.554, 17.641], loss: 0.000172, mean_squared_error: 0.000343, mean_q: 0.714463\n",
      " 1187/2000: episode: 15, duration: 3.236s, episode steps: 73, steps per second: 23, episode reward: 0.433, mean reward: 0.006 [-0.001, 0.014], mean action: 0.371 [-1.094, 1.192], mean observation: 0.125 [-9.772, 17.581], loss: 0.000185, mean_squared_error: 0.000369, mean_q: 0.709655\n",
      " 1261/2000: episode: 16, duration: 3.306s, episode steps: 74, steps per second: 22, episode reward: 0.433, mean reward: 0.006 [-0.001, 0.014], mean action: 0.390 [-1.193, 1.211], mean observation: 0.126 [-9.650, 17.860], loss: 0.000132, mean_squared_error: 0.000265, mean_q: 0.708462\n",
      " 1332/2000: episode: 17, duration: 3.029s, episode steps: 71, steps per second: 23, episode reward: 0.418, mean reward: 0.006 [-0.001, 0.014], mean action: 0.365 [-1.117, 1.233], mean observation: 0.127 [-9.734, 17.760], loss: 0.000096, mean_squared_error: 0.000191, mean_q: 0.717456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1404/2000: episode: 18, duration: 3.046s, episode steps: 72, steps per second: 24, episode reward: 0.425, mean reward: 0.006 [-0.001, 0.014], mean action: 0.369 [-1.157, 1.149], mean observation: 0.125 [-9.621, 17.845], loss: 0.000066, mean_squared_error: 0.000133, mean_q: 0.720553\n",
      " 1475/2000: episode: 19, duration: 2.928s, episode steps: 71, steps per second: 24, episode reward: 0.420, mean reward: 0.006 [-0.001, 0.014], mean action: 0.354 [-1.087, 1.205], mean observation: 0.126 [-9.687, 17.821], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.698341\n",
      " 1548/2000: episode: 20, duration: 3.157s, episode steps: 73, steps per second: 23, episode reward: 0.434, mean reward: 0.006 [-0.001, 0.014], mean action: 0.348 [-1.138, 1.205], mean observation: 0.125 [-9.731, 17.687], loss: 0.000098, mean_squared_error: 0.000197, mean_q: 0.710705\n",
      " 1621/2000: episode: 21, duration: 3.071s, episode steps: 73, steps per second: 24, episode reward: 0.431, mean reward: 0.006 [-0.001, 0.014], mean action: 0.371 [-1.135, 1.171], mean observation: 0.128 [-9.571, 17.253], loss: 0.000249, mean_squared_error: 0.000497, mean_q: 0.704838\n",
      " 1692/2000: episode: 22, duration: 2.985s, episode steps: 71, steps per second: 24, episode reward: 0.416, mean reward: 0.006 [-0.001, 0.014], mean action: 0.352 [-1.124, 1.151], mean observation: 0.126 [-9.673, 17.643], loss: 0.000079, mean_squared_error: 0.000157, mean_q: 0.707994\n",
      " 1764/2000: episode: 23, duration: 3.112s, episode steps: 72, steps per second: 23, episode reward: 0.413, mean reward: 0.006 [-0.001, 0.014], mean action: 0.330 [-1.082, 1.126], mean observation: 0.132 [-9.829, 17.604], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.714889\n",
      " 1837/2000: episode: 24, duration: 3.208s, episode steps: 73, steps per second: 23, episode reward: 0.424, mean reward: 0.006 [-0.001, 0.014], mean action: 0.353 [-1.119, 1.198], mean observation: 0.131 [-9.578, 17.521], loss: 0.000204, mean_squared_error: 0.000407, mean_q: 0.707655\n",
      " 1911/2000: episode: 25, duration: 3.246s, episode steps: 74, steps per second: 23, episode reward: 0.442, mean reward: 0.006 [-0.001, 0.014], mean action: 0.199 [-1.101, 1.149], mean observation: 0.127 [-9.692, 17.478], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.704916\n",
      " 1982/2000: episode: 26, duration: 3.075s, episode steps: 71, steps per second: 23, episode reward: 0.418, mean reward: 0.006 [-0.001, 0.014], mean action: 0.264 [-1.131, 1.156], mean observation: 0.129 [-9.560, 17.713], loss: 0.000291, mean_squared_error: 0.000582, mean_q: 0.725368\n",
      "done, took 79.341 seconds\n",
      "\n",
      "\n",
      "iteration: 13\n",
      "Training for 2000 steps ...\n",
      "   73/2000: episode: 1, duration: 2.620s, episode steps: 73, steps per second: 28, episode reward: 0.433, mean reward: 0.006 [-0.001, 0.014], mean action: 0.265 [-1.180, 1.125], mean observation: 0.130 [-9.765, 17.385], loss: --, mean_squared_error: --, mean_q: --\n",
      "  145/2000: episode: 2, duration: 2.462s, episode steps: 72, steps per second: 29, episode reward: 0.423, mean reward: 0.006 [-0.001, 0.014], mean action: 0.283 [-1.101, 1.164], mean observation: 0.130 [-9.825, 17.218], loss: --, mean_squared_error: --, mean_q: --\n",
      "  218/2000: episode: 3, duration: 2.479s, episode steps: 73, steps per second: 29, episode reward: 0.431, mean reward: 0.006 [-0.001, 0.014], mean action: 0.287 [-1.082, 1.174], mean observation: 0.131 [-9.749, 17.096], loss: --, mean_squared_error: --, mean_q: --\n",
      "  292/2000: episode: 4, duration: 2.525s, episode steps: 74, steps per second: 29, episode reward: 0.438, mean reward: 0.006 [-0.001, 0.014], mean action: 0.298 [-1.087, 1.214], mean observation: 0.131 [-9.674, 17.604], loss: --, mean_squared_error: --, mean_q: --\n",
      "  365/2000: episode: 5, duration: 2.521s, episode steps: 73, steps per second: 29, episode reward: 0.431, mean reward: 0.006 [-0.001, 0.014], mean action: 0.282 [-1.148, 1.164], mean observation: 0.130 [-9.865, 17.239], loss: --, mean_squared_error: --, mean_q: --\n",
      "  438/2000: episode: 6, duration: 2.507s, episode steps: 73, steps per second: 29, episode reward: 0.430, mean reward: 0.006 [-0.001, 0.014], mean action: 0.279 [-1.134, 1.125], mean observation: 0.131 [-9.896, 17.447], loss: --, mean_squared_error: --, mean_q: --\n",
      "  512/2000: episode: 7, duration: 2.505s, episode steps: 74, steps per second: 30, episode reward: 0.437, mean reward: 0.006 [-0.000, 0.014], mean action: 0.289 [-1.111, 1.184], mean observation: 0.131 [-9.705, 17.230], loss: --, mean_squared_error: --, mean_q: --\n",
      "  586/2000: episode: 8, duration: 2.582s, episode steps: 74, steps per second: 29, episode reward: 0.440, mean reward: 0.006 [-0.001, 0.014], mean action: 0.295 [-1.081, 1.173], mean observation: 0.130 [-9.863, 17.166], loss: --, mean_squared_error: --, mean_q: --\n",
      "  659/2000: episode: 9, duration: 2.502s, episode steps: 73, steps per second: 29, episode reward: 0.433, mean reward: 0.006 [-0.001, 0.014], mean action: 0.301 [-1.097, 1.216], mean observation: 0.131 [-9.885, 17.317], loss: --, mean_squared_error: --, mean_q: --\n",
      "  732/2000: episode: 10, duration: 2.484s, episode steps: 73, steps per second: 29, episode reward: 0.430, mean reward: 0.006 [-0.001, 0.014], mean action: 0.272 [-1.165, 1.089], mean observation: 0.131 [-9.750, 17.245], loss: --, mean_squared_error: --, mean_q: --\n",
      "  805/2000: episode: 11, duration: 2.523s, episode steps: 73, steps per second: 29, episode reward: 0.430, mean reward: 0.006 [-0.001, 0.014], mean action: 0.293 [-1.108, 1.166], mean observation: 0.131 [-9.757, 17.045], loss: --, mean_squared_error: --, mean_q: --\n",
      "  877/2000: episode: 12, duration: 2.455s, episode steps: 72, steps per second: 29, episode reward: 0.422, mean reward: 0.006 [-0.001, 0.014], mean action: 0.264 [-1.103, 1.150], mean observation: 0.131 [-9.752, 17.243], loss: --, mean_squared_error: --, mean_q: --\n",
      "  950/2000: episode: 13, duration: 2.461s, episode steps: 73, steps per second: 30, episode reward: 0.431, mean reward: 0.006 [-0.001, 0.014], mean action: 0.297 [-1.087, 1.161], mean observation: 0.130 [-9.775, 17.502], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1022/2000: episode: 14, duration: 2.674s, episode steps: 72, steps per second: 27, episode reward: 0.419, mean reward: 0.006 [-0.001, 0.014], mean action: 0.270 [-1.103, 1.128], mean observation: 0.132 [-9.771, 17.272], loss: 0.000123, mean_squared_error: 0.000246, mean_q: 0.733062\n",
      " 1094/2000: episode: 15, duration: 3.035s, episode steps: 72, steps per second: 24, episode reward: 0.413, mean reward: 0.006 [-0.000, 0.014], mean action: 0.251 [-1.092, 1.143], mean observation: 0.134 [-9.834, 16.947], loss: 0.000298, mean_squared_error: 0.000596, mean_q: 0.732348\n",
      " 1168/2000: episode: 16, duration: 3.262s, episode steps: 74, steps per second: 23, episode reward: 0.434, mean reward: 0.006 [-0.001, 0.014], mean action: 0.323 [-1.128, 1.150], mean observation: 0.132 [-9.890, 18.090], loss: 0.000266, mean_squared_error: 0.000532, mean_q: 0.735824\n",
      " 1241/2000: episode: 17, duration: 3.118s, episode steps: 73, steps per second: 23, episode reward: 0.429, mean reward: 0.006 [-0.000, 0.014], mean action: 0.271 [-1.225, 1.112], mean observation: 0.131 [-10.007, 18.212], loss: 0.000204, mean_squared_error: 0.000407, mean_q: 0.718330\n",
      " 1313/2000: episode: 18, duration: 2.898s, episode steps: 72, steps per second: 25, episode reward: 0.415, mean reward: 0.006 [-0.000, 0.014], mean action: 0.196 [-1.155, 1.143], mean observation: 0.133 [-10.016, 18.018], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.723854\n",
      " 1387/2000: episode: 19, duration: 3.006s, episode steps: 74, steps per second: 25, episode reward: 0.435, mean reward: 0.006 [-0.000, 0.014], mean action: 0.181 [-1.210, 1.123], mean observation: 0.131 [-9.979, 18.301], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.713926\n",
      " 1459/2000: episode: 20, duration: 2.916s, episode steps: 72, steps per second: 25, episode reward: 0.420, mean reward: 0.006 [-0.000, 0.014], mean action: 0.184 [-1.181, 1.162], mean observation: 0.132 [-10.113, 18.118], loss: 0.000072, mean_squared_error: 0.000144, mean_q: 0.718766\n",
      " 1533/2000: episode: 21, duration: 3.047s, episode steps: 74, steps per second: 24, episode reward: 0.429, mean reward: 0.006 [-0.000, 0.014], mean action: 0.256 [-1.132, 1.273], mean observation: 0.134 [-10.110, 18.029], loss: 0.000059, mean_squared_error: 0.000119, mean_q: 0.714258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1607/2000: episode: 22, duration: 3.117s, episode steps: 74, steps per second: 24, episode reward: 0.425, mean reward: 0.006 [-0.000, 0.014], mean action: 0.217 [-1.097, 1.156], mean observation: 0.135 [-9.969, 18.013], loss: 0.000154, mean_squared_error: 0.000307, mean_q: 0.720255\n",
      " 1681/2000: episode: 23, duration: 3.002s, episode steps: 74, steps per second: 25, episode reward: 0.431, mean reward: 0.006 [-0.000, 0.014], mean action: 0.192 [-1.123, 1.126], mean observation: 0.133 [-10.093, 17.961], loss: 0.000135, mean_squared_error: 0.000269, mean_q: 0.733543\n",
      " 1759/2000: episode: 24, duration: 3.341s, episode steps: 78, steps per second: 23, episode reward: 0.457, mean reward: 0.006 [-0.000, 0.014], mean action: 0.196 [-1.091, 1.126], mean observation: 0.130 [-10.076, 18.204], loss: 0.000188, mean_squared_error: 0.000376, mean_q: 0.725469\n",
      " 1835/2000: episode: 25, duration: 3.270s, episode steps: 76, steps per second: 23, episode reward: 0.441, mean reward: 0.006 [0.000, 0.014], mean action: 0.279 [-1.085, 1.171], mean observation: 0.133 [-9.980, 18.058], loss: 0.000107, mean_squared_error: 0.000213, mean_q: 0.731899\n",
      " 1909/2000: episode: 26, duration: 3.042s, episode steps: 74, steps per second: 24, episode reward: 0.426, mean reward: 0.006 [0.000, 0.014], mean action: 0.280 [-1.123, 1.141], mean observation: 0.134 [-10.155, 17.995], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.724355\n",
      " 1985/2000: episode: 27, duration: 3.187s, episode steps: 76, steps per second: 24, episode reward: 0.439, mean reward: 0.006 [0.000, 0.014], mean action: 0.251 [-1.214, 1.102], mean observation: 0.135 [-9.965, 18.140], loss: 0.000129, mean_squared_error: 0.000257, mean_q: 0.728155\n",
      "done, took 76.681 seconds\n",
      "\n",
      "\n",
      "iteration: 14\n",
      "Training for 2000 steps ...\n",
      "   77/2000: episode: 1, duration: 2.544s, episode steps: 77, steps per second: 30, episode reward: 0.445, mean reward: 0.006 [0.000, 0.014], mean action: 0.197 [-1.134, 1.165], mean observation: 0.135 [-9.905, 18.008], loss: --, mean_squared_error: --, mean_q: --\n",
      "  152/2000: episode: 2, duration: 2.476s, episode steps: 75, steps per second: 30, episode reward: 0.435, mean reward: 0.006 [-0.000, 0.014], mean action: 0.201 [-1.110, 1.156], mean observation: 0.133 [-10.223, 18.281], loss: --, mean_squared_error: --, mean_q: --\n",
      "  228/2000: episode: 3, duration: 2.432s, episode steps: 76, steps per second: 31, episode reward: 0.438, mean reward: 0.006 [0.000, 0.014], mean action: 0.200 [-1.245, 1.238], mean observation: 0.135 [-9.960, 18.422], loss: --, mean_squared_error: --, mean_q: --\n",
      "  304/2000: episode: 4, duration: 2.498s, episode steps: 76, steps per second: 30, episode reward: 0.442, mean reward: 0.006 [-0.000, 0.014], mean action: 0.201 [-1.161, 1.174], mean observation: 0.133 [-10.178, 18.187], loss: --, mean_squared_error: --, mean_q: --\n",
      "  379/2000: episode: 5, duration: 2.391s, episode steps: 75, steps per second: 31, episode reward: 0.435, mean reward: 0.006 [-0.000, 0.014], mean action: 0.219 [-1.143, 1.212], mean observation: 0.134 [-10.083, 18.155], loss: --, mean_squared_error: --, mean_q: --\n",
      "  454/2000: episode: 6, duration: 2.387s, episode steps: 75, steps per second: 31, episode reward: 0.431, mean reward: 0.006 [0.000, 0.014], mean action: 0.194 [-1.189, 1.149], mean observation: 0.135 [-10.084, 17.980], loss: --, mean_squared_error: --, mean_q: --\n",
      "  529/2000: episode: 7, duration: 2.404s, episode steps: 75, steps per second: 31, episode reward: 0.433, mean reward: 0.006 [-0.000, 0.014], mean action: 0.196 [-1.083, 1.096], mean observation: 0.134 [-10.074, 18.044], loss: --, mean_squared_error: --, mean_q: --\n",
      "  605/2000: episode: 8, duration: 2.491s, episode steps: 76, steps per second: 31, episode reward: 0.441, mean reward: 0.006 [0.000, 0.014], mean action: 0.184 [-1.155, 1.113], mean observation: 0.135 [-10.016, 18.317], loss: --, mean_squared_error: --, mean_q: --\n",
      "  681/2000: episode: 9, duration: 2.494s, episode steps: 76, steps per second: 30, episode reward: 0.441, mean reward: 0.006 [0.000, 0.014], mean action: 0.203 [-1.156, 1.161], mean observation: 0.134 [-10.031, 18.244], loss: --, mean_squared_error: --, mean_q: --\n",
      "  756/2000: episode: 10, duration: 2.360s, episode steps: 75, steps per second: 32, episode reward: 0.434, mean reward: 0.006 [-0.000, 0.014], mean action: 0.200 [-1.135, 1.184], mean observation: 0.134 [-10.084, 17.886], loss: --, mean_squared_error: --, mean_q: --\n",
      "  832/2000: episode: 11, duration: 2.420s, episode steps: 76, steps per second: 31, episode reward: 0.439, mean reward: 0.006 [0.000, 0.014], mean action: 0.212 [-1.165, 1.169], mean observation: 0.135 [-10.073, 18.217], loss: --, mean_squared_error: --, mean_q: --\n",
      "  908/2000: episode: 12, duration: 2.492s, episode steps: 76, steps per second: 31, episode reward: 0.447, mean reward: 0.006 [-0.000, 0.014], mean action: 0.192 [-1.111, 1.172], mean observation: 0.132 [-10.095, 18.215], loss: --, mean_squared_error: --, mean_q: --\n",
      "  984/2000: episode: 13, duration: 2.502s, episode steps: 76, steps per second: 30, episode reward: 0.440, mean reward: 0.006 [-0.000, 0.014], mean action: 0.214 [-1.086, 1.115], mean observation: 0.133 [-10.103, 17.834], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1060/2000: episode: 14, duration: 3.067s, episode steps: 76, steps per second: 25, episode reward: 0.433, mean reward: 0.006 [0.000, 0.014], mean action: 0.277 [-1.132, 1.259], mean observation: 0.136 [-10.084, 18.297], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.736147\n",
      " 1137/2000: episode: 15, duration: 3.306s, episode steps: 77, steps per second: 23, episode reward: 0.439, mean reward: 0.006 [0.000, 0.014], mean action: 0.233 [-1.050, 1.149], mean observation: 0.136 [-10.022, 18.077], loss: 0.000093, mean_squared_error: 0.000187, mean_q: 0.728106\n",
      " 1215/2000: episode: 16, duration: 3.305s, episode steps: 78, steps per second: 24, episode reward: 0.442, mean reward: 0.006 [0.000, 0.014], mean action: 0.299 [-1.089, 1.136], mean observation: 0.137 [-10.089, 18.080], loss: 0.000095, mean_squared_error: 0.000190, mean_q: 0.731919\n",
      " 1293/2000: episode: 17, duration: 3.392s, episode steps: 78, steps per second: 23, episode reward: 0.449, mean reward: 0.006 [0.000, 0.014], mean action: 0.296 [-1.085, 1.164], mean observation: 0.136 [-10.158, 18.198], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.722371\n",
      " 1370/2000: episode: 18, duration: 3.248s, episode steps: 77, steps per second: 24, episode reward: 0.445, mean reward: 0.006 [0.000, 0.014], mean action: 0.325 [-1.139, 1.156], mean observation: 0.137 [-10.171, 18.107], loss: 0.000082, mean_squared_error: 0.000165, mean_q: 0.731766\n",
      " 1447/2000: episode: 19, duration: 3.220s, episode steps: 77, steps per second: 24, episode reward: 0.435, mean reward: 0.006 [0.000, 0.013], mean action: 0.321 [-1.160, 1.154], mean observation: 0.138 [-10.114, 18.114], loss: 0.000091, mean_squared_error: 0.000183, mean_q: 0.708132\n",
      " 1522/2000: episode: 20, duration: 3.100s, episode steps: 75, steps per second: 24, episode reward: 0.424, mean reward: 0.006 [0.000, 0.014], mean action: 0.311 [-1.190, 1.142], mean observation: 0.138 [-9.995, 18.174], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.730563\n",
      " 1596/2000: episode: 21, duration: 3.028s, episode steps: 74, steps per second: 24, episode reward: 0.421, mean reward: 0.006 [0.000, 0.013], mean action: 0.292 [-1.170, 1.119], mean observation: 0.137 [-10.115, 18.235], loss: 0.000099, mean_squared_error: 0.000197, mean_q: 0.733034\n",
      " 1672/2000: episode: 22, duration: 3.219s, episode steps: 76, steps per second: 24, episode reward: 0.438, mean reward: 0.006 [0.000, 0.014], mean action: 0.371 [-1.056, 1.177], mean observation: 0.136 [-9.970, 18.313], loss: 0.000103, mean_squared_error: 0.000206, mean_q: 0.741104\n",
      " 1746/2000: episode: 23, duration: 3.154s, episode steps: 74, steps per second: 23, episode reward: 0.423, mean reward: 0.006 [0.000, 0.014], mean action: 0.312 [-1.097, 1.074], mean observation: 0.136 [-10.126, 18.178], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.734797\n",
      " 1818/2000: episode: 24, duration: 2.973s, episode steps: 72, steps per second: 24, episode reward: 0.410, mean reward: 0.006 [-0.000, 0.014], mean action: 0.315 [-1.091, 1.247], mean observation: 0.135 [-9.935, 18.158], loss: 0.000104, mean_squared_error: 0.000207, mean_q: 0.733480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1891/2000: episode: 25, duration: 3.113s, episode steps: 73, steps per second: 23, episode reward: 0.414, mean reward: 0.006 [-0.000, 0.014], mean action: 0.225 [-1.110, 1.189], mean observation: 0.135 [-9.910, 18.032], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.739529\n",
      " 1965/2000: episode: 26, duration: 2.957s, episode steps: 74, steps per second: 25, episode reward: 0.423, mean reward: 0.006 [0.000, 0.013], mean action: 0.227 [-1.081, 1.210], mean observation: 0.137 [-10.021, 18.239], loss: 0.000229, mean_squared_error: 0.000457, mean_q: 0.711784\n",
      "done, took 74.676 seconds\n",
      "\n",
      "\n",
      "iteration: 15\n",
      "Training for 2000 steps ...\n",
      "   74/2000: episode: 1, duration: 2.386s, episode steps: 74, steps per second: 31, episode reward: 0.421, mean reward: 0.006 [-0.000, 0.014], mean action: 0.234 [-1.074, 1.242], mean observation: 0.136 [-10.052, 17.912], loss: --, mean_squared_error: --, mean_q: --\n",
      "  147/2000: episode: 2, duration: 2.302s, episode steps: 73, steps per second: 32, episode reward: 0.414, mean reward: 0.006 [-0.000, 0.013], mean action: 0.204 [-1.160, 1.084], mean observation: 0.136 [-9.962, 17.993], loss: --, mean_squared_error: --, mean_q: --\n",
      "  220/2000: episode: 3, duration: 2.347s, episode steps: 73, steps per second: 31, episode reward: 0.415, mean reward: 0.006 [-0.000, 0.014], mean action: 0.220 [-1.098, 1.133], mean observation: 0.136 [-10.121, 18.019], loss: --, mean_squared_error: --, mean_q: --\n",
      "  294/2000: episode: 4, duration: 2.367s, episode steps: 74, steps per second: 31, episode reward: 0.424, mean reward: 0.006 [-0.000, 0.013], mean action: 0.224 [-1.131, 1.149], mean observation: 0.136 [-10.031, 18.197], loss: --, mean_squared_error: --, mean_q: --\n",
      "  367/2000: episode: 5, duration: 2.306s, episode steps: 73, steps per second: 32, episode reward: 0.415, mean reward: 0.006 [-0.000, 0.014], mean action: 0.217 [-1.151, 1.109], mean observation: 0.136 [-9.992, 17.988], loss: --, mean_squared_error: --, mean_q: --\n",
      "  440/2000: episode: 6, duration: 2.326s, episode steps: 73, steps per second: 31, episode reward: 0.415, mean reward: 0.006 [-0.000, 0.013], mean action: 0.193 [-1.172, 1.172], mean observation: 0.136 [-10.057, 18.064], loss: --, mean_squared_error: --, mean_q: --\n",
      "  513/2000: episode: 7, duration: 2.268s, episode steps: 73, steps per second: 32, episode reward: 0.416, mean reward: 0.006 [-0.000, 0.014], mean action: 0.213 [-1.105, 1.123], mean observation: 0.136 [-9.926, 18.306], loss: --, mean_squared_error: --, mean_q: --\n",
      "  586/2000: episode: 8, duration: 2.383s, episode steps: 73, steps per second: 31, episode reward: 0.413, mean reward: 0.006 [-0.000, 0.014], mean action: 0.220 [-1.115, 1.124], mean observation: 0.137 [-10.063, 18.256], loss: --, mean_squared_error: --, mean_q: --\n",
      "  660/2000: episode: 9, duration: 2.403s, episode steps: 74, steps per second: 31, episode reward: 0.424, mean reward: 0.006 [-0.000, 0.013], mean action: 0.211 [-1.187, 1.148], mean observation: 0.136 [-9.870, 18.229], loss: --, mean_squared_error: --, mean_q: --\n",
      "  733/2000: episode: 10, duration: 2.349s, episode steps: 73, steps per second: 31, episode reward: 0.412, mean reward: 0.006 [-0.000, 0.014], mean action: 0.225 [-1.155, 1.186], mean observation: 0.136 [-10.016, 18.102], loss: --, mean_squared_error: --, mean_q: --\n",
      "  807/2000: episode: 11, duration: 2.416s, episode steps: 74, steps per second: 31, episode reward: 0.423, mean reward: 0.006 [-0.000, 0.014], mean action: 0.213 [-1.083, 1.127], mean observation: 0.135 [-9.977, 17.994], loss: --, mean_squared_error: --, mean_q: --\n",
      "  880/2000: episode: 12, duration: 2.347s, episode steps: 73, steps per second: 31, episode reward: 0.414, mean reward: 0.006 [-0.000, 0.013], mean action: 0.228 [-1.133, 1.243], mean observation: 0.136 [-9.965, 18.430], loss: --, mean_squared_error: --, mean_q: --\n",
      "  952/2000: episode: 13, duration: 2.295s, episode steps: 72, steps per second: 31, episode reward: 0.407, mean reward: 0.006 [-0.000, 0.014], mean action: 0.225 [-1.168, 1.150], mean observation: 0.136 [-10.088, 18.146], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1025/2000: episode: 14, duration: 2.539s, episode steps: 73, steps per second: 29, episode reward: 0.414, mean reward: 0.006 [-0.000, 0.013], mean action: 0.224 [-1.091, 1.145], mean observation: 0.137 [-10.041, 18.165], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.745770\n",
      " 1098/2000: episode: 15, duration: 3.002s, episode steps: 73, steps per second: 24, episode reward: 0.416, mean reward: 0.006 [-0.000, 0.014], mean action: 0.246 [-1.069, 1.109], mean observation: 0.136 [-10.124, 18.259], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.748118\n",
      " 1171/2000: episode: 16, duration: 3.044s, episode steps: 73, steps per second: 24, episode reward: 0.415, mean reward: 0.006 [-0.000, 0.014], mean action: 0.260 [-1.189, 1.149], mean observation: 0.136 [-10.120, 18.148], loss: 0.000095, mean_squared_error: 0.000190, mean_q: 0.730827\n",
      " 1244/2000: episode: 17, duration: 3.103s, episode steps: 73, steps per second: 24, episode reward: 0.418, mean reward: 0.006 [-0.000, 0.014], mean action: 0.277 [-1.115, 1.143], mean observation: 0.136 [-10.073, 18.236], loss: 0.000070, mean_squared_error: 0.000139, mean_q: 0.717665\n",
      " 1317/2000: episode: 18, duration: 3.153s, episode steps: 73, steps per second: 23, episode reward: 0.419, mean reward: 0.006 [-0.000, 0.014], mean action: 0.274 [-1.100, 1.144], mean observation: 0.135 [-10.073, 18.213], loss: 0.000113, mean_squared_error: 0.000226, mean_q: 0.738660\n",
      " 1390/2000: episode: 19, duration: 3.339s, episode steps: 73, steps per second: 22, episode reward: 0.395, mean reward: 0.005 [-0.001, 0.014], mean action: 0.143 [-1.100, 1.120], mean observation: 0.136 [-10.069, 18.270], loss: 0.000074, mean_squared_error: 0.000149, mean_q: 0.740017\n",
      " 1464/2000: episode: 20, duration: 3.331s, episode steps: 74, steps per second: 22, episode reward: 0.407, mean reward: 0.005 [-0.001, 0.013], mean action: 0.154 [-1.123, 1.124], mean observation: 0.138 [-9.956, 18.117], loss: 0.000091, mean_squared_error: 0.000182, mean_q: 0.728106\n",
      " 1536/2000: episode: 21, duration: 3.358s, episode steps: 72, steps per second: 21, episode reward: 0.389, mean reward: 0.005 [-0.001, 0.014], mean action: 0.176 [-1.191, 1.163], mean observation: 0.136 [-9.886, 18.337], loss: 0.000123, mean_squared_error: 0.000245, mean_q: 0.735819\n",
      " 1609/2000: episode: 22, duration: 3.069s, episode steps: 73, steps per second: 24, episode reward: 0.399, mean reward: 0.005 [-0.001, 0.014], mean action: 0.231 [-1.185, 1.198], mean observation: 0.138 [-10.035, 18.102], loss: 0.000255, mean_squared_error: 0.000510, mean_q: 0.729538\n",
      " 1680/2000: episode: 23, duration: 3.158s, episode steps: 71, steps per second: 22, episode reward: 0.394, mean reward: 0.006 [-0.002, 0.014], mean action: 0.241 [-1.091, 1.170], mean observation: 0.137 [-9.937, 18.099], loss: 0.000110, mean_squared_error: 0.000220, mean_q: 0.727287\n",
      " 1754/2000: episode: 24, duration: 3.172s, episode steps: 74, steps per second: 23, episode reward: 0.411, mean reward: 0.006 [-0.001, 0.013], mean action: 0.217 [-1.157, 1.199], mean observation: 0.138 [-10.000, 18.241], loss: 0.000137, mean_squared_error: 0.000275, mean_q: 0.741985\n",
      " 1830/2000: episode: 25, duration: 3.369s, episode steps: 76, steps per second: 23, episode reward: 0.423, mean reward: 0.006 [-0.001, 0.013], mean action: 0.194 [-1.148, 1.114], mean observation: 0.138 [-10.029, 17.968], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.730936\n",
      " 1905/2000: episode: 26, duration: 3.286s, episode steps: 75, steps per second: 23, episode reward: 0.412, mean reward: 0.005 [-0.001, 0.013], mean action: 0.230 [-1.126, 1.172], mean observation: 0.140 [-10.094, 18.044], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.719717\n",
      " 1978/2000: episode: 27, duration: 3.294s, episode steps: 73, steps per second: 22, episode reward: 0.397, mean reward: 0.005 [-0.001, 0.013], mean action: 0.222 [-1.067, 1.208], mean observation: 0.139 [-10.018, 18.111], loss: 0.000116, mean_squared_error: 0.000233, mean_q: 0.727124\n",
      "done, took 76.154 seconds\n",
      "\n",
      "\n",
      "iteration: 16\n",
      "Training for 2000 steps ...\n",
      "   72/2000: episode: 1, duration: 2.472s, episode steps: 72, steps per second: 29, episode reward: 0.400, mean reward: 0.006 [-0.002, 0.014], mean action: 0.217 [-1.100, 1.098], mean observation: 0.138 [-10.026, 18.119], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  145/2000: episode: 2, duration: 2.518s, episode steps: 73, steps per second: 29, episode reward: 0.406, mean reward: 0.006 [-0.002, 0.014], mean action: 0.240 [-1.125, 1.163], mean observation: 0.137 [-10.011, 18.040], loss: --, mean_squared_error: --, mean_q: --\n",
      "  216/2000: episode: 3, duration: 2.545s, episode steps: 71, steps per second: 28, episode reward: 0.392, mean reward: 0.006 [-0.002, 0.014], mean action: 0.221 [-1.147, 1.103], mean observation: 0.138 [-10.057, 18.275], loss: --, mean_squared_error: --, mean_q: --\n",
      "  287/2000: episode: 4, duration: 2.427s, episode steps: 71, steps per second: 29, episode reward: 0.393, mean reward: 0.006 [-0.002, 0.014], mean action: 0.229 [-1.175, 1.179], mean observation: 0.138 [-10.093, 18.138], loss: --, mean_squared_error: --, mean_q: --\n",
      "  359/2000: episode: 5, duration: 2.413s, episode steps: 72, steps per second: 30, episode reward: 0.400, mean reward: 0.006 [-0.002, 0.014], mean action: 0.226 [-1.145, 1.155], mean observation: 0.138 [-10.080, 18.193], loss: --, mean_squared_error: --, mean_q: --\n",
      "  431/2000: episode: 6, duration: 2.432s, episode steps: 72, steps per second: 30, episode reward: 0.400, mean reward: 0.006 [-0.002, 0.014], mean action: 0.214 [-1.177, 1.129], mean observation: 0.137 [-9.951, 18.290], loss: --, mean_squared_error: --, mean_q: --\n",
      "  504/2000: episode: 7, duration: 2.461s, episode steps: 73, steps per second: 30, episode reward: 0.406, mean reward: 0.006 [-0.002, 0.014], mean action: 0.212 [-1.127, 1.193], mean observation: 0.138 [-9.975, 18.131], loss: --, mean_squared_error: --, mean_q: --\n",
      "  575/2000: episode: 8, duration: 2.439s, episode steps: 71, steps per second: 29, episode reward: 0.394, mean reward: 0.006 [-0.002, 0.014], mean action: 0.203 [-1.127, 1.096], mean observation: 0.138 [-10.042, 18.008], loss: --, mean_squared_error: --, mean_q: --\n",
      "  647/2000: episode: 9, duration: 2.459s, episode steps: 72, steps per second: 29, episode reward: 0.399, mean reward: 0.006 [-0.002, 0.014], mean action: 0.237 [-1.090, 1.161], mean observation: 0.138 [-9.999, 18.155], loss: --, mean_squared_error: --, mean_q: --\n",
      "  718/2000: episode: 10, duration: 2.366s, episode steps: 71, steps per second: 30, episode reward: 0.392, mean reward: 0.006 [-0.002, 0.014], mean action: 0.226 [-1.155, 1.150], mean observation: 0.138 [-10.051, 18.183], loss: --, mean_squared_error: --, mean_q: --\n",
      "  790/2000: episode: 11, duration: 2.430s, episode steps: 72, steps per second: 30, episode reward: 0.400, mean reward: 0.006 [-0.002, 0.014], mean action: 0.223 [-1.181, 1.152], mean observation: 0.138 [-10.193, 17.949], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 12, duration: 2.446s, episode steps: 72, steps per second: 29, episode reward: 0.400, mean reward: 0.006 [-0.002, 0.014], mean action: 0.226 [-1.110, 1.146], mean observation: 0.138 [-9.947, 18.185], loss: --, mean_squared_error: --, mean_q: --\n",
      "  934/2000: episode: 13, duration: 2.432s, episode steps: 72, steps per second: 30, episode reward: 0.399, mean reward: 0.006 [-0.002, 0.014], mean action: 0.219 [-1.173, 1.147], mean observation: 0.138 [-10.065, 18.250], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1005/2000: episode: 14, duration: 2.463s, episode steps: 71, steps per second: 29, episode reward: 0.392, mean reward: 0.006 [-0.002, 0.014], mean action: 0.234 [-1.136, 1.150], mean observation: 0.138 [-10.053, 18.026], loss: 0.000084, mean_squared_error: 0.000168, mean_q: 0.724490\n",
      " 1077/2000: episode: 15, duration: 3.129s, episode steps: 72, steps per second: 23, episode reward: 0.403, mean reward: 0.006 [-0.002, 0.014], mean action: 0.218 [-1.144, 1.179], mean observation: 0.138 [-9.983, 18.212], loss: 0.000081, mean_squared_error: 0.000163, mean_q: 0.736138\n",
      " 1151/2000: episode: 16, duration: 3.274s, episode steps: 74, steps per second: 23, episode reward: 0.413, mean reward: 0.006 [-0.002, 0.013], mean action: 0.236 [-1.105, 1.145], mean observation: 0.137 [-10.080, 17.948], loss: 0.000072, mean_squared_error: 0.000143, mean_q: 0.737066\n",
      " 1223/2000: episode: 17, duration: 3.441s, episode steps: 72, steps per second: 21, episode reward: 0.401, mean reward: 0.006 [-0.002, 0.014], mean action: 0.233 [-1.106, 1.216], mean observation: 0.137 [-9.982, 18.047], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.738102\n",
      " 1352/2000: episode: 18, duration: 5.979s, episode steps: 129, steps per second: 22, episode reward: 0.540, mean reward: 0.004 [-0.003, 0.013], mean action: 0.265 [-1.171, 1.325], mean observation: 0.127 [-10.102, 18.238], loss: 0.000059, mean_squared_error: 0.000117, mean_q: 0.732546\n",
      " 1448/2000: episode: 19, duration: 4.722s, episode steps: 96, steps per second: 20, episode reward: 0.511, mean reward: 0.005 [-0.002, 0.014], mean action: 0.288 [-1.168, 1.289], mean observation: 0.118 [-10.034, 18.591], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.730195\n",
      " 1542/2000: episode: 20, duration: 4.433s, episode steps: 94, steps per second: 21, episode reward: 0.522, mean reward: 0.006 [-0.002, 0.013], mean action: 0.307 [-1.130, 1.216], mean observation: 0.120 [-10.037, 18.070], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.733142\n",
      " 1641/2000: episode: 21, duration: 4.564s, episode steps: 99, steps per second: 22, episode reward: 0.570, mean reward: 0.006 [-0.001, 0.013], mean action: 0.311 [-1.164, 1.280], mean observation: 0.115 [-9.963, 18.336], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.734255\n",
      " 1738/2000: episode: 22, duration: 5.375s, episode steps: 97, steps per second: 18, episode reward: 0.535, mean reward: 0.006 [-0.001, 0.013], mean action: 0.244 [-1.125, 1.145], mean observation: 0.110 [-21.835, 18.522], loss: 0.000079, mean_squared_error: 0.000157, mean_q: 0.731310\n",
      " 1834/2000: episode: 23, duration: 4.749s, episode steps: 96, steps per second: 20, episode reward: 0.523, mean reward: 0.005 [-0.002, 0.013], mean action: 0.185 [-1.237, 1.207], mean observation: 0.097 [-17.698, 18.128], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.721326\n",
      " 1933/2000: episode: 24, duration: 4.684s, episode steps: 99, steps per second: 21, episode reward: 0.557, mean reward: 0.006 [-0.001, 0.013], mean action: 0.225 [-1.119, 1.143], mean observation: 0.097 [-15.825, 18.212], loss: 0.000155, mean_squared_error: 0.000311, mean_q: 0.723790\n",
      "done, took 81.769 seconds\n",
      "\n",
      "\n",
      "iteration: 17\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 3.492s, episode steps: 105, steps per second: 30, episode reward: 0.585, mean reward: 0.006 [-0.001, 0.013], mean action: 0.257 [-1.171, 1.192], mean observation: 0.108 [-9.925, 18.356], loss: --, mean_squared_error: --, mean_q: --\n",
      "  211/2000: episode: 2, duration: 3.573s, episode steps: 106, steps per second: 30, episode reward: 0.601, mean reward: 0.006 [-0.001, 0.013], mean action: 0.280 [-1.084, 1.116], mean observation: 0.105 [-10.134, 18.263], loss: --, mean_squared_error: --, mean_q: --\n",
      "  317/2000: episode: 3, duration: 3.580s, episode steps: 106, steps per second: 30, episode reward: 0.597, mean reward: 0.006 [-0.001, 0.013], mean action: 0.298 [-1.059, 1.150], mean observation: 0.105 [-10.078, 18.167], loss: --, mean_squared_error: --, mean_q: --\n",
      "  423/2000: episode: 4, duration: 3.523s, episode steps: 106, steps per second: 30, episode reward: 0.593, mean reward: 0.006 [-0.001, 0.013], mean action: 0.286 [-1.111, 1.181], mean observation: 0.106 [-9.974, 18.313], loss: --, mean_squared_error: --, mean_q: --\n",
      "  530/2000: episode: 5, duration: 3.596s, episode steps: 107, steps per second: 30, episode reward: 0.598, mean reward: 0.006 [-0.001, 0.013], mean action: 0.266 [-1.177, 1.213], mean observation: 0.107 [-9.942, 18.526], loss: --, mean_squared_error: --, mean_q: --\n",
      "  638/2000: episode: 6, duration: 3.636s, episode steps: 108, steps per second: 30, episode reward: 0.601, mean reward: 0.006 [-0.001, 0.013], mean action: 0.249 [-1.247, 1.102], mean observation: 0.107 [-9.970, 18.436], loss: --, mean_squared_error: --, mean_q: --\n",
      "  744/2000: episode: 7, duration: 3.509s, episode steps: 106, steps per second: 30, episode reward: 0.594, mean reward: 0.006 [-0.001, 0.013], mean action: 0.282 [-1.144, 1.262], mean observation: 0.106 [-9.918, 18.486], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  852/2000: episode: 8, duration: 3.705s, episode steps: 108, steps per second: 29, episode reward: 0.600, mean reward: 0.006 [-0.001, 0.013], mean action: 0.236 [-1.177, 1.120], mean observation: 0.106 [-9.992, 18.257], loss: --, mean_squared_error: --, mean_q: --\n",
      "  959/2000: episode: 9, duration: 3.652s, episode steps: 107, steps per second: 29, episode reward: 0.597, mean reward: 0.006 [-0.001, 0.014], mean action: 0.306 [-1.139, 1.208], mean observation: 0.106 [-10.001, 18.271], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1065/2000: episode: 10, duration: 4.197s, episode steps: 106, steps per second: 25, episode reward: 0.595, mean reward: 0.006 [-0.001, 0.014], mean action: 0.282 [-1.120, 1.159], mean observation: 0.106 [-10.043, 18.230], loss: 0.000051, mean_squared_error: 0.000101, mean_q: 0.723865\n",
      " 1167/2000: episode: 11, duration: 4.407s, episode steps: 102, steps per second: 23, episode reward: 0.564, mean reward: 0.006 [-0.001, 0.012], mean action: 0.279 [-1.131, 1.203], mean observation: 0.109 [-10.023, 18.293], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.717240\n",
      " 1259/2000: episode: 12, duration: 4.296s, episode steps: 92, steps per second: 21, episode reward: 0.561, mean reward: 0.006 [-0.001, 0.014], mean action: 0.235 [-1.079, 1.171], mean observation: 0.107 [-10.164, 18.283], loss: 0.000109, mean_squared_error: 0.000218, mean_q: 0.715050\n",
      " 1363/2000: episode: 13, duration: 4.659s, episode steps: 104, steps per second: 22, episode reward: 0.647, mean reward: 0.006 [-0.000, 0.015], mean action: 0.208 [-1.143, 1.206], mean observation: 0.120 [-9.371, 18.794], loss: 0.000088, mean_squared_error: 0.000176, mean_q: 0.716939\n",
      " 1465/2000: episode: 14, duration: 4.483s, episode steps: 102, steps per second: 23, episode reward: 0.653, mean reward: 0.006 [-0.001, 0.014], mean action: 0.243 [-1.148, 1.183], mean observation: 0.110 [-10.168, 20.412], loss: 0.000141, mean_squared_error: 0.000281, mean_q: 0.727915\n",
      " 1565/2000: episode: 15, duration: 4.783s, episode steps: 100, steps per second: 21, episode reward: 0.622, mean reward: 0.006 [-0.001, 0.014], mean action: 0.244 [-1.106, 1.213], mean observation: 0.105 [-43.169, 21.756], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.728090\n",
      " 1667/2000: episode: 16, duration: 4.846s, episode steps: 102, steps per second: 21, episode reward: 0.679, mean reward: 0.007 [-0.001, 0.015], mean action: 0.206 [-1.272, 1.186], mean observation: 0.100 [-43.651, 22.544], loss: 0.000110, mean_squared_error: 0.000220, mean_q: 0.716925\n",
      " 1768/2000: episode: 17, duration: 4.897s, episode steps: 101, steps per second: 21, episode reward: 0.688, mean reward: 0.007 [-0.001, 0.015], mean action: 0.173 [-1.164, 1.262], mean observation: 0.103 [-28.520, 20.949], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.706520\n",
      " 1862/2000: episode: 18, duration: 4.166s, episode steps: 94, steps per second: 23, episode reward: 0.543, mean reward: 0.006 [0.000, 0.013], mean action: 0.068 [-1.099, 1.228], mean observation: 0.131 [-23.020, 20.689], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.716679\n",
      " 1956/2000: episode: 19, duration: 3.920s, episode steps: 94, steps per second: 24, episode reward: 0.530, mean reward: 0.006 [0.001, 0.012], mean action: 0.156 [-1.245, 1.125], mean observation: 0.134 [-8.960, 20.244], loss: 0.000110, mean_squared_error: 0.000221, mean_q: 0.709754\n",
      "done, took 78.837 seconds\n",
      "\n",
      "\n",
      "iteration: 18\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 3.006s, episode steps: 95, steps per second: 32, episode reward: 0.524, mean reward: 0.006 [-0.001, 0.012], mean action: 0.178 [-1.194, 1.099], mean observation: 0.125 [-10.142, 20.236], loss: --, mean_squared_error: --, mean_q: --\n",
      "  192/2000: episode: 2, duration: 3.222s, episode steps: 97, steps per second: 30, episode reward: 0.539, mean reward: 0.006 [-0.001, 0.013], mean action: 0.212 [-1.094, 1.204], mean observation: 0.123 [-10.176, 19.787], loss: --, mean_squared_error: --, mean_q: --\n",
      "  287/2000: episode: 3, duration: 3.093s, episode steps: 95, steps per second: 31, episode reward: 0.523, mean reward: 0.006 [-0.001, 0.012], mean action: 0.201 [-1.066, 1.221], mean observation: 0.124 [-10.087, 19.536], loss: --, mean_squared_error: --, mean_q: --\n",
      "  383/2000: episode: 4, duration: 3.106s, episode steps: 96, steps per second: 31, episode reward: 0.535, mean reward: 0.006 [-0.000, 0.013], mean action: 0.199 [-1.157, 1.150], mean observation: 0.123 [-9.978, 19.883], loss: --, mean_squared_error: --, mean_q: --\n",
      "  478/2000: episode: 5, duration: 3.006s, episode steps: 95, steps per second: 32, episode reward: 0.526, mean reward: 0.006 [-0.001, 0.012], mean action: 0.195 [-1.155, 1.130], mean observation: 0.125 [-10.105, 20.417], loss: --, mean_squared_error: --, mean_q: --\n",
      "  573/2000: episode: 6, duration: 3.007s, episode steps: 95, steps per second: 32, episode reward: 0.524, mean reward: 0.006 [-0.001, 0.012], mean action: 0.187 [-1.116, 1.147], mean observation: 0.125 [-10.136, 20.309], loss: --, mean_squared_error: --, mean_q: --\n",
      "  667/2000: episode: 7, duration: 3.037s, episode steps: 94, steps per second: 31, episode reward: 0.518, mean reward: 0.006 [-0.001, 0.012], mean action: 0.226 [-1.103, 1.243], mean observation: 0.126 [-9.928, 20.214], loss: --, mean_squared_error: --, mean_q: --\n",
      "  763/2000: episode: 8, duration: 3.098s, episode steps: 96, steps per second: 31, episode reward: 0.531, mean reward: 0.006 [-0.001, 0.012], mean action: 0.196 [-1.071, 1.135], mean observation: 0.125 [-10.141, 19.393], loss: --, mean_squared_error: --, mean_q: --\n",
      "  859/2000: episode: 9, duration: 3.048s, episode steps: 96, steps per second: 32, episode reward: 0.529, mean reward: 0.006 [-0.000, 0.012], mean action: 0.204 [-1.119, 1.190], mean observation: 0.125 [-10.086, 19.564], loss: --, mean_squared_error: --, mean_q: --\n",
      "  956/2000: episode: 10, duration: 3.246s, episode steps: 97, steps per second: 30, episode reward: 0.536, mean reward: 0.006 [-0.001, 0.012], mean action: 0.217 [-1.141, 1.184], mean observation: 0.123 [-10.213, 19.496], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1051/2000: episode: 11, duration: 3.501s, episode steps: 95, steps per second: 27, episode reward: 0.520, mean reward: 0.005 [-0.001, 0.012], mean action: 0.199 [-1.132, 1.192], mean observation: 0.127 [-10.016, 19.587], loss: 0.000147, mean_squared_error: 0.000293, mean_q: 0.714058\n",
      " 1147/2000: episode: 12, duration: 3.938s, episode steps: 96, steps per second: 24, episode reward: 0.535, mean reward: 0.006 [-0.001, 0.012], mean action: 0.168 [-1.256, 1.132], mean observation: 0.125 [-10.000, 17.926], loss: 0.000124, mean_squared_error: 0.000249, mean_q: 0.725251\n",
      " 1240/2000: episode: 13, duration: 3.628s, episode steps: 93, steps per second: 26, episode reward: 0.519, mean reward: 0.006 [-0.001, 0.012], mean action: 0.183 [-1.127, 1.093], mean observation: 0.127 [-10.016, 21.584], loss: 0.000170, mean_squared_error: 0.000339, mean_q: 0.724913\n",
      " 1338/2000: episode: 14, duration: 4.064s, episode steps: 98, steps per second: 24, episode reward: 0.559, mean reward: 0.006 [-0.000, 0.014], mean action: 0.184 [-1.198, 1.188], mean observation: 0.121 [-10.168, 21.504], loss: 0.000115, mean_squared_error: 0.000230, mean_q: 0.713080\n",
      " 1437/2000: episode: 15, duration: 4.233s, episode steps: 99, steps per second: 23, episode reward: 0.524, mean reward: 0.005 [-0.001, 0.013], mean action: 0.161 [-1.083, 1.155], mean observation: 0.118 [-10.443, 21.621], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.715462\n",
      " 1531/2000: episode: 16, duration: 3.819s, episode steps: 94, steps per second: 25, episode reward: 0.521, mean reward: 0.006 [-0.001, 0.012], mean action: 0.158 [-1.159, 1.203], mean observation: 0.121 [-8.986, 20.758], loss: 0.000081, mean_squared_error: 0.000161, mean_q: 0.714483\n",
      " 1636/2000: episode: 17, duration: 4.233s, episode steps: 105, steps per second: 25, episode reward: 0.514, mean reward: 0.005 [-0.001, 0.012], mean action: 0.129 [-1.148, 1.193], mean observation: 0.119 [-9.256, 14.500], loss: 0.000077, mean_squared_error: 0.000153, mean_q: 0.713651\n",
      " 1731/2000: episode: 18, duration: 4.043s, episode steps: 95, steps per second: 23, episode reward: 0.533, mean reward: 0.006 [-0.001, 0.014], mean action: 0.191 [-1.106, 1.166], mean observation: 0.121 [-9.345, 20.345], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.715005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1825/2000: episode: 19, duration: 3.792s, episode steps: 94, steps per second: 25, episode reward: 0.526, mean reward: 0.006 [-0.001, 0.012], mean action: 0.132 [-1.135, 1.175], mean observation: 0.131 [-9.028, 21.204], loss: 0.000087, mean_squared_error: 0.000174, mean_q: 0.700485\n",
      " 1914/2000: episode: 20, duration: 3.487s, episode steps: 89, steps per second: 26, episode reward: 0.500, mean reward: 0.006 [-0.001, 0.012], mean action: 0.102 [-1.154, 1.171], mean observation: 0.141 [-9.099, 20.042], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.709838\n",
      "done, took 73.056 seconds\n",
      "\n",
      "\n",
      "iteration: 19\n",
      "Training for 2000 steps ...\n",
      "   90/2000: episode: 1, duration: 2.955s, episode steps: 90, steps per second: 30, episode reward: 0.505, mean reward: 0.006 [-0.001, 0.013], mean action: 0.078 [-1.137, 1.167], mean observation: 0.125 [-38.574, 20.801], loss: --, mean_squared_error: --, mean_q: --\n",
      "  179/2000: episode: 2, duration: 2.960s, episode steps: 89, steps per second: 30, episode reward: 0.496, mean reward: 0.006 [-0.001, 0.012], mean action: 0.083 [-1.142, 1.210], mean observation: 0.135 [-15.031, 20.108], loss: --, mean_squared_error: --, mean_q: --\n",
      "  269/2000: episode: 3, duration: 2.882s, episode steps: 90, steps per second: 31, episode reward: 0.505, mean reward: 0.006 [-0.001, 0.014], mean action: 0.095 [-1.154, 1.160], mean observation: 0.128 [-34.667, 20.097], loss: --, mean_squared_error: --, mean_q: --\n",
      "  358/2000: episode: 4, duration: 2.980s, episode steps: 89, steps per second: 30, episode reward: 0.493, mean reward: 0.006 [-0.001, 0.012], mean action: 0.094 [-1.118, 1.283], mean observation: 0.132 [-27.879, 20.568], loss: --, mean_squared_error: --, mean_q: --\n",
      "  448/2000: episode: 5, duration: 2.962s, episode steps: 90, steps per second: 30, episode reward: 0.505, mean reward: 0.006 [-0.001, 0.014], mean action: 0.097 [-1.148, 1.184], mean observation: 0.124 [-42.578, 20.056], loss: --, mean_squared_error: --, mean_q: --\n",
      "  537/2000: episode: 6, duration: 2.850s, episode steps: 89, steps per second: 31, episode reward: 0.493, mean reward: 0.006 [-0.001, 0.012], mean action: 0.070 [-1.222, 1.163], mean observation: 0.131 [-32.365, 20.141], loss: --, mean_squared_error: --, mean_q: --\n",
      "  627/2000: episode: 7, duration: 2.945s, episode steps: 90, steps per second: 31, episode reward: 0.505, mean reward: 0.006 [-0.001, 0.012], mean action: 0.064 [-1.211, 1.145], mean observation: 0.127 [-38.900, 20.175], loss: --, mean_squared_error: --, mean_q: --\n",
      "  717/2000: episode: 8, duration: 2.993s, episode steps: 90, steps per second: 30, episode reward: 0.508, mean reward: 0.006 [-0.001, 0.013], mean action: 0.093 [-1.101, 1.262], mean observation: 0.128 [-36.211, 20.227], loss: --, mean_squared_error: --, mean_q: --\n",
      "  806/2000: episode: 9, duration: 2.995s, episode steps: 89, steps per second: 30, episode reward: 0.495, mean reward: 0.006 [-0.001, 0.013], mean action: 0.089 [-1.165, 1.196], mean observation: 0.131 [-35.611, 20.047], loss: --, mean_squared_error: --, mean_q: --\n",
      "  896/2000: episode: 10, duration: 2.971s, episode steps: 90, steps per second: 30, episode reward: 0.506, mean reward: 0.006 [-0.001, 0.013], mean action: 0.091 [-1.140, 1.180], mean observation: 0.129 [-32.005, 20.082], loss: --, mean_squared_error: --, mean_q: --\n",
      "  985/2000: episode: 11, duration: 3.045s, episode steps: 89, steps per second: 29, episode reward: 0.496, mean reward: 0.006 [-0.001, 0.013], mean action: 0.079 [-1.138, 1.223], mean observation: 0.133 [-20.678, 20.043], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1077/2000: episode: 12, duration: 3.775s, episode steps: 92, steps per second: 24, episode reward: 0.536, mean reward: 0.006 [-0.001, 0.016], mean action: 0.069 [-1.195, 1.166], mean observation: 0.128 [-9.031, 20.133], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.716729\n",
      " 1166/2000: episode: 13, duration: 3.586s, episode steps: 89, steps per second: 25, episode reward: 0.492, mean reward: 0.006 [-0.001, 0.013], mean action: 0.080 [-1.175, 1.130], mean observation: 0.130 [-33.000, 20.290], loss: 0.000111, mean_squared_error: 0.000223, mean_q: 0.719109\n",
      " 1255/2000: episode: 14, duration: 3.835s, episode steps: 89, steps per second: 23, episode reward: 0.508, mean reward: 0.006 [-0.001, 0.014], mean action: 0.108 [-1.152, 1.184], mean observation: 0.126 [-42.789, 20.213], loss: 0.000072, mean_squared_error: 0.000143, mean_q: 0.701552\n",
      " 1356/2000: episode: 15, duration: 4.414s, episode steps: 101, steps per second: 23, episode reward: 0.779, mean reward: 0.008 [-0.001, 0.021], mean action: 0.146 [-1.127, 1.152], mean observation: 0.102 [-9.610, 19.093], loss: 0.000088, mean_squared_error: 0.000176, mean_q: 0.704720\n",
      " 1436/2000: episode: 16, duration: 3.528s, episode steps: 80, steps per second: 23, episode reward: 0.471, mean reward: 0.006 [-0.001, 0.013], mean action: 0.163 [-1.165, 1.195], mean observation: 0.128 [-25.374, 19.097], loss: 0.000183, mean_squared_error: 0.000365, mean_q: 0.718180\n",
      " 1531/2000: episode: 17, duration: 3.969s, episode steps: 95, steps per second: 24, episode reward: 0.545, mean reward: 0.006 [-0.000, 0.014], mean action: 0.050 [-1.259, 1.120], mean observation: 0.134 [-8.314, 20.118], loss: 0.000137, mean_squared_error: 0.000273, mean_q: 0.711701\n",
      " 1622/2000: episode: 18, duration: 3.787s, episode steps: 91, steps per second: 24, episode reward: 0.507, mean reward: 0.006 [0.001, 0.012], mean action: 0.112 [-1.189, 1.215], mean observation: 0.138 [-19.018, 20.096], loss: 0.000072, mean_squared_error: 0.000145, mean_q: 0.716006\n",
      " 1713/2000: episode: 19, duration: 3.577s, episode steps: 91, steps per second: 25, episode reward: 0.503, mean reward: 0.006 [0.001, 0.011], mean action: 0.068 [-1.270, 1.146], mean observation: 0.145 [-8.425, 20.181], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.697492\n",
      " 1810/2000: episode: 20, duration: 4.195s, episode steps: 97, steps per second: 23, episode reward: 0.589, mean reward: 0.006 [0.001, 0.017], mean action: 0.138 [-1.172, 1.209], mean observation: 0.124 [-18.221, 20.036], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.701185\n",
      " 1902/2000: episode: 21, duration: 3.767s, episode steps: 92, steps per second: 24, episode reward: 0.508, mean reward: 0.006 [0.000, 0.012], mean action: 0.078 [-1.149, 1.151], mean observation: 0.140 [-12.779, 15.645], loss: 0.000098, mean_squared_error: 0.000197, mean_q: 0.704328\n",
      " 1991/2000: episode: 22, duration: 3.564s, episode steps: 89, steps per second: 25, episode reward: 0.499, mean reward: 0.006 [-0.000, 0.012], mean action: 0.108 [-1.094, 1.246], mean observation: 0.139 [-14.302, 19.095], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.716007\n",
      "done, took 75.275 seconds\n",
      "\n",
      "\n",
      "iteration: 20\n",
      "Training for 2000 steps ...\n",
      "   81/2000: episode: 1, duration: 2.812s, episode steps: 81, steps per second: 29, episode reward: 0.477, mean reward: 0.006 [-0.001, 0.013], mean action: 0.095 [-1.126, 1.169], mean observation: 0.130 [-9.871, 18.862], loss: --, mean_squared_error: --, mean_q: --\n",
      "  161/2000: episode: 2, duration: 2.754s, episode steps: 80, steps per second: 29, episode reward: 0.466, mean reward: 0.006 [-0.001, 0.012], mean action: 0.089 [-1.148, 1.208], mean observation: 0.125 [-31.182, 18.937], loss: --, mean_squared_error: --, mean_q: --\n",
      "  243/2000: episode: 3, duration: 2.778s, episode steps: 82, steps per second: 30, episode reward: 0.489, mean reward: 0.006 [-0.001, 0.013], mean action: 0.084 [-1.156, 1.085], mean observation: 0.129 [-9.740, 19.069], loss: --, mean_squared_error: --, mean_q: --\n",
      "  323/2000: episode: 4, duration: 2.750s, episode steps: 80, steps per second: 29, episode reward: 0.470, mean reward: 0.006 [-0.001, 0.013], mean action: 0.084 [-1.182, 1.183], mean observation: 0.129 [-9.660, 19.078], loss: --, mean_squared_error: --, mean_q: --\n",
      "  402/2000: episode: 5, duration: 2.680s, episode steps: 79, steps per second: 29, episode reward: 0.463, mean reward: 0.006 [-0.001, 0.012], mean action: 0.088 [-1.188, 1.136], mean observation: 0.126 [-22.146, 18.999], loss: --, mean_squared_error: --, mean_q: --\n",
      "  482/2000: episode: 6, duration: 2.719s, episode steps: 80, steps per second: 29, episode reward: 0.465, mean reward: 0.006 [-0.001, 0.012], mean action: 0.076 [-1.177, 1.131], mean observation: 0.132 [-13.614, 18.919], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  562/2000: episode: 7, duration: 2.630s, episode steps: 80, steps per second: 30, episode reward: 0.462, mean reward: 0.006 [-0.001, 0.012], mean action: 0.073 [-1.209, 1.127], mean observation: 0.133 [-9.593, 19.121], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 8, duration: 2.704s, episode steps: 80, steps per second: 30, episode reward: 0.468, mean reward: 0.006 [-0.001, 0.012], mean action: 0.081 [-1.228, 1.130], mean observation: 0.130 [-9.739, 18.945], loss: --, mean_squared_error: --, mean_q: --\n",
      "  722/2000: episode: 9, duration: 2.717s, episode steps: 80, steps per second: 29, episode reward: 0.465, mean reward: 0.006 [-0.001, 0.012], mean action: 0.106 [-1.075, 1.109], mean observation: 0.132 [-9.781, 18.857], loss: --, mean_squared_error: --, mean_q: --\n",
      "  802/2000: episode: 10, duration: 2.664s, episode steps: 80, steps per second: 30, episode reward: 0.464, mean reward: 0.006 [-0.001, 0.012], mean action: 0.095 [-1.155, 1.200], mean observation: 0.123 [-38.428, 18.818], loss: --, mean_squared_error: --, mean_q: --\n",
      "  883/2000: episode: 11, duration: 2.802s, episode steps: 81, steps per second: 29, episode reward: 0.474, mean reward: 0.006 [-0.001, 0.012], mean action: 0.058 [-1.161, 1.088], mean observation: 0.125 [-29.801, 18.939], loss: --, mean_squared_error: --, mean_q: --\n",
      "  964/2000: episode: 12, duration: 2.769s, episode steps: 81, steps per second: 29, episode reward: 0.477, mean reward: 0.006 [-0.001, 0.013], mean action: 0.096 [-1.102, 1.158], mean observation: 0.130 [-9.607, 18.911], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1045/2000: episode: 13, duration: 3.118s, episode steps: 81, steps per second: 26, episode reward: 0.468, mean reward: 0.006 [-0.001, 0.011], mean action: 0.078 [-1.126, 1.224], mean observation: 0.136 [-9.772, 19.095], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.713642\n",
      " 1122/2000: episode: 14, duration: 3.404s, episode steps: 77, steps per second: 23, episode reward: 0.445, mean reward: 0.006 [-0.001, 0.011], mean action: 0.102 [-1.151, 1.167], mean observation: 0.126 [-26.310, 19.190], loss: 0.000072, mean_squared_error: 0.000145, mean_q: 0.709832\n",
      " 1209/2000: episode: 15, duration: 3.622s, episode steps: 87, steps per second: 24, episode reward: 0.499, mean reward: 0.006 [-0.000, 0.012], mean action: 0.057 [-1.122, 1.125], mean observation: 0.135 [-10.826, 19.203], loss: 0.000198, mean_squared_error: 0.000396, mean_q: 0.704551\n",
      " 1293/2000: episode: 16, duration: 4.102s, episode steps: 84, steps per second: 20, episode reward: 0.542, mean reward: 0.006 [-0.000, 0.016], mean action: 0.130 [-1.158, 1.160], mean observation: 0.106 [-9.745, 16.471], loss: 0.000130, mean_squared_error: 0.000259, mean_q: 0.713930\n",
      " 1395/2000: episode: 17, duration: 4.954s, episode steps: 102, steps per second: 21, episode reward: 0.673, mean reward: 0.007 [-0.001, 0.018], mean action: 0.123 [-1.176, 1.113], mean observation: 0.102 [-9.784, 12.848], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.705682\n",
      " 1492/2000: episode: 18, duration: 4.439s, episode steps: 97, steps per second: 22, episode reward: 0.587, mean reward: 0.006 [-0.000, 0.016], mean action: 0.121 [-1.178, 1.187], mean observation: 0.115 [-9.780, 13.639], loss: 0.000185, mean_squared_error: 0.000370, mean_q: 0.709764\n",
      " 1602/2000: episode: 19, duration: 5.084s, episode steps: 110, steps per second: 22, episode reward: 0.821, mean reward: 0.007 [-0.001, 0.020], mean action: 0.121 [-1.203, 1.182], mean observation: 0.098 [-9.594, 12.683], loss: 0.000182, mean_squared_error: 0.000364, mean_q: 0.701267\n",
      " 1694/2000: episode: 20, duration: 3.839s, episode steps: 92, steps per second: 24, episode reward: 0.511, mean reward: 0.006 [0.000, 0.012], mean action: -0.015 [-1.171, 1.268], mean observation: 0.138 [-9.012, 21.282], loss: 0.000116, mean_squared_error: 0.000231, mean_q: 0.696746\n",
      " 1798/2000: episode: 21, duration: 4.611s, episode steps: 104, steps per second: 23, episode reward: 0.743, mean reward: 0.007 [-0.001, 0.020], mean action: 0.123 [-1.094, 1.206], mean observation: 0.104 [-11.441, 20.021], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.698025\n",
      " 1908/2000: episode: 22, duration: 5.267s, episode steps: 110, steps per second: 21, episode reward: 0.799, mean reward: 0.007 [-0.001, 0.020], mean action: 0.184 [-1.125, 1.316], mean observation: 0.099 [-9.054, 13.462], loss: 0.000074, mean_squared_error: 0.000149, mean_q: 0.691812\n",
      "done, took 79.691 seconds\n",
      "\n",
      "\n",
      "iteration: 21\n",
      "Training for 2000 steps ...\n",
      "  106/2000: episode: 1, duration: 4.045s, episode steps: 106, steps per second: 26, episode reward: 0.772, mean reward: 0.007 [-0.001, 0.020], mean action: 0.161 [-1.163, 1.179], mean observation: 0.097 [-10.254, 14.457], loss: --, mean_squared_error: --, mean_q: --\n",
      "  212/2000: episode: 2, duration: 4.131s, episode steps: 106, steps per second: 26, episode reward: 0.760, mean reward: 0.007 [-0.001, 0.019], mean action: 0.216 [-1.100, 1.232], mean observation: 0.097 [-9.084, 14.719], loss: --, mean_squared_error: --, mean_q: --\n",
      "  318/2000: episode: 3, duration: 3.981s, episode steps: 106, steps per second: 27, episode reward: 0.766, mean reward: 0.007 [-0.001, 0.020], mean action: 0.166 [-1.174, 1.128], mean observation: 0.097 [-9.288, 14.787], loss: --, mean_squared_error: --, mean_q: --\n",
      "  424/2000: episode: 4, duration: 3.959s, episode steps: 106, steps per second: 27, episode reward: 0.772, mean reward: 0.007 [-0.001, 0.020], mean action: 0.218 [-1.114, 1.332], mean observation: 0.096 [-10.857, 14.549], loss: --, mean_squared_error: --, mean_q: --\n",
      "  530/2000: episode: 5, duration: 4.047s, episode steps: 106, steps per second: 26, episode reward: 0.763, mean reward: 0.007 [-0.001, 0.020], mean action: 0.194 [-1.138, 1.236], mean observation: 0.098 [-8.982, 14.709], loss: --, mean_squared_error: --, mean_q: --\n",
      "  632/2000: episode: 6, duration: 3.677s, episode steps: 102, steps per second: 28, episode reward: 0.691, mean reward: 0.007 [-0.001, 0.019], mean action: 0.164 [-1.213, 1.160], mean observation: 0.099 [-11.643, 14.793], loss: --, mean_squared_error: --, mean_q: --\n",
      "  739/2000: episode: 7, duration: 4.118s, episode steps: 107, steps per second: 26, episode reward: 0.774, mean reward: 0.007 [-0.001, 0.020], mean action: 0.197 [-1.128, 1.141], mean observation: 0.097 [-8.762, 13.762], loss: --, mean_squared_error: --, mean_q: --\n",
      "  846/2000: episode: 8, duration: 4.060s, episode steps: 107, steps per second: 26, episode reward: 0.784, mean reward: 0.007 [-0.001, 0.020], mean action: 0.191 [-1.106, 1.177], mean observation: 0.096 [-9.018, 14.517], loss: --, mean_squared_error: --, mean_q: --\n",
      "  950/2000: episode: 9, duration: 3.848s, episode steps: 104, steps per second: 27, episode reward: 0.735, mean reward: 0.007 [-0.001, 0.020], mean action: 0.181 [-1.166, 1.164], mean observation: 0.098 [-9.358, 13.742], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1055/2000: episode: 10, duration: 4.461s, episode steps: 105, steps per second: 24, episode reward: 0.737, mean reward: 0.007 [-0.001, 0.020], mean action: 0.210 [-1.148, 1.256], mean observation: 0.098 [-9.403, 14.295], loss: 0.000045, mean_squared_error: 0.000089, mean_q: 0.705878\n",
      " 1159/2000: episode: 11, duration: 4.870s, episode steps: 104, steps per second: 21, episode reward: 0.774, mean reward: 0.007 [-0.000, 0.019], mean action: 0.237 [-1.127, 1.158], mean observation: 0.097 [-9.693, 18.762], loss: 0.000057, mean_squared_error: 0.000113, mean_q: 0.702744\n",
      " 1266/2000: episode: 12, duration: 5.018s, episode steps: 107, steps per second: 21, episode reward: 0.803, mean reward: 0.008 [-0.000, 0.019], mean action: 0.252 [-1.140, 1.265], mean observation: 0.097 [-9.266, 12.114], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.699958\n",
      " 1375/2000: episode: 13, duration: 5.215s, episode steps: 109, steps per second: 21, episode reward: 0.756, mean reward: 0.007 [-0.000, 0.019], mean action: 0.211 [-1.170, 1.145], mean observation: 0.100 [-9.087, 12.251], loss: 0.000088, mean_squared_error: 0.000177, mean_q: 0.688595\n",
      " 1477/2000: episode: 14, duration: 4.790s, episode steps: 102, steps per second: 21, episode reward: 0.784, mean reward: 0.008 [0.000, 0.020], mean action: 0.202 [-1.136, 1.222], mean observation: 0.095 [-34.598, 20.889], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.696631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1562/2000: episode: 15, duration: 3.662s, episode steps: 85, steps per second: 23, episode reward: 0.481, mean reward: 0.006 [-0.000, 0.014], mean action: 0.161 [-1.126, 1.231], mean observation: 0.121 [-38.707, 20.725], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.693118\n",
      " 1666/2000: episode: 16, duration: 4.761s, episode steps: 104, steps per second: 22, episode reward: 0.833, mean reward: 0.008 [-0.000, 0.020], mean action: 0.231 [-1.113, 1.177], mean observation: 0.101 [-11.256, 22.055], loss: 0.000153, mean_squared_error: 0.000305, mean_q: 0.698670\n",
      " 1753/2000: episode: 17, duration: 3.907s, episode steps: 87, steps per second: 22, episode reward: 0.479, mean reward: 0.006 [-0.000, 0.013], mean action: 0.193 [-1.217, 1.162], mean observation: 0.120 [-44.438, 14.063], loss: 0.000175, mean_squared_error: 0.000351, mean_q: 0.692226\n",
      " 1840/2000: episode: 18, duration: 3.671s, episode steps: 87, steps per second: 24, episode reward: 0.488, mean reward: 0.006 [0.000, 0.013], mean action: 0.158 [-1.136, 1.107], mean observation: 0.133 [-31.551, 20.974], loss: 0.000074, mean_squared_error: 0.000147, mean_q: 0.683925\n",
      " 1927/2000: episode: 19, duration: 3.527s, episode steps: 87, steps per second: 25, episode reward: 0.489, mean reward: 0.006 [0.000, 0.012], mean action: 0.197 [-1.123, 1.186], mean observation: 0.128 [-41.760, 20.759], loss: 0.000206, mean_squared_error: 0.000413, mean_q: 0.686424\n",
      "done, took 82.611 seconds\n",
      "\n",
      "\n",
      "iteration: 22\n",
      "Training for 2000 steps ...\n",
      "   88/2000: episode: 1, duration: 2.705s, episode steps: 88, steps per second: 33, episode reward: 0.496, mean reward: 0.006 [-0.000, 0.013], mean action: 0.222 [-1.205, 1.229], mean observation: 0.128 [-40.507, 17.945], loss: --, mean_squared_error: --, mean_q: --\n",
      "  176/2000: episode: 2, duration: 2.860s, episode steps: 88, steps per second: 31, episode reward: 0.498, mean reward: 0.006 [-0.000, 0.013], mean action: 0.215 [-1.140, 1.093], mean observation: 0.125 [-41.267, 18.016], loss: --, mean_squared_error: --, mean_q: --\n",
      "  264/2000: episode: 3, duration: 2.813s, episode steps: 88, steps per second: 31, episode reward: 0.498, mean reward: 0.006 [-0.000, 0.013], mean action: 0.202 [-1.098, 1.086], mean observation: 0.132 [-29.471, 17.731], loss: --, mean_squared_error: --, mean_q: --\n",
      "  352/2000: episode: 4, duration: 2.862s, episode steps: 88, steps per second: 31, episode reward: 0.498, mean reward: 0.006 [-0.000, 0.014], mean action: 0.229 [-1.085, 1.137], mean observation: 0.132 [-30.473, 17.777], loss: --, mean_squared_error: --, mean_q: --\n",
      "  440/2000: episode: 5, duration: 2.769s, episode steps: 88, steps per second: 32, episode reward: 0.493, mean reward: 0.006 [-0.000, 0.013], mean action: 0.231 [-1.081, 1.147], mean observation: 0.131 [-37.527, 18.264], loss: --, mean_squared_error: --, mean_q: --\n",
      "  529/2000: episode: 6, duration: 2.810s, episode steps: 89, steps per second: 32, episode reward: 0.508, mean reward: 0.006 [-0.000, 0.014], mean action: 0.210 [-1.152, 1.134], mean observation: 0.128 [-36.851, 18.054], loss: --, mean_squared_error: --, mean_q: --\n",
      "  617/2000: episode: 7, duration: 2.796s, episode steps: 88, steps per second: 31, episode reward: 0.496, mean reward: 0.006 [-0.000, 0.013], mean action: 0.213 [-1.125, 1.132], mean observation: 0.132 [-28.285, 18.118], loss: --, mean_squared_error: --, mean_q: --\n",
      "  705/2000: episode: 8, duration: 2.921s, episode steps: 88, steps per second: 30, episode reward: 0.494, mean reward: 0.006 [-0.000, 0.013], mean action: 0.249 [-1.116, 1.239], mean observation: 0.130 [-24.906, 18.262], loss: --, mean_squared_error: --, mean_q: --\n",
      "  794/2000: episode: 9, duration: 2.933s, episode steps: 89, steps per second: 30, episode reward: 0.507, mean reward: 0.006 [-0.000, 0.014], mean action: 0.232 [-1.115, 1.161], mean observation: 0.123 [-45.493, 18.686], loss: --, mean_squared_error: --, mean_q: --\n",
      "  882/2000: episode: 10, duration: 2.773s, episode steps: 88, steps per second: 32, episode reward: 0.499, mean reward: 0.006 [-0.000, 0.013], mean action: 0.218 [-1.153, 1.224], mean observation: 0.126 [-45.006, 17.491], loss: --, mean_squared_error: --, mean_q: --\n",
      "  970/2000: episode: 11, duration: 2.780s, episode steps: 88, steps per second: 32, episode reward: 0.495, mean reward: 0.006 [-0.000, 0.013], mean action: 0.236 [-1.164, 1.226], mean observation: 0.128 [-42.709, 17.840], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1058/2000: episode: 12, duration: 3.279s, episode steps: 88, steps per second: 27, episode reward: 0.496, mean reward: 0.006 [-0.000, 0.013], mean action: 0.208 [-1.155, 1.104], mean observation: 0.132 [-29.582, 17.727], loss: 0.000103, mean_squared_error: 0.000207, mean_q: 0.696601\n",
      " 1172/2000: episode: 13, duration: 5.129s, episode steps: 114, steps per second: 22, episode reward: 0.839, mean reward: 0.007 [0.000, 0.020], mean action: 0.261 [-1.251, 1.199], mean observation: 0.107 [-31.483, 18.076], loss: 0.000148, mean_squared_error: 0.000296, mean_q: 0.687405\n",
      " 1296/2000: episode: 14, duration: 6.028s, episode steps: 124, steps per second: 21, episode reward: 0.742, mean reward: 0.006 [-0.001, 0.017], mean action: 0.220 [-1.251, 1.183], mean observation: 0.110 [-10.098, 18.049], loss: 0.000136, mean_squared_error: 0.000272, mean_q: 0.684145\n",
      " 1415/2000: episode: 15, duration: 5.567s, episode steps: 119, steps per second: 21, episode reward: 0.765, mean reward: 0.006 [-0.000, 0.019], mean action: 0.248 [-1.125, 1.219], mean observation: 0.106 [-9.888, 17.027], loss: 0.000148, mean_squared_error: 0.000296, mean_q: 0.678670\n",
      " 1533/2000: episode: 16, duration: 6.061s, episode steps: 118, steps per second: 19, episode reward: 0.807, mean reward: 0.007 [-0.001, 0.019], mean action: 0.263 [-1.184, 1.189], mean observation: 0.107 [-19.198, 17.966], loss: 0.000282, mean_squared_error: 0.000563, mean_q: 0.684048\n",
      " 1638/2000: episode: 17, duration: 5.245s, episode steps: 105, steps per second: 20, episode reward: 0.678, mean reward: 0.006 [-0.000, 0.015], mean action: 0.293 [-1.265, 1.153], mean observation: 0.126 [-9.967, 17.857], loss: 0.000124, mean_squared_error: 0.000247, mean_q: 0.680408\n",
      " 1759/2000: episode: 18, duration: 6.123s, episode steps: 121, steps per second: 20, episode reward: 0.812, mean reward: 0.007 [-0.001, 0.019], mean action: 0.377 [-1.220, 1.350], mean observation: 0.111 [-15.083, 18.277], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.675734\n",
      " 1907/2000: episode: 19, duration: 8.141s, episode steps: 148, steps per second: 18, episode reward: 0.874, mean reward: 0.006 [-0.001, 0.017], mean action: 0.375 [-1.132, 1.179], mean observation: 0.122 [-23.466, 15.843], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.674394\n",
      "done, took 82.344 seconds\n",
      "\n",
      "\n",
      "iteration: 23\n",
      "Training for 2000 steps ...\n",
      "  125/2000: episode: 1, duration: 6.509s, episode steps: 125, steps per second: 19, episode reward: 0.817, mean reward: 0.007 [-0.001, 0.016], mean action: 0.350 [-1.164, 1.243], mean observation: 0.108 [-41.363, 18.430], loss: --, mean_squared_error: --, mean_q: --\n",
      "  247/2000: episode: 2, duration: 6.457s, episode steps: 122, steps per second: 19, episode reward: 0.781, mean reward: 0.006 [-0.000, 0.015], mean action: 0.389 [-1.151, 1.233], mean observation: 0.096 [-41.939, 18.068], loss: --, mean_squared_error: --, mean_q: --\n",
      "  368/2000: episode: 3, duration: 6.547s, episode steps: 121, steps per second: 18, episode reward: 0.700, mean reward: 0.006 [-0.001, 0.015], mean action: 0.348 [-1.118, 1.277], mean observation: 0.130 [-42.686, 18.285], loss: --, mean_squared_error: --, mean_q: --\n",
      "  493/2000: episode: 4, duration: 6.696s, episode steps: 125, steps per second: 19, episode reward: 0.807, mean reward: 0.006 [-0.001, 0.015], mean action: 0.348 [-1.131, 1.128], mean observation: 0.102 [-42.919, 17.954], loss: --, mean_squared_error: --, mean_q: --\n",
      "  614/2000: episode: 5, duration: 6.588s, episode steps: 121, steps per second: 18, episode reward: 0.753, mean reward: 0.006 [-0.001, 0.015], mean action: 0.342 [-1.259, 1.237], mean observation: 0.113 [-41.929, 18.447], loss: --, mean_squared_error: --, mean_q: --\n",
      "  734/2000: episode: 6, duration: 6.223s, episode steps: 120, steps per second: 19, episode reward: 0.727, mean reward: 0.006 [-0.001, 0.015], mean action: 0.338 [-1.182, 1.231], mean observation: 0.113 [-41.212, 18.096], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  855/2000: episode: 7, duration: 6.604s, episode steps: 121, steps per second: 18, episode reward: 0.726, mean reward: 0.006 [-0.001, 0.015], mean action: 0.328 [-1.144, 1.217], mean observation: 0.122 [-42.801, 18.680], loss: --, mean_squared_error: --, mean_q: --\n",
      "  978/2000: episode: 8, duration: 6.434s, episode steps: 123, steps per second: 19, episode reward: 0.789, mean reward: 0.006 [-0.001, 0.015], mean action: 0.359 [-1.270, 1.256], mean observation: 0.110 [-42.467, 18.069], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1093/2000: episode: 9, duration: 9.399s, episode steps: 115, steps per second: 12, episode reward: 0.708, mean reward: 0.006 [-0.001, 0.016], mean action: 0.357 [-1.155, 1.270], mean observation: 0.082 [-50.280, 18.256], loss: 0.000137, mean_squared_error: 0.000275, mean_q: 0.681926\n",
      " 1233/2000: episode: 10, duration: 7.725s, episode steps: 140, steps per second: 18, episode reward: 0.922, mean reward: 0.007 [-0.001, 0.018], mean action: 0.389 [-1.105, 1.193], mean observation: 0.127 [-22.090, 18.454], loss: 0.000123, mean_squared_error: 0.000245, mean_q: 0.680692\n",
      " 1379/2000: episode: 11, duration: 7.628s, episode steps: 146, steps per second: 19, episode reward: 0.888, mean reward: 0.006 [-0.001, 0.015], mean action: 0.366 [-1.143, 1.223], mean observation: 0.132 [-17.536, 18.225], loss: 0.000174, mean_squared_error: 0.000348, mean_q: 0.677117\n",
      " 1519/2000: episode: 12, duration: 7.270s, episode steps: 140, steps per second: 19, episode reward: 0.899, mean reward: 0.006 [-0.000, 0.017], mean action: 0.402 [-1.199, 1.323], mean observation: 0.125 [-15.690, 18.371], loss: 0.000087, mean_squared_error: 0.000174, mean_q: 0.685762\n",
      " 1644/2000: episode: 13, duration: 6.975s, episode steps: 125, steps per second: 18, episode reward: 0.887, mean reward: 0.007 [-0.001, 0.017], mean action: 0.388 [-1.290, 1.289], mean observation: 0.121 [-19.830, 17.790], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.676206\n",
      " 1772/2000: episode: 14, duration: 6.505s, episode steps: 128, steps per second: 20, episode reward: 0.811, mean reward: 0.006 [-0.001, 0.014], mean action: 0.410 [-1.094, 1.263], mean observation: 0.119 [-11.319, 19.709], loss: 0.000135, mean_squared_error: 0.000271, mean_q: 0.683777\n",
      " 1922/2000: episode: 15, duration: 8.606s, episode steps: 150, steps per second: 17, episode reward: 0.956, mean reward: 0.006 [-0.001, 0.019], mean action: 0.412 [-1.180, 1.170], mean observation: 0.131 [-8.897, 19.789], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.679569\n",
      "done, took 109.695 seconds\n",
      "\n",
      "\n",
      "iteration: 24\n",
      "Training for 2000 steps ...\n",
      "  140/2000: episode: 1, duration: 5.769s, episode steps: 140, steps per second: 24, episode reward: 0.874, mean reward: 0.006 [-0.001, 0.017], mean action: 0.429 [-1.162, 1.192], mean observation: 0.121 [-8.870, 19.358], loss: --, mean_squared_error: --, mean_q: --\n",
      "  280/2000: episode: 2, duration: 5.852s, episode steps: 140, steps per second: 24, episode reward: 0.885, mean reward: 0.006 [-0.001, 0.018], mean action: 0.441 [-1.172, 1.197], mean observation: 0.120 [-8.755, 19.251], loss: --, mean_squared_error: --, mean_q: --\n",
      "  422/2000: episode: 3, duration: 5.826s, episode steps: 142, steps per second: 24, episode reward: 0.909, mean reward: 0.006 [-0.001, 0.017], mean action: 0.463 [-1.105, 1.237], mean observation: 0.122 [-8.902, 19.609], loss: --, mean_squared_error: --, mean_q: --\n",
      "  556/2000: episode: 4, duration: 5.597s, episode steps: 134, steps per second: 24, episode reward: 0.859, mean reward: 0.006 [-0.001, 0.017], mean action: 0.438 [-1.193, 1.295], mean observation: 0.121 [-8.912, 19.362], loss: --, mean_squared_error: --, mean_q: --\n",
      "  694/2000: episode: 5, duration: 5.679s, episode steps: 138, steps per second: 24, episode reward: 0.876, mean reward: 0.006 [-0.001, 0.017], mean action: 0.421 [-1.231, 1.150], mean observation: 0.122 [-8.785, 19.486], loss: --, mean_squared_error: --, mean_q: --\n",
      "  832/2000: episode: 6, duration: 5.851s, episode steps: 138, steps per second: 24, episode reward: 0.888, mean reward: 0.006 [-0.001, 0.018], mean action: 0.448 [-1.152, 1.235], mean observation: 0.122 [-8.869, 19.347], loss: --, mean_squared_error: --, mean_q: --\n",
      "  972/2000: episode: 7, duration: 5.927s, episode steps: 140, steps per second: 24, episode reward: 0.867, mean reward: 0.006 [-0.001, 0.017], mean action: 0.438 [-1.138, 1.246], mean observation: 0.122 [-8.791, 19.465], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1112/2000: episode: 8, duration: 7.156s, episode steps: 140, steps per second: 20, episode reward: 0.937, mean reward: 0.007 [-0.001, 0.019], mean action: 0.458 [-1.121, 1.160], mean observation: 0.126 [-8.822, 19.348], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.684795\n",
      " 1253/2000: episode: 9, duration: 7.163s, episode steps: 141, steps per second: 20, episode reward: 0.941, mean reward: 0.007 [-0.002, 0.019], mean action: 0.406 [-1.206, 1.198], mean observation: 0.125 [-8.668, 18.601], loss: 0.000094, mean_squared_error: 0.000187, mean_q: 0.684969\n",
      " 1379/2000: episode: 10, duration: 6.387s, episode steps: 126, steps per second: 20, episode reward: 0.853, mean reward: 0.007 [-0.001, 0.017], mean action: 0.379 [-1.199, 1.123], mean observation: 0.110 [-9.228, 17.643], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.668501\n",
      " 1502/2000: episode: 11, duration: 6.214s, episode steps: 123, steps per second: 20, episode reward: 0.872, mean reward: 0.007 [-0.001, 0.017], mean action: 0.367 [-1.227, 1.129], mean observation: 0.111 [-9.226, 17.743], loss: 0.000149, mean_squared_error: 0.000297, mean_q: 0.676065\n",
      " 1629/2000: episode: 12, duration: 7.041s, episode steps: 127, steps per second: 18, episode reward: 0.905, mean reward: 0.007 [-0.001, 0.019], mean action: 0.419 [-1.102, 1.163], mean observation: 0.118 [-9.201, 17.517], loss: 0.000070, mean_squared_error: 0.000141, mean_q: 0.669952\n",
      " 1757/2000: episode: 13, duration: 6.645s, episode steps: 128, steps per second: 19, episode reward: 0.852, mean reward: 0.007 [-0.001, 0.018], mean action: 0.334 [-1.269, 1.213], mean observation: 0.116 [-9.375, 18.729], loss: 0.000105, mean_squared_error: 0.000210, mean_q: 0.676330\n",
      " 1886/2000: episode: 14, duration: 7.180s, episode steps: 129, steps per second: 18, episode reward: 0.936, mean reward: 0.007 [-0.001, 0.019], mean action: 0.354 [-1.283, 1.311], mean observation: 0.119 [-8.935, 19.976], loss: 0.000139, mean_squared_error: 0.000277, mean_q: 0.673350\n",
      "done, took 93.381 seconds\n",
      "\n",
      "\n",
      "iteration: 25\n",
      "Training for 2000 steps ...\n",
      "  207/2000: episode: 1, duration: 7.838s, episode steps: 207, steps per second: 26, episode reward: 0.952, mean reward: 0.005 [-0.002, 0.017], mean action: 0.330 [-1.308, 1.259], mean observation: 0.122 [-9.120, 16.574], loss: --, mean_squared_error: --, mean_q: --\n",
      "  397/2000: episode: 2, duration: 7.483s, episode steps: 190, steps per second: 25, episode reward: 0.957, mean reward: 0.005 [-0.002, 0.018], mean action: 0.310 [-1.163, 1.174], mean observation: 0.123 [-9.224, 16.630], loss: --, mean_squared_error: --, mean_q: --\n",
      "  641/2000: episode: 3, duration: 8.654s, episode steps: 244, steps per second: 28, episode reward: 0.911, mean reward: 0.004 [-0.002, 0.019], mean action: 0.316 [-1.298, 1.361], mean observation: 0.121 [-9.140, 16.698], loss: --, mean_squared_error: --, mean_q: --\n",
      "  830/2000: episode: 4, duration: 7.278s, episode steps: 189, steps per second: 26, episode reward: 0.928, mean reward: 0.005 [-0.002, 0.018], mean action: 0.294 [-1.190, 1.249], mean observation: 0.121 [-9.130, 16.722], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1027/2000: episode: 5, duration: 7.996s, episode steps: 197, steps per second: 25, episode reward: 0.974, mean reward: 0.005 [-0.002, 0.018], mean action: 0.294 [-1.274, 1.172], mean observation: 0.123 [-9.241, 16.470], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.674233\n",
      " 1243/2000: episode: 6, duration: 8.159s, episode steps: 216, steps per second: 26, episode reward: -0.746, mean reward: -0.003 [-0.019, 0.011], mean action: 0.180 [-1.179, 1.209], mean observation: 0.060 [-31.810, 12.848], loss: 0.000092, mean_squared_error: 0.000184, mean_q: 0.669733\n",
      " 1373/2000: episode: 7, duration: 7.070s, episode steps: 130, steps per second: 18, episode reward: 0.939, mean reward: 0.007 [-0.000, 0.017], mean action: 0.336 [-1.077, 1.265], mean observation: 0.117 [-9.029, 19.231], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.663937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1518/2000: episode: 8, duration: 8.070s, episode steps: 145, steps per second: 18, episode reward: 0.896, mean reward: 0.006 [-0.001, 0.017], mean action: 0.282 [-1.171, 1.192], mean observation: 0.115 [-9.224, 19.300], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.656648\n",
      " 1653/2000: episode: 9, duration: 6.873s, episode steps: 135, steps per second: 20, episode reward: 0.812, mean reward: 0.006 [-0.001, 0.014], mean action: 0.298 [-1.186, 1.261], mean observation: 0.110 [-9.217, 19.278], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.675534\n",
      " 1792/2000: episode: 10, duration: 7.211s, episode steps: 139, steps per second: 19, episode reward: 0.854, mean reward: 0.006 [-0.001, 0.015], mean action: 0.306 [-1.159, 1.218], mean observation: 0.113 [-8.956, 18.255], loss: 0.000141, mean_squared_error: 0.000281, mean_q: 0.664662\n",
      " 1929/2000: episode: 11, duration: 7.093s, episode steps: 137, steps per second: 19, episode reward: 0.939, mean reward: 0.007 [-0.000, 0.018], mean action: 0.405 [-1.129, 1.198], mean observation: 0.121 [-8.958, 19.022], loss: 0.000133, mean_squared_error: 0.000265, mean_q: 0.667373\n",
      "done, took 87.003 seconds\n",
      "\n",
      "\n",
      "iteration: 26\n",
      "Training for 2000 steps ...\n",
      "  134/2000: episode: 1, duration: 5.699s, episode steps: 134, steps per second: 24, episode reward: 0.986, mean reward: 0.007 [-0.000, 0.019], mean action: 0.417 [-1.164, 1.164], mean observation: 0.122 [-9.045, 19.685], loss: --, mean_squared_error: --, mean_q: --\n",
      "  267/2000: episode: 2, duration: 5.574s, episode steps: 133, steps per second: 24, episode reward: 0.980, mean reward: 0.007 [-0.000, 0.019], mean action: 0.447 [-1.106, 1.156], mean observation: 0.123 [-8.968, 19.534], loss: --, mean_squared_error: --, mean_q: --\n",
      "  399/2000: episode: 3, duration: 5.612s, episode steps: 132, steps per second: 24, episode reward: 0.987, mean reward: 0.007 [-0.000, 0.019], mean action: 0.446 [-1.125, 1.296], mean observation: 0.123 [-9.008, 19.709], loss: --, mean_squared_error: --, mean_q: --\n",
      "  531/2000: episode: 4, duration: 5.459s, episode steps: 132, steps per second: 24, episode reward: 0.990, mean reward: 0.008 [-0.000, 0.018], mean action: 0.445 [-1.189, 1.200], mean observation: 0.124 [-9.098, 19.761], loss: --, mean_squared_error: --, mean_q: --\n",
      "  664/2000: episode: 5, duration: 5.514s, episode steps: 133, steps per second: 24, episode reward: 0.973, mean reward: 0.007 [-0.000, 0.018], mean action: 0.469 [-1.216, 1.274], mean observation: 0.122 [-9.035, 19.722], loss: --, mean_squared_error: --, mean_q: --\n",
      "  798/2000: episode: 6, duration: 6.265s, episode steps: 134, steps per second: 21, episode reward: 0.996, mean reward: 0.007 [-0.000, 0.019], mean action: 0.428 [-1.170, 1.184], mean observation: 0.124 [-11.983, 19.699], loss: --, mean_squared_error: --, mean_q: --\n",
      "  932/2000: episode: 7, duration: 5.564s, episode steps: 134, steps per second: 24, episode reward: 0.993, mean reward: 0.007 [-0.000, 0.019], mean action: 0.439 [-1.133, 1.177], mean observation: 0.124 [-9.000, 19.563], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1064/2000: episode: 8, duration: 6.148s, episode steps: 132, steps per second: 21, episode reward: 1.003, mean reward: 0.008 [-0.000, 0.019], mean action: 0.446 [-1.081, 1.265], mean observation: 0.125 [-8.998, 19.684], loss: 0.000087, mean_squared_error: 0.000174, mean_q: 0.662099\n",
      " 1182/2000: episode: 9, duration: 5.302s, episode steps: 118, steps per second: 22, episode reward: 0.812, mean reward: 0.007 [-0.000, 0.015], mean action: 0.467 [-1.106, 1.266], mean observation: 0.106 [-9.025, 19.587], loss: 0.000104, mean_squared_error: 0.000207, mean_q: 0.660068\n",
      " 1298/2000: episode: 10, duration: 5.764s, episode steps: 116, steps per second: 20, episode reward: 0.867, mean reward: 0.007 [-0.000, 0.016], mean action: 0.438 [-1.130, 1.230], mean observation: 0.108 [-9.160, 19.645], loss: 0.000249, mean_squared_error: 0.000497, mean_q: 0.658724\n",
      " 1418/2000: episode: 11, duration: 5.347s, episode steps: 120, steps per second: 22, episode reward: 0.810, mean reward: 0.007 [-0.000, 0.015], mean action: 0.458 [-1.134, 1.155], mean observation: 0.105 [-9.053, 19.468], loss: 0.000148, mean_squared_error: 0.000297, mean_q: 0.659081\n",
      " 1555/2000: episode: 12, duration: 6.800s, episode steps: 137, steps per second: 20, episode reward: 1.060, mean reward: 0.008 [-0.001, 0.020], mean action: 0.450 [-1.137, 1.212], mean observation: 0.131 [-8.945, 20.242], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.665778\n",
      " 1684/2000: episode: 13, duration: 6.493s, episode steps: 129, steps per second: 20, episode reward: 0.816, mean reward: 0.006 [-0.001, 0.016], mean action: 0.392 [-1.081, 1.168], mean observation: 0.113 [-9.031, 20.320], loss: 0.000116, mean_squared_error: 0.000232, mean_q: 0.669276\n",
      " 1841/2000: episode: 14, duration: 6.702s, episode steps: 157, steps per second: 23, episode reward: 0.822, mean reward: 0.005 [-0.001, 0.015], mean action: 0.326 [-1.183, 1.211], mean observation: 0.118 [-7.550, 17.284], loss: 0.000128, mean_squared_error: 0.000256, mean_q: 0.663464\n",
      "done, took 87.794 seconds\n",
      "\n",
      "\n",
      "iteration: 27\n",
      "Training for 2000 steps ...\n",
      "  141/2000: episode: 1, duration: 4.913s, episode steps: 141, steps per second: 29, episode reward: 0.846, mean reward: 0.006 [-0.001, 0.015], mean action: 0.283 [-1.325, 1.292], mean observation: 0.119 [-8.400, 21.289], loss: --, mean_squared_error: --, mean_q: --\n",
      "  281/2000: episode: 2, duration: 4.730s, episode steps: 140, steps per second: 30, episode reward: 0.850, mean reward: 0.006 [-0.001, 0.015], mean action: 0.264 [-1.201, 1.196], mean observation: 0.116 [-9.781, 21.256], loss: --, mean_squared_error: --, mean_q: --\n",
      "  421/2000: episode: 3, duration: 4.796s, episode steps: 140, steps per second: 29, episode reward: 0.839, mean reward: 0.006 [-0.001, 0.015], mean action: 0.288 [-1.214, 1.241], mean observation: 0.117 [-8.364, 21.431], loss: --, mean_squared_error: --, mean_q: --\n",
      "  563/2000: episode: 4, duration: 4.907s, episode steps: 142, steps per second: 29, episode reward: 0.838, mean reward: 0.006 [-0.001, 0.015], mean action: 0.268 [-1.261, 1.218], mean observation: 0.117 [-8.424, 21.382], loss: --, mean_squared_error: --, mean_q: --\n",
      "  703/2000: episode: 5, duration: 4.696s, episode steps: 140, steps per second: 30, episode reward: 0.834, mean reward: 0.006 [-0.001, 0.015], mean action: 0.274 [-1.219, 1.250], mean observation: 0.117 [-8.244, 21.279], loss: --, mean_squared_error: --, mean_q: --\n",
      "  849/2000: episode: 6, duration: 4.900s, episode steps: 146, steps per second: 30, episode reward: 0.846, mean reward: 0.006 [-0.001, 0.015], mean action: 0.269 [-1.230, 1.132], mean observation: 0.117 [-8.271, 21.140], loss: --, mean_squared_error: --, mean_q: --\n",
      "  992/2000: episode: 7, duration: 4.925s, episode steps: 143, steps per second: 29, episode reward: 0.846, mean reward: 0.006 [-0.001, 0.015], mean action: 0.285 [-1.186, 1.264], mean observation: 0.117 [-8.315, 21.271], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1136/2000: episode: 8, duration: 6.321s, episode steps: 144, steps per second: 23, episode reward: 0.833, mean reward: 0.006 [-0.001, 0.015], mean action: 0.290 [-1.215, 1.175], mean observation: 0.117 [-9.012, 21.539], loss: 0.000094, mean_squared_error: 0.000188, mean_q: 0.655984\n",
      " 1288/2000: episode: 9, duration: 7.231s, episode steps: 152, steps per second: 21, episode reward: 1.012, mean reward: 0.007 [-0.001, 0.018], mean action: 0.364 [-1.113, 1.360], mean observation: 0.129 [-8.048, 20.968], loss: 0.000161, mean_squared_error: 0.000322, mean_q: 0.664986\n",
      " 1431/2000: episode: 10, duration: 6.382s, episode steps: 143, steps per second: 22, episode reward: 0.852, mean reward: 0.006 [-0.001, 0.015], mean action: 0.317 [-1.185, 1.155], mean observation: 0.118 [-8.081, 21.244], loss: 0.000133, mean_squared_error: 0.000266, mean_q: 0.659235\n",
      " 1619/2000: episode: 11, duration: 8.316s, episode steps: 188, steps per second: 23, episode reward: 0.918, mean reward: 0.005 [-0.001, 0.020], mean action: 0.255 [-1.308, 1.180], mean observation: 0.131 [-27.349, 18.945], loss: 0.000135, mean_squared_error: 0.000270, mean_q: 0.667045\n",
      " 1784/2000: episode: 12, duration: 6.980s, episode steps: 165, steps per second: 24, episode reward: -0.883, mean reward: -0.005 [-0.020, 0.006], mean action: 0.287 [-1.211, 1.172], mean observation: 0.032 [-30.267, 19.771], loss: 0.000153, mean_squared_error: 0.000305, mean_q: 0.660286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1952/2000: episode: 13, duration: 7.254s, episode steps: 168, steps per second: 23, episode reward: -0.782, mean reward: -0.005 [-0.020, 0.006], mean action: 0.217 [-1.269, 1.208], mean observation: 0.045 [-29.571, 20.236], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.661821\n",
      "done, took 79.236 seconds\n",
      "\n",
      "\n",
      "iteration: 28\n",
      "Training for 2000 steps ...\n",
      "  141/2000: episode: 1, duration: 5.232s, episode steps: 141, steps per second: 27, episode reward: -0.800, mean reward: -0.006 [-0.020, 0.006], mean action: 0.289 [-1.159, 1.259], mean observation: 0.027 [-27.748, 14.526], loss: --, mean_squared_error: --, mean_q: --\n",
      "  275/2000: episode: 2, duration: 4.865s, episode steps: 134, steps per second: 28, episode reward: -0.774, mean reward: -0.006 [-0.020, 0.007], mean action: 0.335 [-1.168, 1.276], mean observation: 0.023 [-30.822, 15.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  414/2000: episode: 3, duration: 5.044s, episode steps: 139, steps per second: 28, episode reward: -0.790, mean reward: -0.006 [-0.020, 0.007], mean action: 0.320 [-1.236, 1.196], mean observation: 0.026 [-26.603, 15.829], loss: --, mean_squared_error: --, mean_q: --\n",
      "  551/2000: episode: 4, duration: 5.051s, episode steps: 137, steps per second: 27, episode reward: -0.769, mean reward: -0.006 [-0.020, 0.007], mean action: 0.283 [-1.220, 1.172], mean observation: 0.027 [-31.359, 17.507], loss: --, mean_squared_error: --, mean_q: --\n",
      "  690/2000: episode: 5, duration: 5.020s, episode steps: 139, steps per second: 28, episode reward: -0.795, mean reward: -0.006 [-0.021, 0.007], mean action: 0.319 [-1.194, 1.120], mean observation: 0.026 [-29.511, 16.158], loss: --, mean_squared_error: --, mean_q: --\n",
      "  826/2000: episode: 6, duration: 4.935s, episode steps: 136, steps per second: 28, episode reward: -0.785, mean reward: -0.006 [-0.021, 0.007], mean action: 0.338 [-1.196, 1.207], mean observation: 0.024 [-27.110, 14.292], loss: --, mean_squared_error: --, mean_q: --\n",
      "  962/2000: episode: 7, duration: 4.910s, episode steps: 136, steps per second: 28, episode reward: -0.767, mean reward: -0.006 [-0.020, 0.007], mean action: 0.363 [-1.171, 1.332], mean observation: 0.027 [-24.461, 15.740], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1103/2000: episode: 8, duration: 5.925s, episode steps: 141, steps per second: 24, episode reward: -0.785, mean reward: -0.006 [-0.020, 0.007], mean action: 0.312 [-1.225, 1.358], mean observation: 0.029 [-28.110, 16.080], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.663605\n",
      " 1238/2000: episode: 9, duration: 6.616s, episode steps: 135, steps per second: 20, episode reward: -0.796, mean reward: -0.006 [-0.020, 0.006], mean action: 0.287 [-1.221, 1.203], mean observation: 0.025 [-27.004, 15.827], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.658779\n",
      " 1383/2000: episode: 10, duration: 7.103s, episode steps: 145, steps per second: 20, episode reward: -0.868, mean reward: -0.006 [-0.020, 0.007], mean action: 0.244 [-1.195, 1.274], mean observation: 0.025 [-30.441, 14.833], loss: 0.000127, mean_squared_error: 0.000254, mean_q: 0.659464\n",
      " 1507/2000: episode: 11, duration: 6.045s, episode steps: 124, steps per second: 21, episode reward: -0.693, mean reward: -0.006 [-0.021, 0.009], mean action: 0.281 [-1.226, 1.234], mean observation: 0.030 [-31.951, 13.709], loss: 0.000085, mean_squared_error: 0.000170, mean_q: 0.644678\n",
      " 1660/2000: episode: 12, duration: 6.536s, episode steps: 153, steps per second: 23, episode reward: -0.855, mean reward: -0.006 [-0.020, 0.008], mean action: 0.191 [-1.230, 1.260], mean observation: 0.034 [-31.830, 13.597], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.645608\n",
      " 1813/2000: episode: 13, duration: 7.025s, episode steps: 153, steps per second: 22, episode reward: -0.883, mean reward: -0.006 [-0.021, 0.006], mean action: 0.237 [-1.222, 1.246], mean observation: 0.028 [-29.798, 13.711], loss: 0.000102, mean_squared_error: 0.000203, mean_q: 0.652041\n",
      " 1987/2000: episode: 14, duration: 7.681s, episode steps: 174, steps per second: 23, episode reward: -0.858, mean reward: -0.005 [-0.020, 0.006], mean action: 0.291 [-1.167, 1.228], mean observation: 0.039 [-26.967, 13.500], loss: 0.000097, mean_squared_error: 0.000195, mean_q: 0.651160\n",
      "done, took 82.978 seconds\n",
      "\n",
      "\n",
      "iteration: 29\n",
      "Training for 2000 steps ...\n",
      "  202/2000: episode: 1, duration: 8.052s, episode steps: 202, steps per second: 25, episode reward: 0.702, mean reward: 0.003 [-0.000, 0.014], mean action: 0.330 [-1.172, 1.421], mean observation: 0.115 [-29.606, 14.069], loss: --, mean_squared_error: --, mean_q: --\n",
      "  401/2000: episode: 2, duration: 7.900s, episode steps: 199, steps per second: 25, episode reward: 0.724, mean reward: 0.004 [-0.000, 0.014], mean action: 0.345 [-1.183, 1.301], mean observation: 0.116 [-30.789, 14.480], loss: --, mean_squared_error: --, mean_q: --\n",
      "  619/2000: episode: 3, duration: 9.003s, episode steps: 218, steps per second: 24, episode reward: 0.792, mean reward: 0.004 [-0.000, 0.016], mean action: 0.356 [-1.109, 1.277], mean observation: 0.121 [-25.440, 14.119], loss: --, mean_squared_error: --, mean_q: --\n",
      "  835/2000: episode: 4, duration: 8.247s, episode steps: 216, steps per second: 26, episode reward: 0.710, mean reward: 0.003 [-0.001, 0.016], mean action: 0.374 [-1.211, 1.463], mean observation: 0.114 [-30.500, 13.713], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1041/2000: episode: 5, duration: 8.574s, episode steps: 206, steps per second: 24, episode reward: 0.718, mean reward: 0.003 [-0.001, 0.013], mean action: 0.340 [-1.155, 1.289], mean observation: 0.115 [-29.075, 14.172], loss: 0.000091, mean_squared_error: 0.000182, mean_q: 0.644162\n",
      " 1280/2000: episode: 6, duration: 10.687s, episode steps: 239, steps per second: 22, episode reward: 0.791, mean reward: 0.003 [-0.002, 0.014], mean action: 0.334 [-1.305, 1.277], mean observation: 0.116 [-11.118, 14.422], loss: 0.000124, mean_squared_error: 0.000249, mean_q: 0.648931\n",
      " 1417/2000: episode: 7, duration: 6.988s, episode steps: 137, steps per second: 20, episode reward: 0.735, mean reward: 0.005 [-0.001, 0.012], mean action: 0.356 [-1.175, 1.146], mean observation: 0.102 [-9.321, 13.792], loss: 0.000149, mean_squared_error: 0.000297, mean_q: 0.650955\n",
      " 1675/2000: episode: 8, duration: 11.669s, episode steps: 258, steps per second: 22, episode reward: 0.737, mean reward: 0.003 [-0.003, 0.013], mean action: 0.258 [-1.162, 1.193], mean observation: 0.116 [-12.038, 16.752], loss: 0.000117, mean_squared_error: 0.000233, mean_q: 0.644147\n",
      " 1855/2000: episode: 9, duration: 9.219s, episode steps: 180, steps per second: 20, episode reward: 0.758, mean reward: 0.004 [-0.002, 0.016], mean action: 0.200 [-1.318, 1.247], mean observation: 0.114 [-22.204, 18.764], loss: 0.000155, mean_squared_error: 0.000309, mean_q: 0.646902\n",
      "done, took 87.394 seconds\n",
      "\n",
      "\n",
      "iteration: 30\n",
      "Training for 2000 steps ...\n",
      "  180/2000: episode: 1, duration: 7.593s, episode steps: 180, steps per second: 24, episode reward: 0.762, mean reward: 0.004 [-0.002, 0.015], mean action: 0.312 [-1.245, 1.283], mean observation: 0.114 [-15.197, 19.424], loss: --, mean_squared_error: --, mean_q: --\n",
      "  374/2000: episode: 2, duration: 7.797s, episode steps: 194, steps per second: 25, episode reward: 0.780, mean reward: 0.004 [-0.002, 0.017], mean action: 0.338 [-1.091, 1.293], mean observation: 0.116 [-19.142, 19.431], loss: --, mean_squared_error: --, mean_q: --\n",
      "  567/2000: episode: 3, duration: 8.003s, episode steps: 193, steps per second: 24, episode reward: 0.822, mean reward: 0.004 [-0.002, 0.017], mean action: 0.319 [-1.201, 1.243], mean observation: 0.117 [-15.856, 18.332], loss: --, mean_squared_error: --, mean_q: --\n",
      "  748/2000: episode: 4, duration: 7.565s, episode steps: 181, steps per second: 24, episode reward: 0.768, mean reward: 0.004 [-0.002, 0.018], mean action: 0.320 [-1.138, 1.159], mean observation: 0.113 [-11.350, 19.504], loss: --, mean_squared_error: --, mean_q: --\n",
      "  935/2000: episode: 5, duration: 7.945s, episode steps: 187, steps per second: 24, episode reward: 0.868, mean reward: 0.005 [-0.002, 0.019], mean action: 0.307 [-1.250, 1.238], mean observation: 0.119 [-10.897, 19.022], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1132/2000: episode: 6, duration: 9.640s, episode steps: 197, steps per second: 20, episode reward: 0.860, mean reward: 0.004 [-0.002, 0.016], mean action: 0.295 [-1.272, 1.156], mean observation: 0.120 [-11.792, 18.719], loss: 0.000217, mean_squared_error: 0.000434, mean_q: 0.649764\n",
      " 1254/2000: episode: 7, duration: 7.449s, episode steps: 122, steps per second: 16, episode reward: 0.864, mean reward: 0.007 [-0.003, 0.017], mean action: 0.433 [-1.229, 1.184], mean observation: 0.099 [-14.075, 16.701], loss: 0.000234, mean_squared_error: 0.000468, mean_q: 0.646097\n",
      " 1380/2000: episode: 8, duration: 7.427s, episode steps: 126, steps per second: 17, episode reward: 0.705, mean reward: 0.006 [-0.004, 0.013], mean action: 0.438 [-1.129, 1.187], mean observation: 0.088 [-11.495, 16.688], loss: 0.000140, mean_squared_error: 0.000280, mean_q: 0.651383\n",
      " 1487/2000: episode: 9, duration: 5.799s, episode steps: 107, steps per second: 18, episode reward: 0.688, mean reward: 0.006 [-0.003, 0.013], mean action: 0.377 [-1.086, 1.192], mean observation: 0.081 [-15.883, 16.561], loss: 0.000086, mean_squared_error: 0.000173, mean_q: 0.651170\n",
      " 1590/2000: episode: 10, duration: 5.890s, episode steps: 103, steps per second: 17, episode reward: 0.739, mean reward: 0.007 [-0.003, 0.014], mean action: 0.290 [-1.227, 1.154], mean observation: 0.087 [-11.395, 23.262], loss: 0.000104, mean_squared_error: 0.000207, mean_q: 0.638030\n",
      " 1687/2000: episode: 11, duration: 5.441s, episode steps: 97, steps per second: 18, episode reward: 0.670, mean reward: 0.007 [-0.003, 0.014], mean action: 0.306 [-1.204, 1.110], mean observation: 0.073 [-11.280, 21.364], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.645170\n",
      " 1785/2000: episode: 12, duration: 5.457s, episode steps: 98, steps per second: 18, episode reward: 0.681, mean reward: 0.007 [-0.003, 0.014], mean action: 0.315 [-1.188, 1.159], mean observation: 0.077 [-11.066, 18.481], loss: 0.000105, mean_squared_error: 0.000209, mean_q: 0.651201\n",
      " 1883/2000: episode: 13, duration: 5.488s, episode steps: 98, steps per second: 18, episode reward: 0.677, mean reward: 0.007 [-0.003, 0.013], mean action: 0.330 [-1.208, 1.170], mean observation: 0.077 [-11.052, 18.512], loss: 0.000065, mean_squared_error: 0.000131, mean_q: 0.644439\n",
      " 1983/2000: episode: 14, duration: 5.895s, episode steps: 100, steps per second: 17, episode reward: 0.674, mean reward: 0.007 [-0.004, 0.013], mean action: 0.318 [-1.137, 1.156], mean observation: 0.076 [-14.495, 19.200], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.639493\n",
      "done, took 98.899 seconds\n",
      "\n",
      "\n",
      "iteration: 31\n",
      "Training for 2000 steps ...\n",
      "   97/2000: episode: 1, duration: 4.647s, episode steps: 97, steps per second: 21, episode reward: 0.699, mean reward: 0.007 [-0.004, 0.014], mean action: 0.293 [-1.116, 1.279], mean observation: 0.076 [-21.121, 21.959], loss: --, mean_squared_error: --, mean_q: --\n",
      "  197/2000: episode: 2, duration: 4.849s, episode steps: 100, steps per second: 21, episode reward: 0.740, mean reward: 0.007 [-0.004, 0.015], mean action: 0.249 [-1.217, 1.084], mean observation: 0.084 [-20.728, 21.950], loss: --, mean_squared_error: --, mean_q: --\n",
      "  294/2000: episode: 3, duration: 4.758s, episode steps: 97, steps per second: 20, episode reward: 0.709, mean reward: 0.007 [-0.004, 0.013], mean action: 0.297 [-1.099, 1.172], mean observation: 0.078 [-20.881, 22.218], loss: --, mean_squared_error: --, mean_q: --\n",
      "  395/2000: episode: 4, duration: 4.925s, episode steps: 101, steps per second: 21, episode reward: 0.777, mean reward: 0.008 [-0.004, 0.014], mean action: 0.289 [-1.102, 1.173], mean observation: 0.087 [-21.056, 22.132], loss: --, mean_squared_error: --, mean_q: --\n",
      "  492/2000: episode: 5, duration: 4.563s, episode steps: 97, steps per second: 21, episode reward: 0.707, mean reward: 0.007 [-0.004, 0.014], mean action: 0.295 [-1.163, 1.250], mean observation: 0.078 [-20.876, 22.079], loss: --, mean_squared_error: --, mean_q: --\n",
      "  591/2000: episode: 6, duration: 4.816s, episode steps: 99, steps per second: 21, episode reward: 0.728, mean reward: 0.007 [-0.004, 0.013], mean action: 0.285 [-1.136, 1.171], mean observation: 0.081 [-20.862, 21.902], loss: --, mean_squared_error: --, mean_q: --\n",
      "  688/2000: episode: 7, duration: 4.686s, episode steps: 97, steps per second: 21, episode reward: 0.699, mean reward: 0.007 [-0.004, 0.013], mean action: 0.288 [-1.181, 1.246], mean observation: 0.079 [-21.012, 22.184], loss: --, mean_squared_error: --, mean_q: --\n",
      "  787/2000: episode: 8, duration: 4.746s, episode steps: 99, steps per second: 21, episode reward: 0.738, mean reward: 0.007 [-0.004, 0.013], mean action: 0.298 [-1.156, 1.157], mean observation: 0.083 [-21.373, 22.007], loss: --, mean_squared_error: --, mean_q: --\n",
      "  886/2000: episode: 9, duration: 4.734s, episode steps: 99, steps per second: 21, episode reward: 0.727, mean reward: 0.007 [-0.004, 0.013], mean action: 0.297 [-1.168, 1.181], mean observation: 0.081 [-20.774, 22.280], loss: --, mean_squared_error: --, mean_q: --\n",
      "  984/2000: episode: 10, duration: 4.784s, episode steps: 98, steps per second: 20, episode reward: 0.732, mean reward: 0.007 [-0.004, 0.013], mean action: 0.288 [-1.184, 1.157], mean observation: 0.081 [-20.841, 21.888], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1079/2000: episode: 11, duration: 5.438s, episode steps: 95, steps per second: 17, episode reward: 0.674, mean reward: 0.007 [-0.004, 0.014], mean action: 0.313 [-1.104, 1.215], mean observation: 0.074 [-20.699, 22.298], loss: 0.000144, mean_squared_error: 0.000288, mean_q: 0.654994\n",
      " 1178/2000: episode: 12, duration: 5.386s, episode steps: 99, steps per second: 18, episode reward: 0.714, mean reward: 0.007 [-0.002, 0.014], mean action: 0.318 [-1.159, 1.162], mean observation: 0.078 [-19.891, 22.217], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.643242\n",
      " 1270/2000: episode: 13, duration: 5.181s, episode steps: 92, steps per second: 18, episode reward: 0.702, mean reward: 0.008 [-0.002, 0.014], mean action: 0.316 [-1.160, 1.189], mean observation: 0.078 [-17.034, 21.441], loss: 0.000104, mean_squared_error: 0.000209, mean_q: 0.644285\n",
      " 1363/2000: episode: 14, duration: 5.301s, episode steps: 93, steps per second: 18, episode reward: 0.703, mean reward: 0.008 [-0.002, 0.014], mean action: 0.382 [-1.055, 1.144], mean observation: 0.076 [-13.031, 21.414], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.643478\n",
      " 1456/2000: episode: 15, duration: 5.421s, episode steps: 93, steps per second: 17, episode reward: 0.682, mean reward: 0.007 [-0.002, 0.013], mean action: 0.351 [-1.160, 1.186], mean observation: 0.077 [-15.656, 21.601], loss: 0.000100, mean_squared_error: 0.000199, mean_q: 0.637392\n",
      " 1552/2000: episode: 16, duration: 5.313s, episode steps: 96, steps per second: 18, episode reward: 0.689, mean reward: 0.007 [-0.003, 0.013], mean action: 0.381 [-1.172, 1.170], mean observation: 0.074 [-26.463, 22.438], loss: 0.000133, mean_squared_error: 0.000266, mean_q: 0.646213\n",
      " 1641/2000: episode: 17, duration: 5.180s, episode steps: 89, steps per second: 17, episode reward: 0.638, mean reward: 0.007 [-0.003, 0.013], mean action: 0.376 [-1.128, 1.200], mean observation: 0.068 [-28.123, 23.419], loss: 0.000106, mean_squared_error: 0.000212, mean_q: 0.646367\n",
      " 1730/2000: episode: 18, duration: 5.397s, episode steps: 89, steps per second: 16, episode reward: 0.661, mean reward: 0.007 [-0.003, 0.013], mean action: 0.385 [-1.205, 1.186], mean observation: 0.062 [-28.976, 23.543], loss: 0.000095, mean_squared_error: 0.000190, mean_q: 0.639440\n",
      " 1820/2000: episode: 19, duration: 5.442s, episode steps: 90, steps per second: 17, episode reward: 0.645, mean reward: 0.007 [-0.003, 0.013], mean action: 0.372 [-1.100, 1.158], mean observation: 0.069 [-24.150, 23.107], loss: 0.000180, mean_squared_error: 0.000360, mean_q: 0.646242\n",
      " 1909/2000: episode: 20, duration: 5.142s, episode steps: 89, steps per second: 17, episode reward: 0.668, mean reward: 0.008 [-0.002, 0.013], mean action: 0.370 [-1.110, 1.102], mean observation: 0.073 [-19.797, 24.298], loss: 0.000222, mean_squared_error: 0.000445, mean_q: 0.637885\n",
      "done, took 105.852 seconds\n",
      "\n",
      "\n",
      "iteration: 32\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   92/2000: episode: 1, duration: 4.394s, episode steps: 92, steps per second: 21, episode reward: 0.677, mean reward: 0.007 [-0.002, 0.013], mean action: 0.380 [-1.229, 1.149], mean observation: 0.072 [-15.171, 22.966], loss: --, mean_squared_error: --, mean_q: --\n",
      "  183/2000: episode: 2, duration: 4.330s, episode steps: 91, steps per second: 21, episode reward: 0.668, mean reward: 0.007 [-0.003, 0.013], mean action: 0.374 [-1.182, 1.195], mean observation: 0.070 [-19.863, 22.902], loss: --, mean_squared_error: --, mean_q: --\n",
      "  273/2000: episode: 3, duration: 4.615s, episode steps: 90, steps per second: 20, episode reward: 0.656, mean reward: 0.007 [-0.003, 0.013], mean action: 0.340 [-1.201, 1.162], mean observation: 0.068 [-36.168, 22.980], loss: --, mean_squared_error: --, mean_q: --\n",
      "  364/2000: episode: 4, duration: 4.304s, episode steps: 91, steps per second: 21, episode reward: 0.670, mean reward: 0.007 [-0.003, 0.013], mean action: 0.390 [-1.213, 1.166], mean observation: 0.070 [-19.023, 22.784], loss: --, mean_squared_error: --, mean_q: --\n",
      "  454/2000: episode: 5, duration: 4.618s, episode steps: 90, steps per second: 19, episode reward: 0.661, mean reward: 0.007 [-0.003, 0.013], mean action: 0.351 [-1.139, 1.135], mean observation: 0.067 [-30.943, 23.148], loss: --, mean_squared_error: --, mean_q: --\n",
      "  545/2000: episode: 6, duration: 4.354s, episode steps: 91, steps per second: 21, episode reward: 0.668, mean reward: 0.007 [-0.002, 0.013], mean action: 0.371 [-1.126, 1.219], mean observation: 0.073 [-11.730, 23.352], loss: --, mean_squared_error: --, mean_q: --\n",
      "  636/2000: episode: 7, duration: 4.638s, episode steps: 91, steps per second: 20, episode reward: 0.665, mean reward: 0.007 [-0.002, 0.013], mean action: 0.360 [-1.175, 1.176], mean observation: 0.075 [-23.300, 23.114], loss: --, mean_squared_error: --, mean_q: --\n",
      "  727/2000: episode: 8, duration: 4.427s, episode steps: 91, steps per second: 21, episode reward: 0.667, mean reward: 0.007 [-0.003, 0.013], mean action: 0.369 [-1.161, 1.125], mean observation: 0.070 [-21.421, 22.904], loss: --, mean_squared_error: --, mean_q: --\n",
      "  818/2000: episode: 9, duration: 4.424s, episode steps: 91, steps per second: 21, episode reward: 0.666, mean reward: 0.007 [-0.002, 0.013], mean action: 0.373 [-1.101, 1.262], mean observation: 0.075 [-11.665, 22.914], loss: --, mean_squared_error: --, mean_q: --\n",
      "  909/2000: episode: 10, duration: 4.334s, episode steps: 91, steps per second: 21, episode reward: 0.674, mean reward: 0.007 [-0.003, 0.013], mean action: 0.378 [-1.107, 1.358], mean observation: 0.071 [-20.317, 22.813], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1001/2000: episode: 11, duration: 4.513s, episode steps: 92, steps per second: 20, episode reward: 0.674, mean reward: 0.007 [-0.002, 0.013], mean action: 0.361 [-1.211, 1.221], mean observation: 0.075 [-12.130, 23.182], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1088/2000: episode: 12, duration: 5.176s, episode steps: 87, steps per second: 17, episode reward: 0.630, mean reward: 0.007 [-0.003, 0.013], mean action: 0.367 [-1.123, 1.156], mean observation: 0.070 [-30.831, 23.193], loss: 0.000145, mean_squared_error: 0.000291, mean_q: 0.639426\n",
      " 1177/2000: episode: 13, duration: 4.672s, episode steps: 89, steps per second: 19, episode reward: 0.679, mean reward: 0.008 [-0.002, 0.015], mean action: 0.427 [-1.140, 1.166], mean observation: 0.072 [-16.186, 23.814], loss: 0.000235, mean_squared_error: 0.000470, mean_q: 0.640149\n",
      " 1266/2000: episode: 14, duration: 5.087s, episode steps: 89, steps per second: 17, episode reward: 0.677, mean reward: 0.008 [-0.001, 0.014], mean action: 0.355 [-1.168, 1.167], mean observation: 0.070 [-39.038, 22.262], loss: 0.000100, mean_squared_error: 0.000199, mean_q: 0.634875\n",
      " 1356/2000: episode: 15, duration: 4.876s, episode steps: 90, steps per second: 18, episode reward: 0.682, mean reward: 0.008 [-0.001, 0.014], mean action: 0.374 [-1.136, 1.153], mean observation: 0.076 [-13.186, 22.323], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.641865\n",
      " 1445/2000: episode: 16, duration: 4.945s, episode steps: 89, steps per second: 18, episode reward: 0.654, mean reward: 0.007 [-0.002, 0.013], mean action: 0.396 [-1.095, 1.218], mean observation: 0.073 [-24.141, 22.886], loss: 0.000112, mean_squared_error: 0.000225, mean_q: 0.640507\n",
      " 1533/2000: episode: 17, duration: 5.151s, episode steps: 88, steps per second: 17, episode reward: 0.656, mean reward: 0.007 [-0.002, 0.013], mean action: 0.316 [-1.179, 1.172], mean observation: 0.073 [-53.999, 23.059], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.636119\n",
      " 1621/2000: episode: 18, duration: 5.178s, episode steps: 88, steps per second: 17, episode reward: 0.656, mean reward: 0.007 [-0.001, 0.013], mean action: 0.324 [-1.217, 1.139], mean observation: 0.074 [-54.350, 22.996], loss: 0.000152, mean_squared_error: 0.000303, mean_q: 0.638888\n",
      " 1709/2000: episode: 19, duration: 5.210s, episode steps: 88, steps per second: 17, episode reward: 0.654, mean reward: 0.007 [-0.002, 0.013], mean action: 0.365 [-1.177, 1.232], mean observation: 0.063 [-51.138, 23.365], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.647048\n",
      " 1798/2000: episode: 20, duration: 5.316s, episode steps: 89, steps per second: 17, episode reward: 0.669, mean reward: 0.008 [-0.002, 0.013], mean action: 0.345 [-1.173, 1.182], mean observation: 0.065 [-55.538, 22.608], loss: 0.000179, mean_squared_error: 0.000358, mean_q: 0.637739\n",
      " 1888/2000: episode: 21, duration: 5.301s, episode steps: 90, steps per second: 17, episode reward: 0.668, mean reward: 0.007 [-0.001, 0.014], mean action: 0.352 [-1.153, 1.144], mean observation: 0.063 [-54.937, 21.443], loss: 0.000362, mean_squared_error: 0.000724, mean_q: 0.650332\n",
      " 1977/2000: episode: 22, duration: 5.084s, episode steps: 89, steps per second: 18, episode reward: 0.665, mean reward: 0.007 [-0.001, 0.013], mean action: 0.318 [-1.088, 1.156], mean observation: 0.072 [-56.703, 21.483], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.626402\n",
      "done, took 106.397 seconds\n",
      "\n",
      "\n",
      "iteration: 33\n",
      "Training for 2000 steps ...\n",
      "   92/2000: episode: 1, duration: 4.497s, episode steps: 92, steps per second: 20, episode reward: 0.688, mean reward: 0.007 [-0.000, 0.013], mean action: 0.335 [-1.158, 1.150], mean observation: 0.078 [-43.794, 22.734], loss: --, mean_squared_error: --, mean_q: --\n",
      "  184/2000: episode: 2, duration: 4.445s, episode steps: 92, steps per second: 21, episode reward: 0.689, mean reward: 0.007 [-0.000, 0.013], mean action: 0.350 [-1.145, 1.267], mean observation: 0.074 [-52.850, 19.204], loss: --, mean_squared_error: --, mean_q: --\n",
      "  276/2000: episode: 3, duration: 4.438s, episode steps: 92, steps per second: 21, episode reward: 0.689, mean reward: 0.007 [-0.000, 0.013], mean action: 0.358 [-1.110, 1.193], mean observation: 0.075 [-50.981, 24.237], loss: --, mean_squared_error: --, mean_q: --\n",
      "  367/2000: episode: 4, duration: 4.446s, episode steps: 91, steps per second: 20, episode reward: 0.681, mean reward: 0.007 [-0.001, 0.013], mean action: 0.349 [-1.132, 1.189], mean observation: 0.074 [-47.322, 21.632], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 5, duration: 4.527s, episode steps: 92, steps per second: 20, episode reward: 0.691, mean reward: 0.008 [-0.001, 0.013], mean action: 0.337 [-1.197, 1.158], mean observation: 0.074 [-52.065, 20.277], loss: --, mean_squared_error: --, mean_q: --\n",
      "  550/2000: episode: 6, duration: 4.440s, episode steps: 91, steps per second: 20, episode reward: 0.679, mean reward: 0.007 [-0.001, 0.013], mean action: 0.334 [-1.161, 1.161], mean observation: 0.074 [-47.063, 23.104], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 7, duration: 4.470s, episode steps: 92, steps per second: 21, episode reward: 0.680, mean reward: 0.007 [-0.000, 0.013], mean action: 0.337 [-1.171, 1.131], mean observation: 0.076 [-47.212, 18.537], loss: --, mean_squared_error: --, mean_q: --\n",
      "  733/2000: episode: 8, duration: 4.407s, episode steps: 91, steps per second: 21, episode reward: 0.681, mean reward: 0.007 [-0.000, 0.013], mean action: 0.318 [-1.143, 1.138], mean observation: 0.067 [-54.866, 18.315], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  824/2000: episode: 9, duration: 4.334s, episode steps: 91, steps per second: 21, episode reward: 0.677, mean reward: 0.007 [-0.000, 0.013], mean action: 0.338 [-1.116, 1.111], mean observation: 0.071 [-55.405, 18.280], loss: --, mean_squared_error: --, mean_q: --\n",
      "  915/2000: episode: 10, duration: 4.388s, episode steps: 91, steps per second: 21, episode reward: 0.675, mean reward: 0.007 [-0.000, 0.013], mean action: 0.328 [-1.231, 1.152], mean observation: 0.075 [-43.315, 21.954], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1006/2000: episode: 11, duration: 4.475s, episode steps: 91, steps per second: 20, episode reward: 0.675, mean reward: 0.007 [-0.000, 0.013], mean action: 0.321 [-1.133, 1.156], mean observation: 0.076 [-39.113, 22.655], loss: 0.000174, mean_squared_error: 0.000349, mean_q: 0.643793\n",
      " 1097/2000: episode: 12, duration: 5.341s, episode steps: 91, steps per second: 17, episode reward: 0.676, mean reward: 0.007 [-0.000, 0.013], mean action: 0.277 [-1.172, 1.221], mean observation: 0.072 [-48.829, 20.545], loss: 0.000276, mean_squared_error: 0.000552, mean_q: 0.647248\n",
      " 1188/2000: episode: 13, duration: 4.978s, episode steps: 91, steps per second: 18, episode reward: 0.705, mean reward: 0.008 [-0.001, 0.014], mean action: 0.337 [-1.113, 1.356], mean observation: 0.072 [-41.778, 21.371], loss: 0.000151, mean_squared_error: 0.000302, mean_q: 0.637539\n",
      " 1278/2000: episode: 14, duration: 5.116s, episode steps: 90, steps per second: 18, episode reward: 0.670, mean reward: 0.007 [-0.001, 0.014], mean action: 0.326 [-1.186, 1.160], mean observation: 0.073 [-44.899, 21.645], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.646724\n",
      " 1369/2000: episode: 15, duration: 5.166s, episode steps: 91, steps per second: 18, episode reward: 0.678, mean reward: 0.007 [-0.001, 0.014], mean action: 0.374 [-1.146, 1.168], mean observation: 0.070 [-53.177, 20.716], loss: 0.000125, mean_squared_error: 0.000251, mean_q: 0.639060\n",
      " 1460/2000: episode: 16, duration: 5.240s, episode steps: 91, steps per second: 17, episode reward: 0.674, mean reward: 0.007 [-0.000, 0.013], mean action: 0.366 [-1.123, 1.113], mean observation: 0.081 [-55.572, 18.461], loss: 0.000212, mean_squared_error: 0.000425, mean_q: 0.636276\n",
      " 1550/2000: episode: 17, duration: 5.190s, episode steps: 90, steps per second: 17, episode reward: 0.668, mean reward: 0.007 [-0.001, 0.013], mean action: 0.332 [-1.246, 1.188], mean observation: 0.075 [-51.721, 21.532], loss: 0.000126, mean_squared_error: 0.000251, mean_q: 0.644983\n",
      " 1642/2000: episode: 18, duration: 5.248s, episode steps: 92, steps per second: 18, episode reward: 0.699, mean reward: 0.008 [-0.000, 0.014], mean action: 0.293 [-1.131, 1.148], mean observation: 0.081 [-51.710, 18.595], loss: 0.000171, mean_squared_error: 0.000342, mean_q: 0.643086\n",
      " 1733/2000: episode: 19, duration: 5.216s, episode steps: 91, steps per second: 17, episode reward: 0.679, mean reward: 0.007 [-0.000, 0.013], mean action: 0.322 [-1.110, 1.179], mean observation: 0.071 [-50.785, 21.870], loss: 0.000259, mean_squared_error: 0.000519, mean_q: 0.641832\n",
      " 1824/2000: episode: 20, duration: 5.226s, episode steps: 91, steps per second: 17, episode reward: 0.696, mean reward: 0.008 [0.000, 0.014], mean action: 0.292 [-1.138, 1.116], mean observation: 0.079 [-43.360, 17.638], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.639106\n",
      " 1916/2000: episode: 21, duration: 5.202s, episode steps: 92, steps per second: 18, episode reward: 0.699, mean reward: 0.008 [0.000, 0.014], mean action: 0.301 [-1.162, 1.186], mean observation: 0.079 [-38.803, 17.915], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.655252\n",
      "done, took 105.799 seconds\n",
      "\n",
      "\n",
      "iteration: 34\n",
      "Training for 2000 steps ...\n",
      "   91/2000: episode: 1, duration: 4.571s, episode steps: 91, steps per second: 20, episode reward: 0.672, mean reward: 0.007 [-0.000, 0.013], mean action: 0.324 [-1.125, 1.181], mean observation: 0.076 [-57.448, 18.634], loss: --, mean_squared_error: --, mean_q: --\n",
      "  182/2000: episode: 2, duration: 4.575s, episode steps: 91, steps per second: 20, episode reward: 0.673, mean reward: 0.007 [-0.001, 0.013], mean action: 0.330 [-1.138, 1.222], mean observation: 0.076 [-57.145, 18.473], loss: --, mean_squared_error: --, mean_q: --\n",
      "  274/2000: episode: 3, duration: 4.683s, episode steps: 92, steps per second: 20, episode reward: 0.679, mean reward: 0.007 [-0.000, 0.013], mean action: 0.318 [-1.120, 1.208], mean observation: 0.077 [-54.928, 19.133], loss: --, mean_squared_error: --, mean_q: --\n",
      "  366/2000: episode: 4, duration: 4.612s, episode steps: 92, steps per second: 20, episode reward: 0.681, mean reward: 0.007 [-0.000, 0.013], mean action: 0.284 [-1.234, 1.213], mean observation: 0.064 [-52.182, 18.590], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 5, duration: 4.804s, episode steps: 93, steps per second: 19, episode reward: 0.693, mean reward: 0.007 [-0.000, 0.013], mean action: 0.298 [-1.230, 1.203], mean observation: 0.074 [-49.685, 18.755], loss: --, mean_squared_error: --, mean_q: --\n",
      "  551/2000: episode: 6, duration: 4.739s, episode steps: 92, steps per second: 19, episode reward: 0.686, mean reward: 0.007 [-0.000, 0.013], mean action: 0.308 [-1.138, 1.211], mean observation: 0.079 [-45.754, 18.815], loss: --, mean_squared_error: --, mean_q: --\n",
      "  643/2000: episode: 7, duration: 4.657s, episode steps: 92, steps per second: 20, episode reward: 0.687, mean reward: 0.007 [-0.000, 0.013], mean action: 0.288 [-1.123, 1.090], mean observation: 0.076 [-38.739, 18.839], loss: --, mean_squared_error: --, mean_q: --\n",
      "  735/2000: episode: 8, duration: 4.643s, episode steps: 92, steps per second: 20, episode reward: 0.685, mean reward: 0.007 [-0.000, 0.013], mean action: 0.323 [-1.236, 1.306], mean observation: 0.075 [-43.158, 18.926], loss: --, mean_squared_error: --, mean_q: --\n",
      "  827/2000: episode: 9, duration: 4.603s, episode steps: 92, steps per second: 20, episode reward: 0.685, mean reward: 0.007 [-0.000, 0.013], mean action: 0.309 [-1.150, 1.239], mean observation: 0.064 [-54.800, 18.574], loss: --, mean_squared_error: --, mean_q: --\n",
      "  919/2000: episode: 10, duration: 4.555s, episode steps: 92, steps per second: 20, episode reward: 0.681, mean reward: 0.007 [-0.000, 0.013], mean action: 0.291 [-1.265, 1.148], mean observation: 0.068 [-43.599, 21.544], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1011/2000: episode: 11, duration: 4.669s, episode steps: 92, steps per second: 20, episode reward: 0.683, mean reward: 0.007 [-0.000, 0.013], mean action: 0.277 [-1.181, 1.236], mean observation: 0.078 [-48.187, 18.702], loss: 0.000231, mean_squared_error: 0.000462, mean_q: 0.650041\n",
      " 1102/2000: episode: 12, duration: 5.328s, episode steps: 91, steps per second: 17, episode reward: 0.677, mean reward: 0.007 [-0.000, 0.013], mean action: 0.260 [-1.123, 1.270], mean observation: 0.078 [-53.679, 18.806], loss: 0.000158, mean_squared_error: 0.000316, mean_q: 0.645040\n",
      " 1193/2000: episode: 13, duration: 5.433s, episode steps: 91, steps per second: 17, episode reward: 0.685, mean reward: 0.008 [-0.000, 0.014], mean action: 0.310 [-1.096, 1.251], mean observation: 0.067 [-41.925, 19.180], loss: 0.000232, mean_squared_error: 0.000463, mean_q: 0.635430\n",
      " 1285/2000: episode: 14, duration: 5.272s, episode steps: 92, steps per second: 17, episode reward: 0.711, mean reward: 0.008 [-0.001, 0.015], mean action: 0.334 [-1.176, 1.165], mean observation: 0.082 [-12.780, 19.199], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.641404\n",
      " 1376/2000: episode: 15, duration: 5.079s, episode steps: 91, steps per second: 18, episode reward: 0.709, mean reward: 0.008 [-0.001, 0.015], mean action: 0.368 [-1.154, 1.243], mean observation: 0.079 [-22.485, 18.930], loss: 0.000213, mean_squared_error: 0.000425, mean_q: 0.645750\n",
      " 1466/2000: episode: 16, duration: 4.896s, episode steps: 90, steps per second: 18, episode reward: 0.705, mean reward: 0.008 [-0.001, 0.015], mean action: 0.368 [-1.116, 1.190], mean observation: 0.077 [-26.342, 18.507], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.635381\n",
      " 1557/2000: episode: 17, duration: 4.849s, episode steps: 91, steps per second: 19, episode reward: 0.725, mean reward: 0.008 [-0.001, 0.015], mean action: 0.359 [-1.188, 1.173], mean observation: 0.077 [-30.364, 18.488], loss: 0.000190, mean_squared_error: 0.000381, mean_q: 0.650641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1648/2000: episode: 18, duration: 4.816s, episode steps: 91, steps per second: 19, episode reward: 0.706, mean reward: 0.008 [-0.001, 0.014], mean action: 0.353 [-1.176, 1.157], mean observation: 0.076 [-27.041, 18.833], loss: 0.000174, mean_squared_error: 0.000348, mean_q: 0.636109\n",
      " 1739/2000: episode: 19, duration: 5.141s, episode steps: 91, steps per second: 18, episode reward: 0.727, mean reward: 0.008 [-0.001, 0.015], mean action: 0.308 [-1.136, 1.148], mean observation: 0.078 [-29.725, 18.669], loss: 0.000151, mean_squared_error: 0.000301, mean_q: 0.638733\n",
      " 1829/2000: episode: 20, duration: 5.122s, episode steps: 90, steps per second: 18, episode reward: 0.710, mean reward: 0.008 [-0.001, 0.015], mean action: 0.311 [-1.126, 1.201], mean observation: 0.079 [-26.437, 18.458], loss: 0.000142, mean_squared_error: 0.000284, mean_q: 0.653161\n",
      " 1921/2000: episode: 21, duration: 5.116s, episode steps: 92, steps per second: 18, episode reward: 0.725, mean reward: 0.008 [0.000, 0.015], mean action: 0.309 [-1.078, 1.234], mean observation: 0.084 [-25.554, 17.596], loss: 0.000126, mean_squared_error: 0.000252, mean_q: 0.642501\n",
      "done, took 106.891 seconds\n",
      "\n",
      "\n",
      "iteration: 35\n",
      "Training for 2000 steps ...\n",
      "   91/2000: episode: 1, duration: 4.208s, episode steps: 91, steps per second: 22, episode reward: 0.723, mean reward: 0.008 [-0.000, 0.015], mean action: 0.290 [-1.107, 1.161], mean observation: 0.080 [-22.146, 17.586], loss: --, mean_squared_error: --, mean_q: --\n",
      "  183/2000: episode: 2, duration: 4.185s, episode steps: 92, steps per second: 22, episode reward: 0.741, mean reward: 0.008 [-0.000, 0.015], mean action: 0.298 [-1.128, 1.197], mean observation: 0.080 [-25.614, 17.471], loss: --, mean_squared_error: --, mean_q: --\n",
      "  275/2000: episode: 3, duration: 4.144s, episode steps: 92, steps per second: 22, episode reward: 0.740, mean reward: 0.008 [-0.000, 0.016], mean action: 0.284 [-1.134, 1.128], mean observation: 0.081 [-26.194, 17.355], loss: --, mean_squared_error: --, mean_q: --\n",
      "  367/2000: episode: 4, duration: 4.209s, episode steps: 92, steps per second: 22, episode reward: 0.742, mean reward: 0.008 [0.000, 0.016], mean action: 0.284 [-1.170, 1.135], mean observation: 0.075 [-36.416, 17.493], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 5, duration: 4.187s, episode steps: 92, steps per second: 22, episode reward: 0.727, mean reward: 0.008 [-0.000, 0.015], mean action: 0.303 [-1.165, 1.183], mean observation: 0.079 [-18.605, 17.431], loss: --, mean_squared_error: --, mean_q: --\n",
      "  550/2000: episode: 6, duration: 4.253s, episode steps: 91, steps per second: 21, episode reward: 0.728, mean reward: 0.008 [-0.000, 0.016], mean action: 0.304 [-1.147, 1.182], mean observation: 0.083 [-26.998, 17.241], loss: --, mean_squared_error: --, mean_q: --\n",
      "  641/2000: episode: 7, duration: 4.199s, episode steps: 91, steps per second: 22, episode reward: 0.726, mean reward: 0.008 [-0.000, 0.015], mean action: 0.301 [-1.121, 1.167], mean observation: 0.080 [-33.345, 17.191], loss: --, mean_squared_error: --, mean_q: --\n",
      "  733/2000: episode: 8, duration: 4.270s, episode steps: 92, steps per second: 22, episode reward: 0.735, mean reward: 0.008 [0.000, 0.015], mean action: 0.273 [-1.168, 1.140], mean observation: 0.077 [-27.838, 17.404], loss: --, mean_squared_error: --, mean_q: --\n",
      "  824/2000: episode: 9, duration: 4.201s, episode steps: 91, steps per second: 22, episode reward: 0.727, mean reward: 0.008 [-0.000, 0.015], mean action: 0.282 [-1.123, 1.193], mean observation: 0.083 [-35.963, 17.404], loss: --, mean_squared_error: --, mean_q: --\n",
      "  916/2000: episode: 10, duration: 4.254s, episode steps: 92, steps per second: 22, episode reward: 0.725, mean reward: 0.008 [0.000, 0.015], mean action: 0.289 [-1.163, 1.150], mean observation: 0.082 [-24.832, 17.495], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1008/2000: episode: 11, duration: 4.226s, episode steps: 92, steps per second: 22, episode reward: 0.744, mean reward: 0.008 [-0.000, 0.016], mean action: 0.312 [-1.238, 1.187], mean observation: 0.077 [-23.760, 17.687], loss: 0.000217, mean_squared_error: 0.000434, mean_q: 0.666677\n",
      " 1099/2000: episode: 12, duration: 5.157s, episode steps: 91, steps per second: 18, episode reward: 0.706, mean reward: 0.008 [0.000, 0.015], mean action: 0.312 [-1.061, 1.146], mean observation: 0.074 [-37.015, 17.604], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.646185\n",
      " 1190/2000: episode: 13, duration: 5.198s, episode steps: 91, steps per second: 18, episode reward: 0.700, mean reward: 0.008 [-0.000, 0.014], mean action: 0.274 [-1.183, 1.169], mean observation: 0.081 [-22.268, 17.571], loss: 0.000139, mean_squared_error: 0.000279, mean_q: 0.641925\n",
      " 1282/2000: episode: 14, duration: 5.247s, episode steps: 92, steps per second: 18, episode reward: 0.707, mean reward: 0.008 [-0.000, 0.014], mean action: 0.346 [-1.114, 1.134], mean observation: 0.079 [-16.803, 17.587], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.644879\n",
      " 1373/2000: episode: 15, duration: 5.183s, episode steps: 91, steps per second: 18, episode reward: 0.721, mean reward: 0.008 [-0.001, 0.015], mean action: 0.333 [-1.178, 1.176], mean observation: 0.081 [-21.779, 18.172], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.641915\n",
      " 1468/2000: episode: 16, duration: 5.732s, episode steps: 95, steps per second: 17, episode reward: 0.773, mean reward: 0.008 [-0.000, 0.016], mean action: 0.318 [-1.169, 1.252], mean observation: 0.088 [-23.976, 17.545], loss: 0.000197, mean_squared_error: 0.000394, mean_q: 0.648271\n",
      " 1562/2000: episode: 17, duration: 5.385s, episode steps: 94, steps per second: 17, episode reward: 0.768, mean reward: 0.008 [-0.000, 0.014], mean action: 0.311 [-1.120, 1.156], mean observation: 0.082 [-27.023, 17.319], loss: 0.000261, mean_squared_error: 0.000521, mean_q: 0.641316\n",
      " 1653/2000: episode: 18, duration: 5.137s, episode steps: 91, steps per second: 18, episode reward: 0.706, mean reward: 0.008 [-0.000, 0.014], mean action: 0.345 [-1.091, 1.220], mean observation: 0.078 [-26.873, 17.626], loss: 0.000137, mean_squared_error: 0.000274, mean_q: 0.634233\n",
      " 1744/2000: episode: 19, duration: 4.968s, episode steps: 91, steps per second: 18, episode reward: 0.731, mean reward: 0.008 [-0.000, 0.015], mean action: 0.349 [-1.139, 1.204], mean observation: 0.080 [-20.703, 17.376], loss: 0.000104, mean_squared_error: 0.000208, mean_q: 0.646914\n",
      " 1835/2000: episode: 20, duration: 5.030s, episode steps: 91, steps per second: 18, episode reward: 0.709, mean reward: 0.008 [-0.001, 0.014], mean action: 0.327 [-1.171, 1.268], mean observation: 0.075 [-26.368, 17.569], loss: 0.000158, mean_squared_error: 0.000315, mean_q: 0.651419\n",
      " 1925/2000: episode: 21, duration: 5.111s, episode steps: 90, steps per second: 18, episode reward: 0.687, mean reward: 0.008 [-0.001, 0.014], mean action: 0.319 [-1.119, 1.138], mean observation: 0.078 [-23.342, 18.451], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.638220\n",
      "done, took 103.039 seconds\n",
      "\n",
      "\n",
      "iteration: 36\n",
      "Training for 2000 steps ...\n",
      "   90/2000: episode: 1, duration: 4.186s, episode steps: 90, steps per second: 21, episode reward: 0.719, mean reward: 0.008 [-0.001, 0.015], mean action: 0.309 [-1.093, 1.175], mean observation: 0.079 [-29.428, 18.168], loss: --, mean_squared_error: --, mean_q: --\n",
      "  181/2000: episode: 2, duration: 4.296s, episode steps: 91, steps per second: 21, episode reward: 0.720, mean reward: 0.008 [-0.001, 0.015], mean action: 0.303 [-1.173, 1.211], mean observation: 0.079 [-22.500, 18.414], loss: --, mean_squared_error: --, mean_q: --\n",
      "  271/2000: episode: 3, duration: 4.152s, episode steps: 90, steps per second: 22, episode reward: 0.716, mean reward: 0.008 [-0.001, 0.015], mean action: 0.340 [-1.138, 1.269], mean observation: 0.076 [-19.021, 18.598], loss: --, mean_squared_error: --, mean_q: --\n",
      "  361/2000: episode: 4, duration: 4.194s, episode steps: 90, steps per second: 21, episode reward: 0.717, mean reward: 0.008 [-0.001, 0.015], mean action: 0.289 [-1.191, 1.161], mean observation: 0.075 [-16.954, 18.515], loss: --, mean_squared_error: --, mean_q: --\n",
      "  452/2000: episode: 5, duration: 4.267s, episode steps: 91, steps per second: 21, episode reward: 0.724, mean reward: 0.008 [-0.001, 0.014], mean action: 0.320 [-1.086, 1.205], mean observation: 0.077 [-26.030, 18.754], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  543/2000: episode: 6, duration: 4.153s, episode steps: 91, steps per second: 22, episode reward: 0.723, mean reward: 0.008 [-0.001, 0.015], mean action: 0.285 [-1.219, 1.131], mean observation: 0.076 [-22.829, 18.667], loss: --, mean_squared_error: --, mean_q: --\n",
      "  634/2000: episode: 7, duration: 4.088s, episode steps: 91, steps per second: 22, episode reward: 0.717, mean reward: 0.008 [-0.001, 0.015], mean action: 0.315 [-1.157, 1.176], mean observation: 0.075 [-29.435, 18.504], loss: --, mean_squared_error: --, mean_q: --\n",
      "  725/2000: episode: 8, duration: 4.281s, episode steps: 91, steps per second: 21, episode reward: 0.727, mean reward: 0.008 [-0.001, 0.015], mean action: 0.302 [-1.155, 1.156], mean observation: 0.078 [-28.289, 18.427], loss: --, mean_squared_error: --, mean_q: --\n",
      "  816/2000: episode: 9, duration: 4.074s, episode steps: 91, steps per second: 22, episode reward: 0.734, mean reward: 0.008 [-0.001, 0.015], mean action: 0.304 [-1.192, 1.155], mean observation: 0.073 [-21.121, 18.463], loss: --, mean_squared_error: --, mean_q: --\n",
      "  906/2000: episode: 10, duration: 4.295s, episode steps: 90, steps per second: 21, episode reward: 0.714, mean reward: 0.008 [-0.001, 0.015], mean action: 0.297 [-1.114, 1.132], mean observation: 0.077 [-21.196, 18.399], loss: --, mean_squared_error: --, mean_q: --\n",
      "  997/2000: episode: 11, duration: 4.380s, episode steps: 91, steps per second: 21, episode reward: 0.727, mean reward: 0.008 [-0.001, 0.014], mean action: 0.300 [-1.147, 1.161], mean observation: 0.077 [-18.210, 18.411], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1088/2000: episode: 12, duration: 5.107s, episode steps: 91, steps per second: 18, episode reward: 0.725, mean reward: 0.008 [-0.001, 0.014], mean action: 0.307 [-1.083, 1.140], mean observation: 0.076 [-25.535, 18.493], loss: 0.000152, mean_squared_error: 0.000304, mean_q: 0.651265\n",
      " 1180/2000: episode: 13, duration: 5.412s, episode steps: 92, steps per second: 17, episode reward: 0.723, mean reward: 0.008 [-0.001, 0.014], mean action: 0.296 [-1.183, 1.179], mean observation: 0.080 [-28.955, 18.474], loss: 0.000094, mean_squared_error: 0.000189, mean_q: 0.640713\n",
      " 1274/2000: episode: 14, duration: 5.904s, episode steps: 94, steps per second: 16, episode reward: 0.753, mean reward: 0.008 [-0.001, 0.015], mean action: 0.269 [-1.132, 1.211], mean observation: 0.081 [-29.937, 18.227], loss: 0.000113, mean_squared_error: 0.000225, mean_q: 0.637030\n",
      " 1367/2000: episode: 15, duration: 5.712s, episode steps: 93, steps per second: 16, episode reward: 0.767, mean reward: 0.008 [-0.001, 0.016], mean action: 0.283 [-1.229, 1.145], mean observation: 0.091 [-55.370, 17.527], loss: 0.000178, mean_squared_error: 0.000356, mean_q: 0.650525\n",
      " 1458/2000: episode: 16, duration: 5.325s, episode steps: 91, steps per second: 17, episode reward: 0.709, mean reward: 0.008 [-0.001, 0.013], mean action: 0.299 [-1.188, 1.132], mean observation: 0.077 [-52.073, 22.441], loss: 0.000160, mean_squared_error: 0.000321, mean_q: 0.643775\n",
      " 1550/2000: episode: 17, duration: 5.395s, episode steps: 92, steps per second: 17, episode reward: 0.702, mean reward: 0.008 [-0.000, 0.013], mean action: 0.294 [-1.094, 1.153], mean observation: 0.077 [-43.413, 22.669], loss: 0.000188, mean_squared_error: 0.000377, mean_q: 0.641015\n",
      " 1641/2000: episode: 18, duration: 5.275s, episode steps: 91, steps per second: 17, episode reward: 0.708, mean reward: 0.008 [-0.000, 0.014], mean action: 0.300 [-1.152, 1.237], mean observation: 0.077 [-42.691, 17.611], loss: 0.000171, mean_squared_error: 0.000343, mean_q: 0.640323\n",
      " 1732/2000: episode: 19, duration: 5.175s, episode steps: 91, steps per second: 18, episode reward: 0.710, mean reward: 0.008 [-0.000, 0.014], mean action: 0.287 [-1.101, 1.209], mean observation: 0.076 [-52.373, 22.494], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.639167\n",
      " 1823/2000: episode: 20, duration: 5.256s, episode steps: 91, steps per second: 17, episode reward: 0.703, mean reward: 0.008 [-0.000, 0.014], mean action: 0.304 [-1.102, 1.150], mean observation: 0.079 [-32.319, 23.620], loss: 0.000165, mean_squared_error: 0.000330, mean_q: 0.636759\n",
      " 1916/2000: episode: 21, duration: 5.383s, episode steps: 93, steps per second: 17, episode reward: 0.706, mean reward: 0.008 [-0.000, 0.013], mean action: 0.309 [-1.119, 1.181], mean observation: 0.082 [-29.863, 20.758], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.642385\n",
      "done, took 105.175 seconds\n",
      "\n",
      "\n",
      "iteration: 37\n",
      "Training for 2000 steps ...\n",
      "   92/2000: episode: 1, duration: 4.293s, episode steps: 92, steps per second: 21, episode reward: 0.682, mean reward: 0.007 [-0.000, 0.013], mean action: 0.299 [-1.191, 1.114], mean observation: 0.082 [-50.605, 17.698], loss: --, mean_squared_error: --, mean_q: --\n",
      "  184/2000: episode: 2, duration: 4.216s, episode steps: 92, steps per second: 22, episode reward: 0.692, mean reward: 0.008 [-0.000, 0.014], mean action: 0.300 [-1.116, 1.205], mean observation: 0.070 [-50.445, 23.219], loss: --, mean_squared_error: --, mean_q: --\n",
      "  277/2000: episode: 3, duration: 4.493s, episode steps: 93, steps per second: 21, episode reward: 0.706, mean reward: 0.008 [-0.000, 0.013], mean action: 0.312 [-1.103, 1.253], mean observation: 0.080 [-35.658, 18.472], loss: --, mean_squared_error: --, mean_q: --\n",
      "  370/2000: episode: 4, duration: 4.503s, episode steps: 93, steps per second: 21, episode reward: 0.705, mean reward: 0.008 [-0.001, 0.013], mean action: 0.319 [-1.218, 1.223], mean observation: 0.075 [-43.856, 19.958], loss: --, mean_squared_error: --, mean_q: --\n",
      "  463/2000: episode: 5, duration: 4.475s, episode steps: 93, steps per second: 21, episode reward: 0.697, mean reward: 0.007 [-0.000, 0.013], mean action: 0.297 [-1.164, 1.108], mean observation: 0.084 [-40.831, 17.528], loss: --, mean_squared_error: --, mean_q: --\n",
      "  556/2000: episode: 6, duration: 4.419s, episode steps: 93, steps per second: 21, episode reward: 0.701, mean reward: 0.008 [-0.000, 0.013], mean action: 0.308 [-1.146, 1.225], mean observation: 0.084 [-36.881, 21.575], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 7, duration: 4.349s, episode steps: 93, steps per second: 21, episode reward: 0.703, mean reward: 0.008 [-0.001, 0.014], mean action: 0.273 [-1.208, 1.125], mean observation: 0.079 [-52.036, 21.005], loss: --, mean_squared_error: --, mean_q: --\n",
      "  742/2000: episode: 8, duration: 4.348s, episode steps: 93, steps per second: 21, episode reward: 0.698, mean reward: 0.008 [-0.000, 0.014], mean action: 0.290 [-1.196, 1.182], mean observation: 0.071 [-43.202, 22.806], loss: --, mean_squared_error: --, mean_q: --\n",
      "  836/2000: episode: 9, duration: 4.782s, episode steps: 94, steps per second: 20, episode reward: 0.733, mean reward: 0.008 [-0.000, 0.015], mean action: 0.333 [-1.133, 1.180], mean observation: 0.087 [-34.281, 17.542], loss: --, mean_squared_error: --, mean_q: --\n",
      "  928/2000: episode: 10, duration: 4.271s, episode steps: 92, steps per second: 22, episode reward: 0.692, mean reward: 0.008 [-0.000, 0.013], mean action: 0.316 [-1.100, 1.192], mean observation: 0.074 [-39.154, 21.648], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1021/2000: episode: 11, duration: 4.561s, episode steps: 93, steps per second: 20, episode reward: 0.706, mean reward: 0.008 [-0.001, 0.013], mean action: 0.293 [-1.103, 1.136], mean observation: 0.073 [-38.217, 20.720], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.638448\n",
      " 1113/2000: episode: 12, duration: 5.400s, episode steps: 92, steps per second: 17, episode reward: 0.713, mean reward: 0.008 [-0.000, 0.013], mean action: 0.326 [-1.161, 1.210], mean observation: 0.087 [-33.785, 17.652], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.641383\n",
      " 1208/2000: episode: 13, duration: 5.892s, episode steps: 95, steps per second: 16, episode reward: 0.760, mean reward: 0.008 [-0.000, 0.016], mean action: 0.286 [-1.114, 1.188], mean observation: 0.087 [-35.120, 20.517], loss: 0.000263, mean_squared_error: 0.000526, mean_q: 0.643304\n",
      " 1298/2000: episode: 14, duration: 4.908s, episode steps: 90, steps per second: 18, episode reward: 0.674, mean reward: 0.007 [-0.000, 0.014], mean action: 0.293 [-1.077, 1.207], mean observation: 0.079 [-45.217, 17.617], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.636023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1393/2000: episode: 15, duration: 5.289s, episode steps: 95, steps per second: 18, episode reward: 0.672, mean reward: 0.007 [0.000, 0.013], mean action: 0.266 [-1.132, 1.202], mean observation: 0.084 [-53.264, 17.397], loss: 0.000185, mean_squared_error: 0.000369, mean_q: 0.642526\n",
      " 1484/2000: episode: 16, duration: 5.187s, episode steps: 91, steps per second: 18, episode reward: 0.662, mean reward: 0.007 [-0.000, 0.013], mean action: 0.306 [-1.172, 1.158], mean observation: 0.074 [-50.409, 17.256], loss: 0.000179, mean_squared_error: 0.000357, mean_q: 0.639459\n",
      " 1575/2000: episode: 17, duration: 4.836s, episode steps: 91, steps per second: 19, episode reward: 0.696, mean reward: 0.008 [-0.000, 0.014], mean action: 0.290 [-1.105, 1.247], mean observation: 0.072 [-49.820, 17.505], loss: 0.000305, mean_squared_error: 0.000609, mean_q: 0.644381\n",
      " 1665/2000: episode: 18, duration: 4.848s, episode steps: 90, steps per second: 19, episode reward: 0.681, mean reward: 0.008 [-0.000, 0.014], mean action: 0.268 [-1.176, 1.156], mean observation: 0.073 [-50.430, 17.552], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.638513\n",
      " 1756/2000: episode: 19, duration: 4.986s, episode steps: 91, steps per second: 18, episode reward: 0.709, mean reward: 0.008 [-0.000, 0.014], mean action: 0.291 [-1.131, 1.184], mean observation: 0.076 [-50.948, 17.596], loss: 0.000103, mean_squared_error: 0.000207, mean_q: 0.641517\n",
      " 1848/2000: episode: 20, duration: 5.112s, episode steps: 92, steps per second: 18, episode reward: 0.701, mean reward: 0.008 [-0.000, 0.014], mean action: 0.269 [-1.131, 1.214], mean observation: 0.074 [-48.352, 17.559], loss: 0.000181, mean_squared_error: 0.000361, mean_q: 0.645326\n",
      " 1939/2000: episode: 21, duration: 5.049s, episode steps: 91, steps per second: 18, episode reward: 0.696, mean reward: 0.008 [-0.000, 0.014], mean action: 0.290 [-1.202, 1.146], mean observation: 0.074 [-50.529, 17.637], loss: 0.000132, mean_squared_error: 0.000265, mean_q: 0.638008\n",
      "done, took 103.756 seconds\n",
      "\n",
      "\n",
      "iteration: 38\n",
      "Training for 2000 steps ...\n",
      "   92/2000: episode: 1, duration: 4.264s, episode steps: 92, steps per second: 22, episode reward: 0.697, mean reward: 0.008 [-0.000, 0.014], mean action: 0.263 [-1.146, 1.212], mean observation: 0.077 [-45.624, 17.438], loss: --, mean_squared_error: --, mean_q: --\n",
      "  184/2000: episode: 2, duration: 4.095s, episode steps: 92, steps per second: 22, episode reward: 0.690, mean reward: 0.008 [-0.000, 0.014], mean action: 0.251 [-1.176, 1.218], mean observation: 0.075 [-45.694, 17.661], loss: --, mean_squared_error: --, mean_q: --\n",
      "  275/2000: episode: 3, duration: 4.116s, episode steps: 91, steps per second: 22, episode reward: 0.678, mean reward: 0.007 [0.000, 0.014], mean action: 0.267 [-1.108, 1.277], mean observation: 0.073 [-45.473, 17.733], loss: --, mean_squared_error: --, mean_q: --\n",
      "  366/2000: episode: 4, duration: 4.113s, episode steps: 91, steps per second: 22, episode reward: 0.682, mean reward: 0.007 [-0.000, 0.014], mean action: 0.272 [-1.172, 1.163], mean observation: 0.070 [-46.800, 17.594], loss: --, mean_squared_error: --, mean_q: --\n",
      "  457/2000: episode: 5, duration: 4.106s, episode steps: 91, steps per second: 22, episode reward: 0.684, mean reward: 0.008 [-0.000, 0.014], mean action: 0.259 [-1.167, 1.170], mean observation: 0.071 [-44.827, 17.649], loss: --, mean_squared_error: --, mean_q: --\n",
      "  548/2000: episode: 6, duration: 4.116s, episode steps: 91, steps per second: 22, episode reward: 0.687, mean reward: 0.008 [-0.000, 0.014], mean action: 0.234 [-1.296, 1.091], mean observation: 0.069 [-44.348, 17.519], loss: --, mean_squared_error: --, mean_q: --\n",
      "  639/2000: episode: 7, duration: 4.208s, episode steps: 91, steps per second: 22, episode reward: 0.690, mean reward: 0.008 [-0.000, 0.014], mean action: 0.258 [-1.145, 1.141], mean observation: 0.071 [-45.628, 17.421], loss: --, mean_squared_error: --, mean_q: --\n",
      "  730/2000: episode: 8, duration: 4.130s, episode steps: 91, steps per second: 22, episode reward: 0.681, mean reward: 0.007 [-0.000, 0.014], mean action: 0.257 [-1.227, 1.136], mean observation: 0.071 [-44.742, 17.411], loss: --, mean_squared_error: --, mean_q: --\n",
      "  820/2000: episode: 9, duration: 4.040s, episode steps: 90, steps per second: 22, episode reward: 0.674, mean reward: 0.007 [-0.000, 0.014], mean action: 0.254 [-1.115, 1.236], mean observation: 0.074 [-44.503, 17.747], loss: --, mean_squared_error: --, mean_q: --\n",
      "  911/2000: episode: 10, duration: 4.107s, episode steps: 91, steps per second: 22, episode reward: 0.688, mean reward: 0.008 [-0.000, 0.014], mean action: 0.275 [-1.138, 1.286], mean observation: 0.071 [-44.196, 17.462], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1002/2000: episode: 11, duration: 4.121s, episode steps: 91, steps per second: 22, episode reward: 0.677, mean reward: 0.007 [-0.000, 0.014], mean action: 0.274 [-1.128, 1.270], mean observation: 0.070 [-45.554, 17.609], loss: 0.000162, mean_squared_error: 0.000323, mean_q: 0.659437\n",
      " 1093/2000: episode: 12, duration: 5.022s, episode steps: 91, steps per second: 18, episode reward: 0.686, mean reward: 0.008 [-0.000, 0.014], mean action: 0.273 [-1.107, 1.175], mean observation: 0.073 [-48.220, 17.421], loss: 0.000215, mean_squared_error: 0.000431, mean_q: 0.625603\n",
      " 1185/2000: episode: 13, duration: 5.239s, episode steps: 92, steps per second: 18, episode reward: 0.675, mean reward: 0.007 [0.000, 0.014], mean action: 0.239 [-1.135, 1.327], mean observation: 0.077 [-45.940, 17.505], loss: 0.000167, mean_squared_error: 0.000335, mean_q: 0.639603\n",
      " 1277/2000: episode: 14, duration: 4.963s, episode steps: 92, steps per second: 19, episode reward: 0.686, mean reward: 0.007 [0.000, 0.014], mean action: 0.231 [-1.087, 1.190], mean observation: 0.077 [-47.951, 17.477], loss: 0.000152, mean_squared_error: 0.000303, mean_q: 0.639125\n",
      " 1371/2000: episode: 15, duration: 5.101s, episode steps: 94, steps per second: 18, episode reward: 0.720, mean reward: 0.008 [0.000, 0.014], mean action: 0.253 [-1.107, 1.173], mean observation: 0.077 [-46.865, 24.463], loss: 0.000159, mean_squared_error: 0.000318, mean_q: 0.632955\n",
      " 1464/2000: episode: 16, duration: 4.943s, episode steps: 93, steps per second: 19, episode reward: 0.722, mean reward: 0.008 [0.000, 0.014], mean action: 0.269 [-1.145, 1.171], mean observation: 0.077 [-42.340, 21.540], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.641684\n",
      " 1558/2000: episode: 17, duration: 5.200s, episode steps: 94, steps per second: 18, episode reward: 0.702, mean reward: 0.007 [0.000, 0.014], mean action: 0.236 [-1.178, 1.185], mean observation: 0.079 [-43.665, 20.430], loss: 0.000145, mean_squared_error: 0.000290, mean_q: 0.645105\n",
      " 1650/2000: episode: 18, duration: 5.179s, episode steps: 92, steps per second: 18, episode reward: 0.647, mean reward: 0.007 [0.000, 0.013], mean action: 0.225 [-1.069, 1.110], mean observation: 0.069 [-48.926, 17.793], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.632534\n",
      " 1742/2000: episode: 19, duration: 5.092s, episode steps: 92, steps per second: 18, episode reward: 0.663, mean reward: 0.007 [0.000, 0.013], mean action: 0.213 [-1.164, 1.246], mean observation: 0.072 [-44.000, 17.609], loss: 0.000135, mean_squared_error: 0.000270, mean_q: 0.642713\n",
      " 1835/2000: episode: 20, duration: 5.136s, episode steps: 93, steps per second: 18, episode reward: 0.700, mean reward: 0.008 [-0.000, 0.015], mean action: 0.212 [-1.235, 1.168], mean observation: 0.076 [-45.112, 20.637], loss: 0.000116, mean_squared_error: 0.000232, mean_q: 0.623093\n",
      " 1928/2000: episode: 21, duration: 5.177s, episode steps: 93, steps per second: 18, episode reward: 0.702, mean reward: 0.008 [-0.000, 0.014], mean action: 0.244 [-1.128, 1.192], mean observation: 0.076 [-41.809, 17.997], loss: 0.000218, mean_squared_error: 0.000436, mean_q: 0.645577\n",
      "done, took 100.535 seconds\n",
      "\n",
      "\n",
      "iteration: 39\n",
      "Training for 2000 steps ...\n",
      "   94/2000: episode: 1, duration: 4.127s, episode steps: 94, steps per second: 23, episode reward: 0.709, mean reward: 0.008 [-0.001, 0.014], mean action: 0.209 [-1.185, 1.134], mean observation: 0.080 [-42.880, 21.188], loss: --, mean_squared_error: --, mean_q: --\n",
      "  187/2000: episode: 2, duration: 4.137s, episode steps: 93, steps per second: 22, episode reward: 0.697, mean reward: 0.007 [-0.001, 0.014], mean action: 0.219 [-1.136, 1.184], mean observation: 0.078 [-45.369, 19.828], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  280/2000: episode: 3, duration: 4.035s, episode steps: 93, steps per second: 23, episode reward: 0.699, mean reward: 0.008 [-0.001, 0.014], mean action: 0.202 [-1.144, 1.156], mean observation: 0.079 [-40.661, 22.088], loss: --, mean_squared_error: --, mean_q: --\n",
      "  373/2000: episode: 4, duration: 4.167s, episode steps: 93, steps per second: 22, episode reward: 0.694, mean reward: 0.007 [-0.001, 0.014], mean action: 0.211 [-1.101, 1.168], mean observation: 0.079 [-43.710, 21.581], loss: --, mean_squared_error: --, mean_q: --\n",
      "  467/2000: episode: 5, duration: 4.152s, episode steps: 94, steps per second: 23, episode reward: 0.716, mean reward: 0.008 [-0.001, 0.014], mean action: 0.209 [-1.188, 1.248], mean observation: 0.079 [-41.121, 21.948], loss: --, mean_squared_error: --, mean_q: --\n",
      "  560/2000: episode: 6, duration: 4.085s, episode steps: 93, steps per second: 23, episode reward: 0.689, mean reward: 0.007 [-0.001, 0.014], mean action: 0.210 [-1.184, 1.141], mean observation: 0.078 [-44.566, 17.735], loss: --, mean_squared_error: --, mean_q: --\n",
      "  654/2000: episode: 7, duration: 4.128s, episode steps: 94, steps per second: 23, episode reward: 0.705, mean reward: 0.008 [-0.001, 0.015], mean action: 0.218 [-1.102, 1.148], mean observation: 0.079 [-43.424, 18.273], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 8, duration: 4.114s, episode steps: 93, steps per second: 23, episode reward: 0.691, mean reward: 0.007 [-0.001, 0.014], mean action: 0.207 [-1.145, 1.148], mean observation: 0.074 [-44.829, 17.418], loss: --, mean_squared_error: --, mean_q: --\n",
      "  840/2000: episode: 9, duration: 4.163s, episode steps: 93, steps per second: 22, episode reward: 0.691, mean reward: 0.007 [-0.001, 0.014], mean action: 0.199 [-1.168, 1.167], mean observation: 0.075 [-44.711, 17.376], loss: --, mean_squared_error: --, mean_q: --\n",
      "  935/2000: episode: 10, duration: 4.304s, episode steps: 95, steps per second: 22, episode reward: 0.721, mean reward: 0.008 [-0.001, 0.014], mean action: 0.205 [-1.128, 1.135], mean observation: 0.081 [-41.893, 21.121], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1028/2000: episode: 11, duration: 4.427s, episode steps: 93, steps per second: 21, episode reward: 0.692, mean reward: 0.007 [-0.001, 0.014], mean action: 0.208 [-1.175, 1.143], mean observation: 0.077 [-41.690, 17.311], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.630439\n",
      " 1122/2000: episode: 12, duration: 5.215s, episode steps: 94, steps per second: 18, episode reward: 0.705, mean reward: 0.007 [-0.001, 0.014], mean action: 0.233 [-1.162, 1.140], mean observation: 0.082 [-15.354, 17.789], loss: 0.000219, mean_squared_error: 0.000439, mean_q: 0.637557\n",
      " 1215/2000: episode: 13, duration: 4.851s, episode steps: 93, steps per second: 19, episode reward: 0.723, mean reward: 0.008 [-0.001, 0.015], mean action: 0.163 [-1.151, 1.230], mean observation: 0.079 [-45.942, 21.677], loss: 0.000142, mean_squared_error: 0.000285, mean_q: 0.633975\n",
      " 1306/2000: episode: 14, duration: 4.943s, episode steps: 91, steps per second: 18, episode reward: 0.655, mean reward: 0.007 [-0.002, 0.013], mean action: 0.183 [-1.183, 1.150], mean observation: 0.077 [-54.132, 17.478], loss: 0.000304, mean_squared_error: 0.000608, mean_q: 0.645621\n",
      " 1403/2000: episode: 15, duration: 5.477s, episode steps: 97, steps per second: 18, episode reward: 0.680, mean reward: 0.007 [-0.002, 0.013], mean action: 0.189 [-1.252, 1.187], mean observation: 0.076 [-46.421, 17.423], loss: 0.000111, mean_squared_error: 0.000223, mean_q: 0.626950\n",
      " 1498/2000: episode: 16, duration: 5.287s, episode steps: 95, steps per second: 18, episode reward: 0.685, mean reward: 0.007 [-0.001, 0.013], mean action: 0.173 [-1.217, 1.209], mean observation: 0.072 [-55.368, 17.651], loss: 0.000174, mean_squared_error: 0.000349, mean_q: 0.640635\n",
      " 1593/2000: episode: 17, duration: 5.337s, episode steps: 95, steps per second: 18, episode reward: 0.691, mean reward: 0.007 [-0.001, 0.013], mean action: 0.167 [-1.237, 1.148], mean observation: 0.077 [-53.332, 17.709], loss: 0.000235, mean_squared_error: 0.000471, mean_q: 0.632805\n",
      " 1690/2000: episode: 18, duration: 5.568s, episode steps: 97, steps per second: 17, episode reward: 0.699, mean reward: 0.007 [-0.002, 0.013], mean action: 0.227 [-1.105, 1.189], mean observation: 0.093 [-12.177, 17.422], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.636816\n",
      " 1786/2000: episode: 19, duration: 5.623s, episode steps: 96, steps per second: 17, episode reward: 0.687, mean reward: 0.007 [-0.002, 0.013], mean action: 0.227 [-1.155, 1.213], mean observation: 0.080 [-51.871, 17.608], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.624818\n",
      " 1886/2000: episode: 20, duration: 5.978s, episode steps: 100, steps per second: 17, episode reward: 0.701, mean reward: 0.007 [-0.002, 0.013], mean action: 0.284 [-1.119, 1.154], mean observation: 0.086 [-10.368, 17.646], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.630876\n",
      "done, took 101.324 seconds\n",
      "\n",
      "\n",
      "iteration: 40\n",
      "Training for 2000 steps ...\n",
      "  139/2000: episode: 1, duration: 7.361s, episode steps: 139, steps per second: 19, episode reward: 0.814, mean reward: 0.006 [-0.001, 0.014], mean action: 0.333 [-1.118, 1.210], mean observation: 0.104 [-21.478, 16.242], loss: --, mean_squared_error: --, mean_q: --\n",
      "  285/2000: episode: 2, duration: 7.785s, episode steps: 146, steps per second: 19, episode reward: 0.845, mean reward: 0.006 [-0.001, 0.017], mean action: 0.313 [-1.150, 1.180], mean observation: 0.106 [-20.128, 16.486], loss: --, mean_squared_error: --, mean_q: --\n",
      "  428/2000: episode: 3, duration: 7.704s, episode steps: 143, steps per second: 19, episode reward: 0.833, mean reward: 0.006 [-0.001, 0.015], mean action: 0.293 [-1.215, 1.156], mean observation: 0.108 [-21.060, 16.188], loss: --, mean_squared_error: --, mean_q: --\n",
      "  576/2000: episode: 4, duration: 7.697s, episode steps: 148, steps per second: 19, episode reward: 0.852, mean reward: 0.006 [-0.001, 0.018], mean action: 0.294 [-1.173, 1.161], mean observation: 0.108 [-18.052, 16.066], loss: --, mean_squared_error: --, mean_q: --\n",
      "  718/2000: episode: 5, duration: 7.546s, episode steps: 142, steps per second: 19, episode reward: 0.815, mean reward: 0.006 [-0.001, 0.014], mean action: 0.270 [-1.303, 1.133], mean observation: 0.106 [-25.039, 16.644], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 6, duration: 7.708s, episode steps: 144, steps per second: 19, episode reward: 0.839, mean reward: 0.006 [-0.001, 0.014], mean action: 0.303 [-1.148, 1.202], mean observation: 0.108 [-18.964, 16.400], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1006/2000: episode: 7, duration: 7.739s, episode steps: 144, steps per second: 19, episode reward: 0.826, mean reward: 0.006 [-0.001, 0.015], mean action: 0.298 [-1.203, 1.253], mean observation: 0.108 [-21.829, 16.430], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.667588\n",
      " 1233/2000: episode: 8, duration: 13.241s, episode steps: 227, steps per second: 17, episode reward: 1.466, mean reward: 0.006 [-0.002, 0.019], mean action: 0.306 [-1.209, 1.198], mean observation: 0.196 [-24.670, 16.508], loss: 0.000136, mean_squared_error: 0.000271, mean_q: 0.634335\n",
      "done, took 110.690 seconds\n",
      "\n",
      "\n",
      "iteration: 41\n",
      "Training for 2000 steps ...\n",
      "  112/2000: episode: 1, duration: 4.881s, episode steps: 112, steps per second: 23, episode reward: 0.760, mean reward: 0.007 [-0.001, 0.014], mean action: 0.082 [-1.238, 1.140], mean observation: 0.097 [-40.921, 16.099], loss: --, mean_squared_error: --, mean_q: --\n",
      "  223/2000: episode: 2, duration: 4.828s, episode steps: 111, steps per second: 23, episode reward: 0.755, mean reward: 0.007 [-0.001, 0.015], mean action: 0.087 [-1.158, 1.175], mean observation: 0.099 [-39.198, 16.488], loss: --, mean_squared_error: --, mean_q: --\n",
      "  335/2000: episode: 3, duration: 4.894s, episode steps: 112, steps per second: 23, episode reward: 0.753, mean reward: 0.007 [-0.001, 0.013], mean action: 0.097 [-1.243, 1.199], mean observation: 0.096 [-41.334, 15.982], loss: --, mean_squared_error: --, mean_q: --\n",
      "  447/2000: episode: 4, duration: 4.907s, episode steps: 112, steps per second: 23, episode reward: 0.762, mean reward: 0.007 [-0.001, 0.014], mean action: 0.117 [-1.188, 1.293], mean observation: 0.096 [-42.683, 15.926], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  559/2000: episode: 5, duration: 4.918s, episode steps: 112, steps per second: 23, episode reward: 0.759, mean reward: 0.007 [-0.001, 0.013], mean action: 0.118 [-1.162, 1.166], mean observation: 0.097 [-42.050, 16.448], loss: --, mean_squared_error: --, mean_q: --\n",
      "  671/2000: episode: 6, duration: 5.006s, episode steps: 112, steps per second: 22, episode reward: 0.759, mean reward: 0.007 [-0.001, 0.013], mean action: 0.109 [-1.222, 1.183], mean observation: 0.097 [-38.932, 16.331], loss: --, mean_squared_error: --, mean_q: --\n",
      "  783/2000: episode: 7, duration: 4.925s, episode steps: 112, steps per second: 23, episode reward: 0.761, mean reward: 0.007 [-0.001, 0.013], mean action: 0.115 [-1.191, 1.210], mean observation: 0.095 [-43.000, 16.482], loss: --, mean_squared_error: --, mean_q: --\n",
      "  895/2000: episode: 8, duration: 4.950s, episode steps: 112, steps per second: 23, episode reward: 0.765, mean reward: 0.007 [-0.001, 0.014], mean action: 0.119 [-1.168, 1.227], mean observation: 0.100 [-42.463, 16.250], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1007/2000: episode: 9, duration: 4.874s, episode steps: 112, steps per second: 23, episode reward: 0.759, mean reward: 0.007 [-0.001, 0.014], mean action: 0.106 [-1.180, 1.219], mean observation: 0.093 [-44.452, 16.247], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.640363\n",
      " 1116/2000: episode: 10, duration: 6.065s, episode steps: 109, steps per second: 18, episode reward: 0.762, mean reward: 0.007 [-0.001, 0.014], mean action: 0.118 [-1.135, 1.199], mean observation: 0.095 [-45.401, 16.459], loss: 0.000141, mean_squared_error: 0.000283, mean_q: 0.628839\n",
      " 1225/2000: episode: 11, duration: 6.273s, episode steps: 109, steps per second: 17, episode reward: 0.743, mean reward: 0.007 [-0.000, 0.017], mean action: 0.129 [-1.229, 1.194], mean observation: 0.101 [-25.536, 16.561], loss: 0.000307, mean_squared_error: 0.000614, mean_q: 0.627128\n",
      " 1337/2000: episode: 12, duration: 6.381s, episode steps: 112, steps per second: 18, episode reward: 0.787, mean reward: 0.007 [-0.001, 0.022], mean action: 0.170 [-1.080, 1.153], mean observation: 0.105 [-31.026, 16.550], loss: 0.000285, mean_squared_error: 0.000570, mean_q: 0.620738\n",
      " 1445/2000: episode: 13, duration: 6.188s, episode steps: 108, steps per second: 17, episode reward: 0.727, mean reward: 0.007 [-0.001, 0.013], mean action: 0.204 [-1.152, 1.224], mean observation: 0.090 [-50.087, 16.651], loss: 0.000149, mean_squared_error: 0.000299, mean_q: 0.629955\n",
      " 1559/2000: episode: 14, duration: 6.568s, episode steps: 114, steps per second: 17, episode reward: 0.783, mean reward: 0.007 [-0.001, 0.014], mean action: 0.182 [-1.115, 1.134], mean observation: 0.103 [-48.712, 16.457], loss: 0.000107, mean_squared_error: 0.000213, mean_q: 0.634017\n",
      " 1673/2000: episode: 15, duration: 6.489s, episode steps: 114, steps per second: 18, episode reward: 0.751, mean reward: 0.007 [-0.001, 0.013], mean action: 0.253 [-1.104, 1.202], mean observation: 0.104 [-40.723, 16.057], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.615188\n",
      " 1790/2000: episode: 16, duration: 6.688s, episode steps: 117, steps per second: 17, episode reward: 0.765, mean reward: 0.007 [-0.001, 0.014], mean action: 0.299 [-1.251, 1.190], mean observation: 0.100 [-20.767, 16.275], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.623924\n",
      " 1906/2000: episode: 17, duration: 6.803s, episode steps: 116, steps per second: 17, episode reward: 0.750, mean reward: 0.006 [-0.001, 0.014], mean action: 0.286 [-1.172, 1.198], mean observation: 0.106 [-14.024, 16.693], loss: 0.000107, mean_squared_error: 0.000213, mean_q: 0.631086\n",
      "done, took 101.584 seconds\n",
      "\n",
      "\n",
      "iteration: 42\n",
      "Training for 2000 steps ...\n",
      "  185/2000: episode: 1, duration: 9.581s, episode steps: 185, steps per second: 19, episode reward: 0.755, mean reward: 0.004 [-0.007, 0.013], mean action: 0.391 [-1.194, 1.343], mean observation: 0.093 [-34.357, 17.232], loss: --, mean_squared_error: --, mean_q: --\n",
      "  382/2000: episode: 2, duration: 10.180s, episode steps: 197, steps per second: 19, episode reward: 0.758, mean reward: 0.004 [-0.007, 0.014], mean action: 0.413 [-1.146, 1.175], mean observation: 0.096 [-25.320, 17.102], loss: --, mean_squared_error: --, mean_q: --\n",
      "  564/2000: episode: 3, duration: 9.461s, episode steps: 182, steps per second: 19, episode reward: 0.753, mean reward: 0.004 [-0.007, 0.013], mean action: 0.423 [-1.168, 1.265], mean observation: 0.094 [-30.719, 16.818], loss: --, mean_squared_error: --, mean_q: --\n",
      "  759/2000: episode: 4, duration: 9.943s, episode steps: 195, steps per second: 20, episode reward: 0.774, mean reward: 0.004 [-0.007, 0.014], mean action: 0.431 [-1.252, 1.250], mean observation: 0.097 [-26.655, 17.204], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1002/2000: episode: 5, duration: 12.506s, episode steps: 243, steps per second: 19, episode reward: 0.762, mean reward: 0.003 [-0.008, 0.015], mean action: 0.418 [-1.325, 1.359], mean observation: 0.093 [-26.187, 16.687], loss: 0.000040, mean_squared_error: 0.000080, mean_q: 0.564948\n",
      " 1123/2000: episode: 6, duration: 6.946s, episode steps: 121, steps per second: 17, episode reward: 0.783, mean reward: 0.006 [-0.001, 0.014], mean action: 0.281 [-1.096, 1.232], mean observation: 0.104 [-19.934, 16.976], loss: 0.000258, mean_squared_error: 0.000516, mean_q: 0.633939\n",
      " 1243/2000: episode: 7, duration: 6.870s, episode steps: 120, steps per second: 17, episode reward: 0.765, mean reward: 0.006 [-0.001, 0.014], mean action: 0.269 [-1.224, 1.249], mean observation: 0.100 [-22.323, 16.889], loss: 0.000161, mean_squared_error: 0.000321, mean_q: 0.633079\n",
      " 1359/2000: episode: 8, duration: 6.467s, episode steps: 116, steps per second: 18, episode reward: 0.758, mean reward: 0.007 [-0.001, 0.013], mean action: 0.186 [-1.112, 1.258], mean observation: 0.100 [-55.243, 16.603], loss: 0.000107, mean_squared_error: 0.000214, mean_q: 0.632674\n",
      " 1472/2000: episode: 9, duration: 6.273s, episode steps: 113, steps per second: 18, episode reward: 0.755, mean reward: 0.007 [-0.001, 0.014], mean action: 0.193 [-1.170, 1.180], mean observation: 0.092 [-51.176, 16.952], loss: 0.000156, mean_squared_error: 0.000313, mean_q: 0.616229\n",
      " 1587/2000: episode: 10, duration: 6.561s, episode steps: 115, steps per second: 18, episode reward: 0.774, mean reward: 0.007 [-0.001, 0.013], mean action: 0.236 [-1.257, 1.221], mean observation: 0.105 [-54.807, 16.983], loss: 0.000305, mean_squared_error: 0.000609, mean_q: 0.623404\n",
      " 1701/2000: episode: 11, duration: 6.606s, episode steps: 114, steps per second: 17, episode reward: 0.747, mean reward: 0.007 [-0.001, 0.013], mean action: 0.252 [-1.137, 1.198], mean observation: 0.096 [-51.468, 17.090], loss: 0.000152, mean_squared_error: 0.000303, mean_q: 0.620739\n",
      " 1815/2000: episode: 12, duration: 6.481s, episode steps: 114, steps per second: 18, episode reward: 0.773, mean reward: 0.007 [-0.001, 0.013], mean action: 0.199 [-1.226, 1.138], mean observation: 0.103 [-54.748, 15.692], loss: 0.000173, mean_squared_error: 0.000347, mean_q: 0.627886\n",
      " 1929/2000: episode: 13, duration: 6.249s, episode steps: 114, steps per second: 18, episode reward: 0.778, mean reward: 0.007 [0.000, 0.013], mean action: 0.292 [-1.135, 1.212], mean observation: 0.105 [-53.704, 14.141], loss: 0.000116, mean_squared_error: 0.000232, mean_q: 0.625746\n",
      "done, took 108.120 seconds\n",
      "\n",
      "\n",
      "iteration: 43\n",
      "Training for 2000 steps ...\n",
      "  113/2000: episode: 1, duration: 5.276s, episode steps: 113, steps per second: 21, episode reward: 0.762, mean reward: 0.007 [-0.000, 0.013], mean action: 0.263 [-1.157, 1.230], mean observation: 0.102 [-54.376, 15.002], loss: --, mean_squared_error: --, mean_q: --\n",
      "  224/2000: episode: 2, duration: 5.317s, episode steps: 111, steps per second: 21, episode reward: 0.754, mean reward: 0.007 [-0.000, 0.013], mean action: 0.247 [-1.136, 1.133], mean observation: 0.101 [-53.879, 15.401], loss: --, mean_squared_error: --, mean_q: --\n",
      "  336/2000: episode: 3, duration: 5.462s, episode steps: 112, steps per second: 21, episode reward: 0.757, mean reward: 0.007 [-0.000, 0.013], mean action: 0.263 [-1.120, 1.209], mean observation: 0.100 [-53.831, 15.307], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  448/2000: episode: 4, duration: 5.187s, episode steps: 112, steps per second: 22, episode reward: 0.756, mean reward: 0.007 [-0.000, 0.013], mean action: 0.253 [-1.180, 1.167], mean observation: 0.103 [-53.289, 15.248], loss: --, mean_squared_error: --, mean_q: --\n",
      "  559/2000: episode: 5, duration: 5.272s, episode steps: 111, steps per second: 21, episode reward: 0.741, mean reward: 0.007 [-0.000, 0.013], mean action: 0.255 [-1.192, 1.219], mean observation: 0.102 [-52.052, 15.424], loss: --, mean_squared_error: --, mean_q: --\n",
      "  672/2000: episode: 6, duration: 5.277s, episode steps: 113, steps per second: 21, episode reward: 0.769, mean reward: 0.007 [-0.000, 0.013], mean action: 0.277 [-1.150, 1.196], mean observation: 0.102 [-53.228, 15.189], loss: --, mean_squared_error: --, mean_q: --\n",
      "  784/2000: episode: 7, duration: 5.145s, episode steps: 112, steps per second: 22, episode reward: 0.756, mean reward: 0.007 [-0.000, 0.012], mean action: 0.249 [-1.275, 1.136], mean observation: 0.101 [-50.732, 15.085], loss: --, mean_squared_error: --, mean_q: --\n",
      "  894/2000: episode: 8, duration: 5.333s, episode steps: 110, steps per second: 21, episode reward: 0.746, mean reward: 0.007 [-0.001, 0.013], mean action: 0.269 [-1.202, 1.264], mean observation: 0.099 [-53.195, 15.224], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1007/2000: episode: 9, duration: 5.337s, episode steps: 113, steps per second: 21, episode reward: 0.764, mean reward: 0.007 [-0.000, 0.013], mean action: 0.271 [-1.160, 1.213], mean observation: 0.100 [-53.430, 15.027], loss: 0.000126, mean_squared_error: 0.000253, mean_q: 0.637885\n",
      " 1116/2000: episode: 10, duration: 6.244s, episode steps: 109, steps per second: 17, episode reward: 0.774, mean reward: 0.007 [-0.001, 0.014], mean action: 0.294 [-1.115, 1.301], mean observation: 0.093 [-50.641, 18.685], loss: 0.000166, mean_squared_error: 0.000333, mean_q: 0.631206\n",
      " 1220/2000: episode: 11, duration: 6.321s, episode steps: 104, steps per second: 16, episode reward: 0.729, mean reward: 0.007 [-0.001, 0.013], mean action: 0.268 [-1.123, 1.227], mean observation: 0.090 [-54.081, 15.278], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.626626\n",
      " 1329/2000: episode: 12, duration: 6.196s, episode steps: 109, steps per second: 18, episode reward: 0.749, mean reward: 0.007 [-0.001, 0.015], mean action: 0.252 [-1.175, 1.350], mean observation: 0.098 [-55.242, 15.168], loss: 0.000266, mean_squared_error: 0.000532, mean_q: 0.626867\n",
      " 1430/2000: episode: 13, duration: 6.336s, episode steps: 101, steps per second: 16, episode reward: 0.679, mean reward: 0.007 [-0.002, 0.013], mean action: 0.310 [-1.139, 1.311], mean observation: 0.083 [-55.699, 15.916], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.615412\n",
      " 1535/2000: episode: 14, duration: 6.345s, episode steps: 105, steps per second: 17, episode reward: 0.745, mean reward: 0.007 [-0.002, 0.014], mean action: 0.226 [-1.109, 1.108], mean observation: 0.088 [-53.717, 17.018], loss: 0.000144, mean_squared_error: 0.000287, mean_q: 0.634802\n",
      " 1635/2000: episode: 15, duration: 5.884s, episode steps: 100, steps per second: 17, episode reward: 0.719, mean reward: 0.007 [-0.002, 0.014], mean action: 0.294 [-1.183, 1.267], mean observation: 0.085 [-51.886, 19.082], loss: 0.000174, mean_squared_error: 0.000348, mean_q: 0.624055\n",
      " 1733/2000: episode: 16, duration: 5.816s, episode steps: 98, steps per second: 17, episode reward: 0.714, mean reward: 0.007 [-0.003, 0.013], mean action: 0.308 [-1.064, 1.136], mean observation: 0.087 [-54.125, 16.694], loss: 0.000109, mean_squared_error: 0.000219, mean_q: 0.635064\n",
      " 1828/2000: episode: 17, duration: 5.669s, episode steps: 95, steps per second: 17, episode reward: 0.693, mean reward: 0.007 [-0.003, 0.013], mean action: 0.304 [-1.223, 1.145], mean observation: 0.083 [-53.660, 16.822], loss: 0.000125, mean_squared_error: 0.000249, mean_q: 0.623166\n",
      " 1924/2000: episode: 18, duration: 5.959s, episode steps: 96, steps per second: 16, episode reward: 0.712, mean reward: 0.007 [-0.003, 0.014], mean action: 0.305 [-1.124, 1.158], mean observation: 0.076 [-51.876, 16.938], loss: 0.000087, mean_squared_error: 0.000173, mean_q: 0.622775\n",
      "done, took 107.253 seconds\n",
      "\n",
      "\n",
      "iteration: 44\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 4.691s, episode steps: 95, steps per second: 20, episode reward: 0.688, mean reward: 0.007 [-0.003, 0.014], mean action: 0.266 [-1.233, 1.201], mean observation: 0.082 [-54.028, 17.739], loss: --, mean_squared_error: --, mean_q: --\n",
      "  190/2000: episode: 2, duration: 4.598s, episode steps: 95, steps per second: 21, episode reward: 0.697, mean reward: 0.007 [-0.003, 0.015], mean action: 0.270 [-1.147, 1.178], mean observation: 0.084 [-51.986, 17.940], loss: --, mean_squared_error: --, mean_q: --\n",
      "  284/2000: episode: 3, duration: 4.577s, episode steps: 94, steps per second: 21, episode reward: 0.677, mean reward: 0.007 [-0.004, 0.015], mean action: 0.270 [-1.232, 1.295], mean observation: 0.079 [-56.116, 17.844], loss: --, mean_squared_error: --, mean_q: --\n",
      "  378/2000: episode: 4, duration: 4.587s, episode steps: 94, steps per second: 20, episode reward: 0.677, mean reward: 0.007 [-0.003, 0.015], mean action: 0.257 [-1.128, 1.158], mean observation: 0.079 [-54.274, 18.097], loss: --, mean_squared_error: --, mean_q: --\n",
      "  473/2000: episode: 5, duration: 4.668s, episode steps: 95, steps per second: 20, episode reward: 0.688, mean reward: 0.007 [-0.004, 0.015], mean action: 0.271 [-1.128, 1.153], mean observation: 0.081 [-54.421, 17.838], loss: --, mean_squared_error: --, mean_q: --\n",
      "  568/2000: episode: 6, duration: 4.743s, episode steps: 95, steps per second: 20, episode reward: 0.701, mean reward: 0.007 [-0.004, 0.014], mean action: 0.272 [-1.195, 1.177], mean observation: 0.076 [-49.662, 18.146], loss: --, mean_squared_error: --, mean_q: --\n",
      "  662/2000: episode: 7, duration: 4.598s, episode steps: 94, steps per second: 20, episode reward: 0.691, mean reward: 0.007 [-0.003, 0.013], mean action: 0.252 [-1.156, 1.216], mean observation: 0.071 [-53.782, 17.939], loss: --, mean_squared_error: --, mean_q: --\n",
      "  758/2000: episode: 8, duration: 4.721s, episode steps: 96, steps per second: 20, episode reward: 0.706, mean reward: 0.007 [-0.003, 0.014], mean action: 0.287 [-1.123, 1.199], mean observation: 0.074 [-53.408, 17.859], loss: --, mean_squared_error: --, mean_q: --\n",
      "  852/2000: episode: 9, duration: 4.585s, episode steps: 94, steps per second: 21, episode reward: 0.674, mean reward: 0.007 [-0.003, 0.015], mean action: 0.258 [-1.116, 1.099], mean observation: 0.076 [-56.247, 18.141], loss: --, mean_squared_error: --, mean_q: --\n",
      "  948/2000: episode: 10, duration: 4.746s, episode steps: 96, steps per second: 20, episode reward: 0.720, mean reward: 0.008 [-0.004, 0.014], mean action: 0.300 [-1.201, 1.265], mean observation: 0.079 [-52.289, 17.896], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1042/2000: episode: 11, duration: 4.885s, episode steps: 94, steps per second: 19, episode reward: 0.669, mean reward: 0.007 [-0.003, 0.014], mean action: 0.248 [-1.253, 1.133], mean observation: 0.079 [-56.712, 17.870], loss: 0.000155, mean_squared_error: 0.000309, mean_q: 0.635107\n",
      " 1136/2000: episode: 12, duration: 5.599s, episode steps: 94, steps per second: 17, episode reward: 0.694, mean reward: 0.007 [-0.004, 0.014], mean action: 0.297 [-1.107, 1.190], mean observation: 0.075 [-55.063, 17.271], loss: 0.000187, mean_squared_error: 0.000375, mean_q: 0.634682\n",
      " 1232/2000: episode: 13, duration: 5.680s, episode steps: 96, steps per second: 17, episode reward: 0.701, mean reward: 0.007 [-0.003, 0.013], mean action: 0.269 [-1.218, 1.188], mean observation: 0.077 [-46.202, 17.237], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.616894\n",
      " 1329/2000: episode: 14, duration: 5.716s, episode steps: 97, steps per second: 17, episode reward: 0.701, mean reward: 0.007 [-0.003, 0.014], mean action: 0.256 [-1.230, 1.212], mean observation: 0.072 [-51.624, 16.673], loss: 0.000331, mean_squared_error: 0.000662, mean_q: 0.628772\n",
      " 1440/2000: episode: 15, duration: 6.134s, episode steps: 111, steps per second: 18, episode reward: 0.838, mean reward: 0.008 [-0.001, 0.017], mean action: 0.234 [-1.118, 1.232], mean observation: 0.104 [-41.542, 16.936], loss: 0.000327, mean_squared_error: 0.000653, mean_q: 0.627265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1550/2000: episode: 16, duration: 5.799s, episode steps: 110, steps per second: 19, episode reward: 0.814, mean reward: 0.007 [-0.001, 0.015], mean action: 0.264 [-1.155, 1.165], mean observation: 0.105 [-16.653, 19.038], loss: 0.000153, mean_squared_error: 0.000306, mean_q: 0.632864\n",
      " 1689/2000: episode: 17, duration: 6.725s, episode steps: 139, steps per second: 21, episode reward: -0.778, mean reward: -0.006 [-0.021, 0.011], mean action: 0.294 [-1.192, 1.231], mean observation: 0.024 [-17.779, 21.427], loss: 0.000245, mean_squared_error: 0.000490, mean_q: 0.627381\n",
      " 1836/2000: episode: 18, duration: 6.988s, episode steps: 147, steps per second: 21, episode reward: -0.784, mean reward: -0.005 [-0.021, 0.011], mean action: 0.340 [-1.239, 1.175], mean observation: 0.025 [-41.244, 21.944], loss: 0.000174, mean_squared_error: 0.000347, mean_q: 0.629485\n",
      "done, took 102.345 seconds\n",
      "\n",
      "\n",
      "iteration: 45\n",
      "Training for 2000 steps ...\n",
      "  120/2000: episode: 1, duration: 4.975s, episode steps: 120, steps per second: 24, episode reward: 0.880, mean reward: 0.007 [-0.000, 0.017], mean action: 0.250 [-1.158, 1.147], mean observation: 0.112 [-28.833, 21.676], loss: --, mean_squared_error: --, mean_q: --\n",
      "  241/2000: episode: 2, duration: 5.064s, episode steps: 121, steps per second: 24, episode reward: 0.867, mean reward: 0.007 [-0.000, 0.017], mean action: 0.249 [-1.132, 1.205], mean observation: 0.107 [-48.297, 21.554], loss: --, mean_squared_error: --, mean_q: --\n",
      "  361/2000: episode: 3, duration: 5.004s, episode steps: 120, steps per second: 24, episode reward: 0.866, mean reward: 0.007 [-0.000, 0.017], mean action: 0.247 [-1.238, 1.306], mean observation: 0.115 [-15.171, 21.686], loss: --, mean_squared_error: --, mean_q: --\n",
      "  482/2000: episode: 4, duration: 5.108s, episode steps: 121, steps per second: 24, episode reward: 0.875, mean reward: 0.007 [-0.000, 0.017], mean action: 0.268 [-1.200, 1.206], mean observation: 0.118 [-15.702, 21.553], loss: --, mean_squared_error: --, mean_q: --\n",
      "  600/2000: episode: 5, duration: 4.803s, episode steps: 118, steps per second: 25, episode reward: 0.864, mean reward: 0.007 [-0.000, 0.016], mean action: 0.247 [-1.361, 1.267], mean observation: 0.113 [-15.441, 21.666], loss: --, mean_squared_error: --, mean_q: --\n",
      "  720/2000: episode: 6, duration: 4.980s, episode steps: 120, steps per second: 24, episode reward: 0.886, mean reward: 0.007 [-0.000, 0.016], mean action: 0.243 [-1.217, 1.168], mean observation: 0.116 [-16.628, 21.758], loss: --, mean_squared_error: --, mean_q: --\n",
      "  839/2000: episode: 7, duration: 4.842s, episode steps: 119, steps per second: 25, episode reward: 0.879, mean reward: 0.007 [-0.000, 0.017], mean action: 0.253 [-1.125, 1.186], mean observation: 0.115 [-15.483, 21.528], loss: --, mean_squared_error: --, mean_q: --\n",
      "  957/2000: episode: 8, duration: 4.944s, episode steps: 118, steps per second: 24, episode reward: 0.845, mean reward: 0.007 [-0.000, 0.016], mean action: 0.245 [-1.177, 1.142], mean observation: 0.112 [-19.191, 21.712], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1077/2000: episode: 9, duration: 5.877s, episode steps: 120, steps per second: 20, episode reward: 0.886, mean reward: 0.007 [-0.000, 0.017], mean action: 0.231 [-1.129, 1.152], mean observation: 0.111 [-32.251, 21.748], loss: 0.000164, mean_squared_error: 0.000328, mean_q: 0.625760\n",
      " 1194/2000: episode: 10, duration: 6.509s, episode steps: 117, steps per second: 18, episode reward: 0.850, mean reward: 0.007 [-0.001, 0.017], mean action: 0.213 [-1.107, 1.193], mean observation: 0.110 [-21.138, 17.875], loss: 0.000130, mean_squared_error: 0.000260, mean_q: 0.632426\n",
      " 1306/2000: episode: 11, duration: 6.181s, episode steps: 112, steps per second: 18, episode reward: 0.778, mean reward: 0.007 [-0.001, 0.015], mean action: 0.187 [-1.105, 1.195], mean observation: 0.097 [-36.780, 19.424], loss: 0.000148, mean_squared_error: 0.000296, mean_q: 0.630679\n",
      " 1415/2000: episode: 12, duration: 5.919s, episode steps: 109, steps per second: 18, episode reward: 0.791, mean reward: 0.007 [-0.000, 0.014], mean action: 0.213 [-1.165, 1.201], mean observation: 0.103 [-11.620, 19.972], loss: 0.000161, mean_squared_error: 0.000323, mean_q: 0.629806\n",
      " 1525/2000: episode: 13, duration: 5.984s, episode steps: 110, steps per second: 18, episode reward: 0.782, mean reward: 0.007 [-0.001, 0.015], mean action: 0.246 [-1.111, 1.202], mean observation: 0.102 [-11.647, 19.317], loss: 0.000235, mean_squared_error: 0.000470, mean_q: 0.628852\n",
      " 1636/2000: episode: 14, duration: 5.806s, episode steps: 111, steps per second: 19, episode reward: 0.761, mean reward: 0.007 [-0.001, 0.015], mean action: 0.211 [-1.141, 1.189], mean observation: 0.102 [-12.073, 20.254], loss: 0.000173, mean_squared_error: 0.000345, mean_q: 0.623547\n",
      " 1749/2000: episode: 15, duration: 5.906s, episode steps: 113, steps per second: 19, episode reward: 0.769, mean reward: 0.007 [-0.000, 0.016], mean action: 0.226 [-1.117, 1.221], mean observation: 0.096 [-32.629, 18.636], loss: 0.000274, mean_squared_error: 0.000549, mean_q: 0.630480\n",
      " 1856/2000: episode: 16, duration: 5.727s, episode steps: 107, steps per second: 19, episode reward: 0.720, mean reward: 0.007 [0.000, 0.014], mean action: 0.172 [-1.254, 1.153], mean observation: 0.095 [-11.296, 19.091], loss: 0.000149, mean_squared_error: 0.000298, mean_q: 0.633955\n",
      " 1964/2000: episode: 17, duration: 6.006s, episode steps: 108, steps per second: 18, episode reward: 0.701, mean reward: 0.006 [-0.001, 0.014], mean action: 0.229 [-1.179, 1.182], mean observation: 0.094 [-10.942, 20.184], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.621822\n",
      "done, took 95.818 seconds\n",
      "\n",
      "\n",
      "iteration: 46\n",
      "Training for 2000 steps ...\n",
      "  107/2000: episode: 1, duration: 4.833s, episode steps: 107, steps per second: 22, episode reward: 0.728, mean reward: 0.007 [-0.000, 0.014], mean action: 0.264 [-1.164, 1.224], mean observation: 0.097 [-14.687, 18.013], loss: --, mean_squared_error: --, mean_q: --\n",
      "  214/2000: episode: 2, duration: 4.742s, episode steps: 107, steps per second: 23, episode reward: 0.723, mean reward: 0.007 [-0.000, 0.014], mean action: 0.256 [-1.080, 1.210], mean observation: 0.095 [-14.862, 17.628], loss: --, mean_squared_error: --, mean_q: --\n",
      "  322/2000: episode: 3, duration: 4.710s, episode steps: 108, steps per second: 23, episode reward: 0.735, mean reward: 0.007 [-0.000, 0.013], mean action: 0.261 [-1.140, 1.215], mean observation: 0.098 [-15.034, 17.437], loss: --, mean_squared_error: --, mean_q: --\n",
      "  430/2000: episode: 4, duration: 4.634s, episode steps: 108, steps per second: 23, episode reward: 0.744, mean reward: 0.007 [-0.000, 0.013], mean action: 0.223 [-1.295, 1.234], mean observation: 0.098 [-15.096, 17.900], loss: --, mean_squared_error: --, mean_q: --\n",
      "  537/2000: episode: 5, duration: 4.760s, episode steps: 107, steps per second: 22, episode reward: 0.728, mean reward: 0.007 [-0.000, 0.013], mean action: 0.269 [-1.127, 1.191], mean observation: 0.096 [-15.025, 17.634], loss: --, mean_squared_error: --, mean_q: --\n",
      "  644/2000: episode: 6, duration: 4.731s, episode steps: 107, steps per second: 23, episode reward: 0.723, mean reward: 0.007 [-0.000, 0.014], mean action: 0.242 [-1.123, 1.146], mean observation: 0.095 [-14.733, 17.755], loss: --, mean_squared_error: --, mean_q: --\n",
      "  752/2000: episode: 7, duration: 4.561s, episode steps: 108, steps per second: 24, episode reward: 0.749, mean reward: 0.007 [-0.000, 0.014], mean action: 0.243 [-1.167, 1.191], mean observation: 0.097 [-15.139, 17.994], loss: --, mean_squared_error: --, mean_q: --\n",
      "  859/2000: episode: 8, duration: 4.687s, episode steps: 107, steps per second: 23, episode reward: 0.723, mean reward: 0.007 [-0.000, 0.014], mean action: 0.238 [-1.173, 1.219], mean observation: 0.096 [-15.244, 17.760], loss: --, mean_squared_error: --, mean_q: --\n",
      "  966/2000: episode: 9, duration: 4.590s, episode steps: 107, steps per second: 23, episode reward: 0.721, mean reward: 0.007 [-0.000, 0.013], mean action: 0.228 [-1.289, 1.210], mean observation: 0.095 [-14.927, 17.550], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1074/2000: episode: 10, duration: 5.497s, episode steps: 108, steps per second: 20, episode reward: 0.753, mean reward: 0.007 [-0.000, 0.013], mean action: 0.257 [-1.105, 1.161], mean observation: 0.098 [-14.963, 17.523], loss: 0.000229, mean_squared_error: 0.000458, mean_q: 0.639024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1184/2000: episode: 11, duration: 6.056s, episode steps: 110, steps per second: 18, episode reward: 0.755, mean reward: 0.007 [-0.001, 0.013], mean action: 0.226 [-1.122, 1.243], mean observation: 0.100 [-12.179, 18.996], loss: 0.000116, mean_squared_error: 0.000231, mean_q: 0.622618\n",
      " 1289/2000: episode: 12, duration: 5.667s, episode steps: 105, steps per second: 19, episode reward: 0.713, mean reward: 0.007 [0.000, 0.013], mean action: 0.238 [-1.174, 1.165], mean observation: 0.092 [-18.171, 20.123], loss: 0.000118, mean_squared_error: 0.000235, mean_q: 0.628609\n",
      " 1380/2000: episode: 13, duration: 5.581s, episode steps: 91, steps per second: 16, episode reward: 0.637, mean reward: 0.007 [-0.003, 0.012], mean action: 0.324 [-1.088, 1.167], mean observation: 0.072 [-18.691, 19.366], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.630719\n",
      " 1471/2000: episode: 14, duration: 5.287s, episode steps: 91, steps per second: 17, episode reward: 0.636, mean reward: 0.007 [-0.002, 0.013], mean action: 0.283 [-1.177, 1.139], mean observation: 0.062 [-53.867, 19.118], loss: 0.000177, mean_squared_error: 0.000354, mean_q: 0.620165\n",
      " 1580/2000: episode: 15, duration: 5.749s, episode steps: 109, steps per second: 19, episode reward: 0.819, mean reward: 0.008 [0.000, 0.014], mean action: 0.269 [-1.130, 1.185], mean observation: 0.103 [-14.193, 19.082], loss: 0.000231, mean_squared_error: 0.000462, mean_q: 0.636313\n",
      " 1688/2000: episode: 16, duration: 5.686s, episode steps: 108, steps per second: 19, episode reward: 0.779, mean reward: 0.007 [0.000, 0.013], mean action: 0.290 [-1.089, 1.228], mean observation: 0.101 [-11.185, 18.976], loss: 0.000123, mean_squared_error: 0.000245, mean_q: 0.630649\n",
      " 1783/2000: episode: 17, duration: 5.572s, episode steps: 95, steps per second: 17, episode reward: 0.685, mean reward: 0.007 [-0.001, 0.013], mean action: 0.239 [-1.101, 1.140], mean observation: 0.083 [-10.195, 18.093], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.627652\n",
      " 1878/2000: episode: 18, duration: 5.269s, episode steps: 95, steps per second: 18, episode reward: 0.693, mean reward: 0.007 [-0.001, 0.013], mean action: 0.224 [-1.127, 1.131], mean observation: 0.075 [-54.327, 20.246], loss: 0.000123, mean_squared_error: 0.000246, mean_q: 0.634325\n",
      " 1973/2000: episode: 19, duration: 5.365s, episode steps: 95, steps per second: 18, episode reward: 0.682, mean reward: 0.007 [-0.001, 0.013], mean action: 0.259 [-1.160, 1.218], mean observation: 0.073 [-51.532, 20.108], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.626826\n",
      "done, took 99.588 seconds\n",
      "\n",
      "\n",
      "iteration: 47\n",
      "Training for 2000 steps ...\n",
      "  111/2000: episode: 1, duration: 4.491s, episode steps: 111, steps per second: 25, episode reward: 0.749, mean reward: 0.007 [0.000, 0.013], mean action: 0.232 [-1.081, 1.216], mean observation: 0.103 [-10.773, 19.745], loss: --, mean_squared_error: --, mean_q: --\n",
      "  222/2000: episode: 2, duration: 4.623s, episode steps: 111, steps per second: 24, episode reward: 0.751, mean reward: 0.007 [0.000, 0.013], mean action: 0.210 [-1.264, 1.234], mean observation: 0.101 [-10.965, 19.365], loss: --, mean_squared_error: --, mean_q: --\n",
      "  334/2000: episode: 3, duration: 4.553s, episode steps: 112, steps per second: 25, episode reward: 0.756, mean reward: 0.007 [0.000, 0.013], mean action: 0.218 [-1.173, 1.242], mean observation: 0.103 [-10.771, 19.438], loss: --, mean_squared_error: --, mean_q: --\n",
      "  445/2000: episode: 4, duration: 4.603s, episode steps: 111, steps per second: 24, episode reward: 0.748, mean reward: 0.007 [0.000, 0.013], mean action: 0.209 [-1.220, 1.222], mean observation: 0.101 [-10.463, 19.905], loss: --, mean_squared_error: --, mean_q: --\n",
      "  557/2000: episode: 5, duration: 4.710s, episode steps: 112, steps per second: 24, episode reward: 0.761, mean reward: 0.007 [0.000, 0.013], mean action: 0.216 [-1.146, 1.200], mean observation: 0.101 [-11.175, 19.586], loss: --, mean_squared_error: --, mean_q: --\n",
      "  669/2000: episode: 6, duration: 4.466s, episode steps: 112, steps per second: 25, episode reward: 0.770, mean reward: 0.007 [0.000, 0.013], mean action: 0.190 [-1.128, 1.141], mean observation: 0.104 [-11.216, 19.433], loss: --, mean_squared_error: --, mean_q: --\n",
      "  781/2000: episode: 7, duration: 4.483s, episode steps: 112, steps per second: 25, episode reward: 0.779, mean reward: 0.007 [0.000, 0.013], mean action: 0.208 [-1.220, 1.218], mean observation: 0.106 [-10.810, 19.710], loss: --, mean_squared_error: --, mean_q: --\n",
      "  893/2000: episode: 8, duration: 4.632s, episode steps: 112, steps per second: 24, episode reward: 0.771, mean reward: 0.007 [0.000, 0.013], mean action: 0.195 [-1.162, 1.246], mean observation: 0.104 [-11.227, 19.643], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1004/2000: episode: 9, duration: 4.579s, episode steps: 111, steps per second: 24, episode reward: 0.747, mean reward: 0.007 [0.000, 0.013], mean action: 0.214 [-1.127, 1.193], mean observation: 0.101 [-10.482, 19.795], loss: 0.000144, mean_squared_error: 0.000287, mean_q: 0.646875\n",
      " 1115/2000: episode: 10, duration: 5.399s, episode steps: 111, steps per second: 21, episode reward: 0.754, mean reward: 0.007 [0.000, 0.013], mean action: 0.214 [-1.125, 1.298], mean observation: 0.103 [-11.230, 19.226], loss: 0.000143, mean_squared_error: 0.000287, mean_q: 0.620497\n",
      " 1226/2000: episode: 11, duration: 5.563s, episode steps: 111, steps per second: 20, episode reward: 0.749, mean reward: 0.007 [0.000, 0.013], mean action: 0.202 [-1.247, 1.200], mean observation: 0.101 [-13.879, 16.551], loss: 0.000144, mean_squared_error: 0.000287, mean_q: 0.629076\n",
      " 1336/2000: episode: 12, duration: 5.742s, episode steps: 110, steps per second: 19, episode reward: 0.736, mean reward: 0.007 [0.000, 0.013], mean action: 0.197 [-1.197, 1.195], mean observation: 0.099 [-12.985, 17.956], loss: 0.000168, mean_squared_error: 0.000336, mean_q: 0.630746\n",
      " 1446/2000: episode: 13, duration: 5.729s, episode steps: 110, steps per second: 19, episode reward: 0.739, mean reward: 0.007 [0.000, 0.013], mean action: 0.198 [-1.149, 1.248], mean observation: 0.099 [-11.257, 19.373], loss: 0.000152, mean_squared_error: 0.000304, mean_q: 0.633355\n",
      " 1556/2000: episode: 14, duration: 5.770s, episode steps: 110, steps per second: 19, episode reward: 0.740, mean reward: 0.007 [0.000, 0.012], mean action: 0.227 [-1.159, 1.155], mean observation: 0.100 [-12.470, 18.342], loss: 0.000142, mean_squared_error: 0.000283, mean_q: 0.620333\n",
      " 1665/2000: episode: 15, duration: 5.661s, episode steps: 109, steps per second: 19, episode reward: 0.747, mean reward: 0.007 [0.000, 0.013], mean action: 0.230 [-1.191, 1.191], mean observation: 0.102 [-9.362, 19.830], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.630102\n",
      " 1774/2000: episode: 16, duration: 5.595s, episode steps: 109, steps per second: 19, episode reward: 0.739, mean reward: 0.007 [0.000, 0.013], mean action: 0.187 [-1.208, 1.152], mean observation: 0.099 [-8.870, 20.289], loss: 0.000224, mean_squared_error: 0.000449, mean_q: 0.627005\n",
      " 1881/2000: episode: 17, duration: 5.647s, episode steps: 107, steps per second: 19, episode reward: 0.727, mean reward: 0.007 [0.000, 0.012], mean action: 0.242 [-1.112, 1.184], mean observation: 0.097 [-8.667, 19.995], loss: 0.000175, mean_squared_error: 0.000349, mean_q: 0.624120\n",
      " 1991/2000: episode: 18, duration: 5.688s, episode steps: 110, steps per second: 19, episode reward: 0.752, mean reward: 0.007 [0.000, 0.013], mean action: 0.197 [-1.250, 1.180], mean observation: 0.098 [-12.378, 19.238], loss: 0.000210, mean_squared_error: 0.000419, mean_q: 0.630091\n",
      "done, took 92.674 seconds\n",
      "\n",
      "\n",
      "iteration: 48\n",
      "Training for 2000 steps ...\n",
      "  107/2000: episode: 1, duration: 4.604s, episode steps: 107, steps per second: 23, episode reward: 0.737, mean reward: 0.007 [0.000, 0.014], mean action: 0.214 [-1.184, 1.137], mean observation: 0.097 [-11.368, 19.778], loss: --, mean_squared_error: --, mean_q: --\n",
      "  215/2000: episode: 2, duration: 4.453s, episode steps: 108, steps per second: 24, episode reward: 0.764, mean reward: 0.007 [0.000, 0.014], mean action: 0.199 [-1.184, 1.145], mean observation: 0.098 [-9.401, 20.181], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  323/2000: episode: 3, duration: 4.496s, episode steps: 108, steps per second: 24, episode reward: 0.742, mean reward: 0.007 [0.000, 0.013], mean action: 0.226 [-1.172, 1.136], mean observation: 0.098 [-10.645, 19.628], loss: --, mean_squared_error: --, mean_q: --\n",
      "  430/2000: episode: 4, duration: 4.720s, episode steps: 107, steps per second: 23, episode reward: 0.741, mean reward: 0.007 [0.000, 0.013], mean action: 0.254 [-1.075, 1.203], mean observation: 0.096 [-9.613, 20.464], loss: --, mean_squared_error: --, mean_q: --\n",
      "  537/2000: episode: 5, duration: 4.542s, episode steps: 107, steps per second: 24, episode reward: 0.741, mean reward: 0.007 [0.000, 0.013], mean action: 0.230 [-1.150, 1.173], mean observation: 0.097 [-10.513, 20.147], loss: --, mean_squared_error: --, mean_q: --\n",
      "  645/2000: episode: 6, duration: 4.644s, episode steps: 108, steps per second: 23, episode reward: 0.746, mean reward: 0.007 [0.000, 0.013], mean action: 0.214 [-1.139, 1.234], mean observation: 0.097 [-9.805, 19.750], loss: --, mean_squared_error: --, mean_q: --\n",
      "  753/2000: episode: 7, duration: 4.693s, episode steps: 108, steps per second: 23, episode reward: 0.744, mean reward: 0.007 [0.000, 0.013], mean action: 0.226 [-1.218, 1.240], mean observation: 0.096 [-12.326, 19.694], loss: --, mean_squared_error: --, mean_q: --\n",
      "  861/2000: episode: 8, duration: 4.584s, episode steps: 108, steps per second: 24, episode reward: 0.749, mean reward: 0.007 [0.000, 0.013], mean action: 0.205 [-1.142, 1.190], mean observation: 0.097 [-9.627, 19.824], loss: --, mean_squared_error: --, mean_q: --\n",
      "  968/2000: episode: 9, duration: 4.588s, episode steps: 107, steps per second: 23, episode reward: 0.738, mean reward: 0.007 [0.000, 0.013], mean action: 0.209 [-1.193, 1.245], mean observation: 0.096 [-12.049, 19.996], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1076/2000: episode: 10, duration: 5.208s, episode steps: 108, steps per second: 21, episode reward: 0.742, mean reward: 0.007 [0.000, 0.013], mean action: 0.204 [-1.188, 1.194], mean observation: 0.095 [-16.197, 19.703], loss: 0.000102, mean_squared_error: 0.000203, mean_q: 0.621364\n",
      " 1185/2000: episode: 11, duration: 5.864s, episode steps: 109, steps per second: 19, episode reward: 0.748, mean reward: 0.007 [-0.000, 0.013], mean action: 0.234 [-1.086, 1.165], mean observation: 0.099 [-9.253, 19.920], loss: 0.000171, mean_squared_error: 0.000341, mean_q: 0.620763\n",
      " 1294/2000: episode: 12, duration: 5.760s, episode steps: 109, steps per second: 19, episode reward: 0.738, mean reward: 0.007 [-0.000, 0.014], mean action: 0.238 [-1.144, 1.185], mean observation: 0.096 [-13.038, 18.419], loss: 0.000245, mean_squared_error: 0.000490, mean_q: 0.621491\n",
      " 1403/2000: episode: 13, duration: 5.882s, episode steps: 109, steps per second: 19, episode reward: 0.757, mean reward: 0.007 [0.000, 0.014], mean action: 0.292 [-1.116, 1.125], mean observation: 0.099 [-9.650, 19.963], loss: 0.000204, mean_squared_error: 0.000409, mean_q: 0.633117\n",
      " 1510/2000: episode: 14, duration: 5.832s, episode steps: 107, steps per second: 18, episode reward: 0.725, mean reward: 0.007 [-0.000, 0.013], mean action: 0.221 [-1.126, 1.129], mean observation: 0.093 [-11.792, 19.160], loss: 0.000118, mean_squared_error: 0.000236, mean_q: 0.622217\n",
      " 1618/2000: episode: 15, duration: 5.789s, episode steps: 108, steps per second: 19, episode reward: 0.717, mean reward: 0.007 [0.000, 0.013], mean action: 0.201 [-1.147, 1.146], mean observation: 0.094 [-13.788, 17.872], loss: 0.000276, mean_squared_error: 0.000553, mean_q: 0.619083\n",
      " 1726/2000: episode: 16, duration: 5.788s, episode steps: 108, steps per second: 19, episode reward: 0.736, mean reward: 0.007 [0.000, 0.013], mean action: 0.209 [-1.111, 1.233], mean observation: 0.097 [-12.870, 18.149], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.626030\n",
      " 1831/2000: episode: 17, duration: 5.536s, episode steps: 105, steps per second: 19, episode reward: 0.737, mean reward: 0.007 [0.000, 0.013], mean action: 0.244 [-1.088, 1.146], mean observation: 0.089 [-37.506, 21.308], loss: 0.000088, mean_squared_error: 0.000175, mean_q: 0.623667\n",
      " 1936/2000: episode: 18, duration: 5.864s, episode steps: 105, steps per second: 18, episode reward: 0.753, mean reward: 0.007 [0.001, 0.013], mean action: 0.248 [-1.219, 1.208], mean observation: 0.094 [-40.812, 19.254], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.630401\n",
      "done, took 96.429 seconds\n",
      "\n",
      "\n",
      "iteration: 49\n",
      "Training for 2000 steps ...\n",
      "  106/2000: episode: 1, duration: 4.743s, episode steps: 106, steps per second: 22, episode reward: 0.742, mean reward: 0.007 [0.001, 0.013], mean action: 0.226 [-1.140, 1.202], mean observation: 0.098 [-9.293, 19.112], loss: --, mean_squared_error: --, mean_q: --\n",
      "  213/2000: episode: 2, duration: 4.742s, episode steps: 107, steps per second: 23, episode reward: 0.760, mean reward: 0.007 [0.001, 0.014], mean action: 0.227 [-1.205, 1.155], mean observation: 0.092 [-41.514, 19.303], loss: --, mean_squared_error: --, mean_q: --\n",
      "  319/2000: episode: 3, duration: 4.742s, episode steps: 106, steps per second: 22, episode reward: 0.747, mean reward: 0.007 [0.001, 0.014], mean action: 0.228 [-1.132, 1.176], mean observation: 0.098 [-9.367, 19.118], loss: --, mean_squared_error: --, mean_q: --\n",
      "  425/2000: episode: 4, duration: 4.823s, episode steps: 106, steps per second: 22, episode reward: 0.744, mean reward: 0.007 [0.001, 0.013], mean action: 0.239 [-1.132, 1.113], mean observation: 0.099 [-10.849, 19.156], loss: --, mean_squared_error: --, mean_q: --\n",
      "  530/2000: episode: 5, duration: 4.792s, episode steps: 105, steps per second: 22, episode reward: 0.724, mean reward: 0.007 [0.001, 0.013], mean action: 0.230 [-1.168, 1.173], mean observation: 0.092 [-24.770, 19.073], loss: --, mean_squared_error: --, mean_q: --\n",
      "  635/2000: episode: 6, duration: 4.687s, episode steps: 105, steps per second: 22, episode reward: 0.735, mean reward: 0.007 [0.001, 0.013], mean action: 0.212 [-1.124, 1.119], mean observation: 0.095 [-14.500, 19.431], loss: --, mean_squared_error: --, mean_q: --\n",
      "  741/2000: episode: 7, duration: 4.578s, episode steps: 106, steps per second: 23, episode reward: 0.745, mean reward: 0.007 [0.001, 0.013], mean action: 0.222 [-1.147, 1.135], mean observation: 0.092 [-37.712, 19.068], loss: --, mean_squared_error: --, mean_q: --\n",
      "  848/2000: episode: 8, duration: 4.768s, episode steps: 107, steps per second: 22, episode reward: 0.752, mean reward: 0.007 [0.001, 0.014], mean action: 0.256 [-1.163, 1.256], mean observation: 0.100 [-12.147, 19.228], loss: --, mean_squared_error: --, mean_q: --\n",
      "  954/2000: episode: 9, duration: 4.711s, episode steps: 106, steps per second: 22, episode reward: 0.739, mean reward: 0.007 [0.001, 0.013], mean action: 0.240 [-1.097, 1.235], mean observation: 0.101 [-9.552, 19.125], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1061/2000: episode: 10, duration: 5.296s, episode steps: 107, steps per second: 20, episode reward: 0.760, mean reward: 0.007 [0.001, 0.014], mean action: 0.230 [-1.303, 1.220], mean observation: 0.101 [-11.976, 19.201], loss: 0.000208, mean_squared_error: 0.000417, mean_q: 0.621251\n",
      " 1165/2000: episode: 11, duration: 5.747s, episode steps: 104, steps per second: 18, episode reward: 0.717, mean reward: 0.007 [0.001, 0.013], mean action: 0.248 [-1.152, 1.172], mean observation: 0.096 [-16.390, 19.238], loss: 0.000087, mean_squared_error: 0.000175, mean_q: 0.628551\n",
      " 1270/2000: episode: 12, duration: 5.774s, episode steps: 105, steps per second: 18, episode reward: 0.739, mean reward: 0.007 [0.001, 0.013], mean action: 0.251 [-1.174, 1.192], mean observation: 0.096 [-11.086, 19.233], loss: 0.000099, mean_squared_error: 0.000199, mean_q: 0.630356\n",
      " 1374/2000: episode: 13, duration: 5.633s, episode steps: 104, steps per second: 18, episode reward: 0.721, mean reward: 0.007 [0.001, 0.013], mean action: 0.241 [-1.201, 1.151], mean observation: 0.093 [-9.278, 19.267], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.632598\n",
      " 1479/2000: episode: 14, duration: 5.684s, episode steps: 105, steps per second: 18, episode reward: 0.729, mean reward: 0.007 [0.001, 0.013], mean action: 0.200 [-1.208, 1.158], mean observation: 0.094 [-12.524, 18.970], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.623885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1584/2000: episode: 15, duration: 5.755s, episode steps: 105, steps per second: 18, episode reward: 0.725, mean reward: 0.007 [0.001, 0.012], mean action: 0.229 [-1.164, 1.264], mean observation: 0.093 [-9.256, 18.939], loss: 0.000084, mean_squared_error: 0.000168, mean_q: 0.634827\n",
      " 1688/2000: episode: 16, duration: 5.730s, episode steps: 104, steps per second: 18, episode reward: 0.729, mean reward: 0.007 [0.001, 0.012], mean action: 0.223 [-1.185, 1.229], mean observation: 0.095 [-13.508, 19.086], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.618128\n",
      " 1794/2000: episode: 17, duration: 5.644s, episode steps: 106, steps per second: 19, episode reward: 0.742, mean reward: 0.007 [0.001, 0.014], mean action: 0.244 [-1.163, 1.210], mean observation: 0.096 [-9.180, 19.331], loss: 0.000314, mean_squared_error: 0.000628, mean_q: 0.633805\n",
      " 1901/2000: episode: 18, duration: 5.898s, episode steps: 107, steps per second: 18, episode reward: 0.746, mean reward: 0.007 [0.000, 0.013], mean action: 0.237 [-1.208, 1.196], mean observation: 0.099 [-9.188, 18.957], loss: 0.000186, mean_squared_error: 0.000372, mean_q: 0.629327\n",
      "done, took 98.867 seconds\n",
      "\n",
      "\n",
      "iteration: 50\n",
      "Training for 2000 steps ...\n",
      "  112/2000: episode: 1, duration: 4.832s, episode steps: 112, steps per second: 23, episode reward: 0.736, mean reward: 0.007 [0.000, 0.012], mean action: 0.157 [-1.251, 1.233], mean observation: 0.097 [-14.792, 19.199], loss: --, mean_squared_error: --, mean_q: --\n",
      "  224/2000: episode: 2, duration: 4.824s, episode steps: 112, steps per second: 23, episode reward: 0.739, mean reward: 0.007 [0.000, 0.013], mean action: 0.166 [-1.188, 1.126], mean observation: 0.098 [-10.876, 19.107], loss: --, mean_squared_error: --, mean_q: --\n",
      "  336/2000: episode: 3, duration: 4.941s, episode steps: 112, steps per second: 23, episode reward: 0.730, mean reward: 0.007 [0.000, 0.013], mean action: 0.162 [-1.196, 1.173], mean observation: 0.098 [-13.499, 19.111], loss: --, mean_squared_error: --, mean_q: --\n",
      "  448/2000: episode: 4, duration: 4.904s, episode steps: 112, steps per second: 23, episode reward: 0.734, mean reward: 0.007 [-0.000, 0.012], mean action: 0.168 [-1.189, 1.162], mean observation: 0.096 [-13.826, 19.140], loss: --, mean_squared_error: --, mean_q: --\n",
      "  560/2000: episode: 5, duration: 4.789s, episode steps: 112, steps per second: 23, episode reward: 0.763, mean reward: 0.007 [-0.000, 0.013], mean action: 0.148 [-1.252, 1.250], mean observation: 0.102 [-10.479, 19.288], loss: --, mean_squared_error: --, mean_q: --\n",
      "  673/2000: episode: 6, duration: 4.940s, episode steps: 113, steps per second: 23, episode reward: 0.748, mean reward: 0.007 [-0.000, 0.013], mean action: 0.165 [-1.182, 1.187], mean observation: 0.099 [-12.791, 18.901], loss: --, mean_squared_error: --, mean_q: --\n",
      "  785/2000: episode: 7, duration: 4.953s, episode steps: 112, steps per second: 23, episode reward: 0.740, mean reward: 0.007 [0.000, 0.012], mean action: 0.157 [-1.220, 1.187], mean observation: 0.099 [-14.250, 19.207], loss: --, mean_squared_error: --, mean_q: --\n",
      "  894/2000: episode: 8, duration: 5.042s, episode steps: 109, steps per second: 22, episode reward: 0.719, mean reward: 0.007 [0.000, 0.013], mean action: 0.195 [-1.126, 1.197], mean observation: 0.099 [-16.403, 19.165], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1005/2000: episode: 9, duration: 5.204s, episode steps: 111, steps per second: 21, episode reward: 0.731, mean reward: 0.007 [0.000, 0.013], mean action: 0.193 [-1.112, 1.222], mean observation: 0.099 [-15.166, 19.032], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.561457\n",
      " 1112/2000: episode: 10, duration: 6.071s, episode steps: 107, steps per second: 18, episode reward: 0.731, mean reward: 0.007 [-0.000, 0.012], mean action: 0.196 [-1.155, 1.165], mean observation: 0.101 [-10.904, 19.108], loss: 0.000238, mean_squared_error: 0.000477, mean_q: 0.631614\n",
      " 1219/2000: episode: 11, duration: 5.734s, episode steps: 107, steps per second: 19, episode reward: 0.740, mean reward: 0.007 [0.001, 0.013], mean action: 0.164 [-1.195, 1.167], mean observation: 0.097 [-11.171, 19.021], loss: 0.000111, mean_squared_error: 0.000221, mean_q: 0.624401\n",
      " 1325/2000: episode: 12, duration: 5.939s, episode steps: 106, steps per second: 18, episode reward: 0.735, mean reward: 0.007 [0.000, 0.013], mean action: 0.232 [-1.102, 1.232], mean observation: 0.092 [-41.512, 19.193], loss: 0.000091, mean_squared_error: 0.000182, mean_q: 0.621851\n",
      " 1431/2000: episode: 13, duration: 5.855s, episode steps: 106, steps per second: 18, episode reward: 0.714, mean reward: 0.007 [0.001, 0.013], mean action: 0.209 [-1.191, 1.263], mean observation: 0.098 [-13.053, 19.116], loss: 0.000230, mean_squared_error: 0.000459, mean_q: 0.623334\n",
      " 1538/2000: episode: 14, duration: 6.031s, episode steps: 107, steps per second: 18, episode reward: 0.722, mean reward: 0.007 [-0.000, 0.013], mean action: 0.240 [-1.116, 1.186], mean observation: 0.096 [-11.319, 19.212], loss: 0.000129, mean_squared_error: 0.000257, mean_q: 0.632446\n",
      " 1646/2000: episode: 15, duration: 5.726s, episode steps: 108, steps per second: 19, episode reward: 0.745, mean reward: 0.007 [0.001, 0.013], mean action: 0.245 [-1.130, 1.257], mean observation: 0.096 [-10.392, 19.167], loss: 0.000179, mean_squared_error: 0.000357, mean_q: 0.634745\n",
      " 1754/2000: episode: 16, duration: 5.590s, episode steps: 108, steps per second: 19, episode reward: 0.743, mean reward: 0.007 [0.001, 0.014], mean action: 0.257 [-1.206, 1.216], mean observation: 0.097 [-13.277, 19.091], loss: 0.000094, mean_squared_error: 0.000188, mean_q: 0.630037\n",
      " 1862/2000: episode: 17, duration: 5.694s, episode steps: 108, steps per second: 19, episode reward: 0.734, mean reward: 0.007 [-0.001, 0.013], mean action: 0.206 [-1.268, 1.162], mean observation: 0.100 [-11.910, 18.734], loss: 0.000110, mean_squared_error: 0.000221, mean_q: 0.629756\n",
      " 1970/2000: episode: 18, duration: 5.905s, episode steps: 108, steps per second: 18, episode reward: 0.724, mean reward: 0.007 [0.001, 0.013], mean action: 0.239 [-1.134, 1.115], mean observation: 0.095 [-15.010, 18.963], loss: 0.000127, mean_squared_error: 0.000254, mean_q: 0.626926\n",
      "done, took 98.760 seconds\n",
      "\n",
      "\n",
      "iteration: 51\n",
      "Training for 2000 steps ...\n",
      "  108/2000: episode: 1, duration: 4.656s, episode steps: 108, steps per second: 23, episode reward: 0.751, mean reward: 0.007 [-0.000, 0.014], mean action: 0.239 [-1.142, 1.260], mean observation: 0.096 [-10.162, 19.167], loss: --, mean_squared_error: --, mean_q: --\n",
      "  216/2000: episode: 2, duration: 4.592s, episode steps: 108, steps per second: 24, episode reward: 0.751, mean reward: 0.007 [-0.001, 0.014], mean action: 0.232 [-1.145, 1.174], mean observation: 0.096 [-10.103, 18.960], loss: --, mean_squared_error: --, mean_q: --\n",
      "  324/2000: episode: 3, duration: 4.525s, episode steps: 108, steps per second: 24, episode reward: 0.751, mean reward: 0.007 [-0.000, 0.014], mean action: 0.229 [-1.175, 1.118], mean observation: 0.096 [-12.163, 18.841], loss: --, mean_squared_error: --, mean_q: --\n",
      "  433/2000: episode: 4, duration: 4.550s, episode steps: 109, steps per second: 24, episode reward: 0.758, mean reward: 0.007 [-0.001, 0.014], mean action: 0.233 [-1.220, 1.181], mean observation: 0.097 [-10.278, 19.038], loss: --, mean_squared_error: --, mean_q: --\n",
      "  542/2000: episode: 5, duration: 4.490s, episode steps: 109, steps per second: 24, episode reward: 0.750, mean reward: 0.007 [-0.000, 0.014], mean action: 0.219 [-1.195, 1.116], mean observation: 0.097 [-14.630, 18.968], loss: --, mean_squared_error: --, mean_q: --\n",
      "  651/2000: episode: 6, duration: 4.558s, episode steps: 109, steps per second: 24, episode reward: 0.760, mean reward: 0.007 [-0.000, 0.014], mean action: 0.227 [-1.233, 1.218], mean observation: 0.097 [-9.214, 19.028], loss: --, mean_squared_error: --, mean_q: --\n",
      "  759/2000: episode: 7, duration: 4.528s, episode steps: 108, steps per second: 24, episode reward: 0.748, mean reward: 0.007 [-0.000, 0.014], mean action: 0.240 [-1.097, 1.138], mean observation: 0.094 [-11.066, 19.195], loss: --, mean_squared_error: --, mean_q: --\n",
      "  867/2000: episode: 8, duration: 4.735s, episode steps: 108, steps per second: 23, episode reward: 0.745, mean reward: 0.007 [-0.000, 0.014], mean action: 0.246 [-1.210, 1.245], mean observation: 0.096 [-9.241, 18.892], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  975/2000: episode: 9, duration: 4.719s, episode steps: 108, steps per second: 23, episode reward: 0.748, mean reward: 0.007 [-0.000, 0.014], mean action: 0.232 [-1.239, 1.192], mean observation: 0.096 [-12.253, 19.109], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1083/2000: episode: 10, duration: 5.249s, episode steps: 108, steps per second: 21, episode reward: 0.749, mean reward: 0.007 [-0.000, 0.014], mean action: 0.229 [-1.254, 1.171], mean observation: 0.095 [-13.103, 19.028], loss: 0.000133, mean_squared_error: 0.000267, mean_q: 0.633453\n",
      " 1190/2000: episode: 11, duration: 5.566s, episode steps: 107, steps per second: 19, episode reward: 0.737, mean reward: 0.007 [-0.000, 0.014], mean action: 0.211 [-1.128, 1.266], mean observation: 0.095 [-12.583, 18.995], loss: 0.000149, mean_squared_error: 0.000299, mean_q: 0.625206\n",
      " 1298/2000: episode: 12, duration: 5.379s, episode steps: 108, steps per second: 20, episode reward: 0.755, mean reward: 0.007 [-0.000, 0.013], mean action: 0.215 [-1.140, 1.147], mean observation: 0.098 [-9.387, 18.879], loss: 0.000222, mean_squared_error: 0.000444, mean_q: 0.637027\n",
      " 1407/2000: episode: 13, duration: 5.472s, episode steps: 109, steps per second: 20, episode reward: 0.770, mean reward: 0.007 [-0.000, 0.015], mean action: 0.248 [-1.165, 1.164], mean observation: 0.099 [-16.197, 19.495], loss: 0.000180, mean_squared_error: 0.000361, mean_q: 0.636934\n",
      " 1516/2000: episode: 14, duration: 5.510s, episode steps: 109, steps per second: 20, episode reward: 0.767, mean reward: 0.007 [-0.001, 0.014], mean action: 0.217 [-1.168, 1.170], mean observation: 0.097 [-12.520, 19.544], loss: 0.000079, mean_squared_error: 0.000157, mean_q: 0.633123\n",
      " 1623/2000: episode: 15, duration: 5.643s, episode steps: 107, steps per second: 19, episode reward: 0.757, mean reward: 0.007 [0.000, 0.014], mean action: 0.265 [-1.147, 1.293], mean observation: 0.096 [-13.930, 19.551], loss: 0.000087, mean_squared_error: 0.000175, mean_q: 0.631167\n",
      " 1727/2000: episode: 16, duration: 5.612s, episode steps: 104, steps per second: 19, episode reward: 0.721, mean reward: 0.007 [-0.000, 0.013], mean action: 0.275 [-1.179, 1.167], mean observation: 0.088 [-41.468, 19.439], loss: 0.000070, mean_squared_error: 0.000141, mean_q: 0.627734\n",
      " 1834/2000: episode: 17, duration: 5.917s, episode steps: 107, steps per second: 18, episode reward: 0.762, mean reward: 0.007 [-0.000, 0.014], mean action: 0.298 [-1.077, 1.224], mean observation: 0.096 [-16.710, 19.752], loss: 0.000074, mean_squared_error: 0.000148, mean_q: 0.634851\n",
      " 1943/2000: episode: 18, duration: 5.756s, episode steps: 109, steps per second: 19, episode reward: 0.747, mean reward: 0.007 [-0.000, 0.013], mean action: 0.238 [-1.172, 1.139], mean observation: 0.098 [-12.242, 19.410], loss: 0.000150, mean_squared_error: 0.000301, mean_q: 0.637006\n",
      "done, took 94.864 seconds\n",
      "\n",
      "\n",
      "iteration: 52\n",
      "Training for 2000 steps ...\n",
      "  107/2000: episode: 1, duration: 4.709s, episode steps: 107, steps per second: 23, episode reward: 0.735, mean reward: 0.007 [-0.001, 0.013], mean action: 0.194 [-1.150, 1.213], mean observation: 0.093 [-10.401, 19.670], loss: --, mean_squared_error: --, mean_q: --\n",
      "  213/2000: episode: 2, duration: 4.502s, episode steps: 106, steps per second: 24, episode reward: 0.733, mean reward: 0.007 [-0.001, 0.013], mean action: 0.211 [-1.173, 1.212], mean observation: 0.092 [-13.643, 19.643], loss: --, mean_squared_error: --, mean_q: --\n",
      "  319/2000: episode: 3, duration: 4.492s, episode steps: 106, steps per second: 24, episode reward: 0.731, mean reward: 0.007 [-0.001, 0.013], mean action: 0.219 [-1.120, 1.203], mean observation: 0.092 [-13.731, 19.471], loss: --, mean_squared_error: --, mean_q: --\n",
      "  425/2000: episode: 4, duration: 4.565s, episode steps: 106, steps per second: 23, episode reward: 0.734, mean reward: 0.007 [-0.001, 0.014], mean action: 0.196 [-1.199, 1.215], mean observation: 0.092 [-11.143, 19.805], loss: --, mean_squared_error: --, mean_q: --\n",
      "  531/2000: episode: 5, duration: 4.563s, episode steps: 106, steps per second: 23, episode reward: 0.734, mean reward: 0.007 [-0.001, 0.013], mean action: 0.200 [-1.242, 1.214], mean observation: 0.093 [-10.999, 19.630], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 6, duration: 4.613s, episode steps: 106, steps per second: 23, episode reward: 0.734, mean reward: 0.007 [-0.001, 0.014], mean action: 0.213 [-1.168, 1.168], mean observation: 0.091 [-12.134, 19.560], loss: --, mean_squared_error: --, mean_q: --\n",
      "  743/2000: episode: 7, duration: 4.754s, episode steps: 106, steps per second: 22, episode reward: 0.729, mean reward: 0.007 [-0.001, 0.013], mean action: 0.210 [-1.253, 1.202], mean observation: 0.092 [-9.403, 19.464], loss: --, mean_squared_error: --, mean_q: --\n",
      "  849/2000: episode: 8, duration: 4.575s, episode steps: 106, steps per second: 23, episode reward: 0.728, mean reward: 0.007 [-0.001, 0.013], mean action: 0.211 [-1.064, 1.201], mean observation: 0.093 [-9.280, 19.614], loss: --, mean_squared_error: --, mean_q: --\n",
      "  955/2000: episode: 9, duration: 4.439s, episode steps: 106, steps per second: 24, episode reward: 0.734, mean reward: 0.007 [-0.001, 0.013], mean action: 0.221 [-1.132, 1.114], mean observation: 0.093 [-9.359, 19.607], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1060/2000: episode: 10, duration: 5.169s, episode steps: 105, steps per second: 20, episode reward: 0.725, mean reward: 0.007 [-0.001, 0.013], mean action: 0.222 [-1.230, 1.276], mean observation: 0.091 [-9.437, 19.543], loss: 0.000138, mean_squared_error: 0.000275, mean_q: 0.642917\n",
      " 1157/2000: episode: 11, duration: 5.257s, episode steps: 97, steps per second: 18, episode reward: 0.717, mean reward: 0.007 [-0.001, 0.013], mean action: 0.223 [-1.138, 1.178], mean observation: 0.083 [-9.959, 18.289], loss: 0.000183, mean_squared_error: 0.000365, mean_q: 0.628589\n",
      " 1257/2000: episode: 12, duration: 5.605s, episode steps: 100, steps per second: 18, episode reward: 0.743, mean reward: 0.007 [-0.001, 0.013], mean action: 0.210 [-1.191, 1.193], mean observation: 0.085 [-16.710, 18.121], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.646394\n",
      " 1357/2000: episode: 13, duration: 5.897s, episode steps: 100, steps per second: 17, episode reward: 0.726, mean reward: 0.007 [-0.002, 0.013], mean action: 0.188 [-1.255, 1.266], mean observation: 0.083 [-18.675, 18.085], loss: 0.000220, mean_squared_error: 0.000440, mean_q: 0.640487\n",
      " 1457/2000: episode: 14, duration: 5.809s, episode steps: 100, steps per second: 17, episode reward: 0.709, mean reward: 0.007 [-0.002, 0.013], mean action: 0.170 [-1.130, 1.202], mean observation: 0.084 [-11.771, 18.051], loss: 0.000089, mean_squared_error: 0.000179, mean_q: 0.637327\n",
      " 1553/2000: episode: 15, duration: 5.480s, episode steps: 96, steps per second: 18, episode reward: 0.657, mean reward: 0.007 [-0.002, 0.013], mean action: 0.171 [-1.224, 1.156], mean observation: 0.072 [-41.359, 21.380], loss: 0.000080, mean_squared_error: 0.000159, mean_q: 0.650345\n",
      " 1651/2000: episode: 16, duration: 5.536s, episode steps: 98, steps per second: 18, episode reward: 0.694, mean reward: 0.007 [-0.002, 0.013], mean action: 0.176 [-1.125, 1.250], mean observation: 0.088 [-11.135, 18.049], loss: 0.000216, mean_squared_error: 0.000432, mean_q: 0.635434\n",
      " 1753/2000: episode: 17, duration: 5.787s, episode steps: 102, steps per second: 18, episode reward: 0.742, mean reward: 0.007 [-0.002, 0.013], mean action: 0.173 [-1.170, 1.213], mean observation: 0.087 [-17.407, 17.879], loss: 0.000096, mean_squared_error: 0.000191, mean_q: 0.633736\n",
      " 1850/2000: episode: 18, duration: 5.544s, episode steps: 97, steps per second: 17, episode reward: 0.681, mean reward: 0.007 [-0.002, 0.013], mean action: 0.167 [-1.170, 1.208], mean observation: 0.079 [-19.015, 18.174], loss: 0.000099, mean_squared_error: 0.000197, mean_q: 0.632446\n",
      " 1946/2000: episode: 19, duration: 5.843s, episode steps: 96, steps per second: 16, episode reward: 0.646, mean reward: 0.007 [-0.002, 0.013], mean action: 0.148 [-1.157, 1.143], mean observation: 0.078 [-48.252, 18.228], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.643342\n",
      "done, took 100.077 seconds\n",
      "\n",
      "\n",
      "iteration: 53\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  104/2000: episode: 1, duration: 5.214s, episode steps: 104, steps per second: 20, episode reward: 0.751, mean reward: 0.007 [-0.001, 0.014], mean action: 0.156 [-1.134, 1.159], mean observation: 0.088 [-10.166, 17.795], loss: --, mean_squared_error: --, mean_q: --\n",
      "  209/2000: episode: 2, duration: 5.299s, episode steps: 105, steps per second: 20, episode reward: 0.771, mean reward: 0.007 [-0.001, 0.014], mean action: 0.175 [-1.131, 1.206], mean observation: 0.089 [-14.975, 17.915], loss: --, mean_squared_error: --, mean_q: --\n",
      "  313/2000: episode: 3, duration: 5.124s, episode steps: 104, steps per second: 20, episode reward: 0.746, mean reward: 0.007 [-0.002, 0.013], mean action: 0.190 [-1.104, 1.199], mean observation: 0.088 [-10.188, 18.033], loss: --, mean_squared_error: --, mean_q: --\n",
      "  419/2000: episode: 4, duration: 5.366s, episode steps: 106, steps per second: 20, episode reward: 0.781, mean reward: 0.007 [-0.002, 0.014], mean action: 0.168 [-1.239, 1.140], mean observation: 0.092 [-13.293, 17.797], loss: --, mean_squared_error: --, mean_q: --\n",
      "  523/2000: episode: 5, duration: 5.110s, episode steps: 104, steps per second: 20, episode reward: 0.750, mean reward: 0.007 [-0.001, 0.013], mean action: 0.145 [-1.174, 1.151], mean observation: 0.088 [-10.020, 17.928], loss: --, mean_squared_error: --, mean_q: --\n",
      "  627/2000: episode: 6, duration: 5.190s, episode steps: 104, steps per second: 20, episode reward: 0.752, mean reward: 0.007 [-0.001, 0.013], mean action: 0.198 [-1.098, 1.226], mean observation: 0.088 [-10.105, 17.701], loss: --, mean_squared_error: --, mean_q: --\n",
      "  732/2000: episode: 7, duration: 5.383s, episode steps: 105, steps per second: 20, episode reward: 0.773, mean reward: 0.007 [-0.002, 0.014], mean action: 0.167 [-1.175, 1.200], mean observation: 0.090 [-10.218, 18.101], loss: --, mean_squared_error: --, mean_q: --\n",
      "  837/2000: episode: 8, duration: 5.359s, episode steps: 105, steps per second: 20, episode reward: 0.773, mean reward: 0.007 [-0.001, 0.014], mean action: 0.172 [-1.087, 1.156], mean observation: 0.089 [-13.175, 17.913], loss: --, mean_squared_error: --, mean_q: --\n",
      "  941/2000: episode: 9, duration: 5.163s, episode steps: 104, steps per second: 20, episode reward: 0.760, mean reward: 0.007 [-0.001, 0.013], mean action: 0.171 [-1.115, 1.304], mean observation: 0.089 [-12.080, 17.736], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1045/2000: episode: 10, duration: 5.697s, episode steps: 104, steps per second: 18, episode reward: 0.760, mean reward: 0.007 [-0.002, 0.014], mean action: 0.162 [-1.227, 1.222], mean observation: 0.090 [-10.107, 18.166], loss: 0.000111, mean_squared_error: 0.000221, mean_q: 0.644078\n",
      " 1135/2000: episode: 11, duration: 5.770s, episode steps: 90, steps per second: 16, episode reward: 0.586, mean reward: 0.007 [-0.002, 0.014], mean action: 0.122 [-1.106, 1.123], mean observation: 0.054 [-42.544, 17.992], loss: 0.000085, mean_squared_error: 0.000170, mean_q: 0.627431\n",
      " 1232/2000: episode: 12, duration: 5.738s, episode steps: 97, steps per second: 17, episode reward: 0.653, mean reward: 0.007 [-0.002, 0.013], mean action: 0.128 [-1.179, 1.193], mean observation: 0.076 [-12.152, 17.928], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.637807\n",
      " 1330/2000: episode: 13, duration: 5.918s, episode steps: 98, steps per second: 17, episode reward: 0.667, mean reward: 0.007 [-0.002, 0.013], mean action: 0.120 [-1.193, 1.212], mean observation: 0.077 [-12.404, 17.697], loss: 0.000160, mean_squared_error: 0.000320, mean_q: 0.641996\n",
      " 1428/2000: episode: 14, duration: 5.723s, episode steps: 98, steps per second: 17, episode reward: 0.696, mean reward: 0.007 [-0.002, 0.013], mean action: 0.201 [-1.117, 1.240], mean observation: 0.078 [-12.717, 17.585], loss: 0.000098, mean_squared_error: 0.000195, mean_q: 0.638862\n",
      " 1526/2000: episode: 15, duration: 5.600s, episode steps: 98, steps per second: 17, episode reward: 0.726, mean reward: 0.007 [-0.001, 0.013], mean action: 0.222 [-1.178, 1.269], mean observation: 0.083 [-10.387, 17.971], loss: 0.000104, mean_squared_error: 0.000207, mean_q: 0.638987\n",
      " 1617/2000: episode: 16, duration: 5.455s, episode steps: 91, steps per second: 17, episode reward: 0.643, mean reward: 0.007 [-0.001, 0.014], mean action: 0.205 [-1.132, 1.134], mean observation: 0.060 [-47.350, 17.833], loss: 0.000119, mean_squared_error: 0.000237, mean_q: 0.642700\n",
      " 1711/2000: episode: 17, duration: 5.528s, episode steps: 94, steps per second: 17, episode reward: 0.676, mean reward: 0.007 [-0.001, 0.014], mean action: 0.192 [-1.099, 1.201], mean observation: 0.069 [-42.971, 18.019], loss: 0.000171, mean_squared_error: 0.000341, mean_q: 0.646102\n",
      " 1809/2000: episode: 18, duration: 5.663s, episode steps: 98, steps per second: 17, episode reward: 0.716, mean reward: 0.007 [-0.002, 0.013], mean action: 0.186 [-1.147, 1.164], mean observation: 0.082 [-10.405, 17.941], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.631612\n",
      " 1906/2000: episode: 19, duration: 5.817s, episode steps: 97, steps per second: 17, episode reward: 0.677, mean reward: 0.007 [-0.001, 0.013], mean action: 0.130 [-1.311, 1.147], mean observation: 0.073 [-44.343, 17.746], loss: 0.000077, mean_squared_error: 0.000154, mean_q: 0.636871\n",
      "done, took 109.895 seconds\n",
      "\n",
      "\n",
      "iteration: 54\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 4.916s, episode steps: 95, steps per second: 19, episode reward: 0.681, mean reward: 0.007 [-0.002, 0.014], mean action: 0.137 [-1.131, 1.208], mean observation: 0.075 [-29.766, 18.027], loss: --, mean_squared_error: --, mean_q: --\n",
      "  189/2000: episode: 2, duration: 4.835s, episode steps: 94, steps per second: 19, episode reward: 0.669, mean reward: 0.007 [-0.002, 0.014], mean action: 0.151 [-1.141, 1.150], mean observation: 0.068 [-38.693, 17.923], loss: --, mean_squared_error: --, mean_q: --\n",
      "  283/2000: episode: 3, duration: 4.928s, episode steps: 94, steps per second: 19, episode reward: 0.666, mean reward: 0.007 [-0.002, 0.014], mean action: 0.119 [-1.152, 1.104], mean observation: 0.074 [-17.961, 17.738], loss: --, mean_squared_error: --, mean_q: --\n",
      "  377/2000: episode: 4, duration: 4.928s, episode steps: 94, steps per second: 19, episode reward: 0.673, mean reward: 0.007 [-0.002, 0.014], mean action: 0.160 [-1.119, 1.193], mean observation: 0.067 [-43.835, 17.723], loss: --, mean_squared_error: --, mean_q: --\n",
      "  471/2000: episode: 5, duration: 4.796s, episode steps: 94, steps per second: 20, episode reward: 0.666, mean reward: 0.007 [-0.002, 0.014], mean action: 0.159 [-1.173, 1.183], mean observation: 0.072 [-36.305, 17.991], loss: --, mean_squared_error: --, mean_q: --\n",
      "  566/2000: episode: 6, duration: 4.822s, episode steps: 95, steps per second: 20, episode reward: 0.681, mean reward: 0.007 [-0.002, 0.014], mean action: 0.151 [-1.111, 1.212], mean observation: 0.072 [-37.516, 18.038], loss: --, mean_squared_error: --, mean_q: --\n",
      "  660/2000: episode: 7, duration: 4.726s, episode steps: 94, steps per second: 20, episode reward: 0.667, mean reward: 0.007 [-0.002, 0.014], mean action: 0.161 [-1.113, 1.163], mean observation: 0.073 [-32.800, 17.768], loss: --, mean_squared_error: --, mean_q: --\n",
      "  754/2000: episode: 8, duration: 4.742s, episode steps: 94, steps per second: 20, episode reward: 0.668, mean reward: 0.007 [-0.002, 0.014], mean action: 0.132 [-1.215, 1.201], mean observation: 0.070 [-40.971, 17.758], loss: --, mean_squared_error: --, mean_q: --\n",
      "  848/2000: episode: 9, duration: 4.774s, episode steps: 94, steps per second: 20, episode reward: 0.671, mean reward: 0.007 [-0.002, 0.014], mean action: 0.128 [-1.146, 1.253], mean observation: 0.073 [-32.189, 18.030], loss: --, mean_squared_error: --, mean_q: --\n",
      "  943/2000: episode: 10, duration: 4.976s, episode steps: 95, steps per second: 19, episode reward: 0.678, mean reward: 0.007 [-0.002, 0.014], mean action: 0.140 [-1.096, 1.239], mean observation: 0.075 [-24.966, 18.028], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1037/2000: episode: 11, duration: 5.161s, episode steps: 94, steps per second: 18, episode reward: 0.671, mean reward: 0.007 [-0.002, 0.014], mean action: 0.136 [-1.150, 1.154], mean observation: 0.077 [-15.391, 17.930], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.640238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1133/2000: episode: 12, duration: 5.381s, episode steps: 96, steps per second: 18, episode reward: 0.704, mean reward: 0.007 [-0.002, 0.014], mean action: 0.126 [-1.138, 1.149], mean observation: 0.085 [-10.421, 17.854], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.644375\n",
      " 1231/2000: episode: 13, duration: 5.522s, episode steps: 98, steps per second: 18, episode reward: 0.715, mean reward: 0.007 [-0.001, 0.013], mean action: 0.218 [-1.092, 1.163], mean observation: 0.082 [-10.457, 18.042], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.640513\n",
      " 1328/2000: episode: 14, duration: 5.748s, episode steps: 97, steps per second: 17, episode reward: 0.704, mean reward: 0.007 [-0.002, 0.014], mean action: 0.223 [-1.116, 1.226], mean observation: 0.080 [-10.388, 18.031], loss: 0.000161, mean_squared_error: 0.000322, mean_q: 0.640127\n",
      " 1422/2000: episode: 15, duration: 5.456s, episode steps: 94, steps per second: 17, episode reward: 0.692, mean reward: 0.007 [-0.002, 0.014], mean action: 0.212 [-1.182, 1.246], mean observation: 0.075 [-18.663, 17.993], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.635413\n",
      " 1517/2000: episode: 16, duration: 5.580s, episode steps: 95, steps per second: 17, episode reward: 0.673, mean reward: 0.007 [-0.002, 0.014], mean action: 0.187 [-1.127, 1.121], mean observation: 0.077 [-10.396, 17.930], loss: 0.000152, mean_squared_error: 0.000305, mean_q: 0.644060\n",
      " 1611/2000: episode: 17, duration: 5.194s, episode steps: 94, steps per second: 18, episode reward: 0.686, mean reward: 0.007 [-0.002, 0.014], mean action: 0.186 [-1.160, 1.213], mean observation: 0.078 [-10.119, 18.019], loss: 0.000159, mean_squared_error: 0.000319, mean_q: 0.643270\n",
      " 1706/2000: episode: 18, duration: 5.462s, episode steps: 95, steps per second: 17, episode reward: 0.696, mean reward: 0.007 [-0.002, 0.014], mean action: 0.165 [-1.075, 1.222], mean observation: 0.079 [-10.139, 17.935], loss: 0.000074, mean_squared_error: 0.000147, mean_q: 0.644110\n",
      " 1799/2000: episode: 19, duration: 5.409s, episode steps: 93, steps per second: 17, episode reward: 0.660, mean reward: 0.007 [-0.001, 0.013], mean action: 0.177 [-1.097, 1.187], mean observation: 0.072 [-49.007, 18.068], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.630796\n",
      " 1894/2000: episode: 20, duration: 5.487s, episode steps: 95, steps per second: 17, episode reward: 0.680, mean reward: 0.007 [-0.001, 0.014], mean action: 0.167 [-1.125, 1.272], mean observation: 0.079 [-10.849, 19.331], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.641002\n",
      " 1992/2000: episode: 21, duration: 5.426s, episode steps: 98, steps per second: 18, episode reward: 0.711, mean reward: 0.007 [-0.001, 0.014], mean action: 0.171 [-1.197, 1.188], mean observation: 0.085 [-10.072, 18.910], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.643880\n",
      "done, took 108.950 seconds\n",
      "\n",
      "\n",
      "iteration: 55\n",
      "Training for 2000 steps ...\n",
      "   97/2000: episode: 1, duration: 4.567s, episode steps: 97, steps per second: 21, episode reward: 0.688, mean reward: 0.007 [-0.001, 0.013], mean action: 0.179 [-1.119, 1.240], mean observation: 0.081 [-11.016, 17.963], loss: --, mean_squared_error: --, mean_q: --\n",
      "  195/2000: episode: 2, duration: 4.819s, episode steps: 98, steps per second: 20, episode reward: 0.687, mean reward: 0.007 [-0.001, 0.013], mean action: 0.186 [-1.152, 1.207], mean observation: 0.084 [-10.310, 17.848], loss: --, mean_squared_error: --, mean_q: --\n",
      "  293/2000: episode: 3, duration: 4.731s, episode steps: 98, steps per second: 21, episode reward: 0.690, mean reward: 0.007 [-0.001, 0.013], mean action: 0.155 [-1.130, 1.144], mean observation: 0.080 [-15.079, 17.884], loss: --, mean_squared_error: --, mean_q: --\n",
      "  391/2000: episode: 4, duration: 4.808s, episode steps: 98, steps per second: 20, episode reward: 0.695, mean reward: 0.007 [-0.001, 0.013], mean action: 0.172 [-1.098, 1.124], mean observation: 0.084 [-10.256, 18.019], loss: --, mean_squared_error: --, mean_q: --\n",
      "  488/2000: episode: 5, duration: 4.791s, episode steps: 97, steps per second: 20, episode reward: 0.685, mean reward: 0.007 [-0.001, 0.013], mean action: 0.176 [-1.125, 1.294], mean observation: 0.080 [-14.561, 17.867], loss: --, mean_squared_error: --, mean_q: --\n",
      "  585/2000: episode: 6, duration: 4.794s, episode steps: 97, steps per second: 20, episode reward: 0.685, mean reward: 0.007 [-0.001, 0.013], mean action: 0.169 [-1.110, 1.168], mean observation: 0.080 [-12.876, 18.029], loss: --, mean_squared_error: --, mean_q: --\n",
      "  682/2000: episode: 7, duration: 4.804s, episode steps: 97, steps per second: 20, episode reward: 0.681, mean reward: 0.007 [-0.001, 0.013], mean action: 0.170 [-1.192, 1.210], mean observation: 0.077 [-17.130, 17.803], loss: --, mean_squared_error: --, mean_q: --\n",
      "  780/2000: episode: 8, duration: 4.711s, episode steps: 98, steps per second: 21, episode reward: 0.693, mean reward: 0.007 [-0.001, 0.013], mean action: 0.158 [-1.140, 1.140], mean observation: 0.084 [-10.198, 18.005], loss: --, mean_squared_error: --, mean_q: --\n",
      "  878/2000: episode: 9, duration: 4.610s, episode steps: 98, steps per second: 21, episode reward: 0.700, mean reward: 0.007 [-0.001, 0.014], mean action: 0.181 [-1.100, 1.229], mean observation: 0.085 [-10.413, 17.876], loss: --, mean_squared_error: --, mean_q: --\n",
      "  975/2000: episode: 10, duration: 4.646s, episode steps: 97, steps per second: 21, episode reward: 0.681, mean reward: 0.007 [-0.001, 0.013], mean action: 0.168 [-1.271, 1.184], mean observation: 0.081 [-11.982, 17.775], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1072/2000: episode: 11, duration: 5.470s, episode steps: 97, steps per second: 18, episode reward: 0.687, mean reward: 0.007 [-0.001, 0.013], mean action: 0.173 [-1.160, 1.209], mean observation: 0.086 [-10.296, 18.090], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.629906\n",
      " 1170/2000: episode: 12, duration: 5.445s, episode steps: 98, steps per second: 18, episode reward: 0.719, mean reward: 0.007 [-0.002, 0.014], mean action: 0.176 [-1.197, 1.165], mean observation: 0.085 [-10.328, 17.992], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.640993\n",
      " 1270/2000: episode: 13, duration: 5.524s, episode steps: 100, steps per second: 18, episode reward: 0.705, mean reward: 0.007 [-0.001, 0.014], mean action: 0.182 [-1.100, 1.169], mean observation: 0.086 [-10.256, 18.136], loss: 0.000162, mean_squared_error: 0.000325, mean_q: 0.634133\n",
      " 1368/2000: episode: 14, duration: 5.608s, episode steps: 98, steps per second: 17, episode reward: 0.682, mean reward: 0.007 [-0.001, 0.013], mean action: 0.132 [-1.167, 1.168], mean observation: 0.078 [-16.620, 18.202], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.639410\n",
      " 1471/2000: episode: 15, duration: 6.100s, episode steps: 103, steps per second: 17, episode reward: 0.674, mean reward: 0.007 [-0.003, 0.013], mean action: 0.108 [-1.190, 1.164], mean observation: 0.082 [-21.503, 17.476], loss: 0.000101, mean_squared_error: 0.000203, mean_q: 0.643554\n",
      " 1588/2000: episode: 16, duration: 6.501s, episode steps: 117, steps per second: 18, episode reward: 0.672, mean reward: 0.006 [-0.003, 0.013], mean action: 0.106 [-1.199, 1.274], mean observation: 0.084 [-25.880, 17.535], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.649032\n",
      " 1695/2000: episode: 17, duration: 5.578s, episode steps: 107, steps per second: 19, episode reward: 0.685, mean reward: 0.006 [-0.002, 0.013], mean action: 0.072 [-1.123, 1.218], mean observation: 0.085 [-21.179, 17.725], loss: 0.000155, mean_squared_error: 0.000309, mean_q: 0.645367\n",
      " 1798/2000: episode: 18, duration: 5.414s, episode steps: 103, steps per second: 19, episode reward: 0.728, mean reward: 0.007 [-0.002, 0.014], mean action: 0.107 [-1.230, 1.144], mean observation: 0.088 [-18.684, 17.930], loss: 0.000262, mean_squared_error: 0.000524, mean_q: 0.645342\n",
      " 1897/2000: episode: 19, duration: 5.408s, episode steps: 99, steps per second: 18, episode reward: 0.662, mean reward: 0.007 [-0.003, 0.014], mean action: 0.188 [-1.118, 1.255], mean observation: 0.080 [-18.943, 18.091], loss: 0.000151, mean_squared_error: 0.000302, mean_q: 0.645809\n",
      " 1997/2000: episode: 20, duration: 5.432s, episode steps: 100, steps per second: 18, episode reward: 0.708, mean reward: 0.007 [-0.002, 0.013], mean action: 0.131 [-1.125, 1.154], mean observation: 0.087 [-18.758, 18.020], loss: 0.000262, mean_squared_error: 0.000524, mean_q: 0.648050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 104.026 seconds\n",
      "\n",
      "\n",
      "iteration: 56\n",
      "Training for 2000 steps ...\n",
      "   99/2000: episode: 1, duration: 4.536s, episode steps: 99, steps per second: 22, episode reward: 0.690, mean reward: 0.007 [-0.002, 0.013], mean action: 0.123 [-1.093, 1.147], mean observation: 0.084 [-18.466, 17.831], loss: --, mean_squared_error: --, mean_q: --\n",
      "  198/2000: episode: 2, duration: 4.774s, episode steps: 99, steps per second: 21, episode reward: 0.687, mean reward: 0.007 [-0.002, 0.013], mean action: 0.139 [-1.148, 1.169], mean observation: 0.082 [-18.530, 18.117], loss: --, mean_squared_error: --, mean_q: --\n",
      "  298/2000: episode: 3, duration: 4.657s, episode steps: 100, steps per second: 21, episode reward: 0.703, mean reward: 0.007 [-0.002, 0.013], mean action: 0.125 [-1.260, 1.161], mean observation: 0.085 [-19.554, 17.748], loss: --, mean_squared_error: --, mean_q: --\n",
      "  398/2000: episode: 4, duration: 4.479s, episode steps: 100, steps per second: 22, episode reward: 0.700, mean reward: 0.007 [-0.002, 0.013], mean action: 0.124 [-1.208, 1.133], mean observation: 0.085 [-18.551, 18.015], loss: --, mean_squared_error: --, mean_q: --\n",
      "  498/2000: episode: 5, duration: 4.538s, episode steps: 100, steps per second: 22, episode reward: 0.705, mean reward: 0.007 [-0.002, 0.013], mean action: 0.134 [-1.129, 1.230], mean observation: 0.084 [-18.205, 18.023], loss: --, mean_squared_error: --, mean_q: --\n",
      "  597/2000: episode: 6, duration: 4.515s, episode steps: 99, steps per second: 22, episode reward: 0.689, mean reward: 0.007 [-0.002, 0.013], mean action: 0.113 [-1.268, 1.114], mean observation: 0.082 [-18.425, 17.958], loss: --, mean_squared_error: --, mean_q: --\n",
      "  696/2000: episode: 7, duration: 4.559s, episode steps: 99, steps per second: 22, episode reward: 0.688, mean reward: 0.007 [-0.002, 0.013], mean action: 0.124 [-1.167, 1.174], mean observation: 0.085 [-17.953, 18.034], loss: --, mean_squared_error: --, mean_q: --\n",
      "  796/2000: episode: 8, duration: 4.406s, episode steps: 100, steps per second: 23, episode reward: 0.700, mean reward: 0.007 [-0.002, 0.013], mean action: 0.122 [-1.260, 1.235], mean observation: 0.084 [-18.204, 17.992], loss: --, mean_squared_error: --, mean_q: --\n",
      "  895/2000: episode: 9, duration: 4.639s, episode steps: 99, steps per second: 21, episode reward: 0.689, mean reward: 0.007 [-0.002, 0.013], mean action: 0.144 [-1.136, 1.182], mean observation: 0.086 [-18.159, 18.172], loss: --, mean_squared_error: --, mean_q: --\n",
      "  994/2000: episode: 10, duration: 4.426s, episode steps: 99, steps per second: 22, episode reward: 0.694, mean reward: 0.007 [-0.002, 0.013], mean action: 0.122 [-1.233, 1.155], mean observation: 0.084 [-18.563, 18.043], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1095/2000: episode: 11, duration: 5.260s, episode steps: 101, steps per second: 19, episode reward: 0.718, mean reward: 0.007 [-0.002, 0.014], mean action: 0.075 [-1.211, 1.107], mean observation: 0.088 [-17.931, 17.964], loss: 0.000214, mean_squared_error: 0.000428, mean_q: 0.646003\n",
      " 1195/2000: episode: 12, duration: 5.304s, episode steps: 100, steps per second: 19, episode reward: 0.704, mean reward: 0.007 [-0.002, 0.014], mean action: 0.119 [-1.116, 1.163], mean observation: 0.086 [-18.773, 17.968], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.641930\n",
      " 1294/2000: episode: 13, duration: 5.530s, episode steps: 99, steps per second: 18, episode reward: 0.706, mean reward: 0.007 [-0.002, 0.014], mean action: 0.115 [-1.360, 1.173], mean observation: 0.082 [-17.339, 17.832], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.640681\n",
      " 1393/2000: episode: 14, duration: 5.346s, episode steps: 99, steps per second: 19, episode reward: 0.694, mean reward: 0.007 [-0.002, 0.014], mean action: 0.097 [-1.176, 1.176], mean observation: 0.087 [-15.753, 17.999], loss: 0.000071, mean_squared_error: 0.000143, mean_q: 0.625578\n",
      " 1495/2000: episode: 15, duration: 5.256s, episode steps: 102, steps per second: 19, episode reward: 0.710, mean reward: 0.007 [-0.003, 0.014], mean action: 0.061 [-1.265, 1.196], mean observation: 0.087 [-18.005, 18.219], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.634136\n",
      " 1598/2000: episode: 16, duration: 5.425s, episode steps: 103, steps per second: 19, episode reward: 0.700, mean reward: 0.007 [-0.003, 0.014], mean action: 0.067 [-1.181, 1.221], mean observation: 0.086 [-18.140, 18.101], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.647691\n",
      " 1699/2000: episode: 17, duration: 5.364s, episode steps: 101, steps per second: 19, episode reward: 0.695, mean reward: 0.007 [-0.003, 0.013], mean action: 0.121 [-1.169, 1.198], mean observation: 0.087 [-17.624, 18.041], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.630174\n",
      " 1800/2000: episode: 18, duration: 5.344s, episode steps: 101, steps per second: 19, episode reward: 0.705, mean reward: 0.007 [-0.003, 0.014], mean action: 0.107 [-1.176, 1.156], mean observation: 0.087 [-18.737, 18.048], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.631131\n",
      " 1901/2000: episode: 19, duration: 5.217s, episode steps: 101, steps per second: 19, episode reward: 0.717, mean reward: 0.007 [-0.002, 0.014], mean action: 0.155 [-1.183, 1.203], mean observation: 0.089 [-17.969, 18.091], loss: 0.000057, mean_squared_error: 0.000113, mean_q: 0.636971\n",
      "done, took 98.821 seconds\n",
      "\n",
      "\n",
      "iteration: 57\n",
      "Training for 2000 steps ...\n",
      "  102/2000: episode: 1, duration: 5.335s, episode steps: 102, steps per second: 19, episode reward: 0.716, mean reward: 0.007 [-0.003, 0.014], mean action: 0.133 [-1.187, 1.161], mean observation: 0.088 [-19.256, 17.833], loss: --, mean_squared_error: --, mean_q: --\n",
      "  203/2000: episode: 2, duration: 4.324s, episode steps: 101, steps per second: 23, episode reward: 0.706, mean reward: 0.007 [-0.003, 0.014], mean action: 0.100 [-1.141, 1.087], mean observation: 0.088 [-19.138, 17.849], loss: --, mean_squared_error: --, mean_q: --\n",
      "  305/2000: episode: 3, duration: 4.449s, episode steps: 102, steps per second: 23, episode reward: 0.727, mean reward: 0.007 [-0.003, 0.014], mean action: 0.132 [-1.143, 1.231], mean observation: 0.089 [-18.724, 17.837], loss: --, mean_squared_error: --, mean_q: --\n",
      "  406/2000: episode: 4, duration: 4.561s, episode steps: 101, steps per second: 22, episode reward: 0.712, mean reward: 0.007 [-0.003, 0.014], mean action: 0.120 [-1.209, 1.151], mean observation: 0.089 [-18.960, 17.889], loss: --, mean_squared_error: --, mean_q: --\n",
      "  508/2000: episode: 5, duration: 4.475s, episode steps: 102, steps per second: 23, episode reward: 0.716, mean reward: 0.007 [-0.003, 0.014], mean action: 0.137 [-1.165, 1.234], mean observation: 0.088 [-18.923, 17.866], loss: --, mean_squared_error: --, mean_q: --\n",
      "  609/2000: episode: 6, duration: 4.238s, episode steps: 101, steps per second: 24, episode reward: 0.699, mean reward: 0.007 [-0.003, 0.014], mean action: 0.110 [-1.108, 1.136], mean observation: 0.087 [-18.207, 18.042], loss: --, mean_squared_error: --, mean_q: --\n",
      "  710/2000: episode: 7, duration: 4.217s, episode steps: 101, steps per second: 24, episode reward: 0.709, mean reward: 0.007 [-0.003, 0.014], mean action: 0.126 [-1.140, 1.139], mean observation: 0.088 [-18.295, 18.139], loss: --, mean_squared_error: --, mean_q: --\n",
      "  812/2000: episode: 8, duration: 4.478s, episode steps: 102, steps per second: 23, episode reward: 0.721, mean reward: 0.007 [-0.003, 0.014], mean action: 0.142 [-1.152, 1.182], mean observation: 0.086 [-18.170, 17.970], loss: --, mean_squared_error: --, mean_q: --\n",
      "  914/2000: episode: 9, duration: 4.356s, episode steps: 102, steps per second: 23, episode reward: 0.712, mean reward: 0.007 [-0.003, 0.014], mean action: 0.127 [-1.192, 1.227], mean observation: 0.088 [-18.255, 18.110], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1016/2000: episode: 10, duration: 4.538s, episode steps: 102, steps per second: 22, episode reward: 0.720, mean reward: 0.007 [-0.003, 0.014], mean action: 0.131 [-1.238, 1.159], mean observation: 0.090 [-18.842, 17.969], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.636097\n",
      " 1118/2000: episode: 11, duration: 5.619s, episode steps: 102, steps per second: 18, episode reward: 0.700, mean reward: 0.007 [-0.003, 0.014], mean action: 0.153 [-1.180, 1.180], mean observation: 0.086 [-17.891, 17.937], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.639478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1221/2000: episode: 12, duration: 5.642s, episode steps: 103, steps per second: 18, episode reward: 0.719, mean reward: 0.007 [-0.003, 0.014], mean action: 0.155 [-1.206, 1.257], mean observation: 0.084 [-20.258, 17.960], loss: 0.000083, mean_squared_error: 0.000167, mean_q: 0.631239\n",
      " 1325/2000: episode: 13, duration: 5.705s, episode steps: 104, steps per second: 18, episode reward: 0.704, mean reward: 0.007 [-0.003, 0.014], mean action: 0.135 [-1.186, 1.158], mean observation: 0.081 [-18.738, 17.976], loss: 0.000183, mean_squared_error: 0.000366, mean_q: 0.629262\n",
      " 1432/2000: episode: 14, duration: 5.631s, episode steps: 107, steps per second: 19, episode reward: 0.725, mean reward: 0.007 [-0.003, 0.014], mean action: 0.086 [-1.181, 1.218], mean observation: 0.087 [-18.563, 18.025], loss: 0.000098, mean_squared_error: 0.000195, mean_q: 0.640337\n",
      " 1537/2000: episode: 15, duration: 5.530s, episode steps: 105, steps per second: 19, episode reward: 0.738, mean reward: 0.007 [-0.003, 0.014], mean action: 0.125 [-1.160, 1.228], mean observation: 0.089 [-18.545, 17.959], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.633339\n",
      " 1643/2000: episode: 16, duration: 5.591s, episode steps: 106, steps per second: 19, episode reward: 0.713, mean reward: 0.007 [-0.002, 0.013], mean action: 0.137 [-1.112, 1.183], mean observation: 0.087 [-18.824, 17.923], loss: 0.000217, mean_squared_error: 0.000435, mean_q: 0.636284\n",
      " 1746/2000: episode: 17, duration: 5.500s, episode steps: 103, steps per second: 19, episode reward: 0.731, mean reward: 0.007 [-0.003, 0.014], mean action: 0.109 [-1.153, 1.104], mean observation: 0.091 [-17.272, 17.986], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.647780\n",
      " 1849/2000: episode: 18, duration: 5.380s, episode steps: 103, steps per second: 19, episode reward: 0.723, mean reward: 0.007 [-0.003, 0.014], mean action: 0.131 [-1.176, 1.151], mean observation: 0.090 [-19.219, 17.922], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.636073\n",
      " 1952/2000: episode: 19, duration: 5.735s, episode steps: 103, steps per second: 18, episode reward: 0.713, mean reward: 0.007 [-0.003, 0.014], mean action: 0.086 [-1.198, 1.128], mean observation: 0.088 [-18.412, 18.068], loss: 0.000080, mean_squared_error: 0.000159, mean_q: 0.641183\n",
      "done, took 98.157 seconds\n",
      "\n",
      "\n",
      "iteration: 58\n",
      "Training for 2000 steps ...\n",
      "  103/2000: episode: 1, duration: 4.447s, episode steps: 103, steps per second: 23, episode reward: 0.716, mean reward: 0.007 [-0.003, 0.014], mean action: 0.082 [-1.275, 1.187], mean observation: 0.087 [-17.839, 18.023], loss: --, mean_squared_error: --, mean_q: --\n",
      "  206/2000: episode: 2, duration: 4.410s, episode steps: 103, steps per second: 23, episode reward: 0.711, mean reward: 0.007 [-0.003, 0.014], mean action: 0.116 [-1.246, 1.166], mean observation: 0.087 [-18.906, 17.778], loss: --, mean_squared_error: --, mean_q: --\n",
      "  309/2000: episode: 3, duration: 4.364s, episode steps: 103, steps per second: 24, episode reward: 0.714, mean reward: 0.007 [-0.003, 0.014], mean action: 0.118 [-1.095, 1.136], mean observation: 0.087 [-18.892, 17.943], loss: --, mean_squared_error: --, mean_q: --\n",
      "  411/2000: episode: 4, duration: 4.287s, episode steps: 102, steps per second: 24, episode reward: 0.712, mean reward: 0.007 [-0.003, 0.014], mean action: 0.123 [-1.108, 1.099], mean observation: 0.088 [-18.442, 18.020], loss: --, mean_squared_error: --, mean_q: --\n",
      "  513/2000: episode: 5, duration: 4.484s, episode steps: 102, steps per second: 23, episode reward: 0.707, mean reward: 0.007 [-0.003, 0.014], mean action: 0.104 [-1.277, 1.104], mean observation: 0.084 [-19.021, 17.607], loss: --, mean_squared_error: --, mean_q: --\n",
      "  616/2000: episode: 6, duration: 4.443s, episode steps: 103, steps per second: 23, episode reward: 0.708, mean reward: 0.007 [-0.003, 0.014], mean action: 0.115 [-1.087, 1.127], mean observation: 0.087 [-19.000, 18.028], loss: --, mean_squared_error: --, mean_q: --\n",
      "  719/2000: episode: 7, duration: 4.409s, episode steps: 103, steps per second: 23, episode reward: 0.712, mean reward: 0.007 [-0.003, 0.014], mean action: 0.115 [-1.155, 1.157], mean observation: 0.087 [-19.035, 17.881], loss: --, mean_squared_error: --, mean_q: --\n",
      "  822/2000: episode: 8, duration: 4.379s, episode steps: 103, steps per second: 24, episode reward: 0.715, mean reward: 0.007 [-0.003, 0.014], mean action: 0.097 [-1.214, 1.127], mean observation: 0.087 [-18.006, 18.105], loss: --, mean_squared_error: --, mean_q: --\n",
      "  925/2000: episode: 9, duration: 4.367s, episode steps: 103, steps per second: 24, episode reward: 0.714, mean reward: 0.007 [-0.003, 0.014], mean action: 0.074 [-1.154, 1.060], mean observation: 0.087 [-19.332, 17.794], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1028/2000: episode: 10, duration: 4.610s, episode steps: 103, steps per second: 22, episode reward: 0.720, mean reward: 0.007 [-0.003, 0.014], mean action: 0.142 [-1.110, 1.144], mean observation: 0.087 [-18.501, 17.961], loss: 0.000074, mean_squared_error: 0.000147, mean_q: 0.632466\n",
      " 1129/2000: episode: 11, duration: 5.772s, episode steps: 101, steps per second: 17, episode reward: 0.658, mean reward: 0.007 [-0.003, 0.014], mean action: 0.201 [-1.090, 1.208], mean observation: 0.085 [-19.194, 17.798], loss: 0.000279, mean_squared_error: 0.000559, mean_q: 0.636516\n",
      " 1233/2000: episode: 12, duration: 5.875s, episode steps: 104, steps per second: 18, episode reward: 0.709, mean reward: 0.007 [-0.003, 0.014], mean action: 0.200 [-1.136, 1.230], mean observation: 0.083 [-20.999, 18.014], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.629165\n",
      " 1338/2000: episode: 13, duration: 5.696s, episode steps: 105, steps per second: 18, episode reward: 0.720, mean reward: 0.007 [-0.003, 0.014], mean action: 0.178 [-1.202, 1.149], mean observation: 0.080 [-26.913, 18.074], loss: 0.000131, mean_squared_error: 0.000263, mean_q: 0.630277\n",
      " 1445/2000: episode: 14, duration: 5.588s, episode steps: 107, steps per second: 19, episode reward: 0.719, mean reward: 0.007 [-0.003, 0.014], mean action: 0.173 [-1.108, 1.145], mean observation: 0.084 [-20.809, 17.968], loss: 0.000123, mean_squared_error: 0.000246, mean_q: 0.633923\n",
      " 1547/2000: episode: 15, duration: 5.775s, episode steps: 102, steps per second: 18, episode reward: 0.656, mean reward: 0.006 [-0.003, 0.013], mean action: 0.147 [-1.261, 1.171], mean observation: 0.072 [-50.419, 17.911], loss: 0.000103, mean_squared_error: 0.000207, mean_q: 0.636171\n",
      " 1644/2000: episode: 16, duration: 5.515s, episode steps: 97, steps per second: 18, episode reward: 0.627, mean reward: 0.006 [-0.003, 0.013], mean action: 0.205 [-1.123, 1.151], mean observation: 0.071 [-44.485, 18.020], loss: 0.000053, mean_squared_error: 0.000106, mean_q: 0.624808\n",
      " 1749/2000: episode: 17, duration: 5.490s, episode steps: 105, steps per second: 19, episode reward: 0.705, mean reward: 0.007 [-0.003, 0.014], mean action: 0.174 [-1.153, 1.192], mean observation: 0.083 [-20.746, 17.894], loss: 0.000205, mean_squared_error: 0.000410, mean_q: 0.642191\n",
      " 1852/2000: episode: 18, duration: 5.810s, episode steps: 103, steps per second: 18, episode reward: 0.671, mean reward: 0.007 [-0.003, 0.013], mean action: 0.157 [-1.195, 1.186], mean observation: 0.074 [-55.598, 18.126], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.631979\n",
      " 1956/2000: episode: 19, duration: 5.470s, episode steps: 104, steps per second: 19, episode reward: 0.722, mean reward: 0.007 [-0.002, 0.014], mean action: 0.145 [-1.213, 1.126], mean observation: 0.085 [-21.062, 17.859], loss: 0.000086, mean_squared_error: 0.000171, mean_q: 0.633545\n",
      "done, took 97.722 seconds\n",
      "\n",
      "\n",
      "iteration: 59\n",
      "Training for 2000 steps ...\n",
      "  104/2000: episode: 1, duration: 4.433s, episode steps: 104, steps per second: 23, episode reward: 0.742, mean reward: 0.007 [-0.002, 0.014], mean action: 0.141 [-1.199, 1.152], mean observation: 0.089 [-18.418, 17.812], loss: --, mean_squared_error: --, mean_q: --\n",
      "  208/2000: episode: 2, duration: 4.467s, episode steps: 104, steps per second: 23, episode reward: 0.739, mean reward: 0.007 [-0.002, 0.014], mean action: 0.137 [-1.158, 1.148], mean observation: 0.090 [-18.801, 17.822], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  313/2000: episode: 3, duration: 4.541s, episode steps: 105, steps per second: 23, episode reward: 0.748, mean reward: 0.007 [-0.002, 0.014], mean action: 0.170 [-1.157, 1.307], mean observation: 0.090 [-18.965, 18.000], loss: --, mean_squared_error: --, mean_q: --\n",
      "  417/2000: episode: 4, duration: 4.463s, episode steps: 104, steps per second: 23, episode reward: 0.741, mean reward: 0.007 [-0.002, 0.014], mean action: 0.155 [-1.124, 1.190], mean observation: 0.087 [-19.200, 17.841], loss: --, mean_squared_error: --, mean_q: --\n",
      "  521/2000: episode: 5, duration: 4.536s, episode steps: 104, steps per second: 23, episode reward: 0.738, mean reward: 0.007 [-0.002, 0.014], mean action: 0.151 [-1.118, 1.172], mean observation: 0.089 [-17.616, 17.959], loss: --, mean_squared_error: --, mean_q: --\n",
      "  625/2000: episode: 6, duration: 4.472s, episode steps: 104, steps per second: 23, episode reward: 0.742, mean reward: 0.007 [-0.002, 0.014], mean action: 0.174 [-1.113, 1.248], mean observation: 0.089 [-17.971, 18.106], loss: --, mean_squared_error: --, mean_q: --\n",
      "  729/2000: episode: 7, duration: 4.520s, episode steps: 104, steps per second: 23, episode reward: 0.742, mean reward: 0.007 [-0.002, 0.014], mean action: 0.154 [-1.220, 1.139], mean observation: 0.087 [-18.982, 18.037], loss: --, mean_squared_error: --, mean_q: --\n",
      "  833/2000: episode: 8, duration: 4.485s, episode steps: 104, steps per second: 23, episode reward: 0.742, mean reward: 0.007 [-0.002, 0.014], mean action: 0.170 [-1.118, 1.163], mean observation: 0.088 [-18.351, 18.129], loss: --, mean_squared_error: --, mean_q: --\n",
      "  937/2000: episode: 9, duration: 4.435s, episode steps: 104, steps per second: 23, episode reward: 0.734, mean reward: 0.007 [-0.002, 0.014], mean action: 0.171 [-1.229, 1.232], mean observation: 0.089 [-18.686, 17.851], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1040/2000: episode: 10, duration: 4.997s, episode steps: 103, steps per second: 21, episode reward: 0.727, mean reward: 0.007 [-0.002, 0.014], mean action: 0.148 [-1.123, 1.172], mean observation: 0.086 [-23.457, 17.850], loss: 0.000066, mean_squared_error: 0.000131, mean_q: 0.649294\n",
      " 1142/2000: episode: 11, duration: 5.393s, episode steps: 102, steps per second: 19, episode reward: 0.703, mean reward: 0.007 [-0.002, 0.014], mean action: 0.148 [-1.223, 1.183], mean observation: 0.091 [-16.020, 17.871], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.632176\n",
      " 1244/2000: episode: 12, duration: 5.316s, episode steps: 102, steps per second: 19, episode reward: 0.730, mean reward: 0.007 [-0.002, 0.014], mean action: 0.146 [-1.129, 1.121], mean observation: 0.089 [-14.176, 17.928], loss: 0.000057, mean_squared_error: 0.000113, mean_q: 0.630586\n",
      " 1349/2000: episode: 13, duration: 5.794s, episode steps: 105, steps per second: 18, episode reward: 0.714, mean reward: 0.007 [-0.003, 0.013], mean action: 0.182 [-1.184, 1.226], mean observation: 0.085 [-19.883, 17.745], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.622849\n",
      " 1451/2000: episode: 14, duration: 5.676s, episode steps: 102, steps per second: 18, episode reward: 0.710, mean reward: 0.007 [-0.003, 0.013], mean action: 0.193 [-1.113, 1.188], mean observation: 0.081 [-21.202, 18.014], loss: 0.000064, mean_squared_error: 0.000127, mean_q: 0.629576\n",
      " 1552/2000: episode: 15, duration: 5.582s, episode steps: 101, steps per second: 18, episode reward: 0.714, mean reward: 0.007 [-0.002, 0.014], mean action: 0.199 [-1.064, 1.137], mean observation: 0.085 [-12.249, 18.252], loss: 0.000132, mean_squared_error: 0.000263, mean_q: 0.631687\n",
      " 1650/2000: episode: 16, duration: 5.697s, episode steps: 98, steps per second: 17, episode reward: 0.638, mean reward: 0.007 [-0.003, 0.014], mean action: 0.162 [-1.174, 1.109], mean observation: 0.071 [-39.849, 17.889], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.620685\n",
      " 1754/2000: episode: 17, duration: 5.716s, episode steps: 104, steps per second: 18, episode reward: 0.697, mean reward: 0.007 [-0.002, 0.014], mean action: 0.130 [-1.123, 1.132], mean observation: 0.078 [-39.211, 17.827], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.632073\n",
      " 1859/2000: episode: 18, duration: 6.044s, episode steps: 105, steps per second: 17, episode reward: 0.687, mean reward: 0.007 [-0.002, 0.013], mean action: 0.189 [-1.129, 1.137], mean observation: 0.073 [-42.650, 18.122], loss: 0.000163, mean_squared_error: 0.000327, mean_q: 0.626745\n",
      " 1967/2000: episode: 19, duration: 5.948s, episode steps: 108, steps per second: 18, episode reward: 0.715, mean reward: 0.007 [-0.002, 0.013], mean action: 0.144 [-1.116, 1.122], mean observation: 0.080 [-34.387, 17.809], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.623288\n",
      "done, took 98.498 seconds\n",
      "\n",
      "\n",
      "iteration: 60\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 4.742s, episode steps: 105, steps per second: 22, episode reward: 0.680, mean reward: 0.006 [-0.002, 0.013], mean action: 0.126 [-1.120, 1.296], mean observation: 0.079 [-41.623, 18.085], loss: --, mean_squared_error: --, mean_q: --\n",
      "  211/2000: episode: 2, duration: 4.631s, episode steps: 106, steps per second: 23, episode reward: 0.686, mean reward: 0.006 [-0.002, 0.013], mean action: 0.124 [-1.175, 1.190], mean observation: 0.085 [-18.818, 17.934], loss: --, mean_squared_error: --, mean_q: --\n",
      "  318/2000: episode: 3, duration: 4.656s, episode steps: 107, steps per second: 23, episode reward: 0.700, mean reward: 0.007 [-0.002, 0.013], mean action: 0.097 [-1.199, 1.204], mean observation: 0.079 [-42.882, 18.013], loss: --, mean_squared_error: --, mean_q: --\n",
      "  424/2000: episode: 4, duration: 4.697s, episode steps: 106, steps per second: 23, episode reward: 0.686, mean reward: 0.006 [-0.002, 0.013], mean action: 0.143 [-1.214, 1.180], mean observation: 0.079 [-37.382, 17.892], loss: --, mean_squared_error: --, mean_q: --\n",
      "  531/2000: episode: 5, duration: 4.680s, episode steps: 107, steps per second: 23, episode reward: 0.702, mean reward: 0.007 [-0.002, 0.013], mean action: 0.114 [-1.181, 1.226], mean observation: 0.079 [-38.239, 17.933], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 6, duration: 4.661s, episode steps: 106, steps per second: 23, episode reward: 0.692, mean reward: 0.007 [-0.002, 0.013], mean action: 0.124 [-1.154, 1.138], mean observation: 0.078 [-42.488, 17.773], loss: --, mean_squared_error: --, mean_q: --\n",
      "  743/2000: episode: 7, duration: 4.728s, episode steps: 106, steps per second: 22, episode reward: 0.686, mean reward: 0.006 [-0.002, 0.013], mean action: 0.118 [-1.194, 1.310], mean observation: 0.078 [-45.067, 18.024], loss: --, mean_squared_error: --, mean_q: --\n",
      "  849/2000: episode: 8, duration: 4.677s, episode steps: 106, steps per second: 23, episode reward: 0.687, mean reward: 0.006 [-0.002, 0.013], mean action: 0.139 [-1.091, 1.194], mean observation: 0.076 [-43.680, 17.950], loss: --, mean_squared_error: --, mean_q: --\n",
      "  956/2000: episode: 9, duration: 4.747s, episode steps: 107, steps per second: 23, episode reward: 0.699, mean reward: 0.007 [-0.002, 0.013], mean action: 0.131 [-1.155, 1.259], mean observation: 0.080 [-34.178, 17.731], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1063/2000: episode: 10, duration: 5.206s, episode steps: 107, steps per second: 21, episode reward: 0.696, mean reward: 0.007 [-0.002, 0.013], mean action: 0.083 [-1.209, 1.185], mean observation: 0.082 [-41.321, 18.025], loss: 0.000091, mean_squared_error: 0.000183, mean_q: 0.627646\n",
      " 1167/2000: episode: 11, duration: 5.406s, episode steps: 104, steps per second: 19, episode reward: 0.714, mean reward: 0.007 [-0.002, 0.014], mean action: 0.138 [-1.173, 1.165], mean observation: 0.086 [-21.919, 17.929], loss: 0.000154, mean_squared_error: 0.000308, mean_q: 0.624984\n",
      " 1272/2000: episode: 12, duration: 5.309s, episode steps: 105, steps per second: 20, episode reward: 0.723, mean reward: 0.007 [-0.002, 0.013], mean action: 0.159 [-1.175, 1.243], mean observation: 0.088 [-18.705, 17.935], loss: 0.000485, mean_squared_error: 0.000970, mean_q: 0.625724\n",
      " 1376/2000: episode: 13, duration: 5.021s, episode steps: 104, steps per second: 21, episode reward: 0.728, mean reward: 0.007 [-0.002, 0.013], mean action: 0.175 [-1.232, 1.159], mean observation: 0.085 [-20.843, 17.930], loss: 0.000101, mean_squared_error: 0.000201, mean_q: 0.623559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1480/2000: episode: 14, duration: 5.106s, episode steps: 104, steps per second: 20, episode reward: 0.714, mean reward: 0.007 [-0.002, 0.013], mean action: 0.151 [-1.228, 1.121], mean observation: 0.085 [-10.357, 17.406], loss: 0.000083, mean_squared_error: 0.000167, mean_q: 0.628890\n",
      " 1585/2000: episode: 15, duration: 5.449s, episode steps: 105, steps per second: 19, episode reward: 0.709, mean reward: 0.007 [-0.002, 0.013], mean action: 0.125 [-1.113, 1.152], mean observation: 0.086 [-35.046, 17.652], loss: 0.000339, mean_squared_error: 0.000677, mean_q: 0.625982\n",
      " 1690/2000: episode: 16, duration: 5.580s, episode steps: 105, steps per second: 19, episode reward: 0.682, mean reward: 0.006 [-0.001, 0.013], mean action: 0.068 [-1.136, 1.154], mean observation: 0.080 [-41.539, 17.529], loss: 0.000170, mean_squared_error: 0.000340, mean_q: 0.617165\n",
      " 1798/2000: episode: 17, duration: 5.652s, episode steps: 108, steps per second: 19, episode reward: 0.717, mean reward: 0.007 [-0.001, 0.013], mean action: 0.090 [-1.163, 1.142], mean observation: 0.081 [-43.728, 17.608], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.620002\n",
      " 1903/2000: episode: 18, duration: 5.720s, episode steps: 105, steps per second: 18, episode reward: 0.690, mean reward: 0.007 [-0.002, 0.013], mean action: 0.112 [-1.204, 1.236], mean observation: 0.084 [-30.361, 17.487], loss: 0.000061, mean_squared_error: 0.000123, mean_q: 0.633067\n",
      "done, took 96.002 seconds\n",
      "\n",
      "\n",
      "iteration: 61\n",
      "Training for 2000 steps ...\n",
      "  107/2000: episode: 1, duration: 4.929s, episode steps: 107, steps per second: 22, episode reward: 0.691, mean reward: 0.006 [-0.002, 0.013], mean action: 0.143 [-1.130, 1.145], mean observation: 0.077 [-40.426, 17.243], loss: --, mean_squared_error: --, mean_q: --\n",
      "  214/2000: episode: 2, duration: 5.015s, episode steps: 107, steps per second: 21, episode reward: 0.695, mean reward: 0.006 [-0.002, 0.013], mean action: 0.169 [-1.097, 1.197], mean observation: 0.085 [-10.254, 17.802], loss: --, mean_squared_error: --, mean_q: --\n",
      "  320/2000: episode: 3, duration: 4.955s, episode steps: 106, steps per second: 21, episode reward: 0.691, mean reward: 0.007 [-0.002, 0.013], mean action: 0.147 [-1.180, 1.297], mean observation: 0.081 [-33.371, 17.809], loss: --, mean_squared_error: --, mean_q: --\n",
      "  427/2000: episode: 4, duration: 5.001s, episode steps: 107, steps per second: 21, episode reward: 0.692, mean reward: 0.006 [-0.002, 0.013], mean action: 0.155 [-1.196, 1.238], mean observation: 0.079 [-37.329, 17.485], loss: --, mean_squared_error: --, mean_q: --\n",
      "  534/2000: episode: 5, duration: 4.894s, episode steps: 107, steps per second: 22, episode reward: 0.700, mean reward: 0.007 [-0.002, 0.013], mean action: 0.125 [-1.133, 1.185], mean observation: 0.078 [-39.377, 17.684], loss: --, mean_squared_error: --, mean_q: --\n",
      "  641/2000: episode: 6, duration: 5.115s, episode steps: 107, steps per second: 21, episode reward: 0.690, mean reward: 0.006 [-0.002, 0.013], mean action: 0.142 [-1.163, 1.213], mean observation: 0.078 [-41.844, 17.354], loss: --, mean_squared_error: --, mean_q: --\n",
      "  748/2000: episode: 7, duration: 4.951s, episode steps: 107, steps per second: 22, episode reward: 0.694, mean reward: 0.006 [-0.002, 0.013], mean action: 0.151 [-1.104, 1.184], mean observation: 0.081 [-24.927, 17.541], loss: --, mean_squared_error: --, mean_q: --\n",
      "  854/2000: episode: 8, duration: 4.851s, episode steps: 106, steps per second: 22, episode reward: 0.687, mean reward: 0.006 [-0.002, 0.013], mean action: 0.134 [-1.143, 1.137], mean observation: 0.086 [-19.533, 17.789], loss: --, mean_squared_error: --, mean_q: --\n",
      "  960/2000: episode: 9, duration: 4.902s, episode steps: 106, steps per second: 22, episode reward: 0.686, mean reward: 0.006 [-0.002, 0.013], mean action: 0.147 [-1.216, 1.240], mean observation: 0.081 [-37.779, 17.474], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1067/2000: episode: 10, duration: 5.443s, episode steps: 107, steps per second: 20, episode reward: 0.698, mean reward: 0.007 [-0.002, 0.013], mean action: 0.132 [-1.207, 1.133], mean observation: 0.077 [-26.194, 17.726], loss: 0.000080, mean_squared_error: 0.000161, mean_q: 0.622054\n",
      " 1190/2000: episode: 11, duration: 5.789s, episode steps: 123, steps per second: 21, episode reward: -0.755, mean reward: -0.006 [-0.021, 0.013], mean action: 0.158 [-1.106, 1.224], mean observation: 0.013 [-18.664, 17.532], loss: 0.000148, mean_squared_error: 0.000296, mean_q: 0.625168\n",
      " 1312/2000: episode: 12, duration: 5.628s, episode steps: 122, steps per second: 22, episode reward: -0.750, mean reward: -0.006 [-0.021, 0.013], mean action: 0.135 [-1.099, 1.132], mean observation: 0.011 [-21.329, 17.382], loss: 0.000134, mean_squared_error: 0.000269, mean_q: 0.624594\n",
      " 1431/2000: episode: 13, duration: 5.529s, episode steps: 119, steps per second: 22, episode reward: -0.745, mean reward: -0.006 [-0.021, 0.013], mean action: 0.148 [-1.150, 1.154], mean observation: 0.011 [-20.716, 17.958], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.623982\n",
      " 1550/2000: episode: 14, duration: 5.879s, episode steps: 119, steps per second: 20, episode reward: -0.739, mean reward: -0.006 [-0.021, 0.013], mean action: 0.190 [-1.156, 1.161], mean observation: 0.013 [-15.453, 17.517], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.613795\n",
      " 1671/2000: episode: 15, duration: 6.450s, episode steps: 121, steps per second: 19, episode reward: 0.748, mean reward: 0.006 [-0.002, 0.014], mean action: 0.069 [-1.237, 1.148], mean observation: 0.094 [-44.610, 17.323], loss: 0.000166, mean_squared_error: 0.000333, mean_q: 0.616602\n",
      " 1792/2000: episode: 16, duration: 6.531s, episode steps: 121, steps per second: 19, episode reward: 0.765, mean reward: 0.006 [-0.003, 0.014], mean action: 0.110 [-1.257, 1.160], mean observation: 0.102 [-14.224, 17.389], loss: 0.000149, mean_squared_error: 0.000297, mean_q: 0.618530\n",
      " 1910/2000: episode: 17, duration: 6.330s, episode steps: 118, steps per second: 19, episode reward: 0.739, mean reward: 0.006 [-0.002, 0.014], mean action: 0.169 [-1.140, 1.225], mean observation: 0.094 [-41.678, 17.378], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.616622\n",
      "done, took 97.274 seconds\n",
      "\n",
      "\n",
      "iteration: 62\n",
      "Training for 2000 steps ...\n",
      "  108/2000: episode: 1, duration: 4.853s, episode steps: 108, steps per second: 22, episode reward: 0.717, mean reward: 0.007 [-0.001, 0.014], mean action: 0.090 [-1.186, 1.129], mean observation: 0.081 [-23.472, 17.252], loss: --, mean_squared_error: --, mean_q: --\n",
      "  216/2000: episode: 2, duration: 4.922s, episode steps: 108, steps per second: 22, episode reward: 0.716, mean reward: 0.007 [-0.001, 0.014], mean action: 0.117 [-1.153, 1.231], mean observation: 0.080 [-30.045, 17.392], loss: --, mean_squared_error: --, mean_q: --\n",
      "  324/2000: episode: 3, duration: 4.907s, episode steps: 108, steps per second: 22, episode reward: 0.718, mean reward: 0.007 [-0.001, 0.014], mean action: 0.114 [-1.107, 1.121], mean observation: 0.082 [-17.662, 17.405], loss: --, mean_squared_error: --, mean_q: --\n",
      "  431/2000: episode: 4, duration: 4.984s, episode steps: 107, steps per second: 21, episode reward: 0.704, mean reward: 0.007 [-0.002, 0.014], mean action: 0.116 [-1.166, 1.206], mean observation: 0.081 [-18.118, 17.518], loss: --, mean_squared_error: --, mean_q: --\n",
      "  539/2000: episode: 5, duration: 4.840s, episode steps: 108, steps per second: 22, episode reward: 0.716, mean reward: 0.007 [-0.001, 0.014], mean action: 0.105 [-1.151, 1.183], mean observation: 0.085 [-15.555, 17.493], loss: --, mean_squared_error: --, mean_q: --\n",
      "  647/2000: episode: 6, duration: 4.861s, episode steps: 108, steps per second: 22, episode reward: 0.718, mean reward: 0.007 [-0.001, 0.014], mean action: 0.096 [-1.194, 1.143], mean observation: 0.085 [-16.285, 17.697], loss: --, mean_squared_error: --, mean_q: --\n",
      "  755/2000: episode: 7, duration: 4.907s, episode steps: 108, steps per second: 22, episode reward: 0.711, mean reward: 0.007 [-0.001, 0.014], mean action: 0.113 [-1.225, 1.217], mean observation: 0.086 [-15.712, 17.435], loss: --, mean_squared_error: --, mean_q: --\n",
      "  863/2000: episode: 8, duration: 4.934s, episode steps: 108, steps per second: 22, episode reward: 0.717, mean reward: 0.007 [-0.001, 0.014], mean action: 0.115 [-1.159, 1.140], mean observation: 0.085 [-15.402, 17.383], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  970/2000: episode: 9, duration: 4.824s, episode steps: 107, steps per second: 22, episode reward: 0.706, mean reward: 0.007 [-0.001, 0.014], mean action: 0.121 [-1.132, 1.246], mean observation: 0.082 [-26.263, 17.561], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1077/2000: episode: 10, duration: 5.619s, episode steps: 107, steps per second: 19, episode reward: 0.707, mean reward: 0.007 [-0.001, 0.014], mean action: 0.111 [-1.241, 1.169], mean observation: 0.084 [-15.292, 17.511], loss: 0.000202, mean_squared_error: 0.000405, mean_q: 0.619385\n",
      " 1218/2000: episode: 11, duration: 6.309s, episode steps: 141, steps per second: 22, episode reward: -0.801, mean reward: -0.006 [-0.020, 0.013], mean action: 0.167 [-1.240, 1.217], mean observation: 0.009 [-16.838, 13.578], loss: 0.000057, mean_squared_error: 0.000114, mean_q: 0.615956\n",
      " 1352/2000: episode: 12, duration: 6.012s, episode steps: 134, steps per second: 22, episode reward: -0.766, mean reward: -0.006 [-0.021, 0.013], mean action: 0.164 [-1.315, 1.279], mean observation: 0.012 [-16.699, 17.482], loss: 0.000093, mean_squared_error: 0.000187, mean_q: 0.616673\n",
      " 1478/2000: episode: 13, duration: 5.787s, episode steps: 126, steps per second: 22, episode reward: -0.758, mean reward: -0.006 [-0.020, 0.013], mean action: 0.147 [-1.211, 1.131], mean observation: 0.007 [-16.776, 17.569], loss: 0.000195, mean_squared_error: 0.000390, mean_q: 0.621788\n",
      " 1608/2000: episode: 14, duration: 5.787s, episode steps: 130, steps per second: 22, episode reward: -0.779, mean reward: -0.006 [-0.021, 0.013], mean action: 0.138 [-1.236, 1.121], mean observation: 0.007 [-17.250, 17.522], loss: 0.000199, mean_squared_error: 0.000398, mean_q: 0.613478\n",
      " 1738/2000: episode: 15, duration: 5.938s, episode steps: 130, steps per second: 22, episode reward: -0.756, mean reward: -0.006 [-0.020, 0.013], mean action: 0.166 [-1.121, 1.141], mean observation: 0.009 [-16.672, 17.717], loss: 0.000274, mean_squared_error: 0.000547, mean_q: 0.613299\n",
      " 1870/2000: episode: 16, duration: 5.980s, episode steps: 132, steps per second: 22, episode reward: -0.783, mean reward: -0.006 [-0.020, 0.012], mean action: 0.165 [-1.157, 1.147], mean observation: 0.008 [-17.048, 17.771], loss: 0.000050, mean_squared_error: 0.000100, mean_q: 0.615890\n",
      "done, took 91.237 seconds\n",
      "\n",
      "\n",
      "iteration: 63\n",
      "Training for 2000 steps ...\n",
      "  137/2000: episode: 1, duration: 4.807s, episode steps: 137, steps per second: 29, episode reward: -0.800, mean reward: -0.006 [-0.021, 0.013], mean action: 0.148 [-1.343, 1.282], mean observation: 0.008 [-17.153, 17.628], loss: --, mean_squared_error: --, mean_q: --\n",
      "  275/2000: episode: 2, duration: 4.920s, episode steps: 138, steps per second: 28, episode reward: -0.803, mean reward: -0.006 [-0.021, 0.013], mean action: 0.158 [-1.125, 1.194], mean observation: 0.009 [-16.340, 17.213], loss: --, mean_squared_error: --, mean_q: --\n",
      "  413/2000: episode: 3, duration: 4.708s, episode steps: 138, steps per second: 29, episode reward: -0.806, mean reward: -0.006 [-0.021, 0.013], mean action: 0.169 [-1.312, 1.301], mean observation: 0.006 [-16.861, 17.605], loss: --, mean_squared_error: --, mean_q: --\n",
      "  551/2000: episode: 4, duration: 4.745s, episode steps: 138, steps per second: 29, episode reward: -0.815, mean reward: -0.006 [-0.022, 0.013], mean action: 0.178 [-1.196, 1.239], mean observation: 0.006 [-16.617, 17.353], loss: --, mean_squared_error: --, mean_q: --\n",
      "  689/2000: episode: 5, duration: 4.689s, episode steps: 138, steps per second: 29, episode reward: -0.809, mean reward: -0.006 [-0.022, 0.013], mean action: 0.163 [-1.160, 1.257], mean observation: 0.007 [-16.797, 17.427], loss: --, mean_squared_error: --, mean_q: --\n",
      "  826/2000: episode: 6, duration: 4.843s, episode steps: 137, steps per second: 28, episode reward: -0.787, mean reward: -0.006 [-0.021, 0.013], mean action: 0.146 [-1.161, 1.175], mean observation: 0.009 [-16.646, 17.265], loss: --, mean_squared_error: --, mean_q: --\n",
      "  963/2000: episode: 7, duration: 4.802s, episode steps: 137, steps per second: 29, episode reward: -0.794, mean reward: -0.006 [-0.021, 0.013], mean action: 0.165 [-1.117, 1.174], mean observation: 0.009 [-16.569, 17.755], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1101/2000: episode: 8, duration: 5.666s, episode steps: 138, steps per second: 24, episode reward: -0.803, mean reward: -0.006 [-0.021, 0.013], mean action: 0.156 [-1.122, 1.176], mean observation: 0.008 [-16.650, 17.347], loss: 0.000195, mean_squared_error: 0.000389, mean_q: 0.609248\n",
      " 1223/2000: episode: 9, duration: 6.579s, episode steps: 122, steps per second: 19, episode reward: 0.739, mean reward: 0.006 [-0.003, 0.014], mean action: 0.063 [-1.127, 1.179], mean observation: 0.093 [-16.764, 17.467], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.609141\n",
      " 1333/2000: episode: 10, duration: 5.933s, episode steps: 110, steps per second: 19, episode reward: 0.695, mean reward: 0.006 [-0.002, 0.014], mean action: 0.080 [-1.172, 1.244], mean observation: 0.079 [-22.438, 17.422], loss: 0.000135, mean_squared_error: 0.000269, mean_q: 0.600202\n",
      " 1444/2000: episode: 11, duration: 6.127s, episode steps: 111, steps per second: 18, episode reward: 0.706, mean reward: 0.006 [-0.002, 0.014], mean action: 0.049 [-1.211, 1.167], mean observation: 0.085 [-16.397, 17.467], loss: 0.000189, mean_squared_error: 0.000378, mean_q: 0.604191\n",
      " 1566/2000: episode: 12, duration: 6.415s, episode steps: 122, steps per second: 19, episode reward: 0.723, mean reward: 0.006 [-0.003, 0.014], mean action: 0.048 [-1.164, 1.228], mean observation: 0.088 [-39.737, 17.694], loss: 0.000163, mean_squared_error: 0.000327, mean_q: 0.613177\n",
      " 1680/2000: episode: 13, duration: 5.934s, episode steps: 114, steps per second: 19, episode reward: 0.722, mean reward: 0.006 [-0.001, 0.013], mean action: 0.055 [-1.200, 1.175], mean observation: 0.085 [-42.421, 17.551], loss: 0.000147, mean_squared_error: 0.000293, mean_q: 0.616142\n",
      " 1790/2000: episode: 14, duration: 6.015s, episode steps: 110, steps per second: 18, episode reward: 0.711, mean reward: 0.006 [-0.001, 0.014], mean action: 0.109 [-1.103, 1.273], mean observation: 0.086 [-16.826, 17.377], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.607976\n",
      " 1906/2000: episode: 15, duration: 6.069s, episode steps: 116, steps per second: 19, episode reward: 0.757, mean reward: 0.007 [-0.001, 0.014], mean action: 0.131 [-1.259, 1.249], mean observation: 0.092 [-16.981, 17.537], loss: 0.000054, mean_squared_error: 0.000109, mean_q: 0.604671\n",
      "done, took 87.557 seconds\n",
      "\n",
      "\n",
      "iteration: 64\n",
      "Training for 2000 steps ...\n",
      "  112/2000: episode: 1, duration: 4.858s, episode steps: 112, steps per second: 23, episode reward: 0.741, mean reward: 0.007 [-0.001, 0.014], mean action: 0.091 [-1.270, 1.156], mean observation: 0.087 [-16.472, 17.354], loss: --, mean_squared_error: --, mean_q: --\n",
      "  224/2000: episode: 2, duration: 4.779s, episode steps: 112, steps per second: 23, episode reward: 0.733, mean reward: 0.007 [-0.001, 0.014], mean action: 0.116 [-1.200, 1.171], mean observation: 0.088 [-16.860, 17.617], loss: --, mean_squared_error: --, mean_q: --\n",
      "  336/2000: episode: 3, duration: 4.713s, episode steps: 112, steps per second: 24, episode reward: 0.736, mean reward: 0.007 [-0.001, 0.014], mean action: 0.100 [-1.245, 1.184], mean observation: 0.088 [-17.202, 17.455], loss: --, mean_squared_error: --, mean_q: --\n",
      "  448/2000: episode: 4, duration: 4.874s, episode steps: 112, steps per second: 23, episode reward: 0.736, mean reward: 0.007 [-0.001, 0.014], mean action: 0.132 [-1.169, 1.208], mean observation: 0.090 [-16.958, 17.573], loss: --, mean_squared_error: --, mean_q: --\n",
      "  560/2000: episode: 5, duration: 4.875s, episode steps: 112, steps per second: 23, episode reward: 0.740, mean reward: 0.007 [-0.001, 0.014], mean action: 0.077 [-1.206, 1.222], mean observation: 0.088 [-16.739, 17.459], loss: --, mean_squared_error: --, mean_q: --\n",
      "  671/2000: episode: 6, duration: 4.801s, episode steps: 111, steps per second: 23, episode reward: 0.726, mean reward: 0.007 [-0.001, 0.014], mean action: 0.098 [-1.141, 1.136], mean observation: 0.087 [-16.395, 17.633], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  783/2000: episode: 7, duration: 4.894s, episode steps: 112, steps per second: 23, episode reward: 0.736, mean reward: 0.007 [-0.001, 0.014], mean action: 0.094 [-1.275, 1.207], mean observation: 0.091 [-17.013, 17.429], loss: --, mean_squared_error: --, mean_q: --\n",
      "  895/2000: episode: 8, duration: 4.800s, episode steps: 112, steps per second: 23, episode reward: 0.737, mean reward: 0.007 [-0.001, 0.014], mean action: 0.102 [-1.104, 1.127], mean observation: 0.088 [-16.921, 17.512], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1007/2000: episode: 9, duration: 4.888s, episode steps: 112, steps per second: 23, episode reward: 0.738, mean reward: 0.007 [-0.001, 0.014], mean action: 0.095 [-1.199, 1.327], mean observation: 0.088 [-16.519, 17.394], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.630446\n",
      " 1120/2000: episode: 10, duration: 5.895s, episode steps: 113, steps per second: 19, episode reward: 0.732, mean reward: 0.006 [-0.001, 0.014], mean action: 0.109 [-1.194, 1.210], mean observation: 0.088 [-16.973, 17.605], loss: 0.000161, mean_squared_error: 0.000321, mean_q: 0.621773\n",
      " 1234/2000: episode: 11, duration: 6.030s, episode steps: 114, steps per second: 19, episode reward: 0.740, mean reward: 0.006 [-0.001, 0.014], mean action: 0.142 [-1.171, 1.170], mean observation: 0.091 [-18.518, 17.539], loss: 0.000074, mean_squared_error: 0.000148, mean_q: 0.597921\n",
      " 1346/2000: episode: 12, duration: 6.016s, episode steps: 112, steps per second: 19, episode reward: 0.726, mean reward: 0.006 [-0.001, 0.014], mean action: 0.126 [-1.145, 1.178], mean observation: 0.088 [-16.490, 17.419], loss: 0.000371, mean_squared_error: 0.000742, mean_q: 0.607474\n",
      " 1455/2000: episode: 13, duration: 6.227s, episode steps: 109, steps per second: 18, episode reward: 0.713, mean reward: 0.007 [0.000, 0.014], mean action: 0.147 [-1.086, 1.157], mean observation: 0.084 [-48.777, 17.522], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.605777\n",
      " 1565/2000: episode: 14, duration: 5.972s, episode steps: 110, steps per second: 18, episode reward: 0.704, mean reward: 0.006 [0.000, 0.014], mean action: 0.167 [-1.116, 1.284], mean observation: 0.088 [-51.506, 17.707], loss: 0.000195, mean_squared_error: 0.000390, mean_q: 0.598197\n",
      " 1677/2000: episode: 15, duration: 6.162s, episode steps: 112, steps per second: 18, episode reward: 0.705, mean reward: 0.006 [0.000, 0.014], mean action: 0.159 [-1.112, 1.272], mean observation: 0.085 [-52.556, 17.478], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.605551\n",
      " 1786/2000: episode: 16, duration: 6.211s, episode steps: 109, steps per second: 18, episode reward: 0.724, mean reward: 0.007 [-0.000, 0.014], mean action: 0.157 [-1.103, 1.169], mean observation: 0.089 [-17.859, 17.586], loss: 0.000085, mean_squared_error: 0.000170, mean_q: 0.601389\n",
      " 1898/2000: episode: 17, duration: 6.203s, episode steps: 112, steps per second: 18, episode reward: 0.736, mean reward: 0.007 [-0.000, 0.014], mean action: 0.153 [-1.159, 1.257], mean observation: 0.086 [-22.967, 17.745], loss: 0.000329, mean_squared_error: 0.000658, mean_q: 0.622902\n",
      " 1999/2000: episode: 18, duration: 5.844s, episode steps: 101, steps per second: 17, episode reward: 0.669, mean reward: 0.007 [-0.000, 0.014], mean action: 0.160 [-1.147, 1.170], mean observation: 0.074 [-46.181, 17.652], loss: 0.000290, mean_squared_error: 0.000580, mean_q: 0.607338\n",
      "done, took 98.171 seconds\n",
      "\n",
      "\n",
      "iteration: 65\n",
      "Training for 2000 steps ...\n",
      "   98/2000: episode: 1, duration: 4.802s, episode steps: 98, steps per second: 20, episode reward: 0.643, mean reward: 0.007 [-0.001, 0.014], mean action: 0.164 [-1.209, 1.184], mean observation: 0.065 [-39.218, 17.903], loss: --, mean_squared_error: --, mean_q: --\n",
      "  195/2000: episode: 2, duration: 4.856s, episode steps: 97, steps per second: 20, episode reward: 0.634, mean reward: 0.007 [-0.001, 0.014], mean action: 0.144 [-1.210, 1.236], mean observation: 0.052 [-39.230, 17.820], loss: --, mean_squared_error: --, mean_q: --\n",
      "  294/2000: episode: 3, duration: 4.816s, episode steps: 99, steps per second: 21, episode reward: 0.652, mean reward: 0.007 [-0.001, 0.014], mean action: 0.147 [-1.164, 1.164], mean observation: 0.066 [-40.508, 17.731], loss: --, mean_squared_error: --, mean_q: --\n",
      "  394/2000: episode: 4, duration: 4.886s, episode steps: 100, steps per second: 20, episode reward: 0.657, mean reward: 0.007 [-0.001, 0.014], mean action: 0.127 [-1.218, 1.195], mean observation: 0.062 [-41.938, 17.506], loss: --, mean_squared_error: --, mean_q: --\n",
      "  493/2000: episode: 5, duration: 4.750s, episode steps: 99, steps per second: 21, episode reward: 0.647, mean reward: 0.007 [-0.000, 0.014], mean action: 0.141 [-1.080, 1.141], mean observation: 0.064 [-40.056, 17.400], loss: --, mean_squared_error: --, mean_q: --\n",
      "  591/2000: episode: 6, duration: 4.781s, episode steps: 98, steps per second: 20, episode reward: 0.641, mean reward: 0.007 [-0.000, 0.014], mean action: 0.156 [-1.112, 1.241], mean observation: 0.064 [-40.863, 17.493], loss: --, mean_squared_error: --, mean_q: --\n",
      "  689/2000: episode: 7, duration: 4.765s, episode steps: 98, steps per second: 21, episode reward: 0.643, mean reward: 0.007 [-0.001, 0.014], mean action: 0.134 [-1.181, 1.124], mean observation: 0.065 [-38.109, 17.922], loss: --, mean_squared_error: --, mean_q: --\n",
      "  789/2000: episode: 8, duration: 4.794s, episode steps: 100, steps per second: 21, episode reward: 0.652, mean reward: 0.007 [-0.000, 0.014], mean action: 0.152 [-1.221, 1.198], mean observation: 0.062 [-41.726, 17.542], loss: --, mean_squared_error: --, mean_q: --\n",
      "  887/2000: episode: 9, duration: 4.787s, episode steps: 98, steps per second: 20, episode reward: 0.639, mean reward: 0.007 [-0.001, 0.014], mean action: 0.124 [-1.320, 1.158], mean observation: 0.057 [-40.230, 17.579], loss: --, mean_squared_error: --, mean_q: --\n",
      "  986/2000: episode: 10, duration: 4.784s, episode steps: 99, steps per second: 21, episode reward: 0.650, mean reward: 0.007 [-0.000, 0.014], mean action: 0.128 [-1.291, 1.253], mean observation: 0.065 [-38.286, 17.698], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1085/2000: episode: 11, duration: 5.680s, episode steps: 99, steps per second: 17, episode reward: 0.647, mean reward: 0.007 [-0.000, 0.014], mean action: 0.162 [-1.153, 1.151], mean observation: 0.061 [-41.813, 17.471], loss: 0.000252, mean_squared_error: 0.000504, mean_q: 0.606026\n",
      " 1187/2000: episode: 12, duration: 6.449s, episode steps: 102, steps per second: 16, episode reward: 0.667, mean reward: 0.007 [-0.001, 0.014], mean action: 0.123 [-1.166, 1.141], mean observation: 0.067 [-42.898, 18.790], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.600906\n",
      " 1290/2000: episode: 13, duration: 6.360s, episode steps: 103, steps per second: 16, episode reward: 0.679, mean reward: 0.007 [-0.001, 0.014], mean action: 0.154 [-1.099, 1.167], mean observation: 0.070 [-41.292, 17.987], loss: 0.000080, mean_squared_error: 0.000159, mean_q: 0.598077\n",
      " 1393/2000: episode: 14, duration: 6.291s, episode steps: 103, steps per second: 16, episode reward: 0.678, mean reward: 0.007 [-0.001, 0.014], mean action: 0.150 [-1.236, 1.171], mean observation: 0.071 [-43.051, 17.838], loss: 0.000201, mean_squared_error: 0.000402, mean_q: 0.602643\n",
      " 1495/2000: episode: 15, duration: 6.154s, episode steps: 102, steps per second: 17, episode reward: 0.675, mean reward: 0.007 [-0.002, 0.014], mean action: 0.135 [-1.187, 1.180], mean observation: 0.069 [-43.448, 17.902], loss: 0.000249, mean_squared_error: 0.000498, mean_q: 0.605267\n",
      " 1598/2000: episode: 16, duration: 6.310s, episode steps: 103, steps per second: 16, episode reward: 0.673, mean reward: 0.007 [-0.002, 0.014], mean action: 0.149 [-1.152, 1.268], mean observation: 0.070 [-43.590, 17.886], loss: 0.000131, mean_squared_error: 0.000263, mean_q: 0.601639\n",
      " 1702/2000: episode: 17, duration: 6.338s, episode steps: 104, steps per second: 16, episode reward: 0.682, mean reward: 0.007 [-0.002, 0.014], mean action: 0.110 [-1.198, 1.080], mean observation: 0.072 [-40.812, 18.157], loss: 0.000075, mean_squared_error: 0.000151, mean_q: 0.603109\n",
      " 1803/2000: episode: 18, duration: 6.204s, episode steps: 101, steps per second: 16, episode reward: 0.649, mean reward: 0.006 [-0.002, 0.014], mean action: 0.117 [-1.138, 1.118], mean observation: 0.061 [-40.287, 17.861], loss: 0.000287, mean_squared_error: 0.000573, mean_q: 0.598532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1907/2000: episode: 19, duration: 6.035s, episode steps: 104, steps per second: 17, episode reward: 0.678, mean reward: 0.007 [-0.002, 0.014], mean action: 0.088 [-1.204, 1.177], mean observation: 0.078 [-41.362, 17.908], loss: 0.000093, mean_squared_error: 0.000185, mean_q: 0.606218\n",
      "done, took 109.550 seconds\n",
      "\n",
      "\n",
      "iteration: 66\n",
      "Training for 2000 steps ...\n",
      "  103/2000: episode: 1, duration: 5.073s, episode steps: 103, steps per second: 20, episode reward: 0.680, mean reward: 0.007 [-0.002, 0.014], mean action: 0.108 [-1.131, 1.213], mean observation: 0.080 [-37.291, 17.953], loss: --, mean_squared_error: --, mean_q: --\n",
      "  205/2000: episode: 2, duration: 5.200s, episode steps: 102, steps per second: 20, episode reward: 0.663, mean reward: 0.006 [-0.002, 0.014], mean action: 0.101 [-1.124, 1.181], mean observation: 0.075 [-26.622, 17.984], loss: --, mean_squared_error: --, mean_q: --\n",
      "  307/2000: episode: 3, duration: 5.018s, episode steps: 102, steps per second: 20, episode reward: 0.664, mean reward: 0.007 [-0.002, 0.014], mean action: 0.079 [-1.175, 1.154], mean observation: 0.072 [-36.644, 18.079], loss: --, mean_squared_error: --, mean_q: --\n",
      "  409/2000: episode: 4, duration: 5.065s, episode steps: 102, steps per second: 20, episode reward: 0.667, mean reward: 0.007 [-0.002, 0.014], mean action: 0.070 [-1.167, 1.143], mean observation: 0.073 [-39.317, 17.858], loss: --, mean_squared_error: --, mean_q: --\n",
      "  510/2000: episode: 5, duration: 5.018s, episode steps: 101, steps per second: 20, episode reward: 0.659, mean reward: 0.007 [-0.002, 0.014], mean action: 0.079 [-1.231, 1.093], mean observation: 0.073 [-30.305, 18.068], loss: --, mean_squared_error: --, mean_q: --\n",
      "  612/2000: episode: 6, duration: 5.031s, episode steps: 102, steps per second: 20, episode reward: 0.665, mean reward: 0.007 [-0.002, 0.014], mean action: 0.090 [-1.207, 1.317], mean observation: 0.075 [-41.484, 17.986], loss: --, mean_squared_error: --, mean_q: --\n",
      "  713/2000: episode: 7, duration: 5.041s, episode steps: 101, steps per second: 20, episode reward: 0.658, mean reward: 0.007 [-0.002, 0.014], mean action: 0.095 [-1.183, 1.292], mean observation: 0.074 [-33.610, 17.952], loss: --, mean_squared_error: --, mean_q: --\n",
      "  816/2000: episode: 8, duration: 5.003s, episode steps: 103, steps per second: 21, episode reward: 0.676, mean reward: 0.007 [-0.002, 0.014], mean action: 0.074 [-1.222, 1.120], mean observation: 0.075 [-28.468, 17.783], loss: --, mean_squared_error: --, mean_q: --\n",
      "  918/2000: episode: 9, duration: 5.152s, episode steps: 102, steps per second: 20, episode reward: 0.669, mean reward: 0.007 [-0.002, 0.014], mean action: 0.072 [-1.131, 1.164], mean observation: 0.073 [-41.156, 18.093], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1021/2000: episode: 10, duration: 5.163s, episode steps: 103, steps per second: 20, episode reward: 0.675, mean reward: 0.007 [-0.002, 0.014], mean action: 0.080 [-1.153, 1.233], mean observation: 0.073 [-27.941, 18.004], loss: 0.000085, mean_squared_error: 0.000171, mean_q: 0.615467\n",
      " 1124/2000: episode: 11, duration: 6.276s, episode steps: 103, steps per second: 16, episode reward: 0.671, mean reward: 0.007 [-0.002, 0.014], mean action: 0.097 [-1.190, 1.218], mean observation: 0.071 [-39.185, 17.808], loss: 0.000074, mean_squared_error: 0.000149, mean_q: 0.599446\n",
      " 1228/2000: episode: 12, duration: 6.118s, episode steps: 104, steps per second: 17, episode reward: 0.670, mean reward: 0.006 [-0.002, 0.014], mean action: 0.135 [-1.138, 1.290], mean observation: 0.078 [-42.639, 17.556], loss: 0.000084, mean_squared_error: 0.000168, mean_q: 0.597378\n",
      " 1325/2000: episode: 13, duration: 6.306s, episode steps: 97, steps per second: 15, episode reward: 0.624, mean reward: 0.006 [-0.002, 0.014], mean action: 0.105 [-1.111, 1.158], mean observation: 0.057 [-45.884, 17.627], loss: 0.000160, mean_squared_error: 0.000319, mean_q: 0.597528\n",
      " 1423/2000: episode: 14, duration: 6.056s, episode steps: 98, steps per second: 16, episode reward: 0.631, mean reward: 0.006 [-0.002, 0.014], mean action: 0.065 [-1.127, 1.218], mean observation: 0.065 [-41.365, 17.594], loss: 0.000164, mean_squared_error: 0.000329, mean_q: 0.603306\n",
      " 1518/2000: episode: 15, duration: 6.253s, episode steps: 95, steps per second: 15, episode reward: 0.613, mean reward: 0.006 [-0.002, 0.014], mean action: 0.102 [-1.170, 1.131], mean observation: 0.056 [-44.696, 17.767], loss: 0.000066, mean_squared_error: 0.000133, mean_q: 0.602233\n",
      " 1620/2000: episode: 16, duration: 6.295s, episode steps: 102, steps per second: 16, episode reward: 0.645, mean reward: 0.006 [-0.002, 0.014], mean action: 0.104 [-1.175, 1.131], mean observation: 0.072 [-39.876, 18.015], loss: 0.000339, mean_squared_error: 0.000678, mean_q: 0.610257\n",
      " 1725/2000: episode: 17, duration: 6.287s, episode steps: 105, steps per second: 17, episode reward: 0.677, mean reward: 0.006 [-0.002, 0.014], mean action: 0.085 [-1.153, 1.222], mean observation: 0.078 [-39.583, 17.919], loss: 0.000269, mean_squared_error: 0.000538, mean_q: 0.603013\n",
      " 1826/2000: episode: 18, duration: 6.033s, episode steps: 101, steps per second: 17, episode reward: 0.660, mean reward: 0.007 [-0.002, 0.014], mean action: 0.074 [-1.157, 1.139], mean observation: 0.078 [-43.410, 18.005], loss: 0.000084, mean_squared_error: 0.000167, mean_q: 0.602433\n",
      " 1932/2000: episode: 19, duration: 5.956s, episode steps: 106, steps per second: 18, episode reward: 0.692, mean reward: 0.007 [-0.002, 0.014], mean action: 0.086 [-1.115, 1.285], mean observation: 0.080 [-44.177, 18.094], loss: 0.000563, mean_squared_error: 0.001125, mean_q: 0.604193\n",
      "done, took 110.740 seconds\n",
      "\n",
      "\n",
      "iteration: 67\n",
      "Training for 2000 steps ...\n",
      "  103/2000: episode: 1, duration: 5.195s, episode steps: 103, steps per second: 20, episode reward: 0.671, mean reward: 0.007 [-0.002, 0.014], mean action: 0.108 [-1.073, 1.146], mean observation: 0.074 [-48.212, 17.896], loss: --, mean_squared_error: --, mean_q: --\n",
      "  206/2000: episode: 2, duration: 5.235s, episode steps: 103, steps per second: 20, episode reward: 0.668, mean reward: 0.006 [-0.002, 0.014], mean action: 0.096 [-1.232, 1.218], mean observation: 0.072 [-48.656, 18.025], loss: --, mean_squared_error: --, mean_q: --\n",
      "  309/2000: episode: 3, duration: 5.363s, episode steps: 103, steps per second: 19, episode reward: 0.672, mean reward: 0.007 [-0.002, 0.014], mean action: 0.105 [-1.115, 1.252], mean observation: 0.071 [-48.531, 17.918], loss: --, mean_squared_error: --, mean_q: --\n",
      "  412/2000: episode: 4, duration: 5.471s, episode steps: 103, steps per second: 19, episode reward: 0.673, mean reward: 0.007 [-0.002, 0.014], mean action: 0.072 [-1.193, 1.209], mean observation: 0.071 [-46.070, 17.933], loss: --, mean_squared_error: --, mean_q: --\n",
      "  515/2000: episode: 5, duration: 5.244s, episode steps: 103, steps per second: 20, episode reward: 0.676, mean reward: 0.007 [-0.002, 0.014], mean action: 0.084 [-1.163, 1.225], mean observation: 0.072 [-48.435, 18.249], loss: --, mean_squared_error: --, mean_q: --\n",
      "  617/2000: episode: 6, duration: 5.231s, episode steps: 102, steps per second: 19, episode reward: 0.667, mean reward: 0.007 [-0.002, 0.014], mean action: 0.066 [-1.200, 1.211], mean observation: 0.069 [-45.115, 17.898], loss: --, mean_squared_error: --, mean_q: --\n",
      "  721/2000: episode: 7, duration: 5.298s, episode steps: 104, steps per second: 20, episode reward: 0.672, mean reward: 0.006 [-0.002, 0.014], mean action: 0.067 [-1.277, 1.116], mean observation: 0.074 [-42.781, 17.848], loss: --, mean_squared_error: --, mean_q: --\n",
      "  824/2000: episode: 8, duration: 5.354s, episode steps: 103, steps per second: 19, episode reward: 0.671, mean reward: 0.007 [-0.002, 0.014], mean action: 0.051 [-1.204, 1.161], mean observation: 0.073 [-48.111, 17.763], loss: --, mean_squared_error: --, mean_q: --\n",
      "  926/2000: episode: 9, duration: 5.151s, episode steps: 102, steps per second: 20, episode reward: 0.665, mean reward: 0.007 [-0.002, 0.014], mean action: 0.064 [-1.239, 1.154], mean observation: 0.071 [-48.508, 17.860], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1028/2000: episode: 10, duration: 5.516s, episode steps: 102, steps per second: 18, episode reward: 0.667, mean reward: 0.007 [-0.002, 0.014], mean action: 0.070 [-1.129, 1.110], mean observation: 0.067 [-46.352, 17.750], loss: 0.000388, mean_squared_error: 0.000775, mean_q: 0.585269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1130/2000: episode: 11, duration: 6.327s, episode steps: 102, steps per second: 16, episode reward: 0.663, mean reward: 0.006 [-0.002, 0.014], mean action: 0.076 [-1.214, 1.168], mean observation: 0.073 [-47.339, 17.916], loss: 0.000212, mean_squared_error: 0.000425, mean_q: 0.600053\n",
      " 1228/2000: episode: 12, duration: 6.586s, episode steps: 98, steps per second: 15, episode reward: 0.627, mean reward: 0.006 [-0.002, 0.014], mean action: 0.049 [-1.140, 1.206], mean observation: 0.061 [-34.576, 17.563], loss: 0.000057, mean_squared_error: 0.000115, mean_q: 0.592616\n",
      " 1325/2000: episode: 13, duration: 6.443s, episode steps: 97, steps per second: 15, episode reward: 0.620, mean reward: 0.006 [-0.003, 0.014], mean action: 0.038 [-1.149, 1.159], mean observation: 0.058 [-41.165, 17.539], loss: 0.000247, mean_squared_error: 0.000493, mean_q: 0.605548\n",
      " 1429/2000: episode: 14, duration: 6.516s, episode steps: 104, steps per second: 16, episode reward: 0.672, mean reward: 0.006 [-0.002, 0.014], mean action: 0.053 [-1.125, 1.219], mean observation: 0.071 [-34.543, 17.615], loss: 0.000080, mean_squared_error: 0.000160, mean_q: 0.602925\n",
      " 1532/2000: episode: 15, duration: 6.509s, episode steps: 103, steps per second: 16, episode reward: 0.647, mean reward: 0.006 [-0.002, 0.014], mean action: 0.059 [-1.111, 1.238], mean observation: 0.073 [-44.044, 17.507], loss: 0.000386, mean_squared_error: 0.000772, mean_q: 0.598341\n",
      " 1637/2000: episode: 16, duration: 6.365s, episode steps: 105, steps per second: 16, episode reward: 0.668, mean reward: 0.006 [-0.002, 0.014], mean action: 0.025 [-1.205, 1.081], mean observation: 0.076 [-27.737, 17.780], loss: 0.000185, mean_squared_error: 0.000371, mean_q: 0.606523\n",
      " 1740/2000: episode: 17, duration: 6.410s, episode steps: 103, steps per second: 16, episode reward: 0.655, mean reward: 0.006 [-0.003, 0.014], mean action: 0.019 [-1.170, 1.233], mean observation: 0.072 [-42.872, 17.572], loss: 0.000061, mean_squared_error: 0.000123, mean_q: 0.602825\n",
      " 1839/2000: episode: 18, duration: 6.185s, episode steps: 99, steps per second: 16, episode reward: 0.628, mean reward: 0.006 [-0.003, 0.014], mean action: -0.001 [-1.227, 1.175], mean observation: 0.066 [-40.101, 17.576], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.599129\n",
      " 1938/2000: episode: 19, duration: 6.613s, episode steps: 99, steps per second: 15, episode reward: 0.631, mean reward: 0.006 [-0.002, 0.014], mean action: 0.037 [-1.084, 1.131], mean observation: 0.065 [-39.453, 17.960], loss: 0.000116, mean_squared_error: 0.000231, mean_q: 0.599876\n",
      "done, took 115.130 seconds\n",
      "\n",
      "\n",
      "iteration: 68\n",
      "Training for 2000 steps ...\n",
      "  102/2000: episode: 1, duration: 5.510s, episode steps: 102, steps per second: 19, episode reward: 0.652, mean reward: 0.006 [-0.002, 0.014], mean action: -0.006 [-1.276, 1.173], mean observation: 0.075 [-38.393, 17.475], loss: --, mean_squared_error: --, mean_q: --\n",
      "  200/2000: episode: 2, duration: 5.397s, episode steps: 98, steps per second: 18, episode reward: 0.619, mean reward: 0.006 [-0.003, 0.014], mean action: -0.022 [-1.171, 1.108], mean observation: 0.063 [-37.366, 17.558], loss: --, mean_squared_error: --, mean_q: --\n",
      "  299/2000: episode: 3, duration: 5.395s, episode steps: 99, steps per second: 18, episode reward: 0.633, mean reward: 0.006 [-0.003, 0.014], mean action: 0.022 [-1.143, 1.224], mean observation: 0.060 [-39.491, 17.738], loss: --, mean_squared_error: --, mean_q: --\n",
      "  400/2000: episode: 4, duration: 5.488s, episode steps: 101, steps per second: 18, episode reward: 0.641, mean reward: 0.006 [-0.002, 0.014], mean action: -0.054 [-1.257, 1.149], mean observation: 0.071 [-38.760, 17.569], loss: --, mean_squared_error: --, mean_q: --\n",
      "  502/2000: episode: 5, duration: 5.596s, episode steps: 102, steps per second: 18, episode reward: 0.650, mean reward: 0.006 [-0.002, 0.014], mean action: -0.029 [-1.161, 1.180], mean observation: 0.067 [-39.190, 17.524], loss: --, mean_squared_error: --, mean_q: --\n",
      "  604/2000: episode: 6, duration: 5.548s, episode steps: 102, steps per second: 18, episode reward: 0.649, mean reward: 0.006 [-0.002, 0.014], mean action: -0.017 [-1.177, 1.177], mean observation: 0.071 [-39.025, 17.613], loss: --, mean_squared_error: --, mean_q: --\n",
      "  706/2000: episode: 7, duration: 5.591s, episode steps: 102, steps per second: 18, episode reward: 0.646, mean reward: 0.006 [-0.003, 0.014], mean action: -0.058 [-1.199, 1.071], mean observation: 0.073 [-37.633, 17.497], loss: --, mean_squared_error: --, mean_q: --\n",
      "  805/2000: episode: 8, duration: 5.329s, episode steps: 99, steps per second: 19, episode reward: 0.632, mean reward: 0.006 [-0.003, 0.014], mean action: -0.017 [-1.102, 1.089], mean observation: 0.064 [-37.834, 17.513], loss: --, mean_squared_error: --, mean_q: --\n",
      "  906/2000: episode: 9, duration: 5.657s, episode steps: 101, steps per second: 18, episode reward: 0.643, mean reward: 0.006 [-0.002, 0.014], mean action: -0.020 [-1.113, 1.096], mean observation: 0.066 [-38.356, 17.490], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1008/2000: episode: 10, duration: 5.659s, episode steps: 102, steps per second: 18, episode reward: 0.651, mean reward: 0.006 [-0.002, 0.014], mean action: -0.048 [-1.199, 1.113], mean observation: 0.070 [-38.613, 17.592], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.587699\n",
      " 1109/2000: episode: 11, duration: 6.492s, episode steps: 101, steps per second: 16, episode reward: 0.642, mean reward: 0.006 [-0.002, 0.014], mean action: -0.031 [-1.218, 1.131], mean observation: 0.073 [-41.039, 17.540], loss: 0.000277, mean_squared_error: 0.000554, mean_q: 0.600281\n",
      " 1212/2000: episode: 12, duration: 6.217s, episode steps: 103, steps per second: 17, episode reward: 0.675, mean reward: 0.007 [-0.002, 0.014], mean action: -0.056 [-1.175, 1.136], mean observation: 0.078 [-39.285, 18.053], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.590103\n",
      " 1315/2000: episode: 13, duration: 6.448s, episode steps: 103, steps per second: 16, episode reward: 0.655, mean reward: 0.006 [-0.003, 0.014], mean action: -0.045 [-1.225, 1.168], mean observation: 0.070 [-37.077, 18.118], loss: 0.000066, mean_squared_error: 0.000133, mean_q: 0.595325\n",
      " 1406/2000: episode: 14, duration: 6.020s, episode steps: 91, steps per second: 15, episode reward: 0.555, mean reward: 0.006 [-0.003, 0.014], mean action: -0.037 [-1.262, 1.196], mean observation: 0.045 [-32.006, 17.760], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.601997\n",
      " 1512/2000: episode: 15, duration: 6.864s, episode steps: 106, steps per second: 15, episode reward: 0.657, mean reward: 0.006 [-0.003, 0.014], mean action: -0.055 [-1.229, 1.126], mean observation: 0.065 [-39.135, 17.410], loss: 0.000178, mean_squared_error: 0.000357, mean_q: 0.604769\n",
      " 1605/2000: episode: 16, duration: 5.823s, episode steps: 93, steps per second: 16, episode reward: 0.580, mean reward: 0.006 [-0.002, 0.014], mean action: -0.001 [-1.170, 1.186], mean observation: 0.055 [-41.966, 21.731], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.603968\n",
      " 1706/2000: episode: 17, duration: 6.456s, episode steps: 101, steps per second: 16, episode reward: 0.620, mean reward: 0.006 [-0.002, 0.014], mean action: 0.010 [-1.171, 1.103], mean observation: 0.061 [-40.622, 17.817], loss: 0.000172, mean_squared_error: 0.000344, mean_q: 0.606762\n",
      " 1808/2000: episode: 18, duration: 6.247s, episode steps: 102, steps per second: 16, episode reward: 0.635, mean reward: 0.006 [-0.002, 0.014], mean action: -0.009 [-1.270, 1.182], mean observation: 0.064 [-40.770, 18.164], loss: 0.000209, mean_squared_error: 0.000419, mean_q: 0.605151\n",
      " 1913/2000: episode: 19, duration: 6.650s, episode steps: 105, steps per second: 16, episode reward: 0.650, mean reward: 0.006 [-0.003, 0.014], mean action: -0.033 [-1.267, 1.192], mean observation: 0.067 [-42.509, 17.721], loss: 0.000242, mean_squared_error: 0.000484, mean_q: 0.602942\n",
      "done, took 117.788 seconds\n",
      "\n",
      "\n",
      "iteration: 69\n",
      "Training for 2000 steps ...\n",
      "   99/2000: episode: 1, duration: 5.146s, episode steps: 99, steps per second: 19, episode reward: 0.605, mean reward: 0.006 [-0.002, 0.014], mean action: 0.069 [-1.224, 1.150], mean observation: 0.057 [-42.793, 17.521], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  198/2000: episode: 2, duration: 5.091s, episode steps: 99, steps per second: 19, episode reward: 0.608, mean reward: 0.006 [-0.002, 0.014], mean action: 0.077 [-1.158, 1.285], mean observation: 0.053 [-42.979, 17.512], loss: --, mean_squared_error: --, mean_q: --\n",
      "  297/2000: episode: 3, duration: 5.117s, episode steps: 99, steps per second: 19, episode reward: 0.609, mean reward: 0.006 [-0.002, 0.014], mean action: 0.051 [-1.220, 1.295], mean observation: 0.060 [-42.851, 17.390], loss: --, mean_squared_error: --, mean_q: --\n",
      "  396/2000: episode: 4, duration: 5.178s, episode steps: 99, steps per second: 19, episode reward: 0.607, mean reward: 0.006 [-0.002, 0.014], mean action: 0.071 [-1.157, 1.148], mean observation: 0.058 [-42.430, 17.438], loss: --, mean_squared_error: --, mean_q: --\n",
      "  495/2000: episode: 5, duration: 5.084s, episode steps: 99, steps per second: 19, episode reward: 0.610, mean reward: 0.006 [-0.002, 0.014], mean action: 0.088 [-1.190, 1.148], mean observation: 0.060 [-42.068, 17.449], loss: --, mean_squared_error: --, mean_q: --\n",
      "  594/2000: episode: 6, duration: 5.252s, episode steps: 99, steps per second: 19, episode reward: 0.608, mean reward: 0.006 [-0.002, 0.014], mean action: 0.073 [-1.173, 1.189], mean observation: 0.056 [-43.230, 17.566], loss: --, mean_squared_error: --, mean_q: --\n",
      "  692/2000: episode: 7, duration: 5.107s, episode steps: 98, steps per second: 19, episode reward: 0.599, mean reward: 0.006 [-0.002, 0.014], mean action: 0.067 [-1.120, 1.254], mean observation: 0.049 [-42.524, 17.508], loss: --, mean_squared_error: --, mean_q: --\n",
      "  791/2000: episode: 8, duration: 5.131s, episode steps: 99, steps per second: 19, episode reward: 0.607, mean reward: 0.006 [-0.002, 0.014], mean action: 0.059 [-1.207, 1.178], mean observation: 0.060 [-42.057, 17.230], loss: --, mean_squared_error: --, mean_q: --\n",
      "  890/2000: episode: 9, duration: 5.168s, episode steps: 99, steps per second: 19, episode reward: 0.605, mean reward: 0.006 [-0.002, 0.014], mean action: 0.088 [-1.197, 1.190], mean observation: 0.051 [-43.080, 17.384], loss: --, mean_squared_error: --, mean_q: --\n",
      "  989/2000: episode: 10, duration: 5.118s, episode steps: 99, steps per second: 19, episode reward: 0.609, mean reward: 0.006 [-0.002, 0.014], mean action: 0.053 [-1.261, 1.155], mean observation: 0.058 [-43.148, 17.793], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1088/2000: episode: 11, duration: 5.954s, episode steps: 99, steps per second: 17, episode reward: 0.608, mean reward: 0.006 [-0.002, 0.014], mean action: 0.043 [-1.144, 1.120], mean observation: 0.048 [-41.871, 17.351], loss: 0.000094, mean_squared_error: 0.000188, mean_q: 0.595094\n",
      " 1186/2000: episode: 12, duration: 6.023s, episode steps: 98, steps per second: 16, episode reward: 0.601, mean reward: 0.006 [-0.003, 0.014], mean action: 0.070 [-1.162, 1.111], mean observation: 0.059 [-45.358, 17.370], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.608857\n",
      " 1284/2000: episode: 13, duration: 5.941s, episode steps: 98, steps per second: 16, episode reward: 0.604, mean reward: 0.006 [-0.003, 0.014], mean action: 0.074 [-1.242, 1.160], mean observation: 0.062 [-46.197, 17.429], loss: 0.000064, mean_squared_error: 0.000127, mean_q: 0.592380\n",
      " 1383/2000: episode: 14, duration: 6.116s, episode steps: 99, steps per second: 16, episode reward: 0.616, mean reward: 0.006 [-0.003, 0.014], mean action: 0.048 [-1.198, 1.103], mean observation: 0.053 [-47.798, 17.434], loss: 0.000201, mean_squared_error: 0.000403, mean_q: 0.607335\n",
      " 1482/2000: episode: 15, duration: 6.130s, episode steps: 99, steps per second: 16, episode reward: 0.615, mean reward: 0.006 [-0.003, 0.014], mean action: 0.081 [-1.122, 1.201], mean observation: 0.059 [-47.525, 17.429], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.604281\n",
      " 1581/2000: episode: 16, duration: 6.544s, episode steps: 99, steps per second: 15, episode reward: 0.602, mean reward: 0.006 [-0.002, 0.014], mean action: 0.071 [-1.157, 1.221], mean observation: 0.054 [-49.846, 17.523], loss: 0.000100, mean_squared_error: 0.000199, mean_q: 0.593291\n",
      " 1682/2000: episode: 17, duration: 6.349s, episode steps: 101, steps per second: 16, episode reward: 0.615, mean reward: 0.006 [-0.003, 0.014], mean action: 0.126 [-1.152, 1.195], mean observation: 0.067 [-48.858, 17.528], loss: 0.000181, mean_squared_error: 0.000362, mean_q: 0.585394\n",
      " 1781/2000: episode: 18, duration: 6.081s, episode steps: 99, steps per second: 16, episode reward: 0.613, mean reward: 0.006 [-0.002, 0.014], mean action: 0.073 [-1.170, 1.145], mean observation: 0.062 [-47.011, 17.395], loss: 0.000240, mean_squared_error: 0.000480, mean_q: 0.599710\n",
      " 1884/2000: episode: 19, duration: 6.424s, episode steps: 103, steps per second: 16, episode reward: 0.621, mean reward: 0.006 [-0.003, 0.014], mean action: 0.077 [-1.099, 1.244], mean observation: 0.053 [-43.350, 21.804], loss: 0.000075, mean_squared_error: 0.000151, mean_q: 0.603690\n",
      " 1990/2000: episode: 20, duration: 6.629s, episode steps: 106, steps per second: 16, episode reward: 0.645, mean reward: 0.006 [-0.002, 0.014], mean action: 0.149 [-1.209, 1.186], mean observation: 0.062 [-40.668, 20.649], loss: 0.000472, mean_squared_error: 0.000944, mean_q: 0.595488\n",
      "done, took 114.435 seconds\n",
      "\n",
      "\n",
      "iteration: 70\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 5.702s, episode steps: 105, steps per second: 18, episode reward: 0.623, mean reward: 0.006 [-0.001, 0.014], mean action: 0.200 [-1.149, 1.129], mean observation: 0.065 [-44.336, 18.265], loss: --, mean_squared_error: --, mean_q: --\n",
      "  210/2000: episode: 2, duration: 5.759s, episode steps: 105, steps per second: 18, episode reward: 0.622, mean reward: 0.006 [-0.002, 0.014], mean action: 0.244 [-1.107, 1.293], mean observation: 0.061 [-44.257, 18.681], loss: --, mean_squared_error: --, mean_q: --\n",
      "  315/2000: episode: 3, duration: 5.674s, episode steps: 105, steps per second: 19, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.014], mean action: 0.246 [-1.107, 1.190], mean observation: 0.065 [-43.754, 17.288], loss: --, mean_squared_error: --, mean_q: --\n",
      "  422/2000: episode: 4, duration: 5.761s, episode steps: 107, steps per second: 19, episode reward: 0.639, mean reward: 0.006 [-0.001, 0.014], mean action: 0.212 [-1.171, 1.197], mean observation: 0.068 [-44.600, 20.471], loss: --, mean_squared_error: --, mean_q: --\n",
      "  528/2000: episode: 5, duration: 5.753s, episode steps: 106, steps per second: 18, episode reward: 0.631, mean reward: 0.006 [-0.001, 0.014], mean action: 0.228 [-1.111, 1.126], mean observation: 0.065 [-44.097, 19.396], loss: --, mean_squared_error: --, mean_q: --\n",
      "  634/2000: episode: 6, duration: 5.804s, episode steps: 106, steps per second: 18, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.014], mean action: 0.226 [-1.119, 1.314], mean observation: 0.069 [-44.808, 19.120], loss: --, mean_squared_error: --, mean_q: --\n",
      "  742/2000: episode: 7, duration: 5.784s, episode steps: 108, steps per second: 19, episode reward: 0.655, mean reward: 0.006 [-0.001, 0.014], mean action: 0.195 [-1.157, 1.145], mean observation: 0.069 [-44.029, 20.587], loss: --, mean_squared_error: --, mean_q: --\n",
      "  847/2000: episode: 8, duration: 5.591s, episode steps: 105, steps per second: 19, episode reward: 0.627, mean reward: 0.006 [-0.001, 0.014], mean action: 0.238 [-1.146, 1.240], mean observation: 0.062 [-44.503, 18.392], loss: --, mean_squared_error: --, mean_q: --\n",
      "  955/2000: episode: 9, duration: 5.836s, episode steps: 108, steps per second: 19, episode reward: 0.647, mean reward: 0.006 [-0.001, 0.014], mean action: 0.222 [-1.187, 1.246], mean observation: 0.069 [-43.665, 20.091], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1060/2000: episode: 10, duration: 6.312s, episode steps: 105, steps per second: 17, episode reward: 0.620, mean reward: 0.006 [-0.002, 0.014], mean action: 0.228 [-1.157, 1.165], mean observation: 0.061 [-44.515, 18.517], loss: 0.000295, mean_squared_error: 0.000590, mean_q: 0.591691\n",
      " 1169/2000: episode: 11, duration: 7.013s, episode steps: 109, steps per second: 16, episode reward: 0.627, mean reward: 0.006 [-0.001, 0.014], mean action: 0.232 [-1.110, 1.170], mean observation: 0.065 [-45.760, 19.522], loss: 0.000166, mean_squared_error: 0.000333, mean_q: 0.586391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1277/2000: episode: 12, duration: 6.702s, episode steps: 108, steps per second: 16, episode reward: 0.653, mean reward: 0.006 [-0.002, 0.014], mean action: 0.202 [-1.191, 1.159], mean observation: 0.067 [-43.676, 20.114], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.597779\n",
      " 1380/2000: episode: 13, duration: 6.528s, episode steps: 103, steps per second: 16, episode reward: 0.629, mean reward: 0.006 [-0.002, 0.014], mean action: 0.149 [-1.172, 1.109], mean observation: 0.071 [-45.279, 17.335], loss: 0.000200, mean_squared_error: 0.000400, mean_q: 0.595656\n",
      " 1483/2000: episode: 14, duration: 6.524s, episode steps: 103, steps per second: 16, episode reward: 0.624, mean reward: 0.006 [-0.002, 0.014], mean action: 0.153 [-1.198, 1.103], mean observation: 0.057 [-39.147, 19.648], loss: 0.000680, mean_squared_error: 0.001360, mean_q: 0.600285\n",
      " 1587/2000: episode: 15, duration: 6.218s, episode steps: 104, steps per second: 17, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.014], mean action: 0.174 [-1.284, 1.168], mean observation: 0.061 [-42.043, 17.401], loss: 0.000092, mean_squared_error: 0.000184, mean_q: 0.598851\n",
      " 1689/2000: episode: 16, duration: 6.324s, episode steps: 102, steps per second: 16, episode reward: 0.631, mean reward: 0.006 [-0.001, 0.013], mean action: 0.190 [-1.106, 1.234], mean observation: 0.067 [-42.653, 18.479], loss: 0.000290, mean_squared_error: 0.000581, mean_q: 0.605015\n",
      " 1797/2000: episode: 17, duration: 6.681s, episode steps: 108, steps per second: 16, episode reward: 0.664, mean reward: 0.006 [-0.002, 0.013], mean action: 0.139 [-1.144, 1.186], mean observation: 0.084 [-49.166, 17.313], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.600039\n",
      " 1902/2000: episode: 18, duration: 6.665s, episode steps: 105, steps per second: 16, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.159 [-1.205, 1.165], mean observation: 0.075 [-41.270, 17.457], loss: 0.000158, mean_squared_error: 0.000315, mean_q: 0.598620\n",
      "done, took 116.937 seconds\n",
      "\n",
      "\n",
      "iteration: 71\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 5.476s, episode steps: 105, steps per second: 19, episode reward: 0.652, mean reward: 0.006 [-0.001, 0.013], mean action: 0.108 [-1.183, 1.187], mean observation: 0.076 [-38.647, 17.366], loss: --, mean_squared_error: --, mean_q: --\n",
      "  207/2000: episode: 2, duration: 5.379s, episode steps: 102, steps per second: 19, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.013], mean action: 0.105 [-1.173, 1.179], mean observation: 0.077 [-41.411, 17.258], loss: --, mean_squared_error: --, mean_q: --\n",
      "  312/2000: episode: 3, duration: 5.462s, episode steps: 105, steps per second: 19, episode reward: 0.654, mean reward: 0.006 [-0.001, 0.013], mean action: 0.098 [-1.170, 1.126], mean observation: 0.081 [-41.456, 17.423], loss: --, mean_squared_error: --, mean_q: --\n",
      "  419/2000: episode: 4, duration: 5.472s, episode steps: 107, steps per second: 20, episode reward: 0.669, mean reward: 0.006 [-0.001, 0.013], mean action: 0.132 [-1.162, 1.275], mean observation: 0.078 [-42.555, 17.609], loss: --, mean_squared_error: --, mean_q: --\n",
      "  524/2000: episode: 5, duration: 5.484s, episode steps: 105, steps per second: 19, episode reward: 0.655, mean reward: 0.006 [-0.001, 0.013], mean action: 0.109 [-1.247, 1.240], mean observation: 0.074 [-39.078, 17.306], loss: --, mean_squared_error: --, mean_q: --\n",
      "  628/2000: episode: 6, duration: 5.449s, episode steps: 104, steps per second: 19, episode reward: 0.645, mean reward: 0.006 [-0.001, 0.013], mean action: 0.119 [-1.165, 1.199], mean observation: 0.078 [-41.694, 17.637], loss: --, mean_squared_error: --, mean_q: --\n",
      "  735/2000: episode: 7, duration: 5.509s, episode steps: 107, steps per second: 19, episode reward: 0.662, mean reward: 0.006 [-0.001, 0.013], mean action: 0.125 [-1.240, 1.182], mean observation: 0.080 [-40.202, 17.221], loss: --, mean_squared_error: --, mean_q: --\n",
      "  843/2000: episode: 8, duration: 5.465s, episode steps: 108, steps per second: 20, episode reward: 0.672, mean reward: 0.006 [-0.001, 0.013], mean action: 0.122 [-1.149, 1.155], mean observation: 0.080 [-43.947, 17.449], loss: --, mean_squared_error: --, mean_q: --\n",
      "  949/2000: episode: 9, duration: 5.595s, episode steps: 106, steps per second: 19, episode reward: 0.654, mean reward: 0.006 [-0.001, 0.013], mean action: 0.117 [-1.127, 1.155], mean observation: 0.075 [-40.688, 17.358], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1051/2000: episode: 10, duration: 5.928s, episode steps: 102, steps per second: 17, episode reward: 0.638, mean reward: 0.006 [-0.001, 0.013], mean action: 0.096 [-1.152, 1.156], mean observation: 0.076 [-41.846, 18.177], loss: 0.000070, mean_squared_error: 0.000140, mean_q: 0.603740\n",
      " 1155/2000: episode: 11, duration: 6.364s, episode steps: 104, steps per second: 16, episode reward: 0.649, mean reward: 0.006 [-0.001, 0.013], mean action: 0.118 [-1.089, 1.134], mean observation: 0.076 [-45.044, 17.311], loss: 0.000289, mean_squared_error: 0.000579, mean_q: 0.594611\n",
      " 1254/2000: episode: 12, duration: 6.273s, episode steps: 99, steps per second: 16, episode reward: 0.617, mean reward: 0.006 [-0.002, 0.013], mean action: 0.110 [-1.165, 1.246], mean observation: 0.076 [-48.455, 17.491], loss: 0.000086, mean_squared_error: 0.000171, mean_q: 0.585717\n",
      " 1356/2000: episode: 13, duration: 6.506s, episode steps: 102, steps per second: 16, episode reward: 0.639, mean reward: 0.006 [-0.002, 0.013], mean action: 0.089 [-1.162, 1.154], mean observation: 0.080 [-42.608, 17.469], loss: 0.000239, mean_squared_error: 0.000477, mean_q: 0.594207\n",
      " 1462/2000: episode: 14, duration: 6.497s, episode steps: 106, steps per second: 16, episode reward: 0.657, mean reward: 0.006 [-0.002, 0.013], mean action: 0.093 [-1.157, 1.212], mean observation: 0.072 [-41.061, 17.382], loss: 0.000505, mean_squared_error: 0.001009, mean_q: 0.592384\n",
      " 1565/2000: episode: 15, duration: 6.550s, episode steps: 103, steps per second: 16, episode reward: 0.636, mean reward: 0.006 [-0.002, 0.013], mean action: 0.108 [-1.234, 1.133], mean observation: 0.069 [-43.414, 19.043], loss: 0.000355, mean_squared_error: 0.000710, mean_q: 0.607373\n",
      " 1671/2000: episode: 16, duration: 6.519s, episode steps: 106, steps per second: 16, episode reward: 0.642, mean reward: 0.006 [-0.002, 0.013], mean action: 0.106 [-1.128, 1.159], mean observation: 0.073 [-34.700, 17.521], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.599706\n",
      " 1774/2000: episode: 17, duration: 6.415s, episode steps: 103, steps per second: 16, episode reward: 0.631, mean reward: 0.006 [-0.002, 0.013], mean action: 0.081 [-1.206, 1.168], mean observation: 0.068 [-39.824, 17.405], loss: 0.000157, mean_squared_error: 0.000315, mean_q: 0.606466\n",
      " 1883/2000: episode: 18, duration: 6.459s, episode steps: 109, steps per second: 17, episode reward: 0.666, mean reward: 0.006 [-0.002, 0.013], mean action: 0.128 [-1.186, 1.222], mean observation: 0.085 [-44.318, 17.341], loss: 0.000092, mean_squared_error: 0.000183, mean_q: 0.594366\n",
      " 1992/2000: episode: 19, duration: 6.633s, episode steps: 109, steps per second: 16, episode reward: 0.674, mean reward: 0.006 [-0.002, 0.013], mean action: 0.175 [-1.157, 1.263], mean observation: 0.080 [-45.453, 17.479], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.590913\n",
      "done, took 114.122 seconds\n",
      "\n",
      "\n",
      "iteration: 72\n",
      "Training for 2000 steps ...\n",
      "  110/2000: episode: 1, duration: 5.567s, episode steps: 110, steps per second: 20, episode reward: 0.674, mean reward: 0.006 [-0.002, 0.014], mean action: 0.042 [-1.164, 1.167], mean observation: 0.077 [-33.672, 17.301], loss: --, mean_squared_error: --, mean_q: --\n",
      "  221/2000: episode: 2, duration: 5.724s, episode steps: 111, steps per second: 19, episode reward: 0.683, mean reward: 0.006 [-0.002, 0.014], mean action: 0.052 [-1.307, 1.167], mean observation: 0.080 [-27.914, 20.444], loss: --, mean_squared_error: --, mean_q: --\n",
      "  333/2000: episode: 3, duration: 5.653s, episode steps: 112, steps per second: 20, episode reward: 0.689, mean reward: 0.006 [-0.002, 0.014], mean action: 0.053 [-1.180, 1.193], mean observation: 0.080 [-31.069, 22.034], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  443/2000: episode: 4, duration: 5.552s, episode steps: 110, steps per second: 20, episode reward: 0.679, mean reward: 0.006 [-0.002, 0.014], mean action: 0.048 [-1.241, 1.247], mean observation: 0.079 [-38.218, 20.026], loss: --, mean_squared_error: --, mean_q: --\n",
      "  554/2000: episode: 5, duration: 5.613s, episode steps: 111, steps per second: 20, episode reward: 0.685, mean reward: 0.006 [-0.001, 0.014], mean action: 0.068 [-1.172, 1.232], mean observation: 0.080 [-33.934, 17.476], loss: --, mean_squared_error: --, mean_q: --\n",
      "  667/2000: episode: 6, duration: 5.548s, episode steps: 113, steps per second: 20, episode reward: 0.699, mean reward: 0.006 [-0.001, 0.014], mean action: 0.071 [-1.213, 1.243], mean observation: 0.081 [-27.346, 17.701], loss: --, mean_squared_error: --, mean_q: --\n",
      "  778/2000: episode: 7, duration: 5.408s, episode steps: 111, steps per second: 21, episode reward: 0.680, mean reward: 0.006 [-0.002, 0.014], mean action: 0.062 [-1.187, 1.232], mean observation: 0.078 [-33.927, 17.389], loss: --, mean_squared_error: --, mean_q: --\n",
      "  888/2000: episode: 8, duration: 5.713s, episode steps: 110, steps per second: 19, episode reward: 0.681, mean reward: 0.006 [-0.002, 0.014], mean action: 0.102 [-1.139, 1.292], mean observation: 0.079 [-46.356, 19.568], loss: --, mean_squared_error: --, mean_q: --\n",
      "  997/2000: episode: 9, duration: 5.785s, episode steps: 109, steps per second: 19, episode reward: 0.667, mean reward: 0.006 [-0.002, 0.014], mean action: 0.057 [-1.182, 1.130], mean observation: 0.078 [-40.307, 19.425], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1107/2000: episode: 10, duration: 6.732s, episode steps: 110, steps per second: 16, episode reward: 0.674, mean reward: 0.006 [-0.001, 0.014], mean action: 0.036 [-1.197, 1.288], mean observation: 0.079 [-41.548, 19.848], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.609036\n",
      " 1215/2000: episode: 11, duration: 6.463s, episode steps: 108, steps per second: 17, episode reward: 0.681, mean reward: 0.006 [-0.002, 0.014], mean action: 0.084 [-1.135, 1.196], mean observation: 0.081 [-43.355, 17.226], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.595216\n",
      " 1321/2000: episode: 12, duration: 6.501s, episode steps: 106, steps per second: 16, episode reward: 0.648, mean reward: 0.006 [-0.002, 0.014], mean action: 0.095 [-1.142, 1.225], mean observation: 0.075 [-21.354, 17.924], loss: 0.000059, mean_squared_error: 0.000119, mean_q: 0.580087\n",
      " 1428/2000: episode: 13, duration: 6.316s, episode steps: 107, steps per second: 17, episode reward: 0.666, mean reward: 0.006 [-0.002, 0.014], mean action: 0.020 [-1.158, 1.150], mean observation: 0.079 [-23.303, 18.021], loss: 0.000282, mean_squared_error: 0.000564, mean_q: 0.591289\n",
      " 1536/2000: episode: 14, duration: 6.433s, episode steps: 108, steps per second: 17, episode reward: 0.676, mean reward: 0.006 [-0.002, 0.013], mean action: -0.057 [-1.212, 1.176], mean observation: 0.079 [-13.729, 18.070], loss: 0.000246, mean_squared_error: 0.000491, mean_q: 0.591826\n",
      " 1642/2000: episode: 15, duration: 6.317s, episode steps: 106, steps per second: 17, episode reward: 0.673, mean reward: 0.006 [-0.002, 0.013], mean action: -0.083 [-1.408, 1.126], mean observation: 0.079 [-11.556, 17.756], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.594277\n",
      " 1748/2000: episode: 16, duration: 6.273s, episode steps: 106, steps per second: 17, episode reward: 0.666, mean reward: 0.006 [-0.001, 0.013], mean action: -0.043 [-1.199, 1.223], mean observation: 0.077 [-22.083, 17.890], loss: 0.000420, mean_squared_error: 0.000840, mean_q: 0.581931\n",
      " 1853/2000: episode: 17, duration: 6.153s, episode steps: 105, steps per second: 17, episode reward: 0.673, mean reward: 0.006 [-0.002, 0.013], mean action: -0.000 [-1.125, 1.216], mean observation: 0.078 [-22.219, 17.797], loss: 0.000437, mean_squared_error: 0.000874, mean_q: 0.598767\n",
      " 1959/2000: episode: 18, duration: 6.321s, episode steps: 106, steps per second: 17, episode reward: 0.678, mean reward: 0.006 [-0.002, 0.013], mean action: -0.033 [-1.264, 1.179], mean observation: 0.079 [-20.094, 18.026], loss: 0.000103, mean_squared_error: 0.000206, mean_q: 0.607163\n",
      "done, took 110.675 seconds\n",
      "\n",
      "\n",
      "iteration: 73\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 5.337s, episode steps: 105, steps per second: 20, episode reward: 0.670, mean reward: 0.006 [-0.002, 0.013], mean action: -0.020 [-1.136, 1.147], mean observation: 0.084 [-43.000, 17.937], loss: --, mean_squared_error: --, mean_q: --\n",
      "  213/2000: episode: 2, duration: 5.243s, episode steps: 108, steps per second: 21, episode reward: 0.676, mean reward: 0.006 [-0.002, 0.013], mean action: -0.006 [-1.106, 1.328], mean observation: 0.078 [-10.352, 17.918], loss: --, mean_squared_error: --, mean_q: --\n",
      "  319/2000: episode: 3, duration: 5.288s, episode steps: 106, steps per second: 20, episode reward: 0.681, mean reward: 0.006 [-0.002, 0.013], mean action: -0.001 [-1.146, 1.151], mean observation: 0.081 [-42.439, 17.924], loss: --, mean_squared_error: --, mean_q: --\n",
      "  424/2000: episode: 4, duration: 5.281s, episode steps: 105, steps per second: 20, episode reward: 0.673, mean reward: 0.006 [-0.002, 0.013], mean action: -0.022 [-1.174, 1.109], mean observation: 0.076 [-41.977, 17.972], loss: --, mean_squared_error: --, mean_q: --\n",
      "  533/2000: episode: 5, duration: 5.219s, episode steps: 109, steps per second: 21, episode reward: 0.675, mean reward: 0.006 [-0.002, 0.013], mean action: 0.002 [-1.157, 1.222], mean observation: 0.079 [-10.353, 17.855], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 6, duration: 5.334s, episode steps: 109, steps per second: 20, episode reward: 0.677, mean reward: 0.006 [-0.002, 0.013], mean action: -0.004 [-1.231, 1.202], mean observation: 0.079 [-11.214, 17.889], loss: --, mean_squared_error: --, mean_q: --\n",
      "  750/2000: episode: 7, duration: 5.261s, episode steps: 108, steps per second: 21, episode reward: 0.668, mean reward: 0.006 [-0.002, 0.013], mean action: -0.018 [-1.164, 1.243], mean observation: 0.079 [-10.435, 17.882], loss: --, mean_squared_error: --, mean_q: --\n",
      "  859/2000: episode: 8, duration: 5.222s, episode steps: 109, steps per second: 21, episode reward: 0.679, mean reward: 0.006 [-0.002, 0.013], mean action: -0.015 [-1.173, 1.137], mean observation: 0.079 [-10.284, 17.753], loss: --, mean_squared_error: --, mean_q: --\n",
      "  967/2000: episode: 9, duration: 5.144s, episode steps: 108, steps per second: 21, episode reward: 0.665, mean reward: 0.006 [-0.002, 0.013], mean action: -0.017 [-1.109, 1.140], mean observation: 0.079 [-10.242, 17.978], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1076/2000: episode: 10, duration: 6.095s, episode steps: 109, steps per second: 18, episode reward: 0.676, mean reward: 0.006 [-0.002, 0.013], mean action: -0.023 [-1.271, 1.129], mean observation: 0.080 [-10.429, 18.061], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.596424\n",
      " 1184/2000: episode: 11, duration: 6.500s, episode steps: 108, steps per second: 17, episode reward: 0.687, mean reward: 0.006 [-0.002, 0.013], mean action: -0.041 [-1.281, 1.232], mean observation: 0.077 [-41.701, 21.683], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.597862\n",
      " 1289/2000: episode: 12, duration: 6.397s, episode steps: 105, steps per second: 16, episode reward: 0.656, mean reward: 0.006 [-0.001, 0.014], mean action: -0.023 [-1.210, 1.147], mean observation: 0.078 [-45.566, 19.111], loss: 0.000135, mean_squared_error: 0.000271, mean_q: 0.591450\n",
      " 1397/2000: episode: 13, duration: 6.566s, episode steps: 108, steps per second: 16, episode reward: 0.670, mean reward: 0.006 [-0.001, 0.014], mean action: 0.010 [-1.127, 1.141], mean observation: 0.072 [-33.292, 17.832], loss: 0.000252, mean_squared_error: 0.000504, mean_q: 0.590557\n",
      " 1508/2000: episode: 14, duration: 6.889s, episode steps: 111, steps per second: 16, episode reward: 0.684, mean reward: 0.006 [-0.002, 0.014], mean action: 0.004 [-1.113, 1.258], mean observation: 0.082 [-33.071, 18.069], loss: 0.000273, mean_squared_error: 0.000546, mean_q: 0.581999\n",
      " 1623/2000: episode: 15, duration: 6.373s, episode steps: 115, steps per second: 18, episode reward: 0.715, mean reward: 0.006 [0.000, 0.013], mean action: 0.057 [-1.120, 1.142], mean observation: 0.094 [-20.080, 17.821], loss: 0.000288, mean_squared_error: 0.000575, mean_q: 0.590720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1742/2000: episode: 16, duration: 6.693s, episode steps: 119, steps per second: 18, episode reward: 0.735, mean reward: 0.006 [0.000, 0.014], mean action: 0.054 [-1.227, 1.142], mean observation: 0.095 [-36.681, 17.919], loss: 0.000304, mean_squared_error: 0.000608, mean_q: 0.581927\n",
      " 1869/2000: episode: 17, duration: 6.478s, episode steps: 127, steps per second: 20, episode reward: 0.770, mean reward: 0.006 [0.000, 0.013], mean action: 0.034 [-1.217, 1.257], mean observation: 0.102 [-11.112, 17.813], loss: 0.000180, mean_squared_error: 0.000361, mean_q: 0.602786\n",
      " 1990/2000: episode: 18, duration: 6.475s, episode steps: 121, steps per second: 19, episode reward: 0.742, mean reward: 0.006 [-0.000, 0.013], mean action: 0.010 [-1.196, 1.138], mean observation: 0.093 [-39.706, 17.754], loss: 0.000313, mean_squared_error: 0.000627, mean_q: 0.581198\n",
      "done, took 106.680 seconds\n",
      "\n",
      "\n",
      "iteration: 74\n",
      "Training for 2000 steps ...\n",
      "  124/2000: episode: 1, duration: 5.276s, episode steps: 124, steps per second: 24, episode reward: 0.738, mean reward: 0.006 [-0.000, 0.014], mean action: 0.033 [-1.197, 1.137], mean observation: 0.099 [-39.751, 17.862], loss: --, mean_squared_error: --, mean_q: --\n",
      "  250/2000: episode: 2, duration: 5.498s, episode steps: 126, steps per second: 23, episode reward: 0.759, mean reward: 0.006 [-0.000, 0.014], mean action: 0.018 [-1.233, 1.177], mean observation: 0.096 [-40.355, 17.960], loss: --, mean_squared_error: --, mean_q: --\n",
      "  374/2000: episode: 3, duration: 5.279s, episode steps: 124, steps per second: 23, episode reward: 0.737, mean reward: 0.006 [-0.001, 0.014], mean action: 0.042 [-1.109, 1.165], mean observation: 0.099 [-42.573, 17.834], loss: --, mean_squared_error: --, mean_q: --\n",
      "  498/2000: episode: 4, duration: 5.319s, episode steps: 124, steps per second: 23, episode reward: 0.741, mean reward: 0.006 [-0.001, 0.014], mean action: 0.046 [-1.150, 1.210], mean observation: 0.098 [-39.807, 17.973], loss: --, mean_squared_error: --, mean_q: --\n",
      "  623/2000: episode: 5, duration: 5.286s, episode steps: 125, steps per second: 24, episode reward: 0.751, mean reward: 0.006 [-0.001, 0.014], mean action: 0.004 [-1.148, 1.163], mean observation: 0.096 [-38.631, 17.890], loss: --, mean_squared_error: --, mean_q: --\n",
      "  748/2000: episode: 6, duration: 5.497s, episode steps: 125, steps per second: 23, episode reward: 0.743, mean reward: 0.006 [-0.001, 0.014], mean action: 0.050 [-1.115, 1.158], mean observation: 0.101 [-40.260, 18.020], loss: --, mean_squared_error: --, mean_q: --\n",
      "  870/2000: episode: 7, duration: 5.332s, episode steps: 122, steps per second: 23, episode reward: 0.727, mean reward: 0.006 [-0.000, 0.014], mean action: 0.043 [-1.160, 1.174], mean observation: 0.100 [-39.394, 17.605], loss: --, mean_squared_error: --, mean_q: --\n",
      "  997/2000: episode: 8, duration: 5.528s, episode steps: 127, steps per second: 23, episode reward: 0.750, mean reward: 0.006 [-0.001, 0.014], mean action: 0.021 [-1.146, 1.113], mean observation: 0.097 [-32.383, 17.796], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1121/2000: episode: 9, duration: 6.512s, episode steps: 124, steps per second: 19, episode reward: 0.739, mean reward: 0.006 [-0.001, 0.014], mean action: 0.013 [-1.193, 1.157], mean observation: 0.097 [-39.956, 17.890], loss: 0.000291, mean_squared_error: 0.000581, mean_q: 0.587300\n",
      " 1239/2000: episode: 10, duration: 6.476s, episode steps: 118, steps per second: 18, episode reward: 0.718, mean reward: 0.006 [-0.001, 0.014], mean action: -0.028 [-1.193, 1.191], mean observation: 0.088 [-40.468, 17.899], loss: 0.000216, mean_squared_error: 0.000432, mean_q: 0.589138\n",
      " 1361/2000: episode: 11, duration: 6.683s, episode steps: 122, steps per second: 18, episode reward: 0.696, mean reward: 0.006 [-0.001, 0.013], mean action: 0.013 [-1.232, 1.221], mean observation: 0.105 [-40.846, 17.562], loss: 0.000114, mean_squared_error: 0.000227, mean_q: 0.586840\n",
      " 1477/2000: episode: 12, duration: 6.954s, episode steps: 116, steps per second: 17, episode reward: 0.634, mean reward: 0.005 [-0.002, 0.013], mean action: 0.010 [-1.202, 1.163], mean observation: 0.102 [-42.427, 17.814], loss: 0.000152, mean_squared_error: 0.000304, mean_q: 0.587296\n",
      " 1587/2000: episode: 13, duration: 6.989s, episode steps: 110, steps per second: 16, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.014], mean action: -0.011 [-1.233, 1.271], mean observation: 0.094 [-38.289, 17.737], loss: 0.000431, mean_squared_error: 0.000862, mean_q: 0.594495\n",
      " 1699/2000: episode: 14, duration: 7.083s, episode steps: 112, steps per second: 16, episode reward: 0.655, mean reward: 0.006 [0.000, 0.014], mean action: -0.014 [-1.216, 1.165], mean observation: 0.094 [-38.621, 17.814], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.582280\n",
      " 1816/2000: episode: 15, duration: 7.304s, episode steps: 117, steps per second: 16, episode reward: 0.677, mean reward: 0.006 [-0.002, 0.014], mean action: 0.020 [-1.147, 1.203], mean observation: 0.105 [-14.270, 17.988], loss: 0.000171, mean_squared_error: 0.000342, mean_q: 0.597401\n",
      " 1931/2000: episode: 16, duration: 7.160s, episode steps: 115, steps per second: 16, episode reward: 0.674, mean reward: 0.006 [-0.002, 0.014], mean action: 0.006 [-1.192, 1.197], mean observation: 0.103 [-14.117, 17.898], loss: 0.000237, mean_squared_error: 0.000474, mean_q: 0.578660\n",
      "done, took 102.247 seconds\n",
      "\n",
      "\n",
      "iteration: 75\n",
      "Training for 2000 steps ...\n",
      "  118/2000: episode: 1, duration: 6.111s, episode steps: 118, steps per second: 19, episode reward: 0.675, mean reward: 0.006 [-0.002, 0.014], mean action: -0.000 [-1.224, 1.218], mean observation: 0.104 [-14.160, 17.855], loss: --, mean_squared_error: --, mean_q: --\n",
      "  236/2000: episode: 2, duration: 6.045s, episode steps: 118, steps per second: 20, episode reward: 0.682, mean reward: 0.006 [-0.002, 0.014], mean action: 0.006 [-1.177, 1.131], mean observation: 0.104 [-14.089, 17.671], loss: --, mean_squared_error: --, mean_q: --\n",
      "  358/2000: episode: 3, duration: 6.343s, episode steps: 122, steps per second: 19, episode reward: 0.696, mean reward: 0.006 [-0.002, 0.014], mean action: 0.031 [-1.128, 1.195], mean observation: 0.106 [-14.934, 18.063], loss: --, mean_squared_error: --, mean_q: --\n",
      "  477/2000: episode: 4, duration: 6.236s, episode steps: 119, steps per second: 19, episode reward: 0.684, mean reward: 0.006 [-0.002, 0.014], mean action: 0.011 [-1.226, 1.220], mean observation: 0.102 [-14.441, 17.854], loss: --, mean_squared_error: --, mean_q: --\n",
      "  595/2000: episode: 5, duration: 6.016s, episode steps: 118, steps per second: 20, episode reward: 0.686, mean reward: 0.006 [-0.002, 0.014], mean action: -0.001 [-1.192, 1.097], mean observation: 0.104 [-14.409, 17.848], loss: --, mean_squared_error: --, mean_q: --\n",
      "  716/2000: episode: 6, duration: 6.184s, episode steps: 121, steps per second: 20, episode reward: 0.701, mean reward: 0.006 [-0.002, 0.014], mean action: 0.018 [-1.175, 1.138], mean observation: 0.104 [-14.582, 17.787], loss: --, mean_squared_error: --, mean_q: --\n",
      "  834/2000: episode: 7, duration: 6.018s, episode steps: 118, steps per second: 20, episode reward: 0.683, mean reward: 0.006 [-0.002, 0.014], mean action: 0.006 [-1.273, 1.156], mean observation: 0.103 [-13.794, 17.766], loss: --, mean_squared_error: --, mean_q: --\n",
      "  952/2000: episode: 8, duration: 6.009s, episode steps: 118, steps per second: 20, episode reward: 0.691, mean reward: 0.006 [-0.002, 0.014], mean action: 0.025 [-1.140, 1.152], mean observation: 0.105 [-14.542, 17.715], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1070/2000: episode: 9, duration: 6.750s, episode steps: 118, steps per second: 17, episode reward: 0.679, mean reward: 0.006 [-0.002, 0.014], mean action: 0.028 [-1.117, 1.233], mean observation: 0.103 [-14.205, 17.762], loss: 0.000157, mean_squared_error: 0.000315, mean_q: 0.581350\n",
      " 1185/2000: episode: 10, duration: 6.979s, episode steps: 115, steps per second: 16, episode reward: 0.682, mean reward: 0.006 [-0.001, 0.014], mean action: -0.026 [-1.165, 1.132], mean observation: 0.105 [-14.194, 17.733], loss: 0.000285, mean_squared_error: 0.000569, mean_q: 0.592329\n",
      " 1299/2000: episode: 11, duration: 6.809s, episode steps: 114, steps per second: 17, episode reward: 0.670, mean reward: 0.006 [-0.000, 0.014], mean action: -0.054 [-1.229, 1.229], mean observation: 0.102 [-13.339, 17.117], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.586814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1416/2000: episode: 12, duration: 6.973s, episode steps: 117, steps per second: 17, episode reward: 0.680, mean reward: 0.006 [0.000, 0.014], mean action: -0.059 [-1.151, 1.124], mean observation: 0.106 [-14.701, 17.524], loss: 0.000074, mean_squared_error: 0.000148, mean_q: 0.587558\n",
      " 1530/2000: episode: 13, duration: 6.974s, episode steps: 114, steps per second: 16, episode reward: 0.692, mean reward: 0.006 [0.000, 0.014], mean action: -0.046 [-1.255, 1.087], mean observation: 0.104 [-14.318, 17.165], loss: 0.000146, mean_squared_error: 0.000291, mean_q: 0.592864\n",
      " 1641/2000: episode: 14, duration: 6.737s, episode steps: 111, steps per second: 16, episode reward: 0.647, mean reward: 0.006 [0.000, 0.014], mean action: 0.003 [-1.152, 1.245], mean observation: 0.102 [-13.109, 17.333], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.584262\n",
      " 1753/2000: episode: 15, duration: 6.596s, episode steps: 112, steps per second: 17, episode reward: 0.660, mean reward: 0.006 [0.000, 0.014], mean action: 0.050 [-1.169, 1.166], mean observation: 0.107 [-30.176, 17.847], loss: 0.000070, mean_squared_error: 0.000141, mean_q: 0.587827\n",
      " 1863/2000: episode: 16, duration: 6.659s, episode steps: 110, steps per second: 17, episode reward: 0.654, mean reward: 0.006 [0.000, 0.014], mean action: -0.011 [-1.179, 1.164], mean observation: 0.106 [-36.469, 17.835], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.585829\n",
      " 1973/2000: episode: 17, duration: 6.684s, episode steps: 110, steps per second: 16, episode reward: 0.648, mean reward: 0.006 [0.000, 0.014], mean action: 0.015 [-1.213, 1.201], mean observation: 0.103 [-37.090, 18.025], loss: 0.000202, mean_squared_error: 0.000404, mean_q: 0.585140\n",
      "done, took 111.873 seconds\n",
      "\n",
      "\n",
      "iteration: 76\n",
      "Training for 2000 steps ...\n",
      "  113/2000: episode: 1, duration: 5.745s, episode steps: 113, steps per second: 20, episode reward: 0.668, mean reward: 0.006 [0.000, 0.014], mean action: 0.131 [-1.214, 1.188], mean observation: 0.106 [-14.341, 17.745], loss: --, mean_squared_error: --, mean_q: --\n",
      "  225/2000: episode: 2, duration: 5.684s, episode steps: 112, steps per second: 20, episode reward: 0.662, mean reward: 0.006 [0.000, 0.014], mean action: 0.135 [-1.163, 1.180], mean observation: 0.104 [-14.294, 17.903], loss: --, mean_squared_error: --, mean_q: --\n",
      "  337/2000: episode: 3, duration: 5.687s, episode steps: 112, steps per second: 20, episode reward: 0.658, mean reward: 0.006 [0.000, 0.014], mean action: 0.127 [-1.223, 1.173], mean observation: 0.105 [-14.323, 17.828], loss: --, mean_squared_error: --, mean_q: --\n",
      "  449/2000: episode: 4, duration: 5.651s, episode steps: 112, steps per second: 20, episode reward: 0.654, mean reward: 0.006 [0.000, 0.014], mean action: 0.131 [-1.121, 1.162], mean observation: 0.102 [-14.090, 17.838], loss: --, mean_squared_error: --, mean_q: --\n",
      "  562/2000: episode: 5, duration: 5.799s, episode steps: 113, steps per second: 19, episode reward: 0.673, mean reward: 0.006 [0.000, 0.014], mean action: 0.126 [-1.159, 1.153], mean observation: 0.106 [-14.742, 17.823], loss: --, mean_squared_error: --, mean_q: --\n",
      "  675/2000: episode: 6, duration: 5.668s, episode steps: 113, steps per second: 20, episode reward: 0.671, mean reward: 0.006 [0.000, 0.014], mean action: 0.131 [-1.125, 1.167], mean observation: 0.101 [-14.616, 17.878], loss: --, mean_squared_error: --, mean_q: --\n",
      "  789/2000: episode: 7, duration: 5.880s, episode steps: 114, steps per second: 19, episode reward: 0.684, mean reward: 0.006 [0.000, 0.014], mean action: 0.131 [-1.109, 1.201], mean observation: 0.105 [-14.423, 17.850], loss: --, mean_squared_error: --, mean_q: --\n",
      "  902/2000: episode: 8, duration: 5.734s, episode steps: 113, steps per second: 20, episode reward: 0.671, mean reward: 0.006 [0.000, 0.014], mean action: 0.127 [-1.149, 1.148], mean observation: 0.104 [-14.359, 17.861], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1015/2000: episode: 9, duration: 5.854s, episode steps: 113, steps per second: 19, episode reward: 0.673, mean reward: 0.006 [0.000, 0.014], mean action: 0.130 [-1.173, 1.117], mean observation: 0.104 [-14.602, 17.929], loss: 0.000181, mean_squared_error: 0.000363, mean_q: 0.598329\n",
      " 1126/2000: episode: 10, duration: 6.501s, episode steps: 111, steps per second: 17, episode reward: 0.668, mean reward: 0.006 [0.000, 0.014], mean action: 0.159 [-1.183, 1.213], mean observation: 0.101 [-14.306, 17.928], loss: 0.000256, mean_squared_error: 0.000512, mean_q: 0.587415\n",
      " 1238/2000: episode: 11, duration: 6.577s, episode steps: 112, steps per second: 17, episode reward: 0.681, mean reward: 0.006 [0.000, 0.014], mean action: 0.104 [-1.173, 1.168], mean observation: 0.101 [-14.550, 17.820], loss: 0.000052, mean_squared_error: 0.000105, mean_q: 0.586809\n",
      " 1354/2000: episode: 12, duration: 7.041s, episode steps: 116, steps per second: 16, episode reward: 0.701, mean reward: 0.006 [0.000, 0.014], mean action: 0.081 [-1.159, 1.180], mean observation: 0.102 [-14.602, 17.745], loss: 0.000318, mean_squared_error: 0.000637, mean_q: 0.581859\n",
      " 1469/2000: episode: 13, duration: 6.698s, episode steps: 115, steps per second: 17, episode reward: 0.719, mean reward: 0.006 [0.000, 0.014], mean action: 0.007 [-1.258, 1.137], mean observation: 0.097 [-14.886, 17.849], loss: 0.000358, mean_squared_error: 0.000716, mean_q: 0.587329\n",
      " 1587/2000: episode: 14, duration: 6.879s, episode steps: 118, steps per second: 17, episode reward: 0.736, mean reward: 0.006 [0.000, 0.014], mean action: 0.015 [-1.096, 1.126], mean observation: 0.100 [-14.173, 17.910], loss: 0.000295, mean_squared_error: 0.000589, mean_q: 0.585601\n",
      " 1702/2000: episode: 15, duration: 6.988s, episode steps: 115, steps per second: 16, episode reward: 0.718, mean reward: 0.006 [0.000, 0.014], mean action: -0.018 [-1.186, 1.176], mean observation: 0.101 [-14.554, 17.882], loss: 0.000147, mean_squared_error: 0.000295, mean_q: 0.583985\n",
      " 1820/2000: episode: 16, duration: 6.987s, episode steps: 118, steps per second: 17, episode reward: 0.719, mean reward: 0.006 [0.000, 0.013], mean action: -0.013 [-1.177, 1.174], mean observation: 0.106 [-14.373, 17.819], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.584141\n",
      " 1943/2000: episode: 17, duration: 6.801s, episode steps: 123, steps per second: 18, episode reward: 0.741, mean reward: 0.006 [0.000, 0.013], mean action: -0.069 [-1.145, 1.210], mean observation: 0.096 [-14.483, 17.791], loss: 0.000082, mean_squared_error: 0.000165, mean_q: 0.582021\n",
      "done, took 109.234 seconds\n",
      "\n",
      "\n",
      "iteration: 77\n",
      "Training for 2000 steps ...\n",
      "  124/2000: episode: 1, duration: 6.093s, episode steps: 124, steps per second: 20, episode reward: 0.737, mean reward: 0.006 [0.000, 0.014], mean action: -0.093 [-1.116, 1.129], mean observation: 0.107 [-14.377, 17.842], loss: --, mean_squared_error: --, mean_q: --\n",
      "  248/2000: episode: 2, duration: 5.666s, episode steps: 124, steps per second: 22, episode reward: 0.740, mean reward: 0.006 [0.000, 0.014], mean action: -0.110 [-1.231, 1.189], mean observation: 0.105 [-14.498, 18.137], loss: --, mean_squared_error: --, mean_q: --\n",
      "  374/2000: episode: 3, duration: 6.008s, episode steps: 126, steps per second: 21, episode reward: 0.753, mean reward: 0.006 [0.000, 0.014], mean action: -0.139 [-1.120, 1.145], mean observation: 0.097 [-14.828, 18.040], loss: --, mean_squared_error: --, mean_q: --\n",
      "  500/2000: episode: 4, duration: 5.909s, episode steps: 126, steps per second: 21, episode reward: 0.756, mean reward: 0.006 [0.000, 0.014], mean action: -0.092 [-1.207, 1.128], mean observation: 0.109 [-14.424, 17.842], loss: --, mean_squared_error: --, mean_q: --\n",
      "  625/2000: episode: 5, duration: 5.949s, episode steps: 125, steps per second: 21, episode reward: 0.744, mean reward: 0.006 [0.000, 0.014], mean action: -0.101 [-1.194, 1.259], mean observation: 0.106 [-14.707, 17.875], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 6, duration: 6.072s, episode steps: 122, steps per second: 20, episode reward: 0.725, mean reward: 0.006 [0.000, 0.014], mean action: -0.109 [-1.256, 1.155], mean observation: 0.110 [-14.349, 17.969], loss: --, mean_squared_error: --, mean_q: --\n",
      "  872/2000: episode: 7, duration: 5.912s, episode steps: 125, steps per second: 21, episode reward: 0.734, mean reward: 0.006 [0.000, 0.014], mean action: -0.122 [-1.150, 1.229], mean observation: 0.095 [-14.604, 18.152], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  991/2000: episode: 8, duration: 5.935s, episode steps: 119, steps per second: 20, episode reward: 0.699, mean reward: 0.006 [0.000, 0.013], mean action: -0.101 [-1.223, 1.160], mean observation: 0.101 [-44.930, 17.872], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1118/2000: episode: 9, duration: 7.143s, episode steps: 127, steps per second: 18, episode reward: 0.761, mean reward: 0.006 [0.000, 0.014], mean action: -0.139 [-1.232, 1.166], mean observation: 0.098 [-14.226, 17.782], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.581994\n",
      " 1233/2000: episode: 10, duration: 7.101s, episode steps: 115, steps per second: 16, episode reward: 0.685, mean reward: 0.006 [0.000, 0.013], mean action: -0.102 [-1.192, 1.233], mean observation: 0.101 [-37.448, 21.106], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.582332\n",
      " 1358/2000: episode: 11, duration: 7.496s, episode steps: 125, steps per second: 17, episode reward: 0.697, mean reward: 0.006 [0.000, 0.013], mean action: -0.053 [-1.111, 1.364], mean observation: 0.094 [-38.023, 20.218], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.587379\n",
      " 1481/2000: episode: 12, duration: 7.525s, episode steps: 123, steps per second: 16, episode reward: 0.705, mean reward: 0.006 [0.000, 0.013], mean action: -0.044 [-1.140, 1.181], mean observation: 0.094 [-30.935, 17.791], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.595639\n",
      " 1604/2000: episode: 13, duration: 7.752s, episode steps: 123, steps per second: 16, episode reward: 0.687, mean reward: 0.006 [0.000, 0.013], mean action: -0.049 [-1.247, 1.258], mean observation: 0.090 [-28.648, 17.850], loss: 0.000695, mean_squared_error: 0.001391, mean_q: 0.593967\n",
      " 1728/2000: episode: 14, duration: 7.891s, episode steps: 124, steps per second: 16, episode reward: 0.713, mean reward: 0.006 [0.000, 0.013], mean action: -0.107 [-1.209, 1.114], mean observation: 0.096 [-25.451, 17.760], loss: 0.000126, mean_squared_error: 0.000253, mean_q: 0.590733\n",
      " 1841/2000: episode: 15, duration: 7.644s, episode steps: 113, steps per second: 15, episode reward: 0.652, mean reward: 0.006 [-0.000, 0.014], mean action: -0.051 [-1.221, 1.243], mean observation: 0.091 [-44.017, 17.458], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.595435\n",
      " 1948/2000: episode: 16, duration: 7.470s, episode steps: 107, steps per second: 14, episode reward: 0.623, mean reward: 0.006 [-0.000, 0.013], mean action: -0.055 [-1.129, 1.196], mean observation: 0.089 [-50.002, 17.648], loss: 0.000340, mean_squared_error: 0.000680, mean_q: 0.573511\n",
      "done, took 111.187 seconds\n",
      "\n",
      "\n",
      "iteration: 78\n",
      "Training for 2000 steps ...\n",
      "  109/2000: episode: 1, duration: 6.282s, episode steps: 109, steps per second: 17, episode reward: 0.637, mean reward: 0.006 [-0.000, 0.013], mean action: -0.078 [-1.212, 1.131], mean observation: 0.089 [-50.073, 17.314], loss: --, mean_squared_error: --, mean_q: --\n",
      "  218/2000: episode: 2, duration: 6.265s, episode steps: 109, steps per second: 17, episode reward: 0.638, mean reward: 0.006 [-0.000, 0.013], mean action: -0.066 [-1.234, 1.167], mean observation: 0.084 [-48.087, 17.286], loss: --, mean_squared_error: --, mean_q: --\n",
      "  327/2000: episode: 3, duration: 6.437s, episode steps: 109, steps per second: 17, episode reward: 0.629, mean reward: 0.006 [-0.000, 0.013], mean action: -0.061 [-1.243, 1.144], mean observation: 0.087 [-50.352, 17.706], loss: --, mean_squared_error: --, mean_q: --\n",
      "  434/2000: episode: 4, duration: 6.235s, episode steps: 107, steps per second: 17, episode reward: 0.634, mean reward: 0.006 [-0.000, 0.013], mean action: -0.071 [-1.189, 1.123], mean observation: 0.081 [-50.027, 18.596], loss: --, mean_squared_error: --, mean_q: --\n",
      "  542/2000: episode: 5, duration: 6.429s, episode steps: 108, steps per second: 17, episode reward: 0.635, mean reward: 0.006 [-0.000, 0.013], mean action: -0.028 [-1.096, 1.306], mean observation: 0.075 [-36.300, 18.719], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 6, duration: 6.357s, episode steps: 107, steps per second: 17, episode reward: 0.634, mean reward: 0.006 [-0.000, 0.013], mean action: -0.031 [-1.235, 1.178], mean observation: 0.078 [-48.943, 18.629], loss: --, mean_squared_error: --, mean_q: --\n",
      "  756/2000: episode: 7, duration: 6.336s, episode steps: 107, steps per second: 17, episode reward: 0.625, mean reward: 0.006 [-0.000, 0.013], mean action: -0.041 [-1.094, 1.104], mean observation: 0.084 [-49.472, 17.339], loss: --, mean_squared_error: --, mean_q: --\n",
      "  864/2000: episode: 8, duration: 6.294s, episode steps: 108, steps per second: 17, episode reward: 0.631, mean reward: 0.006 [-0.000, 0.013], mean action: -0.057 [-1.208, 1.112], mean observation: 0.088 [-50.269, 17.523], loss: --, mean_squared_error: --, mean_q: --\n",
      "  972/2000: episode: 9, duration: 6.318s, episode steps: 108, steps per second: 17, episode reward: 0.639, mean reward: 0.006 [-0.000, 0.013], mean action: -0.066 [-1.107, 1.099], mean observation: 0.081 [-48.271, 18.332], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1080/2000: episode: 10, duration: 7.120s, episode steps: 108, steps per second: 15, episode reward: 0.629, mean reward: 0.006 [-0.000, 0.013], mean action: -0.051 [-1.131, 1.182], mean observation: 0.084 [-48.972, 17.562], loss: 0.000206, mean_squared_error: 0.000412, mean_q: 0.586219\n",
      " 1190/2000: episode: 11, duration: 7.577s, episode steps: 110, steps per second: 15, episode reward: 0.633, mean reward: 0.006 [-0.001, 0.013], mean action: -0.061 [-1.256, 1.119], mean observation: 0.078 [-39.770, 17.853], loss: 0.000155, mean_squared_error: 0.000311, mean_q: 0.597199\n",
      " 1300/2000: episode: 12, duration: 7.385s, episode steps: 110, steps per second: 15, episode reward: 0.630, mean reward: 0.006 [-0.001, 0.013], mean action: -0.041 [-1.192, 1.186], mean observation: 0.086 [-48.946, 17.895], loss: 0.000167, mean_squared_error: 0.000335, mean_q: 0.592486\n",
      " 1410/2000: episode: 13, duration: 7.342s, episode steps: 110, steps per second: 15, episode reward: 0.630, mean reward: 0.006 [-0.001, 0.013], mean action: -0.014 [-1.166, 1.142], mean observation: 0.087 [-50.268, 17.878], loss: 0.000158, mean_squared_error: 0.000316, mean_q: 0.587793\n",
      " 1518/2000: episode: 14, duration: 7.437s, episode steps: 108, steps per second: 15, episode reward: 0.631, mean reward: 0.006 [-0.001, 0.013], mean action: -0.005 [-1.124, 1.182], mean observation: 0.084 [-48.217, 19.480], loss: 0.000298, mean_squared_error: 0.000597, mean_q: 0.599592\n",
      " 1627/2000: episode: 15, duration: 7.135s, episode steps: 109, steps per second: 15, episode reward: 0.627, mean reward: 0.006 [-0.001, 0.013], mean action: 0.024 [-1.157, 1.198], mean observation: 0.083 [-46.536, 17.778], loss: 0.000224, mean_squared_error: 0.000449, mean_q: 0.596989\n",
      " 1733/2000: episode: 16, duration: 7.062s, episode steps: 106, steps per second: 15, episode reward: 0.634, mean reward: 0.006 [-0.000, 0.013], mean action: -0.004 [-1.154, 1.217], mean observation: 0.086 [-47.805, 18.032], loss: 0.000380, mean_squared_error: 0.000759, mean_q: 0.590372\n",
      " 1836/2000: episode: 17, duration: 6.985s, episode steps: 103, steps per second: 15, episode reward: 0.616, mean reward: 0.006 [-0.001, 0.013], mean action: 0.008 [-1.126, 1.151], mean observation: 0.075 [-44.391, 17.596], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.587782\n",
      " 1941/2000: episode: 18, duration: 7.276s, episode steps: 105, steps per second: 14, episode reward: 0.633, mean reward: 0.006 [-0.000, 0.013], mean action: 0.015 [-1.211, 1.187], mean observation: 0.076 [-40.849, 17.912], loss: 0.000271, mean_squared_error: 0.000542, mean_q: 0.598982\n",
      "done, took 126.580 seconds\n",
      "\n",
      "\n",
      "iteration: 79\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 6.345s, episode steps: 105, steps per second: 17, episode reward: 0.625, mean reward: 0.006 [-0.001, 0.014], mean action: 0.037 [-1.101, 1.289], mean observation: 0.069 [-51.487, 19.313], loss: --, mean_squared_error: --, mean_q: --\n",
      "  211/2000: episode: 2, duration: 6.268s, episode steps: 106, steps per second: 17, episode reward: 0.636, mean reward: 0.006 [-0.001, 0.014], mean action: 0.061 [-1.110, 1.194], mean observation: 0.068 [-52.016, 20.171], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  316/2000: episode: 3, duration: 6.254s, episode steps: 105, steps per second: 17, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.014], mean action: 0.056 [-1.194, 1.232], mean observation: 0.064 [-53.558, 17.295], loss: --, mean_squared_error: --, mean_q: --\n",
      "  422/2000: episode: 4, duration: 6.196s, episode steps: 106, steps per second: 17, episode reward: 0.638, mean reward: 0.006 [-0.001, 0.014], mean action: 0.052 [-1.133, 1.177], mean observation: 0.070 [-52.251, 19.781], loss: --, mean_squared_error: --, mean_q: --\n",
      "  527/2000: episode: 5, duration: 6.296s, episode steps: 105, steps per second: 17, episode reward: 0.621, mean reward: 0.006 [-0.001, 0.014], mean action: 0.026 [-1.249, 1.137], mean observation: 0.077 [-53.057, 17.216], loss: --, mean_squared_error: --, mean_q: --\n",
      "  633/2000: episode: 6, duration: 6.277s, episode steps: 106, steps per second: 17, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.014], mean action: 0.052 [-1.164, 1.184], mean observation: 0.070 [-51.538, 20.779], loss: --, mean_squared_error: --, mean_q: --\n",
      "  736/2000: episode: 7, duration: 6.107s, episode steps: 103, steps per second: 17, episode reward: 0.615, mean reward: 0.006 [-0.001, 0.014], mean action: 0.073 [-1.120, 1.241], mean observation: 0.071 [-53.679, 17.633], loss: --, mean_squared_error: --, mean_q: --\n",
      "  841/2000: episode: 8, duration: 6.185s, episode steps: 105, steps per second: 17, episode reward: 0.627, mean reward: 0.006 [-0.001, 0.014], mean action: 0.029 [-1.189, 1.169], mean observation: 0.069 [-48.157, 19.740], loss: --, mean_squared_error: --, mean_q: --\n",
      "  948/2000: episode: 9, duration: 6.318s, episode steps: 107, steps per second: 17, episode reward: 0.640, mean reward: 0.006 [-0.001, 0.014], mean action: 0.043 [-1.143, 1.179], mean observation: 0.070 [-47.751, 19.466], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1053/2000: episode: 10, duration: 6.903s, episode steps: 105, steps per second: 15, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.014], mean action: 0.054 [-1.255, 1.308], mean observation: 0.072 [-52.261, 18.604], loss: 0.000182, mean_squared_error: 0.000363, mean_q: 0.596705\n",
      " 1159/2000: episode: 11, duration: 7.290s, episode steps: 106, steps per second: 15, episode reward: 0.641, mean reward: 0.006 [-0.001, 0.013], mean action: 0.030 [-1.140, 1.215], mean observation: 0.072 [-52.034, 17.587], loss: 0.000327, mean_squared_error: 0.000654, mean_q: 0.596859\n",
      " 1263/2000: episode: 12, duration: 7.288s, episode steps: 104, steps per second: 14, episode reward: 0.622, mean reward: 0.006 [-0.001, 0.013], mean action: 0.047 [-1.196, 1.223], mean observation: 0.071 [-52.311, 20.535], loss: 0.000292, mean_squared_error: 0.000585, mean_q: 0.587876\n",
      " 1369/2000: episode: 13, duration: 7.115s, episode steps: 106, steps per second: 15, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.060 [-1.240, 1.186], mean observation: 0.070 [-45.200, 20.003], loss: 0.000105, mean_squared_error: 0.000211, mean_q: 0.601034\n",
      " 1473/2000: episode: 14, duration: 7.284s, episode steps: 104, steps per second: 14, episode reward: 0.632, mean reward: 0.006 [-0.000, 0.013], mean action: 0.032 [-1.207, 1.137], mean observation: 0.073 [-43.921, 18.511], loss: 0.000102, mean_squared_error: 0.000205, mean_q: 0.595110\n",
      " 1578/2000: episode: 15, duration: 7.291s, episode steps: 105, steps per second: 14, episode reward: 0.632, mean reward: 0.006 [-0.000, 0.013], mean action: 0.054 [-1.261, 1.142], mean observation: 0.072 [-44.693, 17.943], loss: 0.000204, mean_squared_error: 0.000409, mean_q: 0.586394\n",
      " 1683/2000: episode: 16, duration: 7.442s, episode steps: 105, steps per second: 14, episode reward: 0.635, mean reward: 0.006 [-0.000, 0.013], mean action: 0.062 [-1.199, 1.117], mean observation: 0.070 [-44.680, 19.390], loss: 0.000214, mean_squared_error: 0.000429, mean_q: 0.587923\n",
      " 1787/2000: episode: 17, duration: 7.252s, episode steps: 104, steps per second: 14, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.088 [-1.274, 1.151], mean observation: 0.079 [-50.386, 17.606], loss: 0.000255, mean_squared_error: 0.000510, mean_q: 0.587115\n",
      " 1891/2000: episode: 18, duration: 7.117s, episode steps: 104, steps per second: 15, episode reward: 0.638, mean reward: 0.006 [-0.001, 0.013], mean action: 0.094 [-1.163, 1.306], mean observation: 0.077 [-41.947, 19.426], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.585384\n",
      " 1995/2000: episode: 19, duration: 7.217s, episode steps: 104, steps per second: 14, episode reward: 0.630, mean reward: 0.006 [-0.001, 0.013], mean action: 0.075 [-1.287, 1.181], mean observation: 0.073 [-50.631, 18.006], loss: 0.000248, mean_squared_error: 0.000495, mean_q: 0.589733\n",
      "done, took 128.909 seconds\n",
      "\n",
      "\n",
      "iteration: 80\n",
      "Training for 2000 steps ...\n",
      "  103/2000: episode: 1, duration: 6.075s, episode steps: 103, steps per second: 17, episode reward: 0.630, mean reward: 0.006 [-0.001, 0.014], mean action: 0.093 [-1.148, 1.158], mean observation: 0.068 [-51.803, 17.264], loss: --, mean_squared_error: --, mean_q: --\n",
      "  206/2000: episode: 2, duration: 6.092s, episode steps: 103, steps per second: 17, episode reward: 0.630, mean reward: 0.006 [-0.001, 0.013], mean action: 0.105 [-1.133, 1.124], mean observation: 0.076 [-50.499, 17.281], loss: --, mean_squared_error: --, mean_q: --\n",
      "  310/2000: episode: 3, duration: 6.305s, episode steps: 104, steps per second: 16, episode reward: 0.631, mean reward: 0.006 [-0.001, 0.013], mean action: 0.094 [-1.160, 1.152], mean observation: 0.075 [-49.307, 20.166], loss: --, mean_squared_error: --, mean_q: --\n",
      "  414/2000: episode: 4, duration: 6.262s, episode steps: 104, steps per second: 17, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.086 [-1.118, 1.202], mean observation: 0.071 [-50.411, 18.428], loss: --, mean_squared_error: --, mean_q: --\n",
      "  517/2000: episode: 5, duration: 6.189s, episode steps: 103, steps per second: 17, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.014], mean action: 0.089 [-1.279, 1.224], mean observation: 0.081 [-48.136, 19.771], loss: --, mean_squared_error: --, mean_q: --\n",
      "  620/2000: episode: 6, duration: 6.165s, episode steps: 103, steps per second: 17, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.014], mean action: 0.074 [-1.159, 1.176], mean observation: 0.071 [-49.893, 18.348], loss: --, mean_squared_error: --, mean_q: --\n",
      "  723/2000: episode: 7, duration: 6.239s, episode steps: 103, steps per second: 17, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.013], mean action: 0.066 [-1.225, 1.115], mean observation: 0.074 [-48.953, 21.003], loss: --, mean_squared_error: --, mean_q: --\n",
      "  828/2000: episode: 8, duration: 6.205s, episode steps: 105, steps per second: 17, episode reward: 0.639, mean reward: 0.006 [-0.001, 0.013], mean action: 0.099 [-1.182, 1.253], mean observation: 0.078 [-50.852, 17.605], loss: --, mean_squared_error: --, mean_q: --\n",
      "  932/2000: episode: 9, duration: 6.062s, episode steps: 104, steps per second: 17, episode reward: 0.637, mean reward: 0.006 [-0.001, 0.013], mean action: 0.082 [-1.196, 1.120], mean observation: 0.068 [-50.684, 17.272], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1034/2000: episode: 10, duration: 6.406s, episode steps: 102, steps per second: 16, episode reward: 0.619, mean reward: 0.006 [-0.001, 0.014], mean action: 0.090 [-1.162, 1.232], mean observation: 0.078 [-46.842, 20.968], loss: 0.000605, mean_squared_error: 0.001211, mean_q: 0.584910\n",
      " 1137/2000: episode: 11, duration: 7.165s, episode steps: 103, steps per second: 14, episode reward: 0.621, mean reward: 0.006 [-0.001, 0.014], mean action: 0.103 [-1.099, 1.168], mean observation: 0.074 [-48.293, 20.450], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.593578\n",
      " 1243/2000: episode: 12, duration: 7.202s, episode steps: 106, steps per second: 15, episode reward: 0.644, mean reward: 0.006 [-0.001, 0.013], mean action: 0.097 [-1.218, 1.214], mean observation: 0.072 [-51.887, 17.639], loss: 0.000219, mean_squared_error: 0.000439, mean_q: 0.589586\n",
      " 1349/2000: episode: 13, duration: 7.112s, episode steps: 106, steps per second: 15, episode reward: 0.649, mean reward: 0.006 [-0.001, 0.014], mean action: 0.107 [-1.141, 1.140], mean observation: 0.077 [-49.160, 19.952], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.581545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1451/2000: episode: 14, duration: 6.986s, episode steps: 102, steps per second: 15, episode reward: 0.627, mean reward: 0.006 [-0.001, 0.013], mean action: 0.096 [-1.173, 1.130], mean observation: 0.069 [-51.274, 18.221], loss: 0.000223, mean_squared_error: 0.000447, mean_q: 0.580384\n",
      " 1554/2000: episode: 15, duration: 7.254s, episode steps: 103, steps per second: 14, episode reward: 0.625, mean reward: 0.006 [-0.001, 0.013], mean action: 0.112 [-1.222, 1.192], mean observation: 0.075 [-51.021, 13.576], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.598785\n",
      " 1658/2000: episode: 16, duration: 7.282s, episode steps: 104, steps per second: 14, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.014], mean action: 0.059 [-1.110, 1.134], mean observation: 0.070 [-53.522, 15.068], loss: 0.000082, mean_squared_error: 0.000163, mean_q: 0.590986\n",
      " 1764/2000: episode: 17, duration: 7.599s, episode steps: 106, steps per second: 14, episode reward: 0.645, mean reward: 0.006 [-0.001, 0.013], mean action: -0.019 [-1.202, 1.220], mean observation: 0.068 [-51.983, 14.618], loss: 0.000180, mean_squared_error: 0.000361, mean_q: 0.593753\n",
      " 1868/2000: episode: 18, duration: 7.582s, episode steps: 104, steps per second: 14, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.013], mean action: -0.039 [-1.157, 1.120], mean observation: 0.067 [-51.115, 15.880], loss: 0.000063, mean_squared_error: 0.000126, mean_q: 0.585946\n",
      " 1973/2000: episode: 19, duration: 7.617s, episode steps: 105, steps per second: 14, episode reward: 0.631, mean reward: 0.006 [-0.001, 0.013], mean action: -0.025 [-1.180, 1.202], mean observation: 0.069 [-51.113, 13.910], loss: 0.000234, mean_squared_error: 0.000467, mean_q: 0.595580\n",
      "done, took 129.825 seconds\n",
      "\n",
      "\n",
      "iteration: 81\n",
      "Training for 2000 steps ...\n",
      "  106/2000: episode: 1, duration: 6.379s, episode steps: 106, steps per second: 17, episode reward: 0.649, mean reward: 0.006 [-0.001, 0.013], mean action: -0.060 [-1.292, 1.205], mean observation: 0.073 [-41.176, 13.457], loss: --, mean_squared_error: --, mean_q: --\n",
      "  209/2000: episode: 2, duration: 6.422s, episode steps: 103, steps per second: 16, episode reward: 0.622, mean reward: 0.006 [-0.001, 0.013], mean action: -0.056 [-1.145, 1.199], mean observation: 0.073 [-44.913, 16.478], loss: --, mean_squared_error: --, mean_q: --\n",
      "  312/2000: episode: 3, duration: 6.482s, episode steps: 103, steps per second: 16, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.013], mean action: -0.005 [-1.148, 1.192], mean observation: 0.075 [-46.694, 17.490], loss: --, mean_squared_error: --, mean_q: --\n",
      "  416/2000: episode: 4, duration: 6.398s, episode steps: 104, steps per second: 16, episode reward: 0.634, mean reward: 0.006 [-0.001, 0.013], mean action: -0.026 [-1.139, 1.213], mean observation: 0.071 [-47.327, 14.318], loss: --, mean_squared_error: --, mean_q: --\n",
      "  521/2000: episode: 5, duration: 6.346s, episode steps: 105, steps per second: 17, episode reward: 0.641, mean reward: 0.006 [-0.001, 0.014], mean action: -0.054 [-1.147, 1.178], mean observation: 0.070 [-44.031, 13.523], loss: --, mean_squared_error: --, mean_q: --\n",
      "  626/2000: episode: 6, duration: 6.323s, episode steps: 105, steps per second: 17, episode reward: 0.640, mean reward: 0.006 [-0.001, 0.013], mean action: -0.057 [-1.238, 1.159], mean observation: 0.078 [-45.381, 13.969], loss: --, mean_squared_error: --, mean_q: --\n",
      "  728/2000: episode: 7, duration: 6.395s, episode steps: 102, steps per second: 16, episode reward: 0.620, mean reward: 0.006 [-0.001, 0.014], mean action: -0.044 [-1.112, 1.157], mean observation: 0.068 [-46.348, 18.828], loss: --, mean_squared_error: --, mean_q: --\n",
      "  831/2000: episode: 8, duration: 6.452s, episode steps: 103, steps per second: 16, episode reward: 0.626, mean reward: 0.006 [-0.001, 0.013], mean action: -0.034 [-1.135, 1.274], mean observation: 0.071 [-44.544, 17.149], loss: --, mean_squared_error: --, mean_q: --\n",
      "  934/2000: episode: 9, duration: 6.465s, episode steps: 103, steps per second: 16, episode reward: 0.628, mean reward: 0.006 [-0.001, 0.014], mean action: -0.019 [-1.177, 1.177], mean observation: 0.072 [-45.795, 16.623], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1040/2000: episode: 10, duration: 6.760s, episode steps: 106, steps per second: 16, episode reward: 0.658, mean reward: 0.006 [-0.001, 0.013], mean action: -0.051 [-1.180, 1.243], mean observation: 0.076 [-44.332, 13.767], loss: 0.000684, mean_squared_error: 0.001368, mean_q: 0.613081\n",
      " 1144/2000: episode: 11, duration: 7.250s, episode steps: 104, steps per second: 14, episode reward: 0.623, mean reward: 0.006 [-0.001, 0.013], mean action: -0.019 [-1.198, 1.153], mean observation: 0.071 [-50.753, 14.371], loss: 0.000087, mean_squared_error: 0.000175, mean_q: 0.590996\n",
      " 1247/2000: episode: 12, duration: 6.974s, episode steps: 103, steps per second: 15, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.013], mean action: 0.020 [-1.201, 1.141], mean observation: 0.074 [-49.877, 15.726], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.593224\n",
      " 1351/2000: episode: 13, duration: 6.952s, episode steps: 104, steps per second: 15, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.062 [-1.205, 1.128], mean observation: 0.080 [-46.471, 15.970], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.590706\n",
      " 1454/2000: episode: 14, duration: 6.837s, episode steps: 103, steps per second: 15, episode reward: 0.604, mean reward: 0.006 [-0.001, 0.013], mean action: 0.197 [-1.278, 1.211], mean observation: 0.084 [-50.445, 13.633], loss: 0.000082, mean_squared_error: 0.000163, mean_q: 0.582869\n",
      " 1561/2000: episode: 15, duration: 7.055s, episode steps: 107, steps per second: 15, episode reward: 0.652, mean reward: 0.006 [-0.001, 0.013], mean action: 0.107 [-1.250, 1.172], mean observation: 0.075 [-50.761, 17.346], loss: 0.000176, mean_squared_error: 0.000351, mean_q: 0.597664\n",
      " 1666/2000: episode: 16, duration: 6.930s, episode steps: 105, steps per second: 15, episode reward: 0.642, mean reward: 0.006 [-0.001, 0.013], mean action: 0.085 [-1.181, 1.156], mean observation: 0.075 [-48.280, 13.499], loss: 0.000149, mean_squared_error: 0.000297, mean_q: 0.583176\n",
      " 1772/2000: episode: 17, duration: 6.937s, episode steps: 106, steps per second: 15, episode reward: 0.647, mean reward: 0.006 [-0.001, 0.013], mean action: 0.129 [-1.143, 1.148], mean observation: 0.081 [-49.315, 13.665], loss: 0.000375, mean_squared_error: 0.000749, mean_q: 0.592669\n",
      " 1882/2000: episode: 18, duration: 7.083s, episode steps: 110, steps per second: 16, episode reward: 0.661, mean reward: 0.006 [-0.001, 0.014], mean action: 0.165 [-1.234, 1.217], mean observation: 0.084 [-49.122, 17.845], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.594872\n",
      " 1989/2000: episode: 19, duration: 7.100s, episode steps: 107, steps per second: 15, episode reward: 0.638, mean reward: 0.006 [-0.001, 0.014], mean action: 0.183 [-1.315, 1.221], mean observation: 0.075 [-50.731, 17.618], loss: 0.000294, mean_squared_error: 0.000587, mean_q: 0.601579\n",
      "done, took 128.501 seconds\n",
      "\n",
      "\n",
      "iteration: 82\n",
      "Training for 2000 steps ...\n",
      "  107/2000: episode: 1, duration: 5.966s, episode steps: 107, steps per second: 18, episode reward: 0.649, mean reward: 0.006 [-0.001, 0.013], mean action: 0.195 [-1.083, 1.132], mean observation: 0.080 [-51.895, 18.096], loss: --, mean_squared_error: --, mean_q: --\n",
      "  214/2000: episode: 2, duration: 5.932s, episode steps: 107, steps per second: 18, episode reward: 0.651, mean reward: 0.006 [-0.001, 0.013], mean action: 0.162 [-1.134, 1.197], mean observation: 0.072 [-50.724, 17.805], loss: --, mean_squared_error: --, mean_q: --\n",
      "  321/2000: episode: 3, duration: 5.972s, episode steps: 107, steps per second: 18, episode reward: 0.648, mean reward: 0.006 [-0.001, 0.013], mean action: 0.171 [-1.183, 1.169], mean observation: 0.073 [-50.467, 18.205], loss: --, mean_squared_error: --, mean_q: --\n",
      "  427/2000: episode: 4, duration: 5.896s, episode steps: 106, steps per second: 18, episode reward: 0.640, mean reward: 0.006 [-0.001, 0.013], mean action: 0.194 [-1.145, 1.244], mean observation: 0.071 [-51.848, 17.906], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  534/2000: episode: 5, duration: 5.991s, episode steps: 107, steps per second: 18, episode reward: 0.653, mean reward: 0.006 [-0.001, 0.013], mean action: 0.176 [-1.130, 1.156], mean observation: 0.072 [-51.017, 18.128], loss: --, mean_squared_error: --, mean_q: --\n",
      "  641/2000: episode: 6, duration: 5.960s, episode steps: 107, steps per second: 18, episode reward: 0.650, mean reward: 0.006 [-0.001, 0.013], mean action: 0.156 [-1.103, 1.155], mean observation: 0.073 [-50.127, 18.008], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 7, duration: 5.932s, episode steps: 106, steps per second: 18, episode reward: 0.642, mean reward: 0.006 [-0.001, 0.013], mean action: 0.186 [-1.121, 1.211], mean observation: 0.073 [-50.118, 18.007], loss: --, mean_squared_error: --, mean_q: --\n",
      "  854/2000: episode: 8, duration: 5.981s, episode steps: 107, steps per second: 18, episode reward: 0.649, mean reward: 0.006 [-0.001, 0.013], mean action: 0.157 [-1.164, 1.140], mean observation: 0.072 [-50.974, 18.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  961/2000: episode: 9, duration: 5.945s, episode steps: 107, steps per second: 18, episode reward: 0.654, mean reward: 0.006 [-0.001, 0.013], mean action: 0.177 [-1.177, 1.174], mean observation: 0.072 [-51.255, 17.950], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1068/2000: episode: 10, duration: 6.506s, episode steps: 107, steps per second: 16, episode reward: 0.650, mean reward: 0.006 [-0.001, 0.013], mean action: 0.167 [-1.177, 1.158], mean observation: 0.073 [-49.194, 17.789], loss: 0.000085, mean_squared_error: 0.000169, mean_q: 0.593439\n",
      " 1174/2000: episode: 11, duration: 6.976s, episode steps: 106, steps per second: 15, episode reward: 0.641, mean reward: 0.006 [-0.001, 0.013], mean action: 0.159 [-1.223, 1.186], mean observation: 0.085 [-53.164, 17.166], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.589534\n",
      " 1283/2000: episode: 12, duration: 7.256s, episode steps: 109, steps per second: 15, episode reward: 0.657, mean reward: 0.006 [-0.001, 0.014], mean action: 0.103 [-1.189, 1.152], mean observation: 0.078 [-51.094, 17.844], loss: 0.000088, mean_squared_error: 0.000177, mean_q: 0.588307\n",
      " 1393/2000: episode: 13, duration: 7.463s, episode steps: 110, steps per second: 15, episode reward: 0.663, mean reward: 0.006 [-0.000, 0.014], mean action: 0.072 [-1.110, 1.133], mean observation: 0.087 [-52.349, 17.968], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.600869\n",
      " 1500/2000: episode: 14, duration: 7.465s, episode steps: 107, steps per second: 14, episode reward: 0.653, mean reward: 0.006 [-0.001, 0.014], mean action: 0.036 [-1.200, 1.206], mean observation: 0.078 [-51.465, 17.828], loss: 0.000394, mean_squared_error: 0.000788, mean_q: 0.579665\n",
      " 1606/2000: episode: 15, duration: 7.032s, episode steps: 106, steps per second: 15, episode reward: 0.653, mean reward: 0.006 [-0.001, 0.014], mean action: 0.116 [-1.195, 1.195], mean observation: 0.075 [-49.723, 17.951], loss: 0.000242, mean_squared_error: 0.000483, mean_q: 0.587978\n",
      " 1715/2000: episode: 16, duration: 7.173s, episode steps: 109, steps per second: 15, episode reward: 0.666, mean reward: 0.006 [-0.000, 0.014], mean action: 0.081 [-1.171, 1.160], mean observation: 0.082 [-50.903, 17.848], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.598793\n",
      " 1823/2000: episode: 17, duration: 7.020s, episode steps: 108, steps per second: 15, episode reward: 0.661, mean reward: 0.006 [-0.001, 0.014], mean action: 0.137 [-1.123, 1.170], mean observation: 0.085 [-50.542, 17.973], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.586946\n",
      " 1928/2000: episode: 18, duration: 7.047s, episode steps: 105, steps per second: 15, episode reward: 0.639, mean reward: 0.006 [-0.001, 0.014], mean action: 0.087 [-1.140, 1.167], mean observation: 0.079 [-48.550, 18.231], loss: 0.000117, mean_squared_error: 0.000233, mean_q: 0.584779\n",
      "done, took 122.444 seconds\n",
      "\n",
      "\n",
      "iteration: 83\n",
      "Training for 2000 steps ...\n",
      "  108/2000: episode: 1, duration: 6.475s, episode steps: 108, steps per second: 17, episode reward: 0.652, mean reward: 0.006 [-0.002, 0.014], mean action: 0.078 [-1.145, 1.218], mean observation: 0.077 [-48.410, 21.717], loss: --, mean_squared_error: --, mean_q: --\n",
      "  213/2000: episode: 2, duration: 6.305s, episode steps: 105, steps per second: 17, episode reward: 0.638, mean reward: 0.006 [-0.002, 0.014], mean action: 0.098 [-1.222, 1.161], mean observation: 0.075 [-50.652, 18.057], loss: --, mean_squared_error: --, mean_q: --\n",
      "  318/2000: episode: 3, duration: 6.325s, episode steps: 105, steps per second: 17, episode reward: 0.632, mean reward: 0.006 [-0.002, 0.014], mean action: 0.091 [-1.224, 1.157], mean observation: 0.072 [-49.654, 20.041], loss: --, mean_squared_error: --, mean_q: --\n",
      "  425/2000: episode: 4, duration: 6.374s, episode steps: 107, steps per second: 17, episode reward: 0.646, mean reward: 0.006 [-0.002, 0.014], mean action: 0.062 [-1.226, 1.185], mean observation: 0.080 [-50.059, 18.153], loss: --, mean_squared_error: --, mean_q: --\n",
      "  534/2000: episode: 5, duration: 6.296s, episode steps: 109, steps per second: 17, episode reward: 0.661, mean reward: 0.006 [-0.002, 0.014], mean action: 0.071 [-1.160, 1.214], mean observation: 0.086 [-49.108, 17.917], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 6, duration: 6.407s, episode steps: 108, steps per second: 17, episode reward: 0.653, mean reward: 0.006 [-0.002, 0.014], mean action: 0.090 [-1.129, 1.241], mean observation: 0.078 [-48.215, 21.467], loss: --, mean_squared_error: --, mean_q: --\n",
      "  750/2000: episode: 7, duration: 6.332s, episode steps: 108, steps per second: 17, episode reward: 0.658, mean reward: 0.006 [-0.002, 0.014], mean action: 0.093 [-1.158, 1.107], mean observation: 0.080 [-51.440, 18.054], loss: --, mean_squared_error: --, mean_q: --\n",
      "  858/2000: episode: 8, duration: 6.361s, episode steps: 108, steps per second: 17, episode reward: 0.658, mean reward: 0.006 [-0.002, 0.014], mean action: 0.084 [-1.185, 1.136], mean observation: 0.074 [-47.901, 17.974], loss: --, mean_squared_error: --, mean_q: --\n",
      "  966/2000: episode: 9, duration: 6.348s, episode steps: 108, steps per second: 17, episode reward: 0.648, mean reward: 0.006 [-0.002, 0.014], mean action: 0.090 [-1.193, 1.183], mean observation: 0.087 [-48.888, 18.011], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1072/2000: episode: 10, duration: 7.144s, episode steps: 106, steps per second: 15, episode reward: 0.635, mean reward: 0.006 [-0.002, 0.014], mean action: 0.029 [-1.161, 1.225], mean observation: 0.076 [-50.351, 18.346], loss: 0.000080, mean_squared_error: 0.000159, mean_q: 0.598433\n",
      " 1181/2000: episode: 11, duration: 7.628s, episode steps: 109, steps per second: 14, episode reward: 0.654, mean reward: 0.006 [-0.001, 0.013], mean action: 0.006 [-1.223, 1.133], mean observation: 0.082 [-47.849, 23.098], loss: 0.000102, mean_squared_error: 0.000204, mean_q: 0.599577\n",
      " 1289/2000: episode: 12, duration: 7.515s, episode steps: 108, steps per second: 14, episode reward: 0.661, mean reward: 0.006 [-0.001, 0.014], mean action: 0.044 [-1.200, 1.215], mean observation: 0.077 [-52.546, 17.814], loss: 0.000059, mean_squared_error: 0.000119, mean_q: 0.602566\n",
      " 1398/2000: episode: 13, duration: 7.545s, episode steps: 109, steps per second: 14, episode reward: 0.662, mean reward: 0.006 [-0.001, 0.014], mean action: 0.041 [-1.210, 1.263], mean observation: 0.078 [-50.343, 20.143], loss: 0.000057, mean_squared_error: 0.000115, mean_q: 0.580001\n",
      " 1507/2000: episode: 14, duration: 7.692s, episode steps: 109, steps per second: 14, episode reward: 0.662, mean reward: 0.006 [-0.002, 0.014], mean action: 0.013 [-1.139, 1.249], mean observation: 0.074 [-48.497, 19.605], loss: 0.000110, mean_squared_error: 0.000220, mean_q: 0.593158\n",
      " 1617/2000: episode: 15, duration: 7.458s, episode steps: 110, steps per second: 15, episode reward: 0.675, mean reward: 0.006 [-0.002, 0.014], mean action: -0.041 [-1.197, 1.125], mean observation: 0.089 [-49.369, 18.095], loss: 0.000269, mean_squared_error: 0.000537, mean_q: 0.591501\n",
      " 1727/2000: episode: 16, duration: 7.476s, episode steps: 110, steps per second: 15, episode reward: 0.682, mean reward: 0.006 [-0.002, 0.014], mean action: -0.002 [-1.110, 1.245], mean observation: 0.081 [-52.562, 17.647], loss: 0.000095, mean_squared_error: 0.000189, mean_q: 0.590188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1841/2000: episode: 17, duration: 7.670s, episode steps: 114, steps per second: 15, episode reward: 0.704, mean reward: 0.006 [-0.001, 0.014], mean action: -0.026 [-1.148, 1.094], mean observation: 0.092 [-49.029, 17.954], loss: 0.000332, mean_squared_error: 0.000665, mean_q: 0.589554\n",
      " 1952/2000: episode: 18, duration: 7.713s, episode steps: 111, steps per second: 14, episode reward: 0.682, mean reward: 0.006 [-0.001, 0.014], mean action: -0.028 [-1.155, 1.224], mean observation: 0.082 [-51.346, 17.884], loss: 0.000106, mean_squared_error: 0.000212, mean_q: 0.576160\n",
      "done, took 128.553 seconds\n",
      "\n",
      "\n",
      "iteration: 84\n",
      "Training for 2000 steps ...\n",
      "  113/2000: episode: 1, duration: 6.768s, episode steps: 113, steps per second: 17, episode reward: 0.677, mean reward: 0.006 [-0.001, 0.013], mean action: -0.019 [-1.112, 1.171], mean observation: 0.086 [-49.281, 18.117], loss: --, mean_squared_error: --, mean_q: --\n",
      "  225/2000: episode: 2, duration: 6.616s, episode steps: 112, steps per second: 17, episode reward: 0.670, mean reward: 0.006 [-0.001, 0.013], mean action: -0.021 [-1.150, 1.243], mean observation: 0.084 [-50.316, 17.843], loss: --, mean_squared_error: --, mean_q: --\n",
      "  339/2000: episode: 3, duration: 6.888s, episode steps: 114, steps per second: 17, episode reward: 0.684, mean reward: 0.006 [-0.001, 0.013], mean action: -0.023 [-1.169, 1.232], mean observation: 0.091 [-45.841, 18.061], loss: --, mean_squared_error: --, mean_q: --\n",
      "  451/2000: episode: 4, duration: 6.721s, episode steps: 112, steps per second: 17, episode reward: 0.672, mean reward: 0.006 [-0.002, 0.013], mean action: -0.017 [-1.222, 1.281], mean observation: 0.087 [-49.464, 17.868], loss: --, mean_squared_error: --, mean_q: --\n",
      "  563/2000: episode: 5, duration: 6.696s, episode steps: 112, steps per second: 17, episode reward: 0.670, mean reward: 0.006 [-0.002, 0.013], mean action: -0.036 [-1.140, 1.173], mean observation: 0.083 [-49.212, 17.902], loss: --, mean_squared_error: --, mean_q: --\n",
      "  677/2000: episode: 6, duration: 6.907s, episode steps: 114, steps per second: 17, episode reward: 0.681, mean reward: 0.006 [-0.001, 0.013], mean action: -0.015 [-1.167, 1.229], mean observation: 0.093 [-46.422, 18.178], loss: --, mean_squared_error: --, mean_q: --\n",
      "  790/2000: episode: 7, duration: 6.814s, episode steps: 113, steps per second: 17, episode reward: 0.682, mean reward: 0.006 [-0.002, 0.013], mean action: -0.047 [-1.257, 1.212], mean observation: 0.085 [-48.720, 18.001], loss: --, mean_squared_error: --, mean_q: --\n",
      "  903/2000: episode: 8, duration: 6.694s, episode steps: 113, steps per second: 17, episode reward: 0.673, mean reward: 0.006 [-0.001, 0.013], mean action: -0.011 [-1.180, 1.259], mean observation: 0.086 [-47.360, 17.949], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1017/2000: episode: 9, duration: 6.917s, episode steps: 114, steps per second: 16, episode reward: 0.681, mean reward: 0.006 [-0.001, 0.013], mean action: -0.005 [-1.151, 1.238], mean observation: 0.090 [-48.321, 17.936], loss: 0.000086, mean_squared_error: 0.000173, mean_q: 0.576599\n",
      " 1128/2000: episode: 10, duration: 7.239s, episode steps: 111, steps per second: 15, episode reward: 0.629, mean reward: 0.006 [-0.002, 0.013], mean action: -0.002 [-1.220, 1.180], mean observation: 0.088 [-41.159, 19.791], loss: 0.000084, mean_squared_error: 0.000169, mean_q: 0.600087\n",
      " 1238/2000: episode: 11, duration: 7.303s, episode steps: 110, steps per second: 15, episode reward: 0.646, mean reward: 0.006 [-0.001, 0.013], mean action: 0.020 [-1.141, 1.117], mean observation: 0.088 [-48.341, 17.774], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.584757\n",
      " 1348/2000: episode: 12, duration: 7.209s, episode steps: 110, steps per second: 15, episode reward: 0.644, mean reward: 0.006 [-0.001, 0.013], mean action: 0.024 [-1.183, 1.145], mean observation: 0.081 [-50.251, 17.795], loss: 0.000203, mean_squared_error: 0.000405, mean_q: 0.581400\n",
      " 1456/2000: episode: 13, duration: 7.204s, episode steps: 108, steps per second: 15, episode reward: 0.632, mean reward: 0.006 [-0.001, 0.013], mean action: 0.041 [-1.131, 1.259], mean observation: 0.087 [-48.339, 17.878], loss: 0.000342, mean_squared_error: 0.000684, mean_q: 0.585206\n",
      " 1567/2000: episode: 14, duration: 6.803s, episode steps: 111, steps per second: 16, episode reward: 0.634, mean reward: 0.006 [-0.001, 0.013], mean action: 0.039 [-1.147, 1.316], mean observation: 0.086 [-32.319, 17.990], loss: 0.000098, mean_squared_error: 0.000197, mean_q: 0.592337\n",
      " 1678/2000: episode: 15, duration: 6.652s, episode steps: 111, steps per second: 17, episode reward: 0.655, mean reward: 0.006 [-0.001, 0.013], mean action: 0.084 [-1.160, 1.214], mean observation: 0.091 [-18.367, 18.022], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.597239\n",
      " 1788/2000: episode: 16, duration: 6.947s, episode steps: 110, steps per second: 16, episode reward: 0.650, mean reward: 0.006 [-0.001, 0.013], mean action: 0.107 [-1.279, 1.153], mean observation: 0.093 [-14.718, 17.991], loss: 0.000090, mean_squared_error: 0.000180, mean_q: 0.601386\n",
      " 1895/2000: episode: 17, duration: 6.869s, episode steps: 107, steps per second: 16, episode reward: 0.631, mean reward: 0.006 [-0.001, 0.013], mean action: 0.089 [-1.175, 1.159], mean observation: 0.093 [-37.413, 17.605], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.588088\n",
      " 1999/2000: episode: 18, duration: 6.895s, episode steps: 104, steps per second: 15, episode reward: 0.614, mean reward: 0.006 [-0.001, 0.014], mean action: 0.101 [-1.190, 1.224], mean observation: 0.092 [-40.260, 17.743], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.593932\n",
      "done, took 124.269 seconds\n",
      "\n",
      "\n",
      "iteration: 85\n",
      "Training for 2000 steps ...\n",
      "  108/2000: episode: 1, duration: 5.461s, episode steps: 108, steps per second: 20, episode reward: 0.646, mean reward: 0.006 [-0.001, 0.013], mean action: 0.112 [-1.213, 1.180], mean observation: 0.097 [-11.108, 17.794], loss: --, mean_squared_error: --, mean_q: --\n",
      "  217/2000: episode: 2, duration: 5.541s, episode steps: 109, steps per second: 20, episode reward: 0.651, mean reward: 0.006 [-0.001, 0.013], mean action: 0.112 [-1.126, 1.253], mean observation: 0.090 [-40.376, 21.188], loss: --, mean_squared_error: --, mean_q: --\n",
      "  324/2000: episode: 3, duration: 5.401s, episode steps: 107, steps per second: 20, episode reward: 0.635, mean reward: 0.006 [-0.001, 0.013], mean action: 0.121 [-1.122, 1.164], mean observation: 0.095 [-14.085, 17.213], loss: --, mean_squared_error: --, mean_q: --\n",
      "  433/2000: episode: 4, duration: 5.548s, episode steps: 109, steps per second: 20, episode reward: 0.651, mean reward: 0.006 [-0.001, 0.013], mean action: 0.132 [-1.113, 1.174], mean observation: 0.096 [-14.967, 17.490], loss: --, mean_squared_error: --, mean_q: --\n",
      "  541/2000: episode: 5, duration: 5.441s, episode steps: 108, steps per second: 20, episode reward: 0.640, mean reward: 0.006 [-0.001, 0.013], mean action: 0.113 [-1.108, 1.154], mean observation: 0.096 [-15.486, 17.618], loss: --, mean_squared_error: --, mean_q: --\n",
      "  648/2000: episode: 6, duration: 5.430s, episode steps: 107, steps per second: 20, episode reward: 0.637, mean reward: 0.006 [-0.001, 0.013], mean action: 0.133 [-1.226, 1.322], mean observation: 0.096 [-14.723, 17.550], loss: --, mean_squared_error: --, mean_q: --\n",
      "  755/2000: episode: 7, duration: 5.393s, episode steps: 107, steps per second: 20, episode reward: 0.647, mean reward: 0.006 [-0.001, 0.013], mean action: 0.130 [-1.165, 1.269], mean observation: 0.092 [-45.348, 17.642], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 8, duration: 5.410s, episode steps: 107, steps per second: 20, episode reward: 0.639, mean reward: 0.006 [-0.001, 0.013], mean action: 0.122 [-1.228, 1.167], mean observation: 0.097 [-10.586, 17.545], loss: --, mean_squared_error: --, mean_q: --\n",
      "  970/2000: episode: 9, duration: 5.519s, episode steps: 108, steps per second: 20, episode reward: 0.645, mean reward: 0.006 [-0.001, 0.013], mean action: 0.123 [-1.165, 1.212], mean observation: 0.094 [-14.534, 17.623], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1079/2000: episode: 10, duration: 6.181s, episode steps: 109, steps per second: 18, episode reward: 0.652, mean reward: 0.006 [-0.001, 0.013], mean action: 0.123 [-1.118, 1.128], mean observation: 0.095 [-14.195, 17.633], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.588570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1188/2000: episode: 11, duration: 6.829s, episode steps: 109, steps per second: 16, episode reward: 0.652, mean reward: 0.006 [-0.001, 0.013], mean action: 0.098 [-1.217, 1.167], mean observation: 0.092 [-14.045, 17.575], loss: 0.000068, mean_squared_error: 0.000135, mean_q: 0.596074\n",
      " 1296/2000: episode: 12, duration: 6.860s, episode steps: 108, steps per second: 16, episode reward: 0.642, mean reward: 0.006 [-0.001, 0.014], mean action: 0.088 [-1.160, 1.197], mean observation: 0.090 [-21.741, 17.687], loss: 0.000333, mean_squared_error: 0.000666, mean_q: 0.598362\n",
      " 1402/2000: episode: 13, duration: 7.095s, episode steps: 106, steps per second: 15, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.013], mean action: 0.121 [-1.157, 1.182], mean observation: 0.087 [-41.302, 17.785], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.590015\n",
      " 1510/2000: episode: 14, duration: 6.780s, episode steps: 108, steps per second: 16, episode reward: 0.628, mean reward: 0.006 [-0.002, 0.014], mean action: 0.098 [-1.191, 1.169], mean observation: 0.089 [-34.673, 17.523], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.596206\n",
      " 1619/2000: episode: 15, duration: 6.722s, episode steps: 109, steps per second: 16, episode reward: 0.634, mean reward: 0.006 [-0.001, 0.013], mean action: 0.059 [-1.129, 1.124], mean observation: 0.094 [-18.369, 17.572], loss: 0.000220, mean_squared_error: 0.000441, mean_q: 0.585599\n",
      " 1729/2000: episode: 16, duration: 7.141s, episode steps: 110, steps per second: 15, episode reward: 0.641, mean reward: 0.006 [-0.001, 0.013], mean action: -0.014 [-1.217, 1.083], mean observation: 0.087 [-18.138, 17.591], loss: 0.000060, mean_squared_error: 0.000119, mean_q: 0.588045\n",
      " 1839/2000: episode: 17, duration: 7.214s, episode steps: 110, steps per second: 15, episode reward: 0.666, mean reward: 0.006 [-0.002, 0.014], mean action: -0.032 [-1.184, 1.161], mean observation: 0.090 [-23.506, 17.682], loss: 0.000225, mean_squared_error: 0.000450, mean_q: 0.590904\n",
      " 1945/2000: episode: 18, duration: 7.100s, episode steps: 106, steps per second: 15, episode reward: 0.625, mean reward: 0.006 [-0.002, 0.014], mean action: -0.027 [-1.241, 1.182], mean observation: 0.093 [-25.977, 17.471], loss: 0.000245, mean_squared_error: 0.000490, mean_q: 0.585831\n",
      "done, took 114.769 seconds\n",
      "\n",
      "\n",
      "iteration: 86\n",
      "Training for 2000 steps ...\n",
      "  110/2000: episode: 1, duration: 6.144s, episode steps: 110, steps per second: 18, episode reward: 0.645, mean reward: 0.006 [-0.002, 0.014], mean action: 0.019 [-1.168, 1.219], mean observation: 0.090 [-12.211, 17.574], loss: --, mean_squared_error: --, mean_q: --\n",
      "  218/2000: episode: 2, duration: 6.117s, episode steps: 108, steps per second: 18, episode reward: 0.631, mean reward: 0.006 [-0.002, 0.014], mean action: 0.018 [-1.169, 1.112], mean observation: 0.089 [-10.391, 17.546], loss: --, mean_squared_error: --, mean_q: --\n",
      "  327/2000: episode: 3, duration: 6.112s, episode steps: 109, steps per second: 18, episode reward: 0.639, mean reward: 0.006 [-0.002, 0.014], mean action: 0.008 [-1.244, 1.090], mean observation: 0.092 [-22.353, 17.598], loss: --, mean_squared_error: --, mean_q: --\n",
      "  435/2000: episode: 4, duration: 6.194s, episode steps: 108, steps per second: 17, episode reward: 0.635, mean reward: 0.006 [-0.002, 0.014], mean action: 0.006 [-1.179, 1.240], mean observation: 0.090 [-24.546, 17.463], loss: --, mean_squared_error: --, mean_q: --\n",
      "  543/2000: episode: 5, duration: 6.111s, episode steps: 108, steps per second: 18, episode reward: 0.628, mean reward: 0.006 [-0.002, 0.014], mean action: 0.022 [-1.153, 1.204], mean observation: 0.091 [-21.376, 17.495], loss: --, mean_squared_error: --, mean_q: --\n",
      "  651/2000: episode: 6, duration: 6.163s, episode steps: 108, steps per second: 18, episode reward: 0.626, mean reward: 0.006 [-0.002, 0.014], mean action: 0.027 [-1.108, 1.222], mean observation: 0.088 [-20.373, 17.595], loss: --, mean_squared_error: --, mean_q: --\n",
      "  760/2000: episode: 7, duration: 6.135s, episode steps: 109, steps per second: 18, episode reward: 0.635, mean reward: 0.006 [-0.002, 0.014], mean action: 0.010 [-1.150, 1.169], mean observation: 0.093 [-19.625, 17.732], loss: --, mean_squared_error: --, mean_q: --\n",
      "  868/2000: episode: 8, duration: 6.121s, episode steps: 108, steps per second: 18, episode reward: 0.627, mean reward: 0.006 [-0.002, 0.014], mean action: 0.020 [-1.163, 1.135], mean observation: 0.092 [-13.208, 17.895], loss: --, mean_squared_error: --, mean_q: --\n",
      "  976/2000: episode: 9, duration: 6.069s, episode steps: 108, steps per second: 18, episode reward: 0.629, mean reward: 0.006 [-0.002, 0.014], mean action: 0.019 [-1.163, 1.253], mean observation: 0.091 [-24.303, 17.474], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1083/2000: episode: 10, duration: 6.918s, episode steps: 107, steps per second: 15, episode reward: 0.621, mean reward: 0.006 [-0.002, 0.014], mean action: 0.023 [-1.117, 1.219], mean observation: 0.089 [-22.449, 17.503], loss: 0.000071, mean_squared_error: 0.000141, mean_q: 0.590298\n",
      " 1190/2000: episode: 11, duration: 7.477s, episode steps: 107, steps per second: 14, episode reward: 0.640, mean reward: 0.006 [-0.002, 0.014], mean action: -0.036 [-1.208, 1.117], mean observation: 0.087 [-26.515, 17.628], loss: 0.000082, mean_squared_error: 0.000163, mean_q: 0.590767\n",
      " 1298/2000: episode: 12, duration: 7.441s, episode steps: 108, steps per second: 15, episode reward: 0.678, mean reward: 0.006 [-0.002, 0.014], mean action: -0.090 [-1.232, 1.074], mean observation: 0.086 [-41.501, 20.480], loss: 0.000522, mean_squared_error: 0.001044, mean_q: 0.590324\n",
      " 1413/2000: episode: 13, duration: 7.263s, episode steps: 115, steps per second: 16, episode reward: 0.725, mean reward: 0.006 [-0.002, 0.014], mean action: -0.001 [-1.151, 1.181], mean observation: 0.088 [-14.929, 17.414], loss: 0.000385, mean_squared_error: 0.000771, mean_q: 0.586060\n",
      " 1528/2000: episode: 14, duration: 6.844s, episode steps: 115, steps per second: 17, episode reward: 0.709, mean reward: 0.006 [-0.002, 0.013], mean action: 0.012 [-1.151, 1.147], mean observation: 0.084 [-10.685, 17.551], loss: 0.000205, mean_squared_error: 0.000411, mean_q: 0.574890\n",
      " 1635/2000: episode: 15, duration: 6.686s, episode steps: 107, steps per second: 16, episode reward: 0.679, mean reward: 0.006 [-0.002, 0.014], mean action: 0.110 [-1.139, 1.167], mean observation: 0.077 [-45.622, 18.442], loss: 0.000116, mean_squared_error: 0.000233, mean_q: 0.576423\n",
      " 1750/2000: episode: 16, duration: 6.621s, episode steps: 115, steps per second: 17, episode reward: 0.722, mean reward: 0.006 [-0.002, 0.013], mean action: 0.071 [-1.225, 1.364], mean observation: 0.087 [-10.383, 17.538], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.578822\n",
      " 1866/2000: episode: 17, duration: 6.805s, episode steps: 116, steps per second: 17, episode reward: 0.724, mean reward: 0.006 [-0.002, 0.013], mean action: 0.032 [-1.150, 1.192], mean observation: 0.086 [-10.462, 17.729], loss: 0.000306, mean_squared_error: 0.000612, mean_q: 0.595439\n",
      " 1981/2000: episode: 18, duration: 7.070s, episode steps: 115, steps per second: 16, episode reward: 0.709, mean reward: 0.006 [-0.002, 0.013], mean action: 0.084 [-1.166, 1.188], mean observation: 0.083 [-39.484, 17.532], loss: 0.000240, mean_squared_error: 0.000480, mean_q: 0.592686\n",
      "done, took 119.506 seconds\n",
      "\n",
      "\n",
      "iteration: 87\n",
      "Training for 2000 steps ...\n",
      "  114/2000: episode: 1, duration: 5.976s, episode steps: 114, steps per second: 19, episode reward: 0.696, mean reward: 0.006 [-0.001, 0.013], mean action: 0.099 [-1.173, 1.152], mean observation: 0.081 [-29.380, 21.719], loss: --, mean_squared_error: --, mean_q: --\n",
      "  227/2000: episode: 2, duration: 5.854s, episode steps: 113, steps per second: 19, episode reward: 0.692, mean reward: 0.006 [-0.001, 0.013], mean action: 0.095 [-1.160, 1.174], mean observation: 0.081 [-31.118, 22.617], loss: --, mean_squared_error: --, mean_q: --\n",
      "  341/2000: episode: 3, duration: 5.973s, episode steps: 114, steps per second: 19, episode reward: 0.691, mean reward: 0.006 [-0.001, 0.013], mean action: 0.099 [-1.206, 1.231], mean observation: 0.080 [-31.030, 20.621], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  455/2000: episode: 4, duration: 5.920s, episode steps: 114, steps per second: 19, episode reward: 0.692, mean reward: 0.006 [-0.001, 0.013], mean action: 0.095 [-1.199, 1.150], mean observation: 0.083 [-27.878, 22.192], loss: --, mean_squared_error: --, mean_q: --\n",
      "  568/2000: episode: 5, duration: 5.896s, episode steps: 113, steps per second: 19, episode reward: 0.691, mean reward: 0.006 [-0.001, 0.013], mean action: 0.121 [-1.110, 1.190], mean observation: 0.083 [-36.828, 19.762], loss: --, mean_squared_error: --, mean_q: --\n",
      "  682/2000: episode: 6, duration: 5.933s, episode steps: 114, steps per second: 19, episode reward: 0.703, mean reward: 0.006 [-0.001, 0.013], mean action: 0.106 [-1.070, 1.182], mean observation: 0.082 [-34.392, 24.235], loss: --, mean_squared_error: --, mean_q: --\n",
      "  794/2000: episode: 7, duration: 5.855s, episode steps: 112, steps per second: 19, episode reward: 0.697, mean reward: 0.006 [-0.001, 0.013], mean action: 0.088 [-1.186, 1.170], mean observation: 0.082 [-32.873, 22.765], loss: --, mean_squared_error: --, mean_q: --\n",
      "  905/2000: episode: 8, duration: 5.869s, episode steps: 111, steps per second: 19, episode reward: 0.690, mean reward: 0.006 [-0.001, 0.013], mean action: 0.100 [-1.288, 1.191], mean observation: 0.082 [-38.757, 21.187], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1016/2000: episode: 9, duration: 6.087s, episode steps: 111, steps per second: 18, episode reward: 0.696, mean reward: 0.006 [-0.001, 0.013], mean action: 0.080 [-1.330, 1.198], mean observation: 0.081 [-36.496, 20.866], loss: 0.000202, mean_squared_error: 0.000405, mean_q: 0.602110\n",
      " 1129/2000: episode: 10, duration: 6.743s, episode steps: 113, steps per second: 17, episode reward: 0.702, mean reward: 0.006 [-0.001, 0.013], mean action: 0.096 [-1.138, 1.160], mean observation: 0.082 [-29.760, 21.546], loss: 0.000258, mean_squared_error: 0.000517, mean_q: 0.574438\n",
      " 1241/2000: episode: 11, duration: 6.655s, episode steps: 112, steps per second: 17, episode reward: 0.697, mean reward: 0.006 [-0.002, 0.013], mean action: 0.091 [-1.156, 1.128], mean observation: 0.080 [-30.306, 21.158], loss: 0.000387, mean_squared_error: 0.000774, mean_q: 0.596630\n",
      " 1351/2000: episode: 12, duration: 6.603s, episode steps: 110, steps per second: 17, episode reward: 0.683, mean reward: 0.006 [-0.001, 0.013], mean action: 0.088 [-1.134, 1.230], mean observation: 0.078 [-34.077, 20.228], loss: 0.000232, mean_squared_error: 0.000465, mean_q: 0.585051\n",
      " 1458/2000: episode: 13, duration: 7.000s, episode steps: 107, steps per second: 15, episode reward: 0.657, mean reward: 0.006 [-0.001, 0.014], mean action: 0.173 [-1.246, 1.173], mean observation: 0.068 [-40.695, 20.619], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.576482\n",
      " 1556/2000: episode: 14, duration: 6.687s, episode steps: 98, steps per second: 15, episode reward: 0.598, mean reward: 0.006 [-0.002, 0.013], mean action: 0.133 [-1.144, 1.171], mean observation: 0.057 [-42.312, 17.513], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.585637\n",
      " 1658/2000: episode: 15, duration: 6.925s, episode steps: 102, steps per second: 15, episode reward: 0.617, mean reward: 0.006 [-0.003, 0.013], mean action: 0.108 [-1.137, 1.144], mean observation: 0.067 [-37.920, 17.585], loss: 0.000258, mean_squared_error: 0.000515, mean_q: 0.584858\n",
      " 1764/2000: episode: 16, duration: 6.286s, episode steps: 106, steps per second: 17, episode reward: 0.677, mean reward: 0.006 [-0.002, 0.013], mean action: 0.098 [-1.206, 1.169], mean observation: 0.078 [-22.036, 17.486], loss: 0.000074, mean_squared_error: 0.000149, mean_q: 0.574409\n",
      " 1865/2000: episode: 17, duration: 6.737s, episode steps: 101, steps per second: 15, episode reward: 0.648, mean reward: 0.006 [-0.002, 0.014], mean action: 0.128 [-1.130, 1.169], mean observation: 0.067 [-18.088, 17.857], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.588034\n",
      " 1971/2000: episode: 18, duration: 6.616s, episode steps: 106, steps per second: 16, episode reward: 0.667, mean reward: 0.006 [-0.001, 0.013], mean action: 0.135 [-1.220, 1.136], mean observation: 0.076 [-21.219, 17.476], loss: 0.000078, mean_squared_error: 0.000155, mean_q: 0.580388\n",
      "done, took 115.698 seconds\n",
      "\n",
      "\n",
      "iteration: 88\n",
      "Training for 2000 steps ...\n",
      "  110/2000: episode: 1, duration: 6.028s, episode steps: 110, steps per second: 18, episode reward: 0.675, mean reward: 0.006 [-0.001, 0.013], mean action: 0.160 [-1.121, 1.186], mean observation: 0.081 [-21.473, 17.688], loss: --, mean_squared_error: --, mean_q: --\n",
      "  218/2000: episode: 2, duration: 5.750s, episode steps: 108, steps per second: 19, episode reward: 0.664, mean reward: 0.006 [-0.001, 0.013], mean action: 0.130 [-1.228, 1.103], mean observation: 0.077 [-49.053, 17.805], loss: --, mean_squared_error: --, mean_q: --\n",
      "  332/2000: episode: 3, duration: 5.990s, episode steps: 114, steps per second: 19, episode reward: 0.692, mean reward: 0.006 [-0.001, 0.013], mean action: 0.159 [-1.136, 1.132], mean observation: 0.084 [-15.888, 17.644], loss: --, mean_squared_error: --, mean_q: --\n",
      "  443/2000: episode: 4, duration: 5.812s, episode steps: 111, steps per second: 19, episode reward: 0.682, mean reward: 0.006 [-0.001, 0.013], mean action: 0.119 [-1.155, 1.089], mean observation: 0.080 [-24.558, 17.414], loss: --, mean_squared_error: --, mean_q: --\n",
      "  554/2000: episode: 5, duration: 5.779s, episode steps: 111, steps per second: 19, episode reward: 0.683, mean reward: 0.006 [-0.001, 0.013], mean action: 0.159 [-1.142, 1.226], mean observation: 0.079 [-29.629, 17.465], loss: --, mean_squared_error: --, mean_q: --\n",
      "  668/2000: episode: 6, duration: 6.035s, episode steps: 114, steps per second: 19, episode reward: 0.692, mean reward: 0.006 [-0.001, 0.013], mean action: 0.162 [-1.155, 1.189], mean observation: 0.080 [-22.881, 17.579], loss: --, mean_squared_error: --, mean_q: --\n",
      "  778/2000: episode: 7, duration: 6.001s, episode steps: 110, steps per second: 18, episode reward: 0.675, mean reward: 0.006 [-0.001, 0.013], mean action: 0.146 [-1.195, 1.211], mean observation: 0.075 [-35.921, 17.539], loss: --, mean_squared_error: --, mean_q: --\n",
      "  891/2000: episode: 8, duration: 6.023s, episode steps: 113, steps per second: 19, episode reward: 0.689, mean reward: 0.006 [-0.001, 0.013], mean action: 0.143 [-1.233, 1.135], mean observation: 0.079 [-19.341, 17.364], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1002/2000: episode: 9, duration: 6.124s, episode steps: 111, steps per second: 18, episode reward: 0.677, mean reward: 0.006 [-0.001, 0.013], mean action: 0.157 [-1.097, 1.147], mean observation: 0.078 [-18.153, 17.838], loss: 0.000032, mean_squared_error: 0.000064, mean_q: 0.561343\n",
      " 1113/2000: episode: 10, duration: 7.038s, episode steps: 111, steps per second: 16, episode reward: 0.675, mean reward: 0.006 [-0.001, 0.013], mean action: 0.162 [-1.121, 1.183], mean observation: 0.079 [-23.598, 17.511], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.585560\n",
      " 1224/2000: episode: 11, duration: 7.272s, episode steps: 111, steps per second: 15, episode reward: 0.675, mean reward: 0.006 [-0.001, 0.013], mean action: 0.138 [-1.233, 1.119], mean observation: 0.077 [-48.684, 21.435], loss: 0.000204, mean_squared_error: 0.000409, mean_q: 0.579853\n",
      " 1337/2000: episode: 12, duration: 7.632s, episode steps: 113, steps per second: 15, episode reward: 0.667, mean reward: 0.006 [-0.001, 0.013], mean action: 0.137 [-1.190, 1.180], mean observation: 0.075 [-47.361, 20.046], loss: 0.000083, mean_squared_error: 0.000167, mean_q: 0.588927\n",
      " 1452/2000: episode: 13, duration: 7.381s, episode steps: 115, steps per second: 16, episode reward: 0.697, mean reward: 0.006 [-0.001, 0.013], mean action: 0.142 [-1.188, 1.172], mean observation: 0.084 [-17.908, 17.867], loss: 0.000096, mean_squared_error: 0.000191, mean_q: 0.589318\n",
      " 1572/2000: episode: 14, duration: 7.783s, episode steps: 120, steps per second: 15, episode reward: 0.694, mean reward: 0.006 [-0.001, 0.013], mean action: 0.126 [-1.139, 1.168], mean observation: 0.088 [-31.654, 18.149], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.579515\n",
      " 1682/2000: episode: 15, duration: 7.921s, episode steps: 110, steps per second: 14, episode reward: 0.658, mean reward: 0.006 [-0.001, 0.013], mean action: 0.131 [-1.133, 1.230], mean observation: 0.072 [-44.211, 18.016], loss: 0.000373, mean_squared_error: 0.000745, mean_q: 0.589487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1792/2000: episode: 16, duration: 7.186s, episode steps: 110, steps per second: 15, episode reward: 0.688, mean reward: 0.006 [-0.001, 0.013], mean action: 0.134 [-1.167, 1.242], mean observation: 0.080 [-27.499, 17.583], loss: 0.000240, mean_squared_error: 0.000479, mean_q: 0.578031\n",
      " 1908/2000: episode: 17, duration: 6.896s, episode steps: 116, steps per second: 17, episode reward: 0.712, mean reward: 0.006 [-0.001, 0.013], mean action: 0.119 [-1.222, 1.200], mean observation: 0.085 [-10.448, 17.587], loss: 0.000077, mean_squared_error: 0.000154, mean_q: 0.578037\n",
      "done, took 118.189 seconds\n",
      "\n",
      "\n",
      "iteration: 89\n",
      "Training for 2000 steps ...\n",
      "  115/2000: episode: 1, duration: 6.288s, episode steps: 115, steps per second: 18, episode reward: 0.697, mean reward: 0.006 [-0.001, 0.013], mean action: 0.093 [-1.150, 1.114], mean observation: 0.083 [-14.073, 17.553], loss: --, mean_squared_error: --, mean_q: --\n",
      "  230/2000: episode: 2, duration: 5.851s, episode steps: 115, steps per second: 20, episode reward: 0.704, mean reward: 0.006 [-0.001, 0.013], mean action: 0.130 [-1.139, 1.157], mean observation: 0.083 [-11.661, 17.597], loss: --, mean_squared_error: --, mean_q: --\n",
      "  346/2000: episode: 3, duration: 5.809s, episode steps: 116, steps per second: 20, episode reward: 0.710, mean reward: 0.006 [-0.001, 0.013], mean action: 0.116 [-1.091, 1.200], mean observation: 0.083 [-13.101, 17.502], loss: --, mean_squared_error: --, mean_q: --\n",
      "  461/2000: episode: 4, duration: 5.795s, episode steps: 115, steps per second: 20, episode reward: 0.697, mean reward: 0.006 [-0.001, 0.013], mean action: 0.116 [-1.211, 1.153], mean observation: 0.083 [-10.429, 17.419], loss: --, mean_squared_error: --, mean_q: --\n",
      "  574/2000: episode: 5, duration: 6.111s, episode steps: 113, steps per second: 18, episode reward: 0.696, mean reward: 0.006 [-0.001, 0.013], mean action: 0.102 [-1.216, 1.133], mean observation: 0.080 [-25.563, 19.686], loss: --, mean_squared_error: --, mean_q: --\n",
      "  689/2000: episode: 6, duration: 5.793s, episode steps: 115, steps per second: 20, episode reward: 0.705, mean reward: 0.006 [-0.001, 0.013], mean action: 0.107 [-1.158, 1.269], mean observation: 0.084 [-14.510, 17.651], loss: --, mean_squared_error: --, mean_q: --\n",
      "  804/2000: episode: 7, duration: 6.226s, episode steps: 115, steps per second: 18, episode reward: 0.706, mean reward: 0.006 [-0.001, 0.013], mean action: 0.075 [-1.237, 1.126], mean observation: 0.081 [-26.492, 19.458], loss: --, mean_squared_error: --, mean_q: --\n",
      "  920/2000: episode: 8, duration: 5.918s, episode steps: 116, steps per second: 20, episode reward: 0.706, mean reward: 0.006 [-0.001, 0.013], mean action: 0.091 [-1.200, 1.236], mean observation: 0.084 [-11.781, 17.912], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1035/2000: episode: 9, duration: 6.211s, episode steps: 115, steps per second: 19, episode reward: 0.701, mean reward: 0.006 [-0.001, 0.013], mean action: 0.099 [-1.161, 1.127], mean observation: 0.085 [-12.217, 17.568], loss: 0.000083, mean_squared_error: 0.000165, mean_q: 0.590472\n",
      " 1149/2000: episode: 10, duration: 7.357s, episode steps: 114, steps per second: 15, episode reward: 0.710, mean reward: 0.006 [-0.001, 0.013], mean action: 0.036 [-1.271, 1.190], mean observation: 0.086 [-25.197, 19.492], loss: 0.000151, mean_squared_error: 0.000301, mean_q: 0.568294\n",
      " 1260/2000: episode: 11, duration: 7.145s, episode steps: 111, steps per second: 16, episode reward: 0.696, mean reward: 0.006 [-0.001, 0.013], mean action: 0.037 [-1.138, 1.218], mean observation: 0.085 [-41.428, 17.659], loss: 0.000116, mean_squared_error: 0.000232, mean_q: 0.592799\n",
      " 1367/2000: episode: 12, duration: 7.124s, episode steps: 107, steps per second: 15, episode reward: 0.658, mean reward: 0.006 [-0.001, 0.013], mean action: 0.003 [-1.140, 1.143], mean observation: 0.076 [-34.625, 17.726], loss: 0.000107, mean_squared_error: 0.000215, mean_q: 0.573538\n",
      " 1479/2000: episode: 13, duration: 7.289s, episode steps: 112, steps per second: 15, episode reward: 0.710, mean reward: 0.006 [-0.001, 0.013], mean action: -0.016 [-1.290, 1.207], mean observation: 0.084 [-28.293, 21.676], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.581287\n",
      " 1587/2000: episode: 14, duration: 7.263s, episode steps: 108, steps per second: 15, episode reward: 0.684, mean reward: 0.006 [-0.001, 0.013], mean action: 0.046 [-1.081, 1.157], mean observation: 0.083 [-39.981, 17.457], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.587929\n",
      " 1696/2000: episode: 15, duration: 6.562s, episode steps: 109, steps per second: 17, episode reward: 0.698, mean reward: 0.006 [-0.001, 0.013], mean action: 0.023 [-1.189, 1.134], mean observation: 0.084 [-24.721, 17.576], loss: 0.000097, mean_squared_error: 0.000195, mean_q: 0.571295\n",
      " 1804/2000: episode: 16, duration: 6.959s, episode steps: 108, steps per second: 16, episode reward: 0.668, mean reward: 0.006 [-0.001, 0.013], mean action: 0.077 [-1.146, 1.273], mean observation: 0.083 [-51.158, 17.578], loss: 0.000144, mean_squared_error: 0.000288, mean_q: 0.592624\n",
      " 1915/2000: episode: 17, duration: 6.332s, episode steps: 111, steps per second: 18, episode reward: 0.716, mean reward: 0.006 [-0.000, 0.013], mean action: 0.097 [-1.168, 1.216], mean observation: 0.088 [-10.368, 17.679], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.581768\n",
      "done, took 114.779 seconds\n",
      "\n",
      "\n",
      "iteration: 90\n",
      "Training for 2000 steps ...\n",
      "  112/2000: episode: 1, duration: 4.920s, episode steps: 112, steps per second: 23, episode reward: 0.734, mean reward: 0.007 [-0.001, 0.013], mean action: 0.088 [-1.125, 1.115], mean observation: 0.093 [-10.389, 17.658], loss: --, mean_squared_error: --, mean_q: --\n",
      "  224/2000: episode: 2, duration: 4.958s, episode steps: 112, steps per second: 23, episode reward: 0.723, mean reward: 0.006 [-0.001, 0.013], mean action: 0.094 [-1.187, 1.169], mean observation: 0.091 [-10.282, 17.839], loss: --, mean_squared_error: --, mean_q: --\n",
      "  336/2000: episode: 3, duration: 4.948s, episode steps: 112, steps per second: 23, episode reward: 0.727, mean reward: 0.006 [-0.001, 0.013], mean action: 0.100 [-1.201, 1.186], mean observation: 0.092 [-10.426, 17.506], loss: --, mean_squared_error: --, mean_q: --\n",
      "  449/2000: episode: 4, duration: 5.053s, episode steps: 113, steps per second: 22, episode reward: 0.734, mean reward: 0.006 [-0.001, 0.013], mean action: 0.090 [-1.106, 1.122], mean observation: 0.092 [-10.248, 17.578], loss: --, mean_squared_error: --, mean_q: --\n",
      "  559/2000: episode: 5, duration: 4.926s, episode steps: 110, steps per second: 22, episode reward: 0.723, mean reward: 0.007 [-0.001, 0.013], mean action: 0.101 [-1.172, 1.211], mean observation: 0.090 [-10.522, 17.487], loss: --, mean_squared_error: --, mean_q: --\n",
      "  670/2000: episode: 6, duration: 4.824s, episode steps: 111, steps per second: 23, episode reward: 0.722, mean reward: 0.007 [-0.001, 0.013], mean action: 0.098 [-1.177, 1.177], mean observation: 0.092 [-10.385, 17.620], loss: --, mean_squared_error: --, mean_q: --\n",
      "  782/2000: episode: 7, duration: 5.032s, episode steps: 112, steps per second: 22, episode reward: 0.726, mean reward: 0.006 [-0.001, 0.013], mean action: 0.117 [-1.160, 1.178], mean observation: 0.092 [-10.381, 17.610], loss: --, mean_squared_error: --, mean_q: --\n",
      "  893/2000: episode: 8, duration: 4.877s, episode steps: 111, steps per second: 23, episode reward: 0.724, mean reward: 0.007 [-0.001, 0.013], mean action: 0.123 [-1.141, 1.295], mean observation: 0.092 [-10.437, 17.581], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1006/2000: episode: 9, duration: 5.162s, episode steps: 113, steps per second: 22, episode reward: 0.728, mean reward: 0.006 [-0.001, 0.013], mean action: 0.086 [-1.192, 1.175], mean observation: 0.092 [-10.328, 17.888], loss: 0.000046, mean_squared_error: 0.000092, mean_q: 0.579326\n",
      " 1119/2000: episode: 10, duration: 6.258s, episode steps: 113, steps per second: 18, episode reward: 0.730, mean reward: 0.006 [-0.001, 0.013], mean action: 0.077 [-1.169, 1.159], mean observation: 0.091 [-10.344, 17.612], loss: 0.000145, mean_squared_error: 0.000290, mean_q: 0.587114\n",
      " 1226/2000: episode: 11, duration: 5.964s, episode steps: 107, steps per second: 18, episode reward: 0.723, mean reward: 0.007 [-0.001, 0.014], mean action: 0.086 [-1.160, 1.141], mean observation: 0.087 [-11.111, 21.065], loss: 0.000203, mean_squared_error: 0.000407, mean_q: 0.576138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1373/2000: episode: 12, duration: 7.841s, episode steps: 147, steps per second: 19, episode reward: 0.767, mean reward: 0.005 [0.000, 0.012], mean action: -0.003 [-1.187, 1.150], mean observation: 0.108 [-10.306, 19.646], loss: 0.000205, mean_squared_error: 0.000409, mean_q: 0.593911\n",
      " 1524/2000: episode: 13, duration: 8.428s, episode steps: 151, steps per second: 18, episode reward: 0.798, mean reward: 0.005 [0.000, 0.014], mean action: -0.061 [-1.243, 1.256], mean observation: 0.112 [-34.167, 19.807], loss: 0.000548, mean_squared_error: 0.001095, mean_q: 0.578509\n",
      " 1659/2000: episode: 14, duration: 7.049s, episode steps: 135, steps per second: 19, episode reward: 0.787, mean reward: 0.006 [0.000, 0.013], mean action: -0.103 [-1.202, 1.117], mean observation: 0.111 [-9.644, 18.597], loss: 0.000138, mean_squared_error: 0.000275, mean_q: 0.585843\n",
      " 1793/2000: episode: 15, duration: 7.121s, episode steps: 134, steps per second: 19, episode reward: 0.777, mean reward: 0.006 [0.000, 0.013], mean action: -0.046 [-1.259, 1.182], mean observation: 0.108 [-9.656, 18.692], loss: 0.000123, mean_squared_error: 0.000247, mean_q: 0.569508\n",
      " 1936/2000: episode: 16, duration: 7.398s, episode steps: 143, steps per second: 19, episode reward: 0.813, mean reward: 0.006 [0.001, 0.013], mean action: -0.049 [-1.138, 1.303], mean observation: 0.115 [-9.648, 18.915], loss: 0.000183, mean_squared_error: 0.000366, mean_q: 0.578162\n",
      "done, took 97.818 seconds\n",
      "\n",
      "\n",
      "iteration: 91\n",
      "Training for 2000 steps ...\n",
      "  145/2000: episode: 1, duration: 6.098s, episode steps: 145, steps per second: 24, episode reward: 0.798, mean reward: 0.006 [0.001, 0.013], mean action: -0.084 [-1.254, 1.127], mean observation: 0.115 [-9.707, 19.371], loss: --, mean_squared_error: --, mean_q: --\n",
      "  291/2000: episode: 2, duration: 6.110s, episode steps: 146, steps per second: 24, episode reward: 0.808, mean reward: 0.006 [0.001, 0.013], mean action: -0.071 [-1.152, 1.192], mean observation: 0.115 [-9.633, 18.861], loss: --, mean_squared_error: --, mean_q: --\n",
      "  434/2000: episode: 3, duration: 5.972s, episode steps: 143, steps per second: 24, episode reward: 0.811, mean reward: 0.006 [0.001, 0.013], mean action: -0.096 [-1.168, 1.182], mean observation: 0.116 [-9.663, 19.056], loss: --, mean_squared_error: --, mean_q: --\n",
      "  579/2000: episode: 4, duration: 5.940s, episode steps: 145, steps per second: 24, episode reward: 0.806, mean reward: 0.006 [0.001, 0.013], mean action: -0.076 [-1.183, 1.159], mean observation: 0.116 [-9.658, 18.831], loss: --, mean_squared_error: --, mean_q: --\n",
      "  726/2000: episode: 5, duration: 6.082s, episode steps: 147, steps per second: 24, episode reward: 0.816, mean reward: 0.006 [0.001, 0.013], mean action: -0.109 [-1.354, 1.199], mean observation: 0.118 [-9.618, 19.028], loss: --, mean_squared_error: --, mean_q: --\n",
      "  872/2000: episode: 6, duration: 6.136s, episode steps: 146, steps per second: 24, episode reward: 0.813, mean reward: 0.006 [0.001, 0.013], mean action: -0.059 [-1.179, 1.265], mean observation: 0.117 [-9.603, 19.002], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1019/2000: episode: 7, duration: 6.352s, episode steps: 147, steps per second: 23, episode reward: 0.814, mean reward: 0.006 [0.001, 0.013], mean action: -0.120 [-1.218, 1.206], mean observation: 0.117 [-9.654, 19.156], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.580829\n",
      " 1173/2000: episode: 8, duration: 8.181s, episode steps: 154, steps per second: 19, episode reward: 0.791, mean reward: 0.005 [0.001, 0.013], mean action: -0.116 [-1.274, 1.182], mean observation: 0.114 [-9.580, 18.766], loss: 0.000046, mean_squared_error: 0.000092, mean_q: 0.574922\n",
      " 1319/2000: episode: 9, duration: 7.788s, episode steps: 146, steps per second: 19, episode reward: 0.790, mean reward: 0.005 [0.000, 0.013], mean action: -0.087 [-1.228, 1.211], mean observation: 0.113 [-9.641, 18.900], loss: 0.000105, mean_squared_error: 0.000211, mean_q: 0.574493\n",
      " 1474/2000: episode: 10, duration: 8.927s, episode steps: 155, steps per second: 17, episode reward: 0.777, mean reward: 0.005 [0.001, 0.013], mean action: -0.127 [-1.162, 1.170], mean observation: 0.119 [-9.586, 18.862], loss: 0.000327, mean_squared_error: 0.000655, mean_q: 0.579080\n",
      " 1724/2000: episode: 11, duration: 10.589s, episode steps: 250, steps per second: 24, episode reward: -0.762, mean reward: -0.003 [-0.020, 0.008], mean action: -0.110 [-1.222, 1.323], mean observation: 0.087 [-9.866, 20.107], loss: 0.000146, mean_squared_error: 0.000292, mean_q: 0.579851\n",
      " 1868/2000: episode: 12, duration: 7.575s, episode steps: 144, steps per second: 19, episode reward: 0.789, mean reward: 0.005 [0.001, 0.014], mean action: -0.083 [-1.185, 1.182], mean observation: 0.114 [-9.832, 20.084], loss: 0.000175, mean_squared_error: 0.000350, mean_q: 0.582838\n",
      "done, took 92.474 seconds\n",
      "\n",
      "\n",
      "iteration: 92\n",
      "Training for 2000 steps ...\n",
      "  152/2000: episode: 1, duration: 6.899s, episode steps: 152, steps per second: 22, episode reward: 0.776, mean reward: 0.005 [0.001, 0.013], mean action: -0.008 [-1.212, 1.164], mean observation: 0.114 [-9.901, 19.886], loss: --, mean_squared_error: --, mean_q: --\n",
      "  307/2000: episode: 2, duration: 6.998s, episode steps: 155, steps per second: 22, episode reward: 0.778, mean reward: 0.005 [0.001, 0.013], mean action: -0.007 [-1.164, 1.158], mean observation: 0.114 [-9.948, 19.890], loss: --, mean_squared_error: --, mean_q: --\n",
      "  462/2000: episode: 3, duration: 7.050s, episode steps: 155, steps per second: 22, episode reward: 0.783, mean reward: 0.005 [0.001, 0.013], mean action: -0.020 [-1.260, 1.135], mean observation: 0.114 [-9.956, 20.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  619/2000: episode: 4, duration: 7.215s, episode steps: 157, steps per second: 22, episode reward: 0.776, mean reward: 0.005 [0.001, 0.013], mean action: -0.024 [-1.185, 1.172], mean observation: 0.114 [-9.995, 20.127], loss: --, mean_squared_error: --, mean_q: --\n",
      "  774/2000: episode: 5, duration: 7.089s, episode steps: 155, steps per second: 22, episode reward: 0.775, mean reward: 0.005 [0.001, 0.013], mean action: -0.025 [-1.239, 1.309], mean observation: 0.114 [-9.968, 20.155], loss: --, mean_squared_error: --, mean_q: --\n",
      "  930/2000: episode: 6, duration: 6.965s, episode steps: 156, steps per second: 22, episode reward: 0.791, mean reward: 0.005 [0.001, 0.013], mean action: -0.019 [-1.246, 1.207], mean observation: 0.115 [-9.907, 19.504], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1087/2000: episode: 7, duration: 7.401s, episode steps: 157, steps per second: 21, episode reward: 0.821, mean reward: 0.005 [0.001, 0.014], mean action: -0.063 [-1.204, 1.178], mean observation: 0.118 [-9.798, 19.968], loss: 0.000249, mean_squared_error: 0.000497, mean_q: 0.578344\n",
      " 1250/2000: episode: 8, duration: 8.084s, episode steps: 163, steps per second: 20, episode reward: 0.862, mean reward: 0.005 [0.001, 0.014], mean action: -0.178 [-1.197, 1.165], mean observation: 0.122 [-18.208, 19.847], loss: 0.000268, mean_squared_error: 0.000537, mean_q: 0.581811\n",
      " 1392/2000: episode: 9, duration: 7.331s, episode steps: 142, steps per second: 19, episode reward: 0.849, mean reward: 0.006 [0.000, 0.014], mean action: -0.141 [-1.146, 1.247], mean observation: 0.117 [-40.764, 19.615], loss: 0.000158, mean_squared_error: 0.000316, mean_q: 0.583281\n",
      " 1532/2000: episode: 10, duration: 6.935s, episode steps: 140, steps per second: 20, episode reward: 0.827, mean reward: 0.006 [0.000, 0.014], mean action: -0.062 [-1.204, 1.144], mean observation: 0.117 [-9.713, 19.809], loss: 0.000208, mean_squared_error: 0.000416, mean_q: 0.572801\n",
      " 1678/2000: episode: 11, duration: 7.432s, episode steps: 146, steps per second: 20, episode reward: 0.816, mean reward: 0.006 [0.000, 0.014], mean action: -0.061 [-1.184, 1.241], mean observation: 0.119 [-9.791, 19.893], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.574954\n",
      " 1827/2000: episode: 12, duration: 8.052s, episode steps: 149, steps per second: 19, episode reward: 0.744, mean reward: 0.005 [0.000, 0.012], mean action: -0.051 [-1.327, 1.207], mean observation: 0.117 [-47.741, 19.859], loss: 0.000103, mean_squared_error: 0.000207, mean_q: 0.580818\n",
      " 1968/2000: episode: 13, duration: 7.592s, episode steps: 141, steps per second: 19, episode reward: 0.784, mean reward: 0.006 [-0.000, 0.012], mean action: -0.062 [-1.300, 1.253], mean observation: 0.114 [-41.187, 19.636], loss: 0.000308, mean_squared_error: 0.000617, mean_q: 0.582834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 96.756 seconds\n",
      "\n",
      "\n",
      "iteration: 93\n",
      "Training for 2000 steps ...\n",
      "  133/2000: episode: 1, duration: 6.023s, episode steps: 133, steps per second: 22, episode reward: 0.753, mean reward: 0.006 [0.000, 0.012], mean action: -0.036 [-1.275, 1.236], mean observation: 0.116 [-20.452, 20.039], loss: --, mean_squared_error: --, mean_q: --\n",
      "  269/2000: episode: 2, duration: 6.269s, episode steps: 136, steps per second: 22, episode reward: 0.790, mean reward: 0.006 [0.000, 0.012], mean action: -0.032 [-1.241, 1.212], mean observation: 0.116 [-39.094, 19.367], loss: --, mean_squared_error: --, mean_q: --\n",
      "  404/2000: episode: 3, duration: 6.207s, episode steps: 135, steps per second: 22, episode reward: 0.786, mean reward: 0.006 [0.000, 0.012], mean action: -0.051 [-1.315, 1.148], mean observation: 0.115 [-35.797, 19.875], loss: --, mean_squared_error: --, mean_q: --\n",
      "  540/2000: episode: 4, duration: 6.105s, episode steps: 136, steps per second: 22, episode reward: 0.796, mean reward: 0.006 [0.000, 0.013], mean action: -0.034 [-1.241, 1.104], mean observation: 0.120 [-30.178, 19.567], loss: --, mean_squared_error: --, mean_q: --\n",
      "  675/2000: episode: 5, duration: 6.195s, episode steps: 135, steps per second: 22, episode reward: 0.771, mean reward: 0.006 [0.000, 0.012], mean action: -0.020 [-1.230, 1.264], mean observation: 0.115 [-24.964, 19.410], loss: --, mean_squared_error: --, mean_q: --\n",
      "  810/2000: episode: 6, duration: 6.349s, episode steps: 135, steps per second: 21, episode reward: 0.779, mean reward: 0.006 [0.000, 0.012], mean action: -0.074 [-1.353, 1.193], mean observation: 0.111 [-38.100, 20.083], loss: --, mean_squared_error: --, mean_q: --\n",
      "  948/2000: episode: 7, duration: 6.021s, episode steps: 138, steps per second: 23, episode reward: 0.826, mean reward: 0.006 [0.000, 0.013], mean action: -0.031 [-1.184, 1.262], mean observation: 0.118 [-35.296, 19.838], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1088/2000: episode: 8, duration: 6.772s, episode steps: 140, steps per second: 21, episode reward: 0.835, mean reward: 0.006 [0.000, 0.013], mean action: -0.051 [-1.223, 1.140], mean observation: 0.121 [-18.702, 19.687], loss: 0.000217, mean_squared_error: 0.000435, mean_q: 0.584553\n",
      " 1233/2000: episode: 9, duration: 7.197s, episode steps: 145, steps per second: 20, episode reward: 0.788, mean reward: 0.005 [-0.000, 0.013], mean action: -0.053 [-1.303, 1.199], mean observation: 0.120 [-9.564, 18.754], loss: 0.000337, mean_squared_error: 0.000673, mean_q: 0.577000\n",
      " 1385/2000: episode: 10, duration: 7.323s, episode steps: 152, steps per second: 21, episode reward: 0.790, mean reward: 0.005 [0.000, 0.012], mean action: -0.044 [-1.190, 1.207], mean observation: 0.121 [-9.802, 19.966], loss: 0.000081, mean_squared_error: 0.000163, mean_q: 0.574449\n",
      " 1522/2000: episode: 11, duration: 6.661s, episode steps: 137, steps per second: 21, episode reward: 0.786, mean reward: 0.006 [-0.000, 0.013], mean action: -0.085 [-1.197, 1.204], mean observation: 0.120 [-9.748, 20.108], loss: 0.000304, mean_squared_error: 0.000608, mean_q: 0.576595\n",
      " 1646/2000: episode: 12, duration: 6.085s, episode steps: 124, steps per second: 20, episode reward: 0.787, mean reward: 0.006 [0.000, 0.013], mean action: -0.044 [-1.208, 1.144], mean observation: 0.118 [-10.960, 20.608], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.579925\n",
      " 1778/2000: episode: 13, duration: 6.709s, episode steps: 132, steps per second: 20, episode reward: 0.771, mean reward: 0.006 [-0.000, 0.012], mean action: -0.035 [-1.211, 1.176], mean observation: 0.118 [-13.787, 19.900], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.571760\n",
      " 1911/2000: episode: 14, duration: 7.654s, episode steps: 133, steps per second: 17, episode reward: 0.722, mean reward: 0.005 [0.000, 0.012], mean action: -0.135 [-1.154, 1.163], mean observation: 0.114 [-48.433, 19.485], loss: 0.000302, mean_squared_error: 0.000605, mean_q: 0.570359\n",
      "done, took 96.077 seconds\n",
      "\n",
      "\n",
      "iteration: 94\n",
      "Training for 2000 steps ...\n",
      "  134/2000: episode: 1, duration: 6.009s, episode steps: 134, steps per second: 22, episode reward: 0.730, mean reward: 0.005 [0.000, 0.012], mean action: -0.182 [-1.177, 1.388], mean observation: 0.115 [-48.289, 19.680], loss: --, mean_squared_error: --, mean_q: --\n",
      "  269/2000: episode: 2, duration: 5.850s, episode steps: 135, steps per second: 23, episode reward: 0.726, mean reward: 0.005 [0.000, 0.012], mean action: -0.196 [-1.269, 1.166], mean observation: 0.116 [-45.632, 19.401], loss: --, mean_squared_error: --, mean_q: --\n",
      "  413/2000: episode: 3, duration: 6.640s, episode steps: 144, steps per second: 22, episode reward: 0.765, mean reward: 0.005 [0.000, 0.012], mean action: -0.175 [-1.204, 1.238], mean observation: 0.123 [-48.540, 19.991], loss: --, mean_squared_error: --, mean_q: --\n",
      "  555/2000: episode: 4, duration: 6.561s, episode steps: 142, steps per second: 22, episode reward: 0.756, mean reward: 0.005 [0.000, 0.013], mean action: -0.188 [-1.178, 1.309], mean observation: 0.118 [-48.423, 19.843], loss: --, mean_squared_error: --, mean_q: --\n",
      "  697/2000: episode: 5, duration: 6.434s, episode steps: 142, steps per second: 22, episode reward: 0.750, mean reward: 0.005 [0.000, 0.012], mean action: -0.181 [-1.158, 1.190], mean observation: 0.118 [-46.956, 19.735], loss: --, mean_squared_error: --, mean_q: --\n",
      "  838/2000: episode: 6, duration: 6.307s, episode steps: 141, steps per second: 22, episode reward: 0.771, mean reward: 0.005 [0.000, 0.012], mean action: -0.214 [-1.238, 1.108], mean observation: 0.122 [-47.825, 19.636], loss: --, mean_squared_error: --, mean_q: --\n",
      "  974/2000: episode: 7, duration: 5.907s, episode steps: 136, steps per second: 23, episode reward: 0.729, mean reward: 0.005 [0.000, 0.012], mean action: -0.232 [-1.225, 1.253], mean observation: 0.113 [-45.476, 19.935], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1115/2000: episode: 8, duration: 7.549s, episode steps: 141, steps per second: 19, episode reward: 0.756, mean reward: 0.005 [0.000, 0.012], mean action: -0.195 [-1.178, 1.155], mean observation: 0.119 [-48.056, 19.948], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.578293\n",
      " 1252/2000: episode: 9, duration: 6.985s, episode steps: 137, steps per second: 20, episode reward: 0.783, mean reward: 0.006 [0.001, 0.014], mean action: -0.172 [-1.199, 1.202], mean observation: 0.120 [-9.997, 19.512], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.593558\n",
      " 1382/2000: episode: 10, duration: 6.858s, episode steps: 130, steps per second: 19, episode reward: 0.744, mean reward: 0.006 [0.000, 0.015], mean action: -0.144 [-1.134, 1.161], mean observation: 0.105 [-10.152, 19.968], loss: 0.000072, mean_squared_error: 0.000145, mean_q: 0.572901\n",
      " 1512/2000: episode: 11, duration: 6.673s, episode steps: 130, steps per second: 19, episode reward: 0.764, mean reward: 0.006 [0.001, 0.017], mean action: -0.092 [-1.197, 1.163], mean observation: 0.117 [-10.237, 19.832], loss: 0.000072, mean_squared_error: 0.000144, mean_q: 0.574348\n",
      " 1638/2000: episode: 12, duration: 6.897s, episode steps: 126, steps per second: 18, episode reward: 0.592, mean reward: 0.005 [0.001, 0.012], mean action: -0.048 [-1.203, 1.159], mean observation: 0.118 [-10.033, 19.925], loss: 0.000274, mean_squared_error: 0.000548, mean_q: 0.584587\n",
      " 1760/2000: episode: 13, duration: 6.462s, episode steps: 122, steps per second: 19, episode reward: 0.764, mean reward: 0.006 [0.000, 0.017], mean action: -0.016 [-1.154, 1.198], mean observation: 0.106 [-10.110, 19.995], loss: 0.000216, mean_squared_error: 0.000431, mean_q: 0.579726\n",
      " 1879/2000: episode: 14, duration: 6.213s, episode steps: 119, steps per second: 19, episode reward: 0.739, mean reward: 0.006 [-0.000, 0.016], mean action: 0.010 [-1.192, 1.271], mean observation: 0.106 [-10.033, 20.037], loss: 0.000184, mean_squared_error: 0.000368, mean_q: 0.582448\n",
      " 1984/2000: episode: 15, duration: 5.794s, episode steps: 105, steps per second: 18, episode reward: 0.664, mean reward: 0.006 [-0.001, 0.015], mean action: 0.129 [-1.161, 1.170], mean observation: 0.090 [-10.219, 18.007], loss: 0.000139, mean_squared_error: 0.000279, mean_q: 0.578707\n",
      "done, took 98.270 seconds\n",
      "\n",
      "\n",
      "iteration: 95\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  107/2000: episode: 1, duration: 5.178s, episode steps: 107, steps per second: 21, episode reward: 0.655, mean reward: 0.006 [-0.001, 0.013], mean action: 0.088 [-1.108, 1.242], mean observation: 0.093 [-10.299, 17.898], loss: --, mean_squared_error: --, mean_q: --\n",
      "  218/2000: episode: 2, duration: 5.445s, episode steps: 111, steps per second: 20, episode reward: 0.596, mean reward: 0.005 [-0.001, 0.013], mean action: 0.072 [-1.131, 1.181], mean observation: 0.111 [-10.343, 18.225], loss: --, mean_squared_error: --, mean_q: --\n",
      "  327/2000: episode: 3, duration: 5.396s, episode steps: 109, steps per second: 20, episode reward: 0.602, mean reward: 0.006 [-0.001, 0.013], mean action: 0.072 [-1.173, 1.120], mean observation: 0.110 [-10.287, 17.843], loss: --, mean_squared_error: --, mean_q: --\n",
      "  437/2000: episode: 4, duration: 5.654s, episode steps: 110, steps per second: 19, episode reward: 0.591, mean reward: 0.005 [-0.001, 0.013], mean action: 0.071 [-1.259, 1.210], mean observation: 0.107 [-28.265, 17.742], loss: --, mean_squared_error: --, mean_q: --\n",
      "  547/2000: episode: 5, duration: 5.249s, episode steps: 110, steps per second: 21, episode reward: 0.663, mean reward: 0.006 [-0.001, 0.013], mean action: 0.073 [-1.119, 1.120], mean observation: 0.103 [-10.412, 17.969], loss: --, mean_squared_error: --, mean_q: --\n",
      "  653/2000: episode: 6, duration: 5.255s, episode steps: 106, steps per second: 20, episode reward: 0.604, mean reward: 0.006 [-0.001, 0.013], mean action: 0.044 [-1.259, 1.133], mean observation: 0.098 [-28.875, 17.715], loss: --, mean_squared_error: --, mean_q: --\n",
      "  758/2000: episode: 7, duration: 5.176s, episode steps: 105, steps per second: 20, episode reward: 0.582, mean reward: 0.006 [-0.001, 0.013], mean action: 0.062 [-1.335, 1.162], mean observation: 0.096 [-35.544, 17.855], loss: --, mean_squared_error: --, mean_q: --\n",
      "  868/2000: episode: 8, duration: 5.386s, episode steps: 110, steps per second: 20, episode reward: 0.594, mean reward: 0.005 [-0.001, 0.013], mean action: 0.044 [-1.155, 1.161], mean observation: 0.109 [-10.285, 18.038], loss: --, mean_squared_error: --, mean_q: --\n",
      "  977/2000: episode: 9, duration: 5.487s, episode steps: 109, steps per second: 20, episode reward: 0.612, mean reward: 0.006 [-0.001, 0.013], mean action: 0.055 [-1.192, 1.233], mean observation: 0.107 [-10.364, 17.807], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1092/2000: episode: 10, duration: 6.602s, episode steps: 115, steps per second: 17, episode reward: 0.705, mean reward: 0.006 [-0.001, 0.013], mean action: 0.063 [-1.244, 1.157], mean observation: 0.115 [-10.358, 17.957], loss: 0.000064, mean_squared_error: 0.000129, mean_q: 0.565790\n",
      " 1206/2000: episode: 11, duration: 6.444s, episode steps: 114, steps per second: 18, episode reward: 0.661, mean reward: 0.006 [-0.001, 0.013], mean action: 0.031 [-1.218, 1.142], mean observation: 0.107 [-10.348, 17.889], loss: 0.000279, mean_squared_error: 0.000559, mean_q: 0.577764\n",
      " 1321/2000: episode: 12, duration: 7.331s, episode steps: 115, steps per second: 16, episode reward: 0.571, mean reward: 0.005 [-0.001, 0.012], mean action: 0.069 [-1.186, 1.174], mean observation: 0.109 [-10.283, 17.870], loss: 0.000110, mean_squared_error: 0.000221, mean_q: 0.581635\n",
      " 1431/2000: episode: 13, duration: 6.408s, episode steps: 110, steps per second: 17, episode reward: 0.605, mean reward: 0.005 [-0.001, 0.013], mean action: 0.040 [-1.203, 1.164], mean observation: 0.099 [-10.320, 17.888], loss: 0.000135, mean_squared_error: 0.000269, mean_q: 0.582516\n",
      " 1545/2000: episode: 14, duration: 6.768s, episode steps: 114, steps per second: 17, episode reward: 0.626, mean reward: 0.005 [-0.001, 0.013], mean action: 0.016 [-1.157, 1.096], mean observation: 0.104 [-10.322, 17.849], loss: 0.000059, mean_squared_error: 0.000118, mean_q: 0.581018\n",
      " 1659/2000: episode: 15, duration: 6.788s, episode steps: 114, steps per second: 17, episode reward: 0.573, mean reward: 0.005 [-0.002, 0.013], mean action: 0.062 [-1.138, 1.210], mean observation: 0.101 [-10.384, 17.873], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.565871\n",
      " 1774/2000: episode: 16, duration: 7.738s, episode steps: 115, steps per second: 15, episode reward: 0.495, mean reward: 0.004 [-0.002, 0.014], mean action: 0.066 [-1.121, 1.120], mean observation: 0.094 [-10.308, 17.959], loss: 0.000164, mean_squared_error: 0.000327, mean_q: 0.570974\n",
      " 1891/2000: episode: 17, duration: 8.542s, episode steps: 117, steps per second: 14, episode reward: 0.431, mean reward: 0.004 [-0.002, 0.013], mean action: 0.129 [-1.132, 1.172], mean observation: 0.089 [-10.479, 17.899], loss: 0.000198, mean_squared_error: 0.000396, mean_q: 0.584526\n",
      "done, took 112.217 seconds\n",
      "\n",
      "\n",
      "iteration: 96\n",
      "Training for 2000 steps ...\n",
      "  114/2000: episode: 1, duration: 5.874s, episode steps: 114, steps per second: 19, episode reward: 0.658, mean reward: 0.006 [-0.003, 0.014], mean action: 0.127 [-1.179, 1.103], mean observation: 0.096 [-10.351, 17.968], loss: --, mean_squared_error: --, mean_q: --\n",
      "  229/2000: episode: 2, duration: 6.072s, episode steps: 115, steps per second: 19, episode reward: 0.641, mean reward: 0.006 [-0.003, 0.014], mean action: 0.147 [-1.199, 1.162], mean observation: 0.096 [-10.302, 18.116], loss: --, mean_squared_error: --, mean_q: --\n",
      "  341/2000: episode: 3, duration: 5.633s, episode steps: 112, steps per second: 20, episode reward: 0.662, mean reward: 0.006 [-0.003, 0.014], mean action: 0.141 [-1.212, 1.175], mean observation: 0.094 [-10.324, 17.917], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 4, duration: 6.667s, episode steps: 118, steps per second: 18, episode reward: 0.543, mean reward: 0.005 [-0.003, 0.014], mean action: 0.155 [-1.135, 1.163], mean observation: 0.093 [-10.300, 17.848], loss: --, mean_squared_error: --, mean_q: --\n",
      "  572/2000: episode: 5, duration: 5.698s, episode steps: 113, steps per second: 20, episode reward: 0.659, mean reward: 0.006 [-0.003, 0.014], mean action: 0.152 [-1.117, 1.160], mean observation: 0.095 [-10.329, 17.975], loss: --, mean_squared_error: --, mean_q: --\n",
      "  687/2000: episode: 6, duration: 5.972s, episode steps: 115, steps per second: 19, episode reward: 0.606, mean reward: 0.005 [-0.003, 0.014], mean action: 0.153 [-1.186, 1.195], mean observation: 0.097 [-10.340, 18.025], loss: --, mean_squared_error: --, mean_q: --\n",
      "  802/2000: episode: 7, duration: 5.892s, episode steps: 115, steps per second: 20, episode reward: 0.630, mean reward: 0.005 [-0.003, 0.014], mean action: 0.151 [-1.139, 1.118], mean observation: 0.097 [-10.354, 18.000], loss: --, mean_squared_error: --, mean_q: --\n",
      "  919/2000: episode: 8, duration: 6.941s, episode steps: 117, steps per second: 17, episode reward: 0.496, mean reward: 0.004 [-0.003, 0.014], mean action: 0.133 [-1.148, 1.181], mean observation: 0.093 [-10.431, 17.938], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1033/2000: episode: 9, duration: 6.085s, episode steps: 114, steps per second: 19, episode reward: 0.641, mean reward: 0.006 [-0.003, 0.014], mean action: 0.153 [-1.132, 1.184], mean observation: 0.096 [-10.402, 18.072], loss: 0.000672, mean_squared_error: 0.001343, mean_q: 0.580836\n",
      " 1145/2000: episode: 10, duration: 6.692s, episode steps: 112, steps per second: 17, episode reward: 0.655, mean reward: 0.006 [-0.002, 0.013], mean action: 0.140 [-1.189, 1.147], mean observation: 0.096 [-15.384, 17.959], loss: 0.000210, mean_squared_error: 0.000421, mean_q: 0.561935\n",
      " 1258/2000: episode: 11, duration: 7.067s, episode steps: 113, steps per second: 16, episode reward: 0.680, mean reward: 0.006 [-0.001, 0.013], mean action: 0.099 [-1.228, 1.193], mean observation: 0.093 [-38.131, 17.908], loss: 0.000057, mean_squared_error: 0.000113, mean_q: 0.569161\n",
      " 1383/2000: episode: 12, duration: 9.294s, episode steps: 125, steps per second: 13, episode reward: 0.527, mean reward: 0.004 [-0.001, 0.013], mean action: 0.070 [-1.156, 1.175], mean observation: 0.106 [-52.902, 17.787], loss: 0.000246, mean_squared_error: 0.000493, mean_q: 0.570614\n",
      " 1504/2000: episode: 13, duration: 10.125s, episode steps: 121, steps per second: 12, episode reward: 0.413, mean reward: 0.003 [-0.001, 0.013], mean action: 0.069 [-1.186, 1.172], mean observation: 0.100 [-53.648, 18.046], loss: 0.000318, mean_squared_error: 0.000637, mean_q: 0.576470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1610/2000: episode: 14, duration: 6.582s, episode steps: 106, steps per second: 16, episode reward: 0.573, mean reward: 0.005 [-0.001, 0.013], mean action: 0.105 [-1.215, 1.200], mean observation: 0.081 [-45.343, 17.955], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.575718\n",
      " 1732/2000: episode: 15, duration: 10.215s, episode steps: 122, steps per second: 12, episode reward: 0.394, mean reward: 0.003 [-0.001, 0.013], mean action: 0.092 [-1.146, 1.235], mean observation: 0.098 [-42.069, 17.856], loss: 0.000198, mean_squared_error: 0.000396, mean_q: 0.574846\n",
      " 1845/2000: episode: 16, duration: 6.952s, episode steps: 113, steps per second: 16, episode reward: 0.691, mean reward: 0.006 [-0.001, 0.013], mean action: 0.107 [-1.208, 1.170], mean observation: 0.089 [-14.719, 17.841], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.581787\n",
      " 1955/2000: episode: 17, duration: 6.161s, episode steps: 110, steps per second: 18, episode reward: 0.700, mean reward: 0.006 [-0.001, 0.013], mean action: 0.255 [-1.101, 1.227], mean observation: 0.082 [-19.193, 17.975], loss: 0.000394, mean_squared_error: 0.000788, mean_q: 0.572862\n",
      "done, took 121.093 seconds\n",
      "\n",
      "\n",
      "iteration: 97\n",
      "Training for 2000 steps ...\n",
      "  109/2000: episode: 1, duration: 5.816s, episode steps: 109, steps per second: 19, episode reward: 0.669, mean reward: 0.006 [-0.001, 0.013], mean action: 0.205 [-1.183, 1.263], mean observation: 0.080 [-44.135, 18.039], loss: --, mean_squared_error: --, mean_q: --\n",
      "  222/2000: episode: 2, duration: 5.712s, episode steps: 113, steps per second: 20, episode reward: 0.742, mean reward: 0.007 [-0.001, 0.013], mean action: 0.171 [-1.181, 1.183], mean observation: 0.088 [-50.377, 18.039], loss: --, mean_squared_error: --, mean_q: --\n",
      "  339/2000: episode: 3, duration: 6.084s, episode steps: 117, steps per second: 19, episode reward: 0.738, mean reward: 0.006 [-0.001, 0.013], mean action: 0.169 [-1.231, 1.160], mean observation: 0.090 [-37.691, 17.919], loss: --, mean_squared_error: --, mean_q: --\n",
      "  457/2000: episode: 4, duration: 6.176s, episode steps: 118, steps per second: 19, episode reward: 0.755, mean reward: 0.006 [-0.001, 0.013], mean action: 0.201 [-1.173, 1.161], mean observation: 0.098 [-12.539, 17.882], loss: --, mean_squared_error: --, mean_q: --\n",
      "  568/2000: episode: 5, duration: 5.663s, episode steps: 111, steps per second: 20, episode reward: 0.724, mean reward: 0.007 [-0.001, 0.013], mean action: 0.218 [-1.099, 1.250], mean observation: 0.085 [-49.826, 17.922], loss: --, mean_squared_error: --, mean_q: --\n",
      "  679/2000: episode: 6, duration: 5.670s, episode steps: 111, steps per second: 20, episode reward: 0.720, mean reward: 0.006 [-0.001, 0.013], mean action: 0.203 [-1.153, 1.263], mean observation: 0.086 [-49.731, 18.021], loss: --, mean_squared_error: --, mean_q: --\n",
      "  790/2000: episode: 7, duration: 5.736s, episode steps: 111, steps per second: 19, episode reward: 0.716, mean reward: 0.006 [-0.001, 0.013], mean action: 0.208 [-1.180, 1.261], mean observation: 0.085 [-47.444, 17.973], loss: --, mean_squared_error: --, mean_q: --\n",
      "  902/2000: episode: 8, duration: 5.711s, episode steps: 112, steps per second: 20, episode reward: 0.719, mean reward: 0.006 [-0.001, 0.013], mean action: 0.190 [-1.188, 1.220], mean observation: 0.085 [-46.684, 17.870], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1015/2000: episode: 9, duration: 5.787s, episode steps: 113, steps per second: 20, episode reward: 0.738, mean reward: 0.007 [-0.001, 0.013], mean action: 0.179 [-1.201, 1.125], mean observation: 0.087 [-50.006, 17.987], loss: 0.000501, mean_squared_error: 0.001001, mean_q: 0.534417\n",
      " 1134/2000: episode: 10, duration: 6.936s, episode steps: 119, steps per second: 17, episode reward: 0.759, mean reward: 0.006 [-0.001, 0.013], mean action: 0.206 [-1.099, 1.174], mean observation: 0.095 [-10.114, 17.801], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.568945\n",
      " 1250/2000: episode: 11, duration: 6.251s, episode steps: 116, steps per second: 19, episode reward: 0.717, mean reward: 0.006 [-0.001, 0.013], mean action: 0.259 [-1.169, 1.140], mean observation: 0.086 [-10.117, 17.999], loss: 0.000169, mean_squared_error: 0.000337, mean_q: 0.566665\n",
      " 1358/2000: episode: 12, duration: 5.991s, episode steps: 108, steps per second: 18, episode reward: 0.731, mean reward: 0.007 [-0.001, 0.013], mean action: 0.240 [-1.180, 1.218], mean observation: 0.087 [-9.964, 18.158], loss: 0.000291, mean_squared_error: 0.000582, mean_q: 0.572471\n",
      " 1466/2000: episode: 13, duration: 6.032s, episode steps: 108, steps per second: 18, episode reward: 0.716, mean reward: 0.007 [-0.000, 0.013], mean action: 0.259 [-1.218, 1.242], mean observation: 0.086 [-19.671, 18.003], loss: 0.000088, mean_squared_error: 0.000177, mean_q: 0.566974\n",
      " 1574/2000: episode: 14, duration: 6.163s, episode steps: 108, steps per second: 18, episode reward: 0.709, mean reward: 0.007 [-0.001, 0.013], mean action: 0.239 [-1.168, 1.250], mean observation: 0.084 [-41.069, 18.127], loss: 0.000067, mean_squared_error: 0.000135, mean_q: 0.567235\n",
      " 1683/2000: episode: 15, duration: 6.011s, episode steps: 109, steps per second: 18, episode reward: 0.725, mean reward: 0.007 [-0.001, 0.013], mean action: 0.201 [-1.108, 1.203], mean observation: 0.087 [-10.058, 18.314], loss: 0.000297, mean_squared_error: 0.000595, mean_q: 0.577291\n",
      " 1791/2000: episode: 16, duration: 5.857s, episode steps: 108, steps per second: 18, episode reward: 0.713, mean reward: 0.007 [-0.001, 0.013], mean action: 0.214 [-1.228, 1.218], mean observation: 0.084 [-10.128, 17.970], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.570461\n",
      " 1902/2000: episode: 17, duration: 5.943s, episode steps: 111, steps per second: 19, episode reward: 0.731, mean reward: 0.007 [-0.001, 0.013], mean action: 0.199 [-1.107, 1.132], mean observation: 0.085 [-10.093, 17.873], loss: 0.000249, mean_squared_error: 0.000497, mean_q: 0.568838\n",
      "done, took 106.866 seconds\n",
      "\n",
      "\n",
      "iteration: 98\n",
      "Training for 2000 steps ...\n",
      "  110/2000: episode: 1, duration: 5.269s, episode steps: 110, steps per second: 21, episode reward: 0.709, mean reward: 0.006 [-0.001, 0.013], mean action: 0.243 [-1.170, 1.250], mean observation: 0.084 [-9.976, 18.063], loss: --, mean_squared_error: --, mean_q: --\n",
      "  221/2000: episode: 2, duration: 5.137s, episode steps: 111, steps per second: 22, episode reward: 0.720, mean reward: 0.006 [-0.001, 0.013], mean action: 0.247 [-1.145, 1.168], mean observation: 0.084 [-10.061, 18.052], loss: --, mean_squared_error: --, mean_q: --\n",
      "  332/2000: episode: 3, duration: 5.223s, episode steps: 111, steps per second: 21, episode reward: 0.714, mean reward: 0.006 [-0.001, 0.013], mean action: 0.274 [-1.138, 1.223], mean observation: 0.084 [-9.988, 18.110], loss: --, mean_squared_error: --, mean_q: --\n",
      "  442/2000: episode: 4, duration: 5.194s, episode steps: 110, steps per second: 21, episode reward: 0.704, mean reward: 0.006 [-0.001, 0.013], mean action: 0.239 [-1.124, 1.151], mean observation: 0.083 [-10.014, 18.051], loss: --, mean_squared_error: --, mean_q: --\n",
      "  552/2000: episode: 5, duration: 4.967s, episode steps: 110, steps per second: 22, episode reward: 0.707, mean reward: 0.006 [-0.001, 0.013], mean action: 0.263 [-1.083, 1.221], mean observation: 0.083 [-10.017, 18.029], loss: --, mean_squared_error: --, mean_q: --\n",
      "  663/2000: episode: 6, duration: 5.097s, episode steps: 111, steps per second: 22, episode reward: 0.715, mean reward: 0.006 [-0.001, 0.013], mean action: 0.236 [-1.150, 1.257], mean observation: 0.081 [-17.121, 18.092], loss: --, mean_squared_error: --, mean_q: --\n",
      "  773/2000: episode: 7, duration: 5.162s, episode steps: 110, steps per second: 21, episode reward: 0.707, mean reward: 0.006 [-0.001, 0.013], mean action: 0.258 [-1.154, 1.194], mean observation: 0.081 [-15.789, 17.962], loss: --, mean_squared_error: --, mean_q: --\n",
      "  884/2000: episode: 8, duration: 5.172s, episode steps: 111, steps per second: 21, episode reward: 0.716, mean reward: 0.006 [-0.001, 0.013], mean action: 0.263 [-1.113, 1.228], mean observation: 0.084 [-10.092, 17.829], loss: --, mean_squared_error: --, mean_q: --\n",
      "  994/2000: episode: 9, duration: 4.938s, episode steps: 110, steps per second: 22, episode reward: 0.709, mean reward: 0.006 [-0.001, 0.013], mean action: 0.211 [-1.233, 1.115], mean observation: 0.085 [-10.006, 18.364], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1103/2000: episode: 10, duration: 6.007s, episode steps: 109, steps per second: 18, episode reward: 0.705, mean reward: 0.006 [-0.001, 0.013], mean action: 0.224 [-1.122, 1.256], mean observation: 0.083 [-9.959, 17.988], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.569630\n",
      " 1212/2000: episode: 11, duration: 6.106s, episode steps: 109, steps per second: 18, episode reward: 0.721, mean reward: 0.007 [-0.002, 0.013], mean action: 0.229 [-1.105, 1.145], mean observation: 0.085 [-10.033, 18.109], loss: 0.000354, mean_squared_error: 0.000708, mean_q: 0.562056\n",
      " 1324/2000: episode: 12, duration: 6.087s, episode steps: 112, steps per second: 18, episode reward: 0.736, mean reward: 0.007 [-0.000, 0.013], mean action: 0.227 [-1.152, 1.230], mean observation: 0.088 [-10.172, 17.897], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.565228\n",
      " 1438/2000: episode: 13, duration: 6.185s, episode steps: 114, steps per second: 18, episode reward: 0.763, mean reward: 0.007 [0.000, 0.013], mean action: 0.210 [-1.174, 1.140], mean observation: 0.093 [-9.912, 18.038], loss: 0.000310, mean_squared_error: 0.000620, mean_q: 0.572007\n",
      " 1545/2000: episode: 14, duration: 5.906s, episode steps: 107, steps per second: 18, episode reward: 0.726, mean reward: 0.007 [0.000, 0.013], mean action: 0.246 [-1.112, 1.265], mean observation: 0.088 [-29.525, 18.024], loss: 0.000115, mean_squared_error: 0.000231, mean_q: 0.569127\n",
      " 1656/2000: episode: 15, duration: 6.202s, episode steps: 111, steps per second: 18, episode reward: 0.730, mean reward: 0.007 [0.000, 0.013], mean action: 0.213 [-1.305, 1.281], mean observation: 0.082 [-34.719, 18.065], loss: 0.000067, mean_squared_error: 0.000133, mean_q: 0.561420\n",
      " 1764/2000: episode: 16, duration: 6.305s, episode steps: 108, steps per second: 17, episode reward: 0.701, mean reward: 0.006 [-0.000, 0.013], mean action: 0.228 [-1.238, 1.219], mean observation: 0.081 [-22.624, 17.809], loss: 0.000380, mean_squared_error: 0.000760, mean_q: 0.558112\n",
      " 1877/2000: episode: 17, duration: 6.478s, episode steps: 113, steps per second: 17, episode reward: 0.770, mean reward: 0.007 [-0.000, 0.013], mean action: 0.325 [-1.123, 1.165], mean observation: 0.093 [-10.173, 18.101], loss: 0.000303, mean_squared_error: 0.000605, mean_q: 0.562591\n",
      " 1989/2000: episode: 18, duration: 6.290s, episode steps: 112, steps per second: 18, episode reward: 0.756, mean reward: 0.007 [0.000, 0.013], mean action: 0.263 [-1.247, 1.189], mean observation: 0.089 [-9.900, 18.050], loss: 0.000348, mean_squared_error: 0.000695, mean_q: 0.563930\n",
      "done, took 102.628 seconds\n",
      "\n",
      "\n",
      "iteration: 99\n",
      "Training for 2000 steps ...\n",
      "  111/2000: episode: 1, duration: 4.983s, episode steps: 111, steps per second: 22, episode reward: 0.746, mean reward: 0.007 [0.000, 0.013], mean action: 0.212 [-1.261, 1.162], mean observation: 0.090 [-10.108, 17.909], loss: --, mean_squared_error: --, mean_q: --\n",
      "  222/2000: episode: 2, duration: 4.949s, episode steps: 111, steps per second: 22, episode reward: 0.741, mean reward: 0.007 [0.000, 0.013], mean action: 0.219 [-1.208, 1.234], mean observation: 0.091 [-10.134, 17.932], loss: --, mean_squared_error: --, mean_q: --\n",
      "  334/2000: episode: 3, duration: 4.895s, episode steps: 112, steps per second: 23, episode reward: 0.751, mean reward: 0.007 [0.000, 0.013], mean action: 0.236 [-1.073, 1.191], mean observation: 0.088 [-10.103, 17.875], loss: --, mean_squared_error: --, mean_q: --\n",
      "  445/2000: episode: 4, duration: 5.033s, episode steps: 111, steps per second: 22, episode reward: 0.745, mean reward: 0.007 [0.000, 0.013], mean action: 0.236 [-1.186, 1.190], mean observation: 0.087 [-15.039, 18.158], loss: --, mean_squared_error: --, mean_q: --\n",
      "  556/2000: episode: 5, duration: 4.976s, episode steps: 111, steps per second: 22, episode reward: 0.746, mean reward: 0.007 [0.000, 0.013], mean action: 0.242 [-1.112, 1.175], mean observation: 0.088 [-10.025, 17.806], loss: --, mean_squared_error: --, mean_q: --\n",
      "  667/2000: episode: 6, duration: 4.989s, episode steps: 111, steps per second: 22, episode reward: 0.744, mean reward: 0.007 [0.000, 0.013], mean action: 0.208 [-1.214, 1.209], mean observation: 0.086 [-16.694, 17.892], loss: --, mean_squared_error: --, mean_q: --\n",
      "  778/2000: episode: 7, duration: 4.970s, episode steps: 111, steps per second: 22, episode reward: 0.740, mean reward: 0.007 [0.000, 0.013], mean action: 0.213 [-1.179, 1.140], mean observation: 0.089 [-9.941, 17.980], loss: --, mean_squared_error: --, mean_q: --\n",
      "  888/2000: episode: 8, duration: 4.959s, episode steps: 110, steps per second: 22, episode reward: 0.733, mean reward: 0.007 [0.000, 0.013], mean action: 0.225 [-1.235, 1.201], mean observation: 0.088 [-10.068, 17.786], loss: --, mean_squared_error: --, mean_q: --\n",
      "  999/2000: episode: 9, duration: 5.041s, episode steps: 111, steps per second: 22, episode reward: 0.740, mean reward: 0.007 [0.000, 0.013], mean action: 0.244 [-1.142, 1.258], mean observation: 0.089 [-9.908, 18.034], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1108/2000: episode: 10, duration: 5.947s, episode steps: 109, steps per second: 18, episode reward: 0.711, mean reward: 0.007 [0.000, 0.013], mean action: 0.174 [-1.104, 1.149], mean observation: 0.085 [-9.903, 18.028], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.562487\n",
      " 1217/2000: episode: 11, duration: 5.939s, episode steps: 109, steps per second: 18, episode reward: 0.719, mean reward: 0.007 [0.000, 0.013], mean action: 0.147 [-1.216, 1.167], mean observation: 0.088 [-10.384, 17.540], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.562368\n",
      " 1326/2000: episode: 12, duration: 5.899s, episode steps: 109, steps per second: 18, episode reward: 0.727, mean reward: 0.007 [0.000, 0.013], mean action: 0.195 [-1.162, 1.149], mean observation: 0.086 [-28.902, 17.902], loss: 0.000182, mean_squared_error: 0.000365, mean_q: 0.562255\n",
      " 1435/2000: episode: 13, duration: 5.750s, episode steps: 109, steps per second: 19, episode reward: 0.723, mean reward: 0.007 [0.000, 0.013], mean action: 0.276 [-1.153, 1.273], mean observation: 0.086 [-32.829, 17.963], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.567433\n",
      " 1546/2000: episode: 14, duration: 6.113s, episode steps: 111, steps per second: 18, episode reward: 0.706, mean reward: 0.006 [-0.000, 0.013], mean action: 0.239 [-1.186, 1.197], mean observation: 0.082 [-38.795, 18.198], loss: 0.000134, mean_squared_error: 0.000269, mean_q: 0.560041\n",
      " 1658/2000: episode: 15, duration: 6.117s, episode steps: 112, steps per second: 18, episode reward: 0.705, mean reward: 0.006 [-0.000, 0.013], mean action: 0.227 [-1.204, 1.272], mean observation: 0.084 [-14.550, 17.904], loss: 0.000516, mean_squared_error: 0.001031, mean_q: 0.559565\n",
      " 1768/2000: episode: 16, duration: 5.830s, episode steps: 110, steps per second: 19, episode reward: 0.709, mean reward: 0.006 [-0.001, 0.013], mean action: 0.192 [-1.181, 1.212], mean observation: 0.084 [-12.274, 17.917], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.558390\n",
      " 1876/2000: episode: 17, duration: 5.753s, episode steps: 108, steps per second: 19, episode reward: 0.718, mean reward: 0.007 [-0.000, 0.014], mean action: 0.234 [-1.113, 1.154], mean observation: 0.087 [-10.823, 17.856], loss: 0.000064, mean_squared_error: 0.000129, mean_q: 0.561219\n",
      " 1987/2000: episode: 18, duration: 5.961s, episode steps: 111, steps per second: 19, episode reward: 0.739, mean reward: 0.007 [-0.000, 0.013], mean action: 0.214 [-1.215, 1.267], mean observation: 0.094 [-15.487, 18.057], loss: 0.000344, mean_squared_error: 0.000688, mean_q: 0.564350\n",
      "done, took 99.127 seconds\n",
      "\n",
      "\n",
      "iteration: 100\n",
      "Training for 2000 steps ...\n",
      "  109/2000: episode: 1, duration: 4.779s, episode steps: 109, steps per second: 23, episode reward: 0.722, mean reward: 0.007 [-0.001, 0.014], mean action: 0.242 [-1.104, 1.233], mean observation: 0.088 [-12.716, 17.937], loss: --, mean_squared_error: --, mean_q: --\n",
      "  218/2000: episode: 2, duration: 4.648s, episode steps: 109, steps per second: 23, episode reward: 0.723, mean reward: 0.007 [-0.001, 0.014], mean action: 0.245 [-1.220, 1.184], mean observation: 0.088 [-13.050, 17.774], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  326/2000: episode: 3, duration: 4.622s, episode steps: 108, steps per second: 23, episode reward: 0.715, mean reward: 0.007 [-0.001, 0.014], mean action: 0.231 [-1.114, 1.169], mean observation: 0.086 [-17.076, 18.155], loss: --, mean_squared_error: --, mean_q: --\n",
      "  435/2000: episode: 4, duration: 4.771s, episode steps: 109, steps per second: 23, episode reward: 0.721, mean reward: 0.007 [-0.001, 0.014], mean action: 0.225 [-1.147, 1.155], mean observation: 0.088 [-11.967, 17.800], loss: --, mean_squared_error: --, mean_q: --\n",
      "  544/2000: episode: 5, duration: 4.802s, episode steps: 109, steps per second: 23, episode reward: 0.723, mean reward: 0.007 [-0.001, 0.014], mean action: 0.260 [-1.149, 1.312], mean observation: 0.089 [-11.733, 17.866], loss: --, mean_squared_error: --, mean_q: --\n",
      "  652/2000: episode: 6, duration: 4.660s, episode steps: 108, steps per second: 23, episode reward: 0.716, mean reward: 0.007 [-0.001, 0.014], mean action: 0.251 [-1.074, 1.180], mean observation: 0.087 [-12.279, 17.742], loss: --, mean_squared_error: --, mean_q: --\n",
      "  762/2000: episode: 7, duration: 4.770s, episode steps: 110, steps per second: 23, episode reward: 0.743, mean reward: 0.007 [-0.001, 0.014], mean action: 0.246 [-1.198, 1.160], mean observation: 0.089 [-14.232, 17.861], loss: --, mean_squared_error: --, mean_q: --\n",
      "  870/2000: episode: 8, duration: 4.700s, episode steps: 108, steps per second: 23, episode reward: 0.716, mean reward: 0.007 [-0.001, 0.014], mean action: 0.243 [-1.169, 1.156], mean observation: 0.087 [-10.527, 17.827], loss: --, mean_squared_error: --, mean_q: --\n",
      "  979/2000: episode: 9, duration: 4.808s, episode steps: 109, steps per second: 23, episode reward: 0.721, mean reward: 0.007 [-0.001, 0.014], mean action: 0.240 [-1.139, 1.181], mean observation: 0.087 [-17.134, 18.103], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1087/2000: episode: 10, duration: 5.587s, episode steps: 108, steps per second: 19, episode reward: 0.713, mean reward: 0.007 [-0.001, 0.014], mean action: 0.229 [-1.211, 1.175], mean observation: 0.087 [-11.599, 17.802], loss: 0.000171, mean_squared_error: 0.000342, mean_q: 0.555427\n",
      " 1196/2000: episode: 11, duration: 5.765s, episode steps: 109, steps per second: 19, episode reward: 0.734, mean reward: 0.007 [-0.000, 0.014], mean action: 0.259 [-1.284, 1.287], mean observation: 0.089 [-12.824, 17.766], loss: 0.000195, mean_squared_error: 0.000391, mean_q: 0.553835\n",
      " 1305/2000: episode: 12, duration: 5.722s, episode steps: 109, steps per second: 19, episode reward: 0.712, mean reward: 0.007 [-0.001, 0.014], mean action: 0.286 [-1.228, 1.147], mean observation: 0.086 [-12.387, 17.860], loss: 0.000058, mean_squared_error: 0.000115, mean_q: 0.556647\n",
      " 1416/2000: episode: 13, duration: 5.789s, episode steps: 111, steps per second: 19, episode reward: 0.710, mean reward: 0.006 [-0.002, 0.014], mean action: 0.279 [-1.150, 1.162], mean observation: 0.084 [-14.338, 18.063], loss: 0.000166, mean_squared_error: 0.000331, mean_q: 0.559521\n",
      " 1530/2000: episode: 14, duration: 5.953s, episode steps: 114, steps per second: 19, episode reward: 0.709, mean reward: 0.006 [-0.001, 0.014], mean action: 0.311 [-1.181, 1.138], mean observation: 0.083 [-16.991, 18.249], loss: 0.000142, mean_squared_error: 0.000283, mean_q: 0.551653\n",
      " 1643/2000: episode: 15, duration: 6.134s, episode steps: 113, steps per second: 18, episode reward: 0.709, mean reward: 0.006 [-0.002, 0.014], mean action: 0.271 [-1.172, 1.139], mean observation: 0.083 [-16.823, 18.077], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.556462\n",
      " 1756/2000: episode: 16, duration: 6.257s, episode steps: 113, steps per second: 18, episode reward: 0.720, mean reward: 0.006 [-0.002, 0.014], mean action: 0.273 [-1.147, 1.209], mean observation: 0.085 [-15.862, 18.415], loss: 0.000158, mean_squared_error: 0.000315, mean_q: 0.557849\n",
      " 1870/2000: episode: 17, duration: 6.345s, episode steps: 114, steps per second: 18, episode reward: 0.702, mean reward: 0.006 [-0.001, 0.014], mean action: 0.347 [-1.122, 1.221], mean observation: 0.083 [-16.991, 18.106], loss: 0.000142, mean_squared_error: 0.000285, mean_q: 0.553226\n",
      " 1989/2000: episode: 18, duration: 6.455s, episode steps: 119, steps per second: 18, episode reward: 0.706, mean reward: 0.006 [-0.001, 0.014], mean action: 0.340 [-1.142, 1.227], mean observation: 0.083 [-15.493, 18.408], loss: 0.000115, mean_squared_error: 0.000229, mean_q: 0.557027\n",
      "done, took 97.489 seconds\n",
      "\n",
      "\n",
      "iteration: 101\n",
      "Training for 2000 steps ...\n",
      "  114/2000: episode: 1, duration: 5.031s, episode steps: 114, steps per second: 23, episode reward: 0.695, mean reward: 0.006 [-0.001, 0.014], mean action: 0.351 [-1.143, 1.290], mean observation: 0.080 [-10.625, 17.710], loss: --, mean_squared_error: --, mean_q: --\n",
      "  229/2000: episode: 2, duration: 5.043s, episode steps: 115, steps per second: 23, episode reward: 0.700, mean reward: 0.006 [-0.001, 0.014], mean action: 0.338 [-1.167, 1.224], mean observation: 0.080 [-12.775, 17.954], loss: --, mean_squared_error: --, mean_q: --\n",
      "  344/2000: episode: 3, duration: 4.997s, episode steps: 115, steps per second: 23, episode reward: 0.703, mean reward: 0.006 [-0.001, 0.013], mean action: 0.351 [-1.148, 1.290], mean observation: 0.080 [-16.521, 18.090], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 4, duration: 5.055s, episode steps: 115, steps per second: 23, episode reward: 0.698, mean reward: 0.006 [-0.001, 0.014], mean action: 0.348 [-1.068, 1.131], mean observation: 0.080 [-14.426, 18.180], loss: --, mean_squared_error: --, mean_q: --\n",
      "  574/2000: episode: 5, duration: 5.024s, episode steps: 115, steps per second: 23, episode reward: 0.701, mean reward: 0.006 [-0.001, 0.013], mean action: 0.349 [-1.131, 1.140], mean observation: 0.081 [-10.427, 17.857], loss: --, mean_squared_error: --, mean_q: --\n",
      "  689/2000: episode: 6, duration: 5.050s, episode steps: 115, steps per second: 23, episode reward: 0.702, mean reward: 0.006 [-0.001, 0.013], mean action: 0.303 [-1.169, 1.155], mean observation: 0.081 [-13.206, 17.976], loss: --, mean_squared_error: --, mean_q: --\n",
      "  803/2000: episode: 7, duration: 5.010s, episode steps: 114, steps per second: 23, episode reward: 0.694, mean reward: 0.006 [-0.001, 0.013], mean action: 0.316 [-1.117, 1.132], mean observation: 0.080 [-14.289, 17.925], loss: --, mean_squared_error: --, mean_q: --\n",
      "  918/2000: episode: 8, duration: 4.975s, episode steps: 115, steps per second: 23, episode reward: 0.694, mean reward: 0.006 [-0.001, 0.013], mean action: 0.330 [-1.088, 1.100], mean observation: 0.081 [-12.612, 17.974], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1033/2000: episode: 9, duration: 5.320s, episode steps: 115, steps per second: 22, episode reward: 0.703, mean reward: 0.006 [-0.001, 0.013], mean action: 0.355 [-1.082, 1.218], mean observation: 0.080 [-13.009, 17.961], loss: 0.000055, mean_squared_error: 0.000110, mean_q: 0.547361\n",
      " 1146/2000: episode: 10, duration: 6.142s, episode steps: 113, steps per second: 18, episode reward: 0.700, mean reward: 0.006 [-0.001, 0.013], mean action: 0.342 [-1.165, 1.164], mean observation: 0.082 [-17.693, 17.877], loss: 0.000085, mean_squared_error: 0.000169, mean_q: 0.561651\n",
      " 1267/2000: episode: 11, duration: 6.630s, episode steps: 121, steps per second: 18, episode reward: 0.699, mean reward: 0.006 [-0.001, 0.013], mean action: 0.277 [-1.163, 1.202], mean observation: 0.084 [-13.303, 17.861], loss: 0.000288, mean_squared_error: 0.000576, mean_q: 0.559551\n",
      " 1389/2000: episode: 12, duration: 6.614s, episode steps: 122, steps per second: 18, episode reward: 0.697, mean reward: 0.006 [-0.001, 0.013], mean action: 0.297 [-1.333, 1.264], mean observation: 0.081 [-13.241, 17.914], loss: 0.000212, mean_squared_error: 0.000425, mean_q: 0.542886\n",
      " 1520/2000: episode: 13, duration: 7.328s, episode steps: 131, steps per second: 18, episode reward: 0.692, mean reward: 0.005 [-0.001, 0.013], mean action: 0.172 [-1.267, 1.264], mean observation: 0.083 [-15.697, 17.981], loss: 0.000064, mean_squared_error: 0.000129, mean_q: 0.548579\n",
      " 1638/2000: episode: 14, duration: 6.577s, episode steps: 118, steps per second: 18, episode reward: 0.691, mean reward: 0.006 [-0.001, 0.013], mean action: 0.222 [-1.084, 1.303], mean observation: 0.079 [-16.170, 18.140], loss: 0.000058, mean_squared_error: 0.000117, mean_q: 0.552009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1761/2000: episode: 15, duration: 6.944s, episode steps: 123, steps per second: 18, episode reward: 0.693, mean reward: 0.006 [-0.000, 0.014], mean action: 0.193 [-1.165, 1.182], mean observation: 0.081 [-16.097, 18.443], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.544518\n",
      " 1876/2000: episode: 16, duration: 6.106s, episode steps: 115, steps per second: 19, episode reward: 0.689, mean reward: 0.006 [-0.001, 0.014], mean action: 0.169 [-1.201, 1.223], mean observation: 0.081 [-13.954, 18.030], loss: 0.000126, mean_squared_error: 0.000252, mean_q: 0.551589\n",
      " 1993/2000: episode: 17, duration: 6.166s, episode steps: 117, steps per second: 19, episode reward: 0.706, mean reward: 0.006 [-0.000, 0.013], mean action: 0.141 [-1.118, 1.167], mean observation: 0.084 [-17.755, 18.076], loss: 0.000065, mean_squared_error: 0.000131, mean_q: 0.556598\n",
      "done, took 98.583 seconds\n",
      "\n",
      "\n",
      "iteration: 102\n",
      "Training for 2000 steps ...\n",
      "  114/2000: episode: 1, duration: 5.001s, episode steps: 114, steps per second: 23, episode reward: 0.693, mean reward: 0.006 [-0.001, 0.013], mean action: 0.095 [-1.246, 1.196], mean observation: 0.084 [-10.373, 18.015], loss: --, mean_squared_error: --, mean_q: --\n",
      "  228/2000: episode: 2, duration: 5.008s, episode steps: 114, steps per second: 23, episode reward: 0.696, mean reward: 0.006 [-0.001, 0.013], mean action: 0.100 [-1.139, 1.186], mean observation: 0.083 [-10.476, 17.732], loss: --, mean_squared_error: --, mean_q: --\n",
      "  342/2000: episode: 3, duration: 5.041s, episode steps: 114, steps per second: 23, episode reward: 0.694, mean reward: 0.006 [-0.001, 0.013], mean action: 0.101 [-1.146, 1.331], mean observation: 0.084 [-11.858, 18.061], loss: --, mean_squared_error: --, mean_q: --\n",
      "  456/2000: episode: 4, duration: 4.949s, episode steps: 114, steps per second: 23, episode reward: 0.689, mean reward: 0.006 [-0.001, 0.013], mean action: 0.119 [-1.216, 1.217], mean observation: 0.084 [-10.413, 17.830], loss: --, mean_squared_error: --, mean_q: --\n",
      "  570/2000: episode: 5, duration: 5.006s, episode steps: 114, steps per second: 23, episode reward: 0.695, mean reward: 0.006 [-0.001, 0.013], mean action: 0.111 [-1.140, 1.173], mean observation: 0.084 [-10.500, 17.846], loss: --, mean_squared_error: --, mean_q: --\n",
      "  684/2000: episode: 6, duration: 4.927s, episode steps: 114, steps per second: 23, episode reward: 0.692, mean reward: 0.006 [-0.001, 0.013], mean action: 0.101 [-1.130, 1.165], mean observation: 0.084 [-10.386, 17.789], loss: --, mean_squared_error: --, mean_q: --\n",
      "  798/2000: episode: 7, duration: 5.018s, episode steps: 114, steps per second: 23, episode reward: 0.695, mean reward: 0.006 [-0.001, 0.013], mean action: 0.101 [-1.124, 1.231], mean observation: 0.084 [-10.307, 17.709], loss: --, mean_squared_error: --, mean_q: --\n",
      "  912/2000: episode: 8, duration: 4.982s, episode steps: 114, steps per second: 23, episode reward: 0.691, mean reward: 0.006 [-0.001, 0.013], mean action: 0.094 [-1.212, 1.108], mean observation: 0.084 [-10.383, 17.727], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1025/2000: episode: 9, duration: 5.220s, episode steps: 113, steps per second: 22, episode reward: 0.684, mean reward: 0.006 [-0.001, 0.013], mean action: 0.110 [-1.133, 1.176], mean observation: 0.083 [-10.224, 17.823], loss: 0.000328, mean_squared_error: 0.000656, mean_q: 0.567477\n",
      " 1140/2000: episode: 10, duration: 6.162s, episode steps: 115, steps per second: 19, episode reward: 0.698, mean reward: 0.006 [-0.001, 0.014], mean action: 0.022 [-1.130, 1.140], mean observation: 0.084 [-10.442, 17.781], loss: 0.000214, mean_squared_error: 0.000429, mean_q: 0.540508\n",
      " 1255/2000: episode: 11, duration: 6.314s, episode steps: 115, steps per second: 18, episode reward: 0.686, mean reward: 0.006 [-0.000, 0.013], mean action: 0.011 [-1.222, 1.215], mean observation: 0.084 [-10.274, 18.028], loss: 0.000074, mean_squared_error: 0.000148, mean_q: 0.546807\n",
      " 1368/2000: episode: 12, duration: 6.410s, episode steps: 113, steps per second: 18, episode reward: 0.663, mean reward: 0.006 [-0.001, 0.014], mean action: 0.036 [-1.162, 1.173], mean observation: 0.079 [-10.582, 18.037], loss: 0.000293, mean_squared_error: 0.000587, mean_q: 0.554654\n",
      " 1476/2000: episode: 13, duration: 6.140s, episode steps: 108, steps per second: 18, episode reward: 0.655, mean reward: 0.006 [-0.001, 0.014], mean action: 0.015 [-1.311, 1.233], mean observation: 0.080 [-14.396, 17.896], loss: 0.000206, mean_squared_error: 0.000412, mean_q: 0.553362\n",
      " 1579/2000: episode: 14, duration: 6.212s, episode steps: 103, steps per second: 17, episode reward: 0.662, mean reward: 0.006 [-0.001, 0.014], mean action: 0.096 [-1.144, 1.117], mean observation: 0.083 [-10.433, 17.810], loss: 0.000053, mean_squared_error: 0.000106, mean_q: 0.546774\n",
      " 1690/2000: episode: 15, duration: 6.641s, episode steps: 111, steps per second: 17, episode reward: 0.680, mean reward: 0.006 [0.000, 0.013], mean action: 0.122 [-1.194, 1.156], mean observation: 0.089 [-16.357, 18.212], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.550246\n",
      " 1812/2000: episode: 16, duration: 7.336s, episode steps: 122, steps per second: 17, episode reward: 0.721, mean reward: 0.006 [-0.000, 0.014], mean action: 0.125 [-1.168, 1.216], mean observation: 0.093 [-17.255, 18.083], loss: 0.000279, mean_squared_error: 0.000557, mean_q: 0.551957\n",
      " 1922/2000: episode: 17, duration: 6.628s, episode steps: 110, steps per second: 17, episode reward: 0.677, mean reward: 0.006 [-0.001, 0.013], mean action: 0.122 [-1.146, 1.259], mean observation: 0.083 [-10.351, 18.089], loss: 0.000452, mean_squared_error: 0.000904, mean_q: 0.537490\n",
      "done, took 101.672 seconds\n",
      "\n",
      "\n",
      "iteration: 103\n",
      "Training for 2000 steps ...\n",
      "  100/2000: episode: 1, duration: 5.528s, episode steps: 100, steps per second: 18, episode reward: 0.601, mean reward: 0.006 [-0.001, 0.013], mean action: 0.033 [-1.214, 1.217], mean observation: 0.080 [-36.639, 20.242], loss: --, mean_squared_error: --, mean_q: --\n",
      "  206/2000: episode: 2, duration: 5.707s, episode steps: 106, steps per second: 19, episode reward: 0.634, mean reward: 0.006 [-0.001, 0.014], mean action: 0.048 [-1.095, 1.171], mean observation: 0.078 [-40.255, 17.260], loss: --, mean_squared_error: --, mean_q: --\n",
      "  316/2000: episode: 3, duration: 5.793s, episode steps: 110, steps per second: 19, episode reward: 0.642, mean reward: 0.006 [-0.001, 0.014], mean action: 0.045 [-1.223, 1.177], mean observation: 0.077 [-39.838, 17.261], loss: --, mean_squared_error: --, mean_q: --\n",
      "  421/2000: episode: 4, duration: 5.607s, episode steps: 105, steps per second: 19, episode reward: 0.624, mean reward: 0.006 [-0.001, 0.013], mean action: 0.030 [-1.197, 1.152], mean observation: 0.069 [-40.597, 17.272], loss: --, mean_squared_error: --, mean_q: --\n",
      "  520/2000: episode: 5, duration: 5.456s, episode steps: 99, steps per second: 18, episode reward: 0.589, mean reward: 0.006 [-0.001, 0.013], mean action: 0.039 [-1.196, 1.134], mean observation: 0.072 [-40.283, 24.680], loss: --, mean_squared_error: --, mean_q: --\n",
      "  623/2000: episode: 6, duration: 5.730s, episode steps: 103, steps per second: 18, episode reward: 0.611, mean reward: 0.006 [-0.001, 0.013], mean action: 0.048 [-1.167, 1.141], mean observation: 0.074 [-41.391, 17.356], loss: --, mean_squared_error: --, mean_q: --\n",
      "  731/2000: episode: 7, duration: 5.629s, episode steps: 108, steps per second: 19, episode reward: 0.644, mean reward: 0.006 [-0.001, 0.014], mean action: 0.061 [-1.154, 1.197], mean observation: 0.083 [-28.041, 17.488], loss: --, mean_squared_error: --, mean_q: --\n",
      "  839/2000: episode: 8, duration: 5.735s, episode steps: 108, steps per second: 19, episode reward: 0.634, mean reward: 0.006 [-0.001, 0.014], mean action: 0.048 [-1.270, 1.191], mean observation: 0.083 [-23.256, 17.464], loss: --, mean_squared_error: --, mean_q: --\n",
      "  941/2000: episode: 9, duration: 5.595s, episode steps: 102, steps per second: 18, episode reward: 0.603, mean reward: 0.006 [-0.001, 0.013], mean action: 0.046 [-1.180, 1.238], mean observation: 0.071 [-41.229, 21.873], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1047/2000: episode: 10, duration: 6.044s, episode steps: 106, steps per second: 18, episode reward: 0.629, mean reward: 0.006 [-0.001, 0.013], mean action: 0.046 [-1.099, 1.113], mean observation: 0.075 [-37.465, 17.360], loss: 0.000505, mean_squared_error: 0.001010, mean_q: 0.533321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1163/2000: episode: 11, duration: 6.728s, episode steps: 116, steps per second: 17, episode reward: 0.669, mean reward: 0.006 [-0.001, 0.013], mean action: 0.007 [-1.102, 1.221], mean observation: 0.088 [-14.412, 17.434], loss: 0.000150, mean_squared_error: 0.000300, mean_q: 0.548491\n",
      " 1282/2000: episode: 12, duration: 7.219s, episode steps: 119, steps per second: 16, episode reward: 0.677, mean reward: 0.006 [-0.001, 0.014], mean action: 0.003 [-1.171, 1.251], mean observation: 0.089 [-14.975, 17.484], loss: 0.000314, mean_squared_error: 0.000627, mean_q: 0.554476\n",
      " 1377/2000: episode: 13, duration: 6.158s, episode steps: 95, steps per second: 15, episode reward: 0.575, mean reward: 0.006 [0.000, 0.013], mean action: 0.036 [-1.155, 1.147], mean observation: 0.069 [-45.801, 26.344], loss: 0.000201, mean_squared_error: 0.000401, mean_q: 0.546562\n",
      " 1478/2000: episode: 14, duration: 6.342s, episode steps: 101, steps per second: 16, episode reward: 0.646, mean reward: 0.006 [-0.000, 0.014], mean action: 0.111 [-1.219, 1.157], mean observation: 0.079 [-34.310, 17.563], loss: 0.000260, mean_squared_error: 0.000520, mean_q: 0.555301\n",
      " 1578/2000: episode: 15, duration: 6.206s, episode steps: 100, steps per second: 16, episode reward: 0.667, mean reward: 0.007 [0.000, 0.014], mean action: 0.139 [-1.166, 1.184], mean observation: 0.082 [-39.568, 17.747], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.550263\n",
      " 1690/2000: episode: 16, duration: 6.849s, episode steps: 112, steps per second: 16, episode reward: 0.662, mean reward: 0.006 [-0.001, 0.014], mean action: 0.152 [-1.227, 1.172], mean observation: 0.088 [-15.543, 17.848], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.546609\n",
      " 1793/2000: episode: 17, duration: 6.344s, episode steps: 103, steps per second: 16, episode reward: 0.676, mean reward: 0.007 [-0.000, 0.014], mean action: 0.107 [-1.130, 1.124], mean observation: 0.082 [-33.935, 17.930], loss: 0.000217, mean_squared_error: 0.000435, mean_q: 0.543514\n",
      " 1898/2000: episode: 18, duration: 6.368s, episode steps: 105, steps per second: 16, episode reward: 0.677, mean reward: 0.006 [-0.000, 0.013], mean action: 0.095 [-1.141, 1.201], mean observation: 0.081 [-30.102, 17.637], loss: 0.000119, mean_squared_error: 0.000239, mean_q: 0.551401\n",
      "done, took 115.077 seconds\n",
      "\n",
      "\n",
      "iteration: 104\n",
      "Training for 2000 steps ...\n",
      "  117/2000: episode: 1, duration: 5.636s, episode steps: 117, steps per second: 21, episode reward: 0.702, mean reward: 0.006 [-0.001, 0.013], mean action: -0.016 [-1.184, 1.151], mean observation: 0.093 [-19.861, 18.022], loss: --, mean_squared_error: --, mean_q: --\n",
      "  241/2000: episode: 2, duration: 6.220s, episode steps: 124, steps per second: 20, episode reward: 0.754, mean reward: 0.006 [-0.001, 0.013], mean action: 0.025 [-1.143, 1.236], mean observation: 0.100 [-15.300, 17.975], loss: --, mean_squared_error: --, mean_q: --\n",
      "  362/2000: episode: 3, duration: 6.045s, episode steps: 121, steps per second: 20, episode reward: 0.728, mean reward: 0.006 [-0.001, 0.013], mean action: 0.009 [-1.101, 1.221], mean observation: 0.099 [-16.505, 18.015], loss: --, mean_squared_error: --, mean_q: --\n",
      "  474/2000: episode: 4, duration: 5.763s, episode steps: 112, steps per second: 19, episode reward: 0.660, mean reward: 0.006 [-0.001, 0.013], mean action: 0.001 [-1.149, 1.140], mean observation: 0.079 [-42.860, 17.917], loss: --, mean_squared_error: --, mean_q: --\n",
      "  590/2000: episode: 5, duration: 5.808s, episode steps: 116, steps per second: 20, episode reward: 0.699, mean reward: 0.006 [-0.001, 0.013], mean action: -0.013 [-1.244, 1.138], mean observation: 0.090 [-37.098, 17.970], loss: --, mean_squared_error: --, mean_q: --\n",
      "  704/2000: episode: 6, duration: 5.741s, episode steps: 114, steps per second: 20, episode reward: 0.687, mean reward: 0.006 [-0.001, 0.013], mean action: 0.022 [-1.093, 1.136], mean observation: 0.089 [-42.841, 17.870], loss: --, mean_squared_error: --, mean_q: --\n",
      "  812/2000: episode: 7, duration: 5.526s, episode steps: 108, steps per second: 20, episode reward: 0.654, mean reward: 0.006 [-0.001, 0.013], mean action: 0.008 [-1.337, 1.127], mean observation: 0.072 [-41.497, 17.944], loss: --, mean_squared_error: --, mean_q: --\n",
      "  926/2000: episode: 8, duration: 5.758s, episode steps: 114, steps per second: 20, episode reward: 0.677, mean reward: 0.006 [-0.001, 0.013], mean action: 0.029 [-1.131, 1.197], mean observation: 0.086 [-42.308, 17.955], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1046/2000: episode: 9, duration: 6.369s, episode steps: 120, steps per second: 19, episode reward: 0.726, mean reward: 0.006 [-0.001, 0.013], mean action: -0.010 [-1.307, 1.185], mean observation: 0.098 [-20.686, 18.048], loss: 0.000308, mean_squared_error: 0.000617, mean_q: 0.549224\n",
      " 1168/2000: episode: 10, duration: 6.977s, episode steps: 122, steps per second: 17, episode reward: 0.721, mean reward: 0.006 [-0.001, 0.013], mean action: 0.003 [-1.122, 1.128], mean observation: 0.092 [-32.101, 18.046], loss: 0.000066, mean_squared_error: 0.000131, mean_q: 0.545633\n",
      " 1290/2000: episode: 11, duration: 6.961s, episode steps: 122, steps per second: 18, episode reward: 0.701, mean reward: 0.006 [-0.001, 0.013], mean action: -0.024 [-1.245, 1.289], mean observation: 0.091 [-10.258, 17.907], loss: 0.000047, mean_squared_error: 0.000094, mean_q: 0.539493\n",
      " 1411/2000: episode: 12, duration: 6.771s, episode steps: 121, steps per second: 18, episode reward: 0.698, mean reward: 0.006 [-0.001, 0.013], mean action: 0.017 [-1.166, 1.244], mean observation: 0.092 [-12.618, 17.872], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.542252\n",
      " 1533/2000: episode: 13, duration: 6.761s, episode steps: 122, steps per second: 18, episode reward: 0.686, mean reward: 0.006 [-0.001, 0.013], mean action: 0.063 [-1.314, 1.176], mean observation: 0.089 [-13.403, 17.922], loss: 0.000178, mean_squared_error: 0.000355, mean_q: 0.555283\n",
      " 1645/2000: episode: 14, duration: 6.285s, episode steps: 112, steps per second: 18, episode reward: 0.670, mean reward: 0.006 [-0.001, 0.014], mean action: 0.118 [-1.113, 1.193], mean observation: 0.085 [-23.746, 18.061], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.537341\n",
      " 1760/2000: episode: 15, duration: 6.518s, episode steps: 115, steps per second: 18, episode reward: 0.691, mean reward: 0.006 [-0.001, 0.014], mean action: 0.159 [-1.121, 1.144], mean observation: 0.087 [-25.710, 18.095], loss: 0.000171, mean_squared_error: 0.000341, mean_q: 0.541060\n",
      " 1878/2000: episode: 16, duration: 7.157s, episode steps: 118, steps per second: 16, episode reward: 0.690, mean reward: 0.006 [-0.001, 0.014], mean action: 0.139 [-1.200, 1.159], mean observation: 0.086 [-17.119, 18.107], loss: 0.000141, mean_squared_error: 0.000281, mean_q: 0.549217\n",
      " 1989/2000: episode: 17, duration: 6.555s, episode steps: 111, steps per second: 17, episode reward: 0.671, mean reward: 0.006 [-0.001, 0.013], mean action: 0.184 [-1.248, 1.137], mean observation: 0.085 [-14.753, 17.944], loss: 0.000092, mean_squared_error: 0.000184, mean_q: 0.548343\n",
      "done, took 107.765 seconds\n",
      "\n",
      "\n",
      "iteration: 105\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 5.327s, episode steps: 105, steps per second: 20, episode reward: 0.680, mean reward: 0.006 [-0.000, 0.013], mean action: 0.274 [-1.191, 1.218], mean observation: 0.083 [-36.700, 18.051], loss: --, mean_squared_error: --, mean_q: --\n",
      "  211/2000: episode: 2, duration: 5.248s, episode steps: 106, steps per second: 20, episode reward: 0.677, mean reward: 0.006 [-0.000, 0.013], mean action: 0.253 [-1.167, 1.124], mean observation: 0.077 [-39.078, 18.008], loss: --, mean_squared_error: --, mean_q: --\n",
      "  317/2000: episode: 3, duration: 5.320s, episode steps: 106, steps per second: 20, episode reward: 0.681, mean reward: 0.006 [-0.000, 0.013], mean action: 0.261 [-1.103, 1.116], mean observation: 0.087 [-13.270, 17.911], loss: --, mean_squared_error: --, mean_q: --\n",
      "  422/2000: episode: 4, duration: 5.249s, episode steps: 105, steps per second: 20, episode reward: 0.660, mean reward: 0.006 [-0.000, 0.013], mean action: 0.284 [-1.109, 1.297], mean observation: 0.076 [-43.625, 18.112], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  527/2000: episode: 5, duration: 5.280s, episode steps: 105, steps per second: 20, episode reward: 0.675, mean reward: 0.006 [-0.000, 0.013], mean action: 0.263 [-1.251, 1.200], mean observation: 0.087 [-35.641, 17.886], loss: --, mean_squared_error: --, mean_q: --\n",
      "  625/2000: episode: 6, duration: 5.000s, episode steps: 98, steps per second: 20, episode reward: 0.602, mean reward: 0.006 [-0.000, 0.013], mean action: 0.260 [-1.146, 1.159], mean observation: 0.073 [-39.889, 22.357], loss: --, mean_squared_error: --, mean_q: --\n",
      "  729/2000: episode: 7, duration: 5.292s, episode steps: 104, steps per second: 20, episode reward: 0.641, mean reward: 0.006 [-0.000, 0.013], mean action: 0.238 [-1.142, 1.189], mean observation: 0.067 [-44.970, 18.083], loss: --, mean_squared_error: --, mean_q: --\n",
      "  835/2000: episode: 8, duration: 5.351s, episode steps: 106, steps per second: 20, episode reward: 0.680, mean reward: 0.006 [-0.000, 0.013], mean action: 0.288 [-1.118, 1.176], mean observation: 0.079 [-30.791, 17.915], loss: --, mean_squared_error: --, mean_q: --\n",
      "  939/2000: episode: 9, duration: 5.318s, episode steps: 104, steps per second: 20, episode reward: 0.650, mean reward: 0.006 [-0.000, 0.013], mean action: 0.273 [-1.151, 1.213], mean observation: 0.083 [-39.072, 18.030], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1044/2000: episode: 10, duration: 5.791s, episode steps: 105, steps per second: 18, episode reward: 0.668, mean reward: 0.006 [-0.000, 0.013], mean action: 0.264 [-1.103, 1.210], mean observation: 0.078 [-39.887, 18.120], loss: 0.000146, mean_squared_error: 0.000291, mean_q: 0.544656\n",
      " 1148/2000: episode: 11, duration: 6.197s, episode steps: 104, steps per second: 17, episode reward: 0.676, mean reward: 0.007 [-0.000, 0.013], mean action: 0.296 [-1.144, 1.194], mean observation: 0.081 [-33.052, 17.856], loss: 0.000252, mean_squared_error: 0.000503, mean_q: 0.537900\n",
      " 1251/2000: episode: 12, duration: 6.475s, episode steps: 103, steps per second: 16, episode reward: 0.656, mean reward: 0.006 [-0.001, 0.014], mean action: 0.217 [-1.114, 1.188], mean observation: 0.073 [-41.252, 17.922], loss: 0.000292, mean_squared_error: 0.000585, mean_q: 0.539267\n",
      " 1349/2000: episode: 13, duration: 6.306s, episode steps: 98, steps per second: 16, episode reward: 0.602, mean reward: 0.006 [-0.001, 0.013], mean action: 0.151 [-1.172, 1.107], mean observation: 0.069 [-40.983, 24.453], loss: 0.000244, mean_squared_error: 0.000488, mean_q: 0.547007\n",
      " 1470/2000: episode: 14, duration: 7.266s, episode steps: 121, steps per second: 17, episode reward: 0.693, mean reward: 0.006 [-0.002, 0.013], mean action: 0.110 [-1.122, 1.142], mean observation: 0.089 [-15.554, 18.028], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.531080\n",
      " 1598/2000: episode: 15, duration: 7.606s, episode steps: 128, steps per second: 17, episode reward: 0.785, mean reward: 0.006 [-0.001, 0.014], mean action: 0.127 [-1.168, 1.207], mean observation: 0.105 [-15.190, 17.895], loss: 0.000178, mean_squared_error: 0.000356, mean_q: 0.537537\n",
      " 1718/2000: episode: 16, duration: 6.996s, episode steps: 120, steps per second: 17, episode reward: 0.692, mean reward: 0.006 [-0.002, 0.014], mean action: 0.071 [-1.191, 1.410], mean observation: 0.085 [-23.556, 18.172], loss: 0.000272, mean_squared_error: 0.000545, mean_q: 0.545054\n",
      " 1855/2000: episode: 17, duration: 7.658s, episode steps: 137, steps per second: 18, episode reward: 0.824, mean reward: 0.006 [-0.001, 0.014], mean action: 0.064 [-1.108, 1.289], mean observation: 0.114 [-20.960, 12.761], loss: 0.000102, mean_squared_error: 0.000204, mean_q: 0.541492\n",
      " 1993/2000: episode: 18, duration: 7.435s, episode steps: 138, steps per second: 19, episode reward: 0.827, mean reward: 0.006 [-0.002, 0.015], mean action: 0.049 [-1.199, 1.178], mean observation: 0.115 [-27.240, 12.880], loss: 0.000094, mean_squared_error: 0.000188, mean_q: 0.533456\n",
      "done, took 109.733 seconds\n",
      "\n",
      "\n",
      "iteration: 106\n",
      "Training for 2000 steps ...\n",
      "  142/2000: episode: 1, duration: 6.341s, episode steps: 142, steps per second: 22, episode reward: 0.790, mean reward: 0.006 [-0.003, 0.014], mean action: 0.056 [-1.170, 1.182], mean observation: 0.113 [-19.386, 12.972], loss: --, mean_squared_error: --, mean_q: --\n",
      "  286/2000: episode: 2, duration: 6.631s, episode steps: 144, steps per second: 22, episode reward: 0.814, mean reward: 0.006 [-0.002, 0.015], mean action: 0.065 [-1.182, 1.188], mean observation: 0.117 [-26.755, 13.028], loss: --, mean_squared_error: --, mean_q: --\n",
      "  430/2000: episode: 3, duration: 6.589s, episode steps: 144, steps per second: 22, episode reward: 0.785, mean reward: 0.005 [-0.002, 0.014], mean action: 0.101 [-1.133, 1.220], mean observation: 0.113 [-27.052, 12.986], loss: --, mean_squared_error: --, mean_q: --\n",
      "  569/2000: episode: 4, duration: 6.524s, episode steps: 139, steps per second: 21, episode reward: 0.769, mean reward: 0.006 [-0.002, 0.013], mean action: 0.071 [-1.221, 1.183], mean observation: 0.112 [-34.576, 13.038], loss: --, mean_squared_error: --, mean_q: --\n",
      "  707/2000: episode: 5, duration: 6.232s, episode steps: 138, steps per second: 22, episode reward: 0.830, mean reward: 0.006 [-0.002, 0.015], mean action: 0.057 [-1.197, 1.210], mean observation: 0.116 [-25.369, 13.022], loss: --, mean_squared_error: --, mean_q: --\n",
      "  854/2000: episode: 6, duration: 6.845s, episode steps: 147, steps per second: 21, episode reward: 0.807, mean reward: 0.005 [-0.002, 0.014], mean action: 0.068 [-1.166, 1.191], mean observation: 0.114 [-27.266, 12.971], loss: --, mean_squared_error: --, mean_q: --\n",
      "  989/2000: episode: 7, duration: 6.113s, episode steps: 135, steps per second: 22, episode reward: 0.799, mean reward: 0.006 [-0.002, 0.014], mean action: 0.062 [-1.199, 1.219], mean observation: 0.112 [-20.799, 13.794], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1135/2000: episode: 8, duration: 7.869s, episode steps: 146, steps per second: 19, episode reward: 0.795, mean reward: 0.005 [-0.002, 0.014], mean action: 0.069 [-1.148, 1.272], mean observation: 0.114 [-26.732, 13.312], loss: 0.000054, mean_squared_error: 0.000107, mean_q: 0.547940\n",
      " 1270/2000: episode: 9, duration: 7.357s, episode steps: 135, steps per second: 18, episode reward: 0.798, mean reward: 0.006 [-0.002, 0.013], mean action: 0.041 [-1.374, 1.167], mean observation: 0.111 [-24.782, 13.105], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.535174\n",
      " 1383/2000: episode: 10, duration: 6.661s, episode steps: 113, steps per second: 17, episode reward: 0.648, mean reward: 0.006 [-0.002, 0.013], mean action: 0.099 [-1.227, 1.258], mean observation: 0.086 [-41.201, 13.209], loss: 0.000102, mean_squared_error: 0.000205, mean_q: 0.546366\n",
      " 1493/2000: episode: 11, duration: 6.544s, episode steps: 110, steps per second: 17, episode reward: 0.638, mean reward: 0.006 [-0.001, 0.013], mean action: 0.090 [-1.127, 1.298], mean observation: 0.081 [-44.248, 13.451], loss: 0.000050, mean_squared_error: 0.000100, mean_q: 0.536925\n",
      " 1598/2000: episode: 12, duration: 6.249s, episode steps: 105, steps per second: 17, episode reward: 0.623, mean reward: 0.006 [-0.002, 0.013], mean action: 0.045 [-1.101, 1.185], mean observation: 0.072 [-41.459, 25.229], loss: 0.000144, mean_squared_error: 0.000287, mean_q: 0.542928\n",
      " 1723/2000: episode: 13, duration: 7.098s, episode steps: 125, steps per second: 18, episode reward: 0.666, mean reward: 0.005 [-0.002, 0.013], mean action: -0.029 [-1.183, 1.203], mean observation: 0.087 [-10.065, 12.989], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.537056\n",
      " 1850/2000: episode: 14, duration: 6.840s, episode steps: 127, steps per second: 19, episode reward: 0.689, mean reward: 0.005 [-0.002, 0.013], mean action: -0.081 [-1.192, 1.315], mean observation: 0.091 [-10.045, 12.938], loss: 0.000100, mean_squared_error: 0.000201, mean_q: 0.538335\n",
      " 1961/2000: episode: 15, duration: 6.100s, episode steps: 111, steps per second: 18, episode reward: 0.582, mean reward: 0.005 [-0.002, 0.012], mean action: -0.014 [-1.208, 1.264], mean observation: 0.079 [-45.898, 14.382], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.544276\n",
      "done, took 102.172 seconds\n",
      "\n",
      "\n",
      "iteration: 107\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  126/2000: episode: 1, duration: 6.339s, episode steps: 126, steps per second: 20, episode reward: 0.681, mean reward: 0.005 [-0.002, 0.013], mean action: -0.088 [-1.161, 1.140], mean observation: 0.089 [-31.017, 13.001], loss: --, mean_squared_error: --, mean_q: --\n",
      "  256/2000: episode: 2, duration: 5.862s, episode steps: 130, steps per second: 22, episode reward: 0.688, mean reward: 0.005 [-0.002, 0.013], mean action: -0.109 [-1.206, 1.161], mean observation: 0.092 [-10.053, 12.905], loss: --, mean_squared_error: --, mean_q: --\n",
      "  380/2000: episode: 3, duration: 5.982s, episode steps: 124, steps per second: 21, episode reward: 0.676, mean reward: 0.005 [-0.002, 0.013], mean action: -0.053 [-1.107, 1.169], mean observation: 0.089 [-43.020, 12.979], loss: --, mean_squared_error: --, mean_q: --\n",
      "  501/2000: episode: 4, duration: 5.765s, episode steps: 121, steps per second: 21, episode reward: 0.657, mean reward: 0.005 [-0.002, 0.013], mean action: -0.064 [-1.284, 1.201], mean observation: 0.086 [-41.103, 13.130], loss: --, mean_squared_error: --, mean_q: --\n",
      "  633/2000: episode: 5, duration: 6.058s, episode steps: 132, steps per second: 22, episode reward: 0.687, mean reward: 0.005 [-0.002, 0.013], mean action: -0.121 [-1.235, 1.180], mean observation: 0.092 [-9.981, 12.825], loss: --, mean_squared_error: --, mean_q: --\n",
      "  762/2000: episode: 6, duration: 5.899s, episode steps: 129, steps per second: 22, episode reward: 0.679, mean reward: 0.005 [-0.002, 0.013], mean action: -0.099 [-1.239, 1.167], mean observation: 0.090 [-17.848, 13.123], loss: --, mean_squared_error: --, mean_q: --\n",
      "  893/2000: episode: 7, duration: 6.122s, episode steps: 131, steps per second: 21, episode reward: 0.696, mean reward: 0.005 [-0.002, 0.013], mean action: -0.094 [-1.194, 1.209], mean observation: 0.091 [-17.213, 12.899], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1017/2000: episode: 8, duration: 6.398s, episode steps: 124, steps per second: 19, episode reward: 0.666, mean reward: 0.005 [-0.002, 0.013], mean action: -0.084 [-1.156, 1.159], mean observation: 0.087 [-38.187, 12.823], loss: 0.000057, mean_squared_error: 0.000114, mean_q: 0.556218\n",
      " 1145/2000: episode: 9, duration: 7.059s, episode steps: 128, steps per second: 18, episode reward: 0.679, mean reward: 0.005 [-0.002, 0.013], mean action: -0.082 [-1.212, 1.168], mean observation: 0.090 [-17.962, 12.894], loss: 0.000272, mean_squared_error: 0.000544, mean_q: 0.534952\n",
      " 1278/2000: episode: 10, duration: 7.461s, episode steps: 133, steps per second: 18, episode reward: 0.723, mean reward: 0.005 [-0.002, 0.013], mean action: -0.065 [-1.176, 1.223], mean observation: 0.101 [-45.621, 13.028], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.540862\n",
      " 1389/2000: episode: 11, duration: 6.397s, episode steps: 111, steps per second: 17, episode reward: 0.701, mean reward: 0.006 [-0.001, 0.013], mean action: -0.036 [-1.307, 1.138], mean observation: 0.113 [-25.881, 16.425], loss: 0.000057, mean_squared_error: 0.000115, mean_q: 0.533457\n",
      " 1507/2000: episode: 12, duration: 6.476s, episode steps: 118, steps per second: 18, episode reward: 0.689, mean reward: 0.006 [-0.001, 0.017], mean action: -0.155 [-1.146, 1.164], mean observation: 0.128 [-18.733, 13.113], loss: 0.000043, mean_squared_error: 0.000085, mean_q: 0.540288\n",
      " 1616/2000: episode: 13, duration: 6.443s, episode steps: 109, steps per second: 17, episode reward: 0.594, mean reward: 0.005 [-0.001, 0.017], mean action: -0.083 [-1.186, 1.209], mean observation: 0.125 [-23.518, 12.830], loss: 0.000090, mean_squared_error: 0.000179, mean_q: 0.530665\n",
      " 1730/2000: episode: 14, duration: 6.367s, episode steps: 114, steps per second: 18, episode reward: 0.672, mean reward: 0.006 [-0.001, 0.014], mean action: -0.064 [-1.185, 1.125], mean observation: 0.124 [-10.146, 13.169], loss: 0.000285, mean_squared_error: 0.000570, mean_q: 0.526295\n",
      " 1843/2000: episode: 15, duration: 6.256s, episode steps: 113, steps per second: 18, episode reward: 0.641, mean reward: 0.006 [-0.001, 0.015], mean action: -0.061 [-1.166, 1.218], mean observation: 0.122 [-10.164, 15.093], loss: 0.000376, mean_squared_error: 0.000753, mean_q: 0.527923\n",
      "done, took 103.359 seconds\n",
      "\n",
      "\n",
      "iteration: 108\n",
      "Training for 2000 steps ...\n",
      "  147/2000: episode: 1, duration: 6.361s, episode steps: 147, steps per second: 23, episode reward: 0.675, mean reward: 0.005 [-0.003, 0.013], mean action: -0.032 [-1.208, 1.209], mean observation: 0.112 [-11.658, 12.955], loss: --, mean_squared_error: --, mean_q: --\n",
      "  302/2000: episode: 2, duration: 6.552s, episode steps: 155, steps per second: 24, episode reward: 0.690, mean reward: 0.004 [-0.003, 0.013], mean action: -0.080 [-1.190, 1.164], mean observation: 0.112 [-12.561, 12.752], loss: --, mean_squared_error: --, mean_q: --\n",
      "  455/2000: episode: 3, duration: 6.491s, episode steps: 153, steps per second: 24, episode reward: 0.686, mean reward: 0.004 [-0.003, 0.013], mean action: -0.047 [-1.274, 1.208], mean observation: 0.111 [-11.404, 13.111], loss: --, mean_squared_error: --, mean_q: --\n",
      "  602/2000: episode: 4, duration: 6.314s, episode steps: 147, steps per second: 23, episode reward: 0.686, mean reward: 0.005 [-0.003, 0.013], mean action: -0.057 [-1.211, 1.233], mean observation: 0.110 [-10.115, 13.087], loss: --, mean_squared_error: --, mean_q: --\n",
      "  749/2000: episode: 5, duration: 6.383s, episode steps: 147, steps per second: 23, episode reward: 0.684, mean reward: 0.005 [-0.003, 0.013], mean action: -0.047 [-1.096, 1.227], mean observation: 0.111 [-11.272, 12.872], loss: --, mean_squared_error: --, mean_q: --\n",
      "  901/2000: episode: 6, duration: 6.523s, episode steps: 152, steps per second: 23, episode reward: 0.699, mean reward: 0.005 [-0.003, 0.013], mean action: -0.060 [-1.276, 1.217], mean observation: 0.112 [-12.566, 12.839], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1054/2000: episode: 7, duration: 7.009s, episode steps: 153, steps per second: 22, episode reward: 0.697, mean reward: 0.005 [-0.003, 0.013], mean action: -0.061 [-1.188, 1.213], mean observation: 0.110 [-12.403, 12.916], loss: 0.000077, mean_squared_error: 0.000153, mean_q: 0.530993\n",
      " 1193/2000: episode: 8, duration: 7.403s, episode steps: 139, steps per second: 19, episode reward: 0.703, mean reward: 0.005 [-0.002, 0.014], mean action: -0.119 [-1.212, 1.129], mean observation: 0.106 [-12.059, 13.100], loss: 0.000105, mean_squared_error: 0.000209, mean_q: 0.533574\n",
      " 1309/2000: episode: 9, duration: 6.733s, episode steps: 116, steps per second: 17, episode reward: 0.665, mean reward: 0.006 [-0.002, 0.014], mean action: -0.081 [-1.177, 1.181], mean observation: 0.093 [-10.197, 13.222], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.534109\n",
      " 1444/2000: episode: 10, duration: 7.511s, episode steps: 135, steps per second: 18, episode reward: 0.681, mean reward: 0.005 [-0.002, 0.012], mean action: -0.167 [-1.263, 1.191], mean observation: 0.107 [-41.609, 12.878], loss: 0.000063, mean_squared_error: 0.000126, mean_q: 0.515681\n",
      " 1563/2000: episode: 11, duration: 7.259s, episode steps: 119, steps per second: 16, episode reward: 0.681, mean reward: 0.006 [-0.001, 0.012], mean action: -0.128 [-1.175, 1.171], mean observation: 0.103 [-28.434, 12.930], loss: 0.000203, mean_squared_error: 0.000405, mean_q: 0.530550\n",
      " 1712/2000: episode: 12, duration: 8.643s, episode steps: 149, steps per second: 17, episode reward: 0.685, mean reward: 0.005 [-0.002, 0.012], mean action: -0.158 [-1.241, 1.136], mean observation: 0.112 [-29.009, 15.368], loss: 0.000057, mean_squared_error: 0.000113, mean_q: 0.528939\n",
      " 1873/2000: episode: 13, duration: 9.467s, episode steps: 161, steps per second: 17, episode reward: 0.658, mean reward: 0.004 [-0.002, 0.012], mean action: -0.118 [-1.251, 1.153], mean observation: 0.107 [-18.415, 12.977], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.532147\n",
      " 1985/2000: episode: 14, duration: 5.812s, episode steps: 112, steps per second: 19, episode reward: 0.574, mean reward: 0.005 [-0.001, 0.012], mean action: -0.044 [-1.227, 1.223], mean observation: 0.108 [-10.026, 13.460], loss: 0.000190, mean_squared_error: 0.000381, mean_q: 0.532265\n",
      "done, took 99.637 seconds\n",
      "\n",
      "\n",
      "iteration: 109\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  103/2000: episode: 1, duration: 4.442s, episode steps: 103, steps per second: 23, episode reward: 0.576, mean reward: 0.006 [-0.001, 0.013], mean action: -0.053 [-1.204, 1.207], mean observation: 0.105 [-10.130, 13.137], loss: --, mean_squared_error: --, mean_q: --\n",
      "  205/2000: episode: 2, duration: 4.300s, episode steps: 102, steps per second: 24, episode reward: 0.574, mean reward: 0.006 [-0.001, 0.013], mean action: -0.030 [-1.199, 1.180], mean observation: 0.104 [-10.071, 13.016], loss: --, mean_squared_error: --, mean_q: --\n",
      "  309/2000: episode: 3, duration: 4.462s, episode steps: 104, steps per second: 23, episode reward: 0.581, mean reward: 0.006 [-0.001, 0.012], mean action: -0.025 [-1.174, 1.218], mean observation: 0.107 [-10.074, 13.008], loss: --, mean_squared_error: --, mean_q: --\n",
      "  413/2000: episode: 4, duration: 4.426s, episode steps: 104, steps per second: 23, episode reward: 0.572, mean reward: 0.005 [-0.001, 0.012], mean action: -0.060 [-1.166, 1.126], mean observation: 0.107 [-9.981, 13.066], loss: --, mean_squared_error: --, mean_q: --\n",
      "  517/2000: episode: 5, duration: 4.571s, episode steps: 104, steps per second: 23, episode reward: 0.579, mean reward: 0.006 [-0.001, 0.013], mean action: -0.049 [-1.154, 1.264], mean observation: 0.106 [-9.969, 13.167], loss: --, mean_squared_error: --, mean_q: --\n",
      "  621/2000: episode: 6, duration: 4.476s, episode steps: 104, steps per second: 23, episode reward: 0.583, mean reward: 0.006 [-0.001, 0.013], mean action: -0.062 [-1.181, 1.116], mean observation: 0.107 [-10.114, 13.127], loss: --, mean_squared_error: --, mean_q: --\n",
      "  725/2000: episode: 7, duration: 4.495s, episode steps: 104, steps per second: 23, episode reward: 0.586, mean reward: 0.006 [-0.001, 0.013], mean action: -0.062 [-1.212, 1.128], mean observation: 0.105 [-10.175, 13.186], loss: --, mean_squared_error: --, mean_q: --\n",
      "  828/2000: episode: 8, duration: 4.516s, episode steps: 103, steps per second: 23, episode reward: 0.576, mean reward: 0.006 [-0.001, 0.013], mean action: -0.055 [-1.206, 1.123], mean observation: 0.105 [-10.061, 13.009], loss: --, mean_squared_error: --, mean_q: --\n",
      "  930/2000: episode: 9, duration: 4.402s, episode steps: 102, steps per second: 23, episode reward: 0.578, mean reward: 0.006 [-0.001, 0.013], mean action: -0.030 [-1.118, 1.163], mean observation: 0.101 [-17.465, 13.010], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1033/2000: episode: 10, duration: 4.764s, episode steps: 103, steps per second: 22, episode reward: 0.571, mean reward: 0.006 [-0.001, 0.012], mean action: -0.097 [-1.221, 1.157], mean observation: 0.108 [-9.964, 12.937], loss: 0.000053, mean_squared_error: 0.000105, mean_q: 0.526153\n",
      " 1144/2000: episode: 11, duration: 5.658s, episode steps: 111, steps per second: 20, episode reward: 0.555, mean reward: 0.005 [-0.002, 0.012], mean action: -0.113 [-1.155, 1.134], mean observation: 0.110 [-10.048, 13.052], loss: 0.000197, mean_squared_error: 0.000395, mean_q: 0.530132\n",
      " 1251/2000: episode: 12, duration: 5.901s, episode steps: 107, steps per second: 18, episode reward: 0.645, mean reward: 0.006 [-0.001, 0.013], mean action: -0.168 [-1.168, 1.175], mean observation: 0.105 [-9.978, 13.367], loss: 0.000049, mean_squared_error: 0.000098, mean_q: 0.534561\n",
      " 1366/2000: episode: 13, duration: 6.286s, episode steps: 115, steps per second: 18, episode reward: 0.647, mean reward: 0.006 [-0.001, 0.013], mean action: -0.210 [-1.198, 1.125], mean observation: 0.106 [-10.032, 12.899], loss: 0.000096, mean_squared_error: 0.000191, mean_q: 0.524441\n",
      " 1502/2000: episode: 14, duration: 7.242s, episode steps: 136, steps per second: 19, episode reward: 0.578, mean reward: 0.004 [-0.001, 0.011], mean action: -0.163 [-1.199, 1.191], mean observation: 0.115 [-10.089, 13.043], loss: 0.000078, mean_squared_error: 0.000157, mean_q: 0.522873\n",
      " 1654/2000: episode: 15, duration: 7.932s, episode steps: 152, steps per second: 19, episode reward: 0.563, mean reward: 0.004 [-0.002, 0.012], mean action: -0.126 [-1.197, 1.213], mean observation: 0.102 [-9.986, 13.796], loss: 0.000285, mean_squared_error: 0.000571, mean_q: 0.531864\n",
      " 1782/2000: episode: 16, duration: 7.336s, episode steps: 128, steps per second: 17, episode reward: 0.552, mean reward: 0.004 [-0.001, 0.011], mean action: -0.141 [-1.207, 1.223], mean observation: 0.124 [-10.216, 13.447], loss: 0.000280, mean_squared_error: 0.000560, mean_q: 0.526684\n",
      " 1884/2000: episode: 17, duration: 5.681s, episode steps: 102, steps per second: 18, episode reward: 0.545, mean reward: 0.005 [-0.001, 0.013], mean action: -0.147 [-1.187, 1.145], mean observation: 0.136 [-14.689, 13.031], loss: 0.000350, mean_squared_error: 0.000701, mean_q: 0.516866\n",
      " 1989/2000: episode: 18, duration: 5.058s, episode steps: 105, steps per second: 21, episode reward: 0.497, mean reward: 0.005 [-0.001, 0.010], mean action: -0.158 [-1.364, 1.114], mean observation: 0.125 [-23.738, 13.105], loss: 0.000151, mean_squared_error: 0.000303, mean_q: 0.531552\n",
      "done, took 96.793 seconds\n",
      "\n",
      "\n",
      "iteration: 110\n",
      "Training for 2000 steps ...\n",
      "  118/2000: episode: 1, duration: 5.330s, episode steps: 118, steps per second: 22, episode reward: 0.542, mean reward: 0.005 [-0.001, 0.011], mean action: -0.139 [-1.151, 1.247], mean observation: 0.139 [-15.758, 12.857], loss: --, mean_squared_error: --, mean_q: --\n",
      "  233/2000: episode: 2, duration: 5.281s, episode steps: 115, steps per second: 22, episode reward: 0.526, mean reward: 0.005 [-0.001, 0.010], mean action: -0.160 [-1.147, 1.197], mean observation: 0.130 [-13.723, 13.110], loss: --, mean_squared_error: --, mean_q: --\n",
      "  349/2000: episode: 3, duration: 5.235s, episode steps: 116, steps per second: 22, episode reward: 0.521, mean reward: 0.004 [-0.001, 0.011], mean action: -0.164 [-1.273, 1.137], mean observation: 0.129 [-11.960, 12.845], loss: --, mean_squared_error: --, mean_q: --\n",
      "  481/2000: episode: 4, duration: 5.414s, episode steps: 132, steps per second: 24, episode reward: 0.681, mean reward: 0.005 [-0.001, 0.012], mean action: -0.142 [-1.216, 1.221], mean observation: 0.149 [-10.438, 12.907], loss: --, mean_squared_error: --, mean_q: --\n",
      "  623/2000: episode: 5, duration: 5.517s, episode steps: 142, steps per second: 26, episode reward: 0.707, mean reward: 0.005 [-0.001, 0.012], mean action: -0.118 [-1.192, 1.247], mean observation: 0.150 [-17.806, 13.081], loss: --, mean_squared_error: --, mean_q: --\n",
      "  752/2000: episode: 6, duration: 7.322s, episode steps: 129, steps per second: 18, episode reward: 0.645, mean reward: 0.005 [-0.001, 0.012], mean action: -0.144 [-1.306, 1.248], mean observation: 0.146 [-15.591, 13.138], loss: --, mean_squared_error: --, mean_q: --\n",
      "  870/2000: episode: 7, duration: 5.385s, episode steps: 118, steps per second: 22, episode reward: 0.539, mean reward: 0.005 [-0.001, 0.012], mean action: -0.140 [-1.216, 1.213], mean observation: 0.135 [-15.297, 12.765], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1001/2000: episode: 8, duration: 5.390s, episode steps: 131, steps per second: 24, episode reward: 0.661, mean reward: 0.005 [-0.001, 0.012], mean action: -0.147 [-1.256, 1.152], mean observation: 0.146 [-17.121, 12.938], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1123/2000: episode: 9, duration: 6.729s, episode steps: 122, steps per second: 18, episode reward: 0.561, mean reward: 0.005 [-0.001, 0.011], mean action: -0.095 [-1.155, 1.192], mean observation: 0.115 [-10.109, 13.046], loss: 0.000242, mean_squared_error: 0.000485, mean_q: 0.526086\n",
      " 1351/2000: episode: 10, duration: 9.178s, episode steps: 228, steps per second: 25, episode reward: -0.862, mean reward: -0.004 [-0.022, 0.010], mean action: 0.030 [-1.224, 1.198], mean observation: 0.063 [-28.856, 12.725], loss: 0.000196, mean_squared_error: 0.000392, mean_q: 0.528865\n",
      " 1609/2000: episode: 11, duration: 10.771s, episode steps: 258, steps per second: 24, episode reward: -0.848, mean reward: -0.003 [-0.021, 0.011], mean action: 0.007 [-1.309, 1.185], mean observation: 0.071 [-10.000, 12.848], loss: 0.000229, mean_squared_error: 0.000458, mean_q: 0.528074\n",
      " 1758/2000: episode: 12, duration: 6.846s, episode steps: 149, steps per second: 22, episode reward: -0.847, mean reward: -0.006 [-0.021, 0.010], mean action: 0.042 [-1.217, 1.196], mean observation: 0.044 [-9.973, 13.826], loss: 0.000056, mean_squared_error: 0.000113, mean_q: 0.522368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1951/2000: episode: 13, duration: 8.461s, episode steps: 193, steps per second: 23, episode reward: -0.885, mean reward: -0.005 [-0.021, 0.011], mean action: 0.019 [-1.231, 1.166], mean observation: 0.051 [-10.001, 12.836], loss: 0.000127, mean_squared_error: 0.000255, mean_q: 0.520581\n",
      "done, took 89.968 seconds\n",
      "\n",
      "\n",
      "iteration: 111\n",
      "Training for 2000 steps ...\n",
      "  178/2000: episode: 1, duration: 6.047s, episode steps: 178, steps per second: 29, episode reward: -0.879, mean reward: -0.005 [-0.021, 0.011], mean action: 0.017 [-1.159, 1.134], mean observation: 0.049 [-10.064, 12.996], loss: --, mean_squared_error: --, mean_q: --\n",
      "  354/2000: episode: 2, duration: 6.268s, episode steps: 176, steps per second: 28, episode reward: -0.843, mean reward: -0.005 [-0.021, 0.011], mean action: 0.030 [-1.173, 1.207], mean observation: 0.051 [-10.173, 13.149], loss: --, mean_squared_error: --, mean_q: --\n",
      "  531/2000: episode: 3, duration: 6.054s, episode steps: 177, steps per second: 29, episode reward: -0.878, mean reward: -0.005 [-0.021, 0.010], mean action: 0.036 [-1.183, 1.317], mean observation: 0.049 [-10.134, 13.173], loss: --, mean_squared_error: --, mean_q: --\n",
      "  718/2000: episode: 4, duration: 6.469s, episode steps: 187, steps per second: 29, episode reward: -0.885, mean reward: -0.005 [-0.021, 0.011], mean action: 0.024 [-1.237, 1.305], mean observation: 0.051 [-10.111, 13.092], loss: --, mean_squared_error: --, mean_q: --\n",
      "  898/2000: episode: 5, duration: 6.215s, episode steps: 180, steps per second: 29, episode reward: -0.860, mean reward: -0.005 [-0.021, 0.011], mean action: 0.001 [-1.220, 1.228], mean observation: 0.051 [-10.070, 12.977], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1081/2000: episode: 6, duration: 7.411s, episode steps: 183, steps per second: 25, episode reward: -0.898, mean reward: -0.005 [-0.021, 0.010], mean action: 0.020 [-1.270, 1.189], mean observation: 0.049 [-10.202, 13.267], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.502902\n",
      " 1211/2000: episode: 7, duration: 5.527s, episode steps: 130, steps per second: 24, episode reward: -0.818, mean reward: -0.006 [-0.021, 0.010], mean action: 0.022 [-1.288, 1.318], mean observation: 0.044 [-10.093, 13.030], loss: 0.000256, mean_squared_error: 0.000513, mean_q: 0.517364\n",
      " 1434/2000: episode: 8, duration: 12.624s, episode steps: 223, steps per second: 18, episode reward: 0.612, mean reward: 0.003 [-0.003, 0.013], mean action: 0.043 [-1.195, 1.253], mean observation: 0.092 [-10.105, 12.701], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.518508\n",
      " 1618/2000: episode: 9, duration: 7.989s, episode steps: 184, steps per second: 23, episode reward: -0.855, mean reward: -0.005 [-0.020, 0.011], mean action: 0.049 [-1.267, 1.223], mean observation: 0.056 [-10.083, 13.199], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.524201\n",
      " 1776/2000: episode: 10, duration: 6.367s, episode steps: 158, steps per second: 25, episode reward: -0.864, mean reward: -0.005 [-0.021, 0.011], mean action: 0.040 [-1.259, 1.180], mean observation: 0.046 [-10.063, 13.002], loss: 0.000254, mean_squared_error: 0.000507, mean_q: 0.521009\n",
      " 1944/2000: episode: 11, duration: 6.916s, episode steps: 168, steps per second: 24, episode reward: -0.883, mean reward: -0.005 [-0.021, 0.011], mean action: 0.066 [-1.157, 1.292], mean observation: 0.047 [-10.144, 13.151], loss: 0.000123, mean_squared_error: 0.000245, mean_q: 0.517297\n",
      "done, took 81.241 seconds\n",
      "\n",
      "\n",
      "iteration: 112\n",
      "Training for 2000 steps ...\n",
      "  177/2000: episode: 1, duration: 5.480s, episode steps: 177, steps per second: 32, episode reward: -0.922, mean reward: -0.005 [-0.019, 0.012], mean action: 0.084 [-1.354, 1.115], mean observation: 0.044 [-10.111, 13.068], loss: --, mean_squared_error: --, mean_q: --\n",
      "  349/2000: episode: 2, duration: 5.274s, episode steps: 172, steps per second: 33, episode reward: -0.911, mean reward: -0.005 [-0.019, 0.012], mean action: 0.081 [-1.258, 1.161], mean observation: 0.043 [-10.077, 13.021], loss: --, mean_squared_error: --, mean_q: --\n",
      "  522/2000: episode: 3, duration: 5.437s, episode steps: 173, steps per second: 32, episode reward: -0.912, mean reward: -0.005 [-0.019, 0.012], mean action: 0.062 [-1.207, 1.289], mean observation: 0.043 [-9.864, 12.622], loss: --, mean_squared_error: --, mean_q: --\n",
      "  694/2000: episode: 4, duration: 5.144s, episode steps: 172, steps per second: 33, episode reward: -0.892, mean reward: -0.005 [-0.019, 0.012], mean action: 0.081 [-1.157, 1.174], mean observation: 0.045 [-9.981, 12.839], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 5, duration: 5.249s, episode steps: 168, steps per second: 32, episode reward: -0.890, mean reward: -0.005 [-0.019, 0.012], mean action: 0.069 [-1.299, 1.237], mean observation: 0.045 [-10.074, 13.067], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1034/2000: episode: 6, duration: 5.456s, episode steps: 172, steps per second: 32, episode reward: -0.912, mean reward: -0.005 [-0.020, 0.012], mean action: 0.097 [-1.236, 1.183], mean observation: 0.043 [-10.027, 12.892], loss: 0.000224, mean_squared_error: 0.000448, mean_q: 0.518938\n",
      " 1218/2000: episode: 7, duration: 7.422s, episode steps: 184, steps per second: 25, episode reward: -0.916, mean reward: -0.005 [-0.019, 0.011], mean action: 0.115 [-1.119, 1.264], mean observation: 0.049 [-10.036, 12.938], loss: 0.000177, mean_squared_error: 0.000355, mean_q: 0.520433\n",
      " 1395/2000: episode: 8, duration: 10.766s, episode steps: 177, steps per second: 16, episode reward: 0.723, mean reward: 0.004 [-0.004, 0.014], mean action: 0.219 [-1.179, 1.198], mean observation: 0.104 [-25.068, 18.097], loss: 0.000194, mean_squared_error: 0.000389, mean_q: 0.522995\n",
      " 1579/2000: episode: 9, duration: 10.816s, episode steps: 184, steps per second: 17, episode reward: 0.694, mean reward: 0.004 [-0.004, 0.013], mean action: 0.143 [-1.204, 1.177], mean observation: 0.095 [-25.435, 17.961], loss: 0.000314, mean_squared_error: 0.000627, mean_q: 0.523790\n",
      " 1732/2000: episode: 10, duration: 6.517s, episode steps: 153, steps per second: 23, episode reward: -0.870, mean reward: -0.006 [-0.020, 0.012], mean action: 0.048 [-1.263, 1.252], mean observation: 0.046 [-10.089, 17.955], loss: 0.000311, mean_squared_error: 0.000622, mean_q: 0.522090\n",
      " 1869/2000: episode: 11, duration: 6.213s, episode steps: 137, steps per second: 22, episode reward: -0.838, mean reward: -0.006 [-0.019, 0.013], mean action: -0.007 [-1.182, 1.218], mean observation: 0.031 [-10.030, 18.089], loss: 0.000145, mean_squared_error: 0.000291, mean_q: 0.512067\n",
      "done, took 79.769 seconds\n",
      "\n",
      "\n",
      "iteration: 113\n",
      "Training for 2000 steps ...\n",
      "  134/2000: episode: 1, duration: 4.471s, episode steps: 134, steps per second: 30, episode reward: -0.786, mean reward: -0.006 [-0.019, 0.014], mean action: 0.006 [-1.224, 1.196], mean observation: 0.036 [-15.420, 18.041], loss: --, mean_squared_error: --, mean_q: --\n",
      "  266/2000: episode: 2, duration: 4.437s, episode steps: 132, steps per second: 30, episode reward: -0.768, mean reward: -0.006 [-0.019, 0.013], mean action: -0.015 [-1.211, 1.080], mean observation: 0.035 [-15.737, 17.997], loss: --, mean_squared_error: --, mean_q: --\n",
      "  400/2000: episode: 3, duration: 4.497s, episode steps: 134, steps per second: 30, episode reward: -0.780, mean reward: -0.006 [-0.018, 0.014], mean action: 0.007 [-1.253, 1.248], mean observation: 0.037 [-15.411, 18.234], loss: --, mean_squared_error: --, mean_q: --\n",
      "  532/2000: episode: 4, duration: 4.454s, episode steps: 132, steps per second: 30, episode reward: -0.758, mean reward: -0.006 [-0.018, 0.014], mean action: -0.014 [-1.140, 1.162], mean observation: 0.036 [-15.369, 18.070], loss: --, mean_squared_error: --, mean_q: --\n",
      "  665/2000: episode: 5, duration: 4.474s, episode steps: 133, steps per second: 30, episode reward: -0.778, mean reward: -0.006 [-0.018, 0.014], mean action: 0.039 [-1.222, 1.156], mean observation: 0.039 [-15.083, 18.121], loss: --, mean_squared_error: --, mean_q: --\n",
      "  799/2000: episode: 6, duration: 4.413s, episode steps: 134, steps per second: 30, episode reward: -0.785, mean reward: -0.006 [-0.018, 0.013], mean action: -0.022 [-1.271, 1.090], mean observation: 0.037 [-15.822, 18.254], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  932/2000: episode: 7, duration: 4.453s, episode steps: 133, steps per second: 30, episode reward: -0.775, mean reward: -0.006 [-0.019, 0.013], mean action: 0.026 [-1.136, 1.229], mean observation: 0.035 [-15.428, 17.867], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1068/2000: episode: 8, duration: 5.122s, episode steps: 136, steps per second: 27, episode reward: -0.831, mean reward: -0.006 [-0.019, 0.014], mean action: -0.005 [-1.146, 1.229], mean observation: 0.055 [-14.880, 18.035], loss: 0.000063, mean_squared_error: 0.000126, mean_q: 0.523137\n",
      " 1204/2000: episode: 9, duration: 5.570s, episode steps: 136, steps per second: 24, episode reward: -0.804, mean reward: -0.006 [-0.018, 0.013], mean action: -0.004 [-1.206, 1.164], mean observation: 0.039 [-14.933, 18.133], loss: 0.000172, mean_squared_error: 0.000344, mean_q: 0.526089\n",
      " 1338/2000: episode: 10, duration: 6.013s, episode steps: 134, steps per second: 22, episode reward: -0.868, mean reward: -0.006 [-0.019, 0.013], mean action: 0.057 [-1.173, 1.222], mean observation: 0.031 [-17.063, 17.980], loss: 0.000166, mean_squared_error: 0.000332, mean_q: 0.526585\n",
      " 1464/2000: episode: 11, duration: 5.708s, episode steps: 126, steps per second: 22, episode reward: -0.741, mean reward: -0.006 [-0.019, 0.014], mean action: 0.096 [-1.169, 1.181], mean observation: 0.037 [-16.840, 18.344], loss: 0.000154, mean_squared_error: 0.000307, mean_q: 0.526245\n",
      " 1597/2000: episode: 12, duration: 5.675s, episode steps: 133, steps per second: 23, episode reward: -0.824, mean reward: -0.006 [-0.019, 0.013], mean action: 0.042 [-1.254, 1.201], mean observation: 0.030 [-17.104, 17.938], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.523883\n",
      " 1726/2000: episode: 13, duration: 5.708s, episode steps: 129, steps per second: 23, episode reward: -0.735, mean reward: -0.006 [-0.018, 0.013], mean action: 0.018 [-1.178, 1.164], mean observation: 0.039 [-16.390, 18.180], loss: 0.000254, mean_squared_error: 0.000509, mean_q: 0.519083\n",
      " 1858/2000: episode: 14, duration: 5.619s, episode steps: 132, steps per second: 23, episode reward: -0.779, mean reward: -0.006 [-0.019, 0.012], mean action: -0.006 [-1.198, 1.142], mean observation: 0.033 [-17.118, 14.705], loss: 0.000144, mean_squared_error: 0.000288, mean_q: 0.524099\n",
      " 1987/2000: episode: 15, duration: 5.159s, episode steps: 129, steps per second: 25, episode reward: -0.765, mean reward: -0.006 [-0.019, 0.011], mean action: -0.059 [-1.175, 1.206], mean observation: 0.037 [-19.865, 18.123], loss: 0.000056, mean_squared_error: 0.000113, mean_q: 0.516674\n",
      "done, took 76.817 seconds\n",
      "\n",
      "\n",
      "iteration: 114\n",
      "Training for 2000 steps ...\n",
      "  129/2000: episode: 1, duration: 4.040s, episode steps: 129, steps per second: 32, episode reward: -0.751, mean reward: -0.006 [-0.019, 0.011], mean action: -0.039 [-1.361, 1.162], mean observation: 0.034 [-19.683, 18.302], loss: --, mean_squared_error: --, mean_q: --\n",
      "  258/2000: episode: 2, duration: 3.920s, episode steps: 129, steps per second: 33, episode reward: -0.764, mean reward: -0.006 [-0.019, 0.011], mean action: -0.035 [-1.148, 1.147], mean observation: 0.035 [-19.647, 18.115], loss: --, mean_squared_error: --, mean_q: --\n",
      "  386/2000: episode: 3, duration: 3.933s, episode steps: 128, steps per second: 33, episode reward: -0.755, mean reward: -0.006 [-0.019, 0.011], mean action: -0.028 [-1.289, 1.269], mean observation: 0.035 [-19.784, 18.152], loss: --, mean_squared_error: --, mean_q: --\n",
      "  515/2000: episode: 4, duration: 3.946s, episode steps: 129, steps per second: 33, episode reward: -0.764, mean reward: -0.006 [-0.019, 0.011], mean action: -0.036 [-1.184, 1.210], mean observation: 0.033 [-19.635, 18.380], loss: --, mean_squared_error: --, mean_q: --\n",
      "  644/2000: episode: 5, duration: 4.073s, episode steps: 129, steps per second: 32, episode reward: -0.763, mean reward: -0.006 [-0.019, 0.011], mean action: -0.044 [-1.204, 1.158], mean observation: 0.033 [-19.311, 18.256], loss: --, mean_squared_error: --, mean_q: --\n",
      "  773/2000: episode: 6, duration: 3.959s, episode steps: 129, steps per second: 33, episode reward: -0.764, mean reward: -0.006 [-0.019, 0.011], mean action: -0.046 [-1.182, 1.281], mean observation: 0.035 [-19.213, 18.387], loss: --, mean_squared_error: --, mean_q: --\n",
      "  902/2000: episode: 7, duration: 3.920s, episode steps: 129, steps per second: 33, episode reward: -0.765, mean reward: -0.006 [-0.019, 0.011], mean action: -0.040 [-1.141, 1.186], mean observation: 0.033 [-19.574, 17.998], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1031/2000: episode: 8, duration: 4.137s, episode steps: 129, steps per second: 31, episode reward: -0.757, mean reward: -0.006 [-0.019, 0.011], mean action: -0.036 [-1.211, 1.214], mean observation: 0.035 [-19.637, 18.083], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.506578\n",
      " 1167/2000: episode: 9, duration: 5.253s, episode steps: 136, steps per second: 26, episode reward: -0.800, mean reward: -0.006 [-0.019, 0.011], mean action: 0.024 [-1.101, 1.130], mean observation: 0.031 [-21.604, 20.205], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.518152\n",
      " 1297/2000: episode: 10, duration: 5.339s, episode steps: 130, steps per second: 24, episode reward: -0.800, mean reward: -0.006 [-0.018, 0.012], mean action: -0.004 [-1.153, 1.208], mean observation: 0.036 [-21.464, 18.578], loss: 0.000218, mean_squared_error: 0.000435, mean_q: 0.517094\n",
      " 1424/2000: episode: 11, duration: 5.434s, episode steps: 127, steps per second: 23, episode reward: -0.786, mean reward: -0.006 [-0.019, 0.012], mean action: 0.014 [-1.197, 1.186], mean observation: 0.029 [-21.990, 20.364], loss: 0.000068, mean_squared_error: 0.000135, mean_q: 0.514570\n",
      " 1553/2000: episode: 12, duration: 5.646s, episode steps: 129, steps per second: 23, episode reward: -0.784, mean reward: -0.006 [-0.018, 0.013], mean action: 0.028 [-1.235, 1.177], mean observation: 0.035 [-17.496, 18.071], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.511387\n",
      " 1685/2000: episode: 13, duration: 5.662s, episode steps: 132, steps per second: 23, episode reward: -0.796, mean reward: -0.006 [-0.018, 0.012], mean action: -0.011 [-1.381, 1.214], mean observation: 0.032 [-24.862, 16.325], loss: 0.000155, mean_squared_error: 0.000309, mean_q: 0.511385\n",
      " 1817/2000: episode: 14, duration: 5.402s, episode steps: 132, steps per second: 24, episode reward: -0.854, mean reward: -0.006 [-0.019, 0.011], mean action: -0.031 [-1.211, 1.338], mean observation: 0.048 [-21.288, 20.428], loss: 0.000082, mean_squared_error: 0.000165, mean_q: 0.520209\n",
      " 1952/2000: episode: 15, duration: 5.679s, episode steps: 135, steps per second: 24, episode reward: -0.849, mean reward: -0.006 [-0.019, 0.011], mean action: -0.027 [-1.209, 1.179], mean observation: 0.050 [-24.368, 20.118], loss: 0.000249, mean_squared_error: 0.000497, mean_q: 0.504480\n",
      "done, took 73.312 seconds\n",
      "\n",
      "\n",
      "iteration: 115\n",
      "Training for 2000 steps ...\n",
      "  146/2000: episode: 1, duration: 4.305s, episode steps: 146, steps per second: 34, episode reward: -0.873, mean reward: -0.006 [-0.019, 0.011], mean action: -0.002 [-1.131, 1.248], mean observation: 0.050 [-20.674, 20.176], loss: --, mean_squared_error: --, mean_q: --\n",
      "  290/2000: episode: 2, duration: 4.118s, episode steps: 144, steps per second: 35, episode reward: -0.849, mean reward: -0.006 [-0.019, 0.011], mean action: -0.022 [-1.251, 1.218], mean observation: 0.052 [-21.555, 20.225], loss: --, mean_squared_error: --, mean_q: --\n",
      "  433/2000: episode: 3, duration: 4.139s, episode steps: 143, steps per second: 35, episode reward: -0.839, mean reward: -0.006 [-0.019, 0.011], mean action: -0.038 [-1.195, 1.280], mean observation: 0.054 [-22.398, 20.254], loss: --, mean_squared_error: --, mean_q: --\n",
      "  577/2000: episode: 4, duration: 4.176s, episode steps: 144, steps per second: 34, episode reward: -0.855, mean reward: -0.006 [-0.019, 0.011], mean action: -0.029 [-1.221, 1.231], mean observation: 0.052 [-21.985, 20.249], loss: --, mean_squared_error: --, mean_q: --\n",
      "  720/2000: episode: 5, duration: 4.053s, episode steps: 143, steps per second: 35, episode reward: -0.839, mean reward: -0.006 [-0.019, 0.011], mean action: -0.024 [-1.170, 1.239], mean observation: 0.054 [-21.206, 20.504], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  865/2000: episode: 6, duration: 4.204s, episode steps: 145, steps per second: 34, episode reward: -0.860, mean reward: -0.006 [-0.019, 0.011], mean action: -0.012 [-1.138, 1.160], mean observation: 0.052 [-21.204, 20.269], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1008/2000: episode: 7, duration: 4.189s, episode steps: 143, steps per second: 34, episode reward: -0.843, mean reward: -0.006 [-0.019, 0.011], mean action: -0.015 [-1.122, 1.234], mean observation: 0.052 [-21.796, 20.456], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.502070\n",
      " 1153/2000: episode: 8, duration: 5.499s, episode steps: 145, steps per second: 26, episode reward: -0.851, mean reward: -0.006 [-0.019, 0.011], mean action: -0.041 [-1.224, 1.274], mean observation: 0.054 [-21.044, 20.409], loss: 0.000119, mean_squared_error: 0.000237, mean_q: 0.500857\n",
      " 1298/2000: episode: 9, duration: 5.595s, episode steps: 145, steps per second: 26, episode reward: -0.832, mean reward: -0.006 [-0.019, 0.011], mean action: -0.036 [-1.146, 1.145], mean observation: 0.056 [-22.028, 20.380], loss: 0.000158, mean_squared_error: 0.000315, mean_q: 0.509782\n",
      " 1446/2000: episode: 10, duration: 5.670s, episode steps: 148, steps per second: 26, episode reward: -0.822, mean reward: -0.006 [-0.019, 0.010], mean action: 0.032 [-1.128, 1.318], mean observation: 0.053 [-21.220, 18.415], loss: 0.000489, mean_squared_error: 0.000979, mean_q: 0.516123\n",
      " 1597/2000: episode: 11, duration: 5.757s, episode steps: 151, steps per second: 26, episode reward: -0.791, mean reward: -0.005 [-0.019, 0.011], mean action: -0.007 [-1.254, 1.177], mean observation: 0.039 [-22.640, 20.275], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.504932\n",
      " 1739/2000: episode: 12, duration: 5.846s, episode steps: 142, steps per second: 24, episode reward: -0.822, mean reward: -0.006 [-0.020, 0.011], mean action: 0.071 [-1.232, 1.238], mean observation: 0.026 [-24.879, 20.285], loss: 0.000154, mean_squared_error: 0.000308, mean_q: 0.516869\n",
      " 1908/2000: episode: 13, duration: 6.076s, episode steps: 169, steps per second: 28, episode reward: -0.844, mean reward: -0.005 [-0.018, 0.011], mean action: 0.011 [-1.274, 1.250], mean observation: 0.038 [-22.979, 20.121], loss: 0.000172, mean_squared_error: 0.000344, mean_q: 0.503407\n",
      "done, took 67.577 seconds\n",
      "\n",
      "\n",
      "iteration: 116\n",
      "Training for 2000 steps ...\n",
      "  176/2000: episode: 1, duration: 4.654s, episode steps: 176, steps per second: 38, episode reward: -0.816, mean reward: -0.005 [-0.018, 0.011], mean action: 0.045 [-1.250, 1.190], mean observation: 0.042 [-22.073, 20.275], loss: --, mean_squared_error: --, mean_q: --\n",
      "  351/2000: episode: 2, duration: 4.617s, episode steps: 175, steps per second: 38, episode reward: -0.813, mean reward: -0.005 [-0.018, 0.011], mean action: 0.056 [-1.143, 1.281], mean observation: 0.042 [-21.449, 20.033], loss: --, mean_squared_error: --, mean_q: --\n",
      "  527/2000: episode: 3, duration: 4.805s, episode steps: 176, steps per second: 37, episode reward: -0.807, mean reward: -0.005 [-0.018, 0.011], mean action: 0.059 [-1.249, 1.289], mean observation: 0.044 [-21.068, 20.346], loss: --, mean_squared_error: --, mean_q: --\n",
      "  702/2000: episode: 4, duration: 4.724s, episode steps: 175, steps per second: 37, episode reward: -0.815, mean reward: -0.005 [-0.018, 0.011], mean action: 0.062 [-1.213, 1.258], mean observation: 0.043 [-22.084, 20.102], loss: --, mean_squared_error: --, mean_q: --\n",
      "  878/2000: episode: 5, duration: 4.722s, episode steps: 176, steps per second: 37, episode reward: -0.819, mean reward: -0.005 [-0.018, 0.011], mean action: 0.042 [-1.256, 1.191], mean observation: 0.043 [-21.546, 20.149], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1054/2000: episode: 6, duration: 5.245s, episode steps: 176, steps per second: 34, episode reward: -0.816, mean reward: -0.005 [-0.018, 0.011], mean action: 0.066 [-1.156, 1.227], mean observation: 0.044 [-21.482, 20.269], loss: 0.000443, mean_squared_error: 0.000886, mean_q: 0.504138\n",
      " 1306/2000: episode: 7, duration: 10.489s, episode steps: 252, steps per second: 24, episode reward: 0.574, mean reward: 0.002 [-0.003, 0.011], mean action: 0.023 [-1.335, 1.173], mean observation: 0.111 [-23.837, 20.289], loss: 0.000186, mean_squared_error: 0.000372, mean_q: 0.512123\n",
      " 1427/2000: episode: 8, duration: 6.418s, episode steps: 121, steps per second: 19, episode reward: 0.546, mean reward: 0.005 [-0.001, 0.012], mean action: 0.115 [-1.117, 1.177], mean observation: 0.106 [-10.515, 20.230], loss: 0.000070, mean_squared_error: 0.000140, mean_q: 0.497177\n",
      " 1543/2000: episode: 9, duration: 6.278s, episode steps: 116, steps per second: 18, episode reward: 0.539, mean reward: 0.005 [-0.001, 0.012], mean action: 0.073 [-1.123, 1.228], mean observation: 0.106 [-10.933, 20.370], loss: 0.000110, mean_squared_error: 0.000219, mean_q: 0.500899\n",
      " 1659/2000: episode: 10, duration: 6.492s, episode steps: 116, steps per second: 18, episode reward: 0.504, mean reward: 0.004 [0.000, 0.012], mean action: 0.077 [-1.123, 1.207], mean observation: 0.104 [-15.867, 20.372], loss: 0.000061, mean_squared_error: 0.000123, mean_q: 0.497271\n",
      " 1779/2000: episode: 11, duration: 6.659s, episode steps: 120, steps per second: 18, episode reward: 0.521, mean reward: 0.004 [-0.000, 0.012], mean action: 0.047 [-1.219, 1.118], mean observation: 0.102 [-33.695, 20.537], loss: 0.000178, mean_squared_error: 0.000356, mean_q: 0.505969\n",
      " 1906/2000: episode: 12, duration: 6.754s, episode steps: 127, steps per second: 19, episode reward: 0.549, mean reward: 0.004 [-0.001, 0.011], mean action: 0.016 [-1.209, 1.180], mean observation: 0.105 [-34.374, 20.087], loss: 0.000115, mean_squared_error: 0.000230, mean_q: 0.504757\n",
      "done, took 76.634 seconds\n",
      "\n",
      "\n",
      "iteration: 117\n",
      "Training for 2000 steps ...\n",
      "  122/2000: episode: 1, duration: 5.501s, episode steps: 122, steps per second: 22, episode reward: 0.508, mean reward: 0.004 [-0.000, 0.011], mean action: 0.037 [-1.239, 1.176], mean observation: 0.112 [-16.156, 20.273], loss: --, mean_squared_error: --, mean_q: --\n",
      "  246/2000: episode: 2, duration: 5.773s, episode steps: 124, steps per second: 21, episode reward: 0.512, mean reward: 0.004 [-0.000, 0.011], mean action: 0.025 [-1.221, 1.104], mean observation: 0.113 [-16.174, 20.388], loss: --, mean_squared_error: --, mean_q: --\n",
      "  368/2000: episode: 3, duration: 5.742s, episode steps: 122, steps per second: 21, episode reward: 0.504, mean reward: 0.004 [0.000, 0.011], mean action: 0.036 [-1.113, 1.297], mean observation: 0.110 [-16.455, 20.152], loss: --, mean_squared_error: --, mean_q: --\n",
      "  493/2000: episode: 4, duration: 5.846s, episode steps: 125, steps per second: 21, episode reward: 0.508, mean reward: 0.004 [-0.000, 0.011], mean action: 0.049 [-1.225, 1.284], mean observation: 0.112 [-17.072, 20.450], loss: --, mean_squared_error: --, mean_q: --\n",
      "  614/2000: episode: 5, duration: 5.568s, episode steps: 121, steps per second: 22, episode reward: 0.534, mean reward: 0.004 [-0.000, 0.011], mean action: 0.023 [-1.140, 1.263], mean observation: 0.111 [-16.094, 20.357], loss: --, mean_squared_error: --, mean_q: --\n",
      "  737/2000: episode: 6, duration: 5.790s, episode steps: 123, steps per second: 21, episode reward: 0.502, mean reward: 0.004 [0.000, 0.011], mean action: 0.016 [-1.211, 1.104], mean observation: 0.113 [-15.616, 20.369], loss: --, mean_squared_error: --, mean_q: --\n",
      "  859/2000: episode: 7, duration: 5.679s, episode steps: 122, steps per second: 21, episode reward: 0.491, mean reward: 0.004 [-0.000, 0.011], mean action: 0.023 [-1.213, 1.191], mean observation: 0.112 [-16.183, 20.333], loss: --, mean_squared_error: --, mean_q: --\n",
      "  983/2000: episode: 8, duration: 5.764s, episode steps: 124, steps per second: 22, episode reward: 0.491, mean reward: 0.004 [-0.000, 0.011], mean action: 0.046 [-1.117, 1.156], mean observation: 0.113 [-16.295, 19.849], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1106/2000: episode: 9, duration: 6.757s, episode steps: 123, steps per second: 18, episode reward: 0.514, mean reward: 0.004 [0.000, 0.011], mean action: 0.008 [-1.149, 1.146], mean observation: 0.109 [-15.793, 20.394], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.508433\n",
      " 1219/2000: episode: 10, duration: 6.493s, episode steps: 113, steps per second: 17, episode reward: 0.529, mean reward: 0.005 [-0.000, 0.011], mean action: -0.016 [-1.164, 1.184], mean observation: 0.108 [-15.924, 20.288], loss: 0.000486, mean_squared_error: 0.000972, mean_q: 0.507108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1327/2000: episode: 11, duration: 6.068s, episode steps: 108, steps per second: 18, episode reward: 0.542, mean reward: 0.005 [-0.001, 0.011], mean action: -0.003 [-1.162, 1.169], mean observation: 0.107 [-33.769, 20.171], loss: 0.000159, mean_squared_error: 0.000319, mean_q: 0.505943\n",
      " 1437/2000: episode: 12, duration: 6.229s, episode steps: 110, steps per second: 18, episode reward: 0.546, mean reward: 0.005 [-0.001, 0.011], mean action: 0.023 [-1.126, 1.106], mean observation: 0.103 [-13.237, 20.251], loss: 0.000077, mean_squared_error: 0.000153, mean_q: 0.509858\n",
      " 1548/2000: episode: 13, duration: 5.881s, episode steps: 111, steps per second: 19, episode reward: 0.535, mean reward: 0.005 [-0.000, 0.011], mean action: -0.011 [-1.177, 1.146], mean observation: 0.107 [-13.808, 20.247], loss: 0.000054, mean_squared_error: 0.000107, mean_q: 0.503434\n",
      " 1661/2000: episode: 14, duration: 6.784s, episode steps: 113, steps per second: 17, episode reward: 0.413, mean reward: 0.004 [-0.000, 0.011], mean action: -0.034 [-1.204, 1.205], mean observation: 0.099 [-44.550, 20.349], loss: 0.000055, mean_squared_error: 0.000111, mean_q: 0.502644\n",
      " 1776/2000: episode: 15, duration: 6.710s, episode steps: 115, steps per second: 17, episode reward: 0.451, mean reward: 0.004 [-0.001, 0.011], mean action: -0.022 [-1.250, 1.190], mean observation: 0.104 [-44.068, 20.352], loss: 0.000070, mean_squared_error: 0.000139, mean_q: 0.506757\n",
      " 1889/2000: episode: 16, duration: 6.485s, episode steps: 113, steps per second: 17, episode reward: 0.516, mean reward: 0.005 [-0.001, 0.011], mean action: 0.003 [-1.259, 1.162], mean observation: 0.103 [-35.567, 20.476], loss: 0.000203, mean_squared_error: 0.000406, mean_q: 0.503282\n",
      " 1996/2000: episode: 17, duration: 5.889s, episode steps: 107, steps per second: 18, episode reward: 0.524, mean reward: 0.005 [-0.001, 0.010], mean action: 0.018 [-1.155, 1.115], mean observation: 0.105 [-31.326, 20.146], loss: 0.000153, mean_squared_error: 0.000307, mean_q: 0.507986\n",
      "done, took 103.291 seconds\n",
      "\n",
      "\n",
      "iteration: 118\n",
      "Training for 2000 steps ...\n",
      "  113/2000: episode: 1, duration: 5.123s, episode steps: 113, steps per second: 22, episode reward: 0.510, mean reward: 0.005 [-0.002, 0.011], mean action: 0.010 [-1.219, 1.144], mean observation: 0.103 [-45.237, 20.366], loss: --, mean_squared_error: --, mean_q: --\n",
      "  229/2000: episode: 2, duration: 5.386s, episode steps: 116, steps per second: 22, episode reward: 0.504, mean reward: 0.004 [-0.002, 0.011], mean action: 0.003 [-1.150, 1.179], mean observation: 0.101 [-41.881, 20.359], loss: --, mean_squared_error: --, mean_q: --\n",
      "  342/2000: episode: 3, duration: 5.169s, episode steps: 113, steps per second: 22, episode reward: 0.524, mean reward: 0.005 [-0.002, 0.011], mean action: 0.023 [-1.180, 1.196], mean observation: 0.103 [-38.437, 20.217], loss: --, mean_squared_error: --, mean_q: --\n",
      "  457/2000: episode: 4, duration: 5.505s, episode steps: 115, steps per second: 21, episode reward: 0.498, mean reward: 0.004 [-0.002, 0.010], mean action: -0.004 [-1.192, 1.154], mean observation: 0.106 [-33.716, 20.260], loss: --, mean_squared_error: --, mean_q: --\n",
      "  570/2000: episode: 5, duration: 5.102s, episode steps: 113, steps per second: 22, episode reward: 0.526, mean reward: 0.005 [-0.002, 0.011], mean action: 0.020 [-1.157, 1.132], mean observation: 0.109 [-14.532, 20.362], loss: --, mean_squared_error: --, mean_q: --\n",
      "  683/2000: episode: 6, duration: 5.054s, episode steps: 113, steps per second: 22, episode reward: 0.532, mean reward: 0.005 [-0.002, 0.011], mean action: 0.032 [-1.136, 1.197], mean observation: 0.101 [-35.691, 20.404], loss: --, mean_squared_error: --, mean_q: --\n",
      "  799/2000: episode: 7, duration: 5.707s, episode steps: 116, steps per second: 20, episode reward: 0.496, mean reward: 0.004 [-0.002, 0.011], mean action: 0.022 [-1.114, 1.164], mean observation: 0.101 [-45.450, 20.370], loss: --, mean_squared_error: --, mean_q: --\n",
      "  914/2000: episode: 8, duration: 5.536s, episode steps: 115, steps per second: 21, episode reward: 0.483, mean reward: 0.004 [-0.002, 0.010], mean action: -0.014 [-1.230, 1.274], mean observation: 0.107 [-42.740, 20.183], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1027/2000: episode: 9, duration: 5.459s, episode steps: 113, steps per second: 21, episode reward: 0.531, mean reward: 0.005 [-0.002, 0.011], mean action: 0.021 [-1.111, 1.289], mean observation: 0.106 [-25.119, 20.221], loss: 0.000113, mean_squared_error: 0.000227, mean_q: 0.511453\n",
      " 1141/2000: episode: 10, duration: 6.172s, episode steps: 114, steps per second: 18, episode reward: 0.560, mean reward: 0.005 [-0.002, 0.012], mean action: 0.043 [-1.127, 1.153], mean observation: 0.112 [-14.621, 20.484], loss: 0.000201, mean_squared_error: 0.000402, mean_q: 0.503029\n",
      " 1246/2000: episode: 11, duration: 5.650s, episode steps: 105, steps per second: 19, episode reward: 0.511, mean reward: 0.005 [-0.002, 0.011], mean action: 0.058 [-1.156, 1.286], mean observation: 0.134 [-30.786, 20.175], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.506486\n",
      " 1355/2000: episode: 12, duration: 5.801s, episode steps: 109, steps per second: 19, episode reward: 0.450, mean reward: 0.004 [-0.001, 0.010], mean action: 0.013 [-1.134, 1.211], mean observation: 0.139 [-16.263, 20.112], loss: 0.000262, mean_squared_error: 0.000525, mean_q: 0.503213\n",
      " 1466/2000: episode: 13, duration: 5.638s, episode steps: 111, steps per second: 20, episode reward: 0.492, mean reward: 0.004 [-0.002, 0.011], mean action: 0.011 [-1.142, 1.227], mean observation: 0.141 [-15.908, 20.390], loss: 0.000107, mean_squared_error: 0.000215, mean_q: 0.518514\n",
      " 1574/2000: episode: 14, duration: 5.683s, episode steps: 108, steps per second: 19, episode reward: 0.464, mean reward: 0.004 [-0.001, 0.011], mean action: -0.006 [-1.197, 1.202], mean observation: 0.139 [-20.889, 20.283], loss: 0.000069, mean_squared_error: 0.000139, mean_q: 0.504990\n",
      " 1683/2000: episode: 15, duration: 5.613s, episode steps: 109, steps per second: 19, episode reward: 0.470, mean reward: 0.004 [-0.001, 0.011], mean action: -0.003 [-1.217, 1.154], mean observation: 0.141 [-14.828, 20.317], loss: 0.000144, mean_squared_error: 0.000288, mean_q: 0.517510\n",
      " 1792/2000: episode: 16, duration: 5.538s, episode steps: 109, steps per second: 20, episode reward: 0.461, mean reward: 0.004 [-0.001, 0.012], mean action: 0.004 [-1.134, 1.171], mean observation: 0.144 [-16.012, 20.354], loss: 0.000168, mean_squared_error: 0.000336, mean_q: 0.510391\n",
      " 1898/2000: episode: 17, duration: 5.320s, episode steps: 106, steps per second: 20, episode reward: 0.450, mean reward: 0.004 [-0.001, 0.011], mean action: -0.024 [-1.101, 1.192], mean observation: 0.143 [-14.805, 20.307], loss: 0.000239, mean_squared_error: 0.000479, mean_q: 0.518152\n",
      " 2000/2000: episode: 18, duration: 5.105s, episode steps: 102, steps per second: 20, episode reward: 0.436, mean reward: 0.004 [-0.001, 0.010], mean action: -0.014 [-1.215, 1.219], mean observation: 0.142 [-20.224, 20.446], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.506743\n",
      "done, took 98.585 seconds\n",
      "\n",
      "\n",
      "iteration: 119\n",
      "Training for 2000 steps ...\n",
      "   99/2000: episode: 1, duration: 4.189s, episode steps: 99, steps per second: 24, episode reward: 0.432, mean reward: 0.004 [-0.001, 0.010], mean action: -0.002 [-1.131, 1.197], mean observation: 0.140 [-21.920, 19.915], loss: --, mean_squared_error: --, mean_q: --\n",
      "  198/2000: episode: 2, duration: 4.246s, episode steps: 99, steps per second: 23, episode reward: 0.425, mean reward: 0.004 [-0.001, 0.011], mean action: -0.006 [-1.114, 1.256], mean observation: 0.143 [-22.159, 20.097], loss: --, mean_squared_error: --, mean_q: --\n",
      "  298/2000: episode: 3, duration: 4.264s, episode steps: 100, steps per second: 23, episode reward: 0.431, mean reward: 0.004 [-0.001, 0.011], mean action: 0.007 [-1.242, 1.147], mean observation: 0.143 [-20.962, 20.137], loss: --, mean_squared_error: --, mean_q: --\n",
      "  397/2000: episode: 4, duration: 4.258s, episode steps: 99, steps per second: 23, episode reward: 0.429, mean reward: 0.004 [-0.001, 0.011], mean action: -0.000 [-1.174, 1.197], mean observation: 0.144 [-19.444, 19.703], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  499/2000: episode: 5, duration: 4.369s, episode steps: 102, steps per second: 23, episode reward: 0.454, mean reward: 0.004 [-0.001, 0.011], mean action: 0.005 [-1.134, 1.186], mean observation: 0.142 [-21.920, 19.911], loss: --, mean_squared_error: --, mean_q: --\n",
      "  598/2000: episode: 6, duration: 4.241s, episode steps: 99, steps per second: 23, episode reward: 0.451, mean reward: 0.005 [-0.001, 0.011], mean action: -0.014 [-1.276, 1.147], mean observation: 0.140 [-22.468, 19.882], loss: --, mean_squared_error: --, mean_q: --\n",
      "  696/2000: episode: 7, duration: 4.200s, episode steps: 98, steps per second: 23, episode reward: 0.416, mean reward: 0.004 [-0.001, 0.011], mean action: -0.006 [-1.205, 1.152], mean observation: 0.143 [-16.593, 19.638], loss: --, mean_squared_error: --, mean_q: --\n",
      "  795/2000: episode: 8, duration: 4.350s, episode steps: 99, steps per second: 23, episode reward: 0.422, mean reward: 0.004 [-0.001, 0.011], mean action: -0.015 [-1.245, 1.185], mean observation: 0.142 [-22.171, 19.945], loss: --, mean_squared_error: --, mean_q: --\n",
      "  897/2000: episode: 9, duration: 4.452s, episode steps: 102, steps per second: 23, episode reward: 0.451, mean reward: 0.004 [-0.001, 0.011], mean action: 0.005 [-1.192, 1.246], mean observation: 0.140 [-21.834, 19.904], loss: --, mean_squared_error: --, mean_q: --\n",
      "  996/2000: episode: 10, duration: 4.371s, episode steps: 99, steps per second: 23, episode reward: 0.449, mean reward: 0.005 [-0.001, 0.011], mean action: -0.017 [-1.171, 1.117], mean observation: 0.138 [-22.360, 19.912], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1095/2000: episode: 11, duration: 5.225s, episode steps: 99, steps per second: 19, episode reward: 0.459, mean reward: 0.005 [-0.001, 0.011], mean action: 0.013 [-1.132, 1.147], mean observation: 0.138 [-21.681, 19.604], loss: 0.000057, mean_squared_error: 0.000114, mean_q: 0.509623\n",
      " 1194/2000: episode: 12, duration: 4.969s, episode steps: 99, steps per second: 20, episode reward: 0.443, mean reward: 0.004 [-0.001, 0.010], mean action: -0.009 [-1.147, 1.152], mean observation: 0.136 [-14.884, 15.277], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.505379\n",
      " 1294/2000: episode: 13, duration: 5.030s, episode steps: 100, steps per second: 20, episode reward: 0.447, mean reward: 0.004 [-0.001, 0.010], mean action: -0.013 [-1.174, 1.117], mean observation: 0.136 [-10.606, 15.215], loss: 0.000197, mean_squared_error: 0.000394, mean_q: 0.512108\n",
      " 1397/2000: episode: 14, duration: 5.050s, episode steps: 103, steps per second: 20, episode reward: 0.399, mean reward: 0.004 [-0.001, 0.010], mean action: -0.068 [-1.126, 1.165], mean observation: 0.137 [-10.774, 15.231], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.513719\n",
      " 1493/2000: episode: 15, duration: 4.761s, episode steps: 96, steps per second: 20, episode reward: 0.379, mean reward: 0.004 [-0.001, 0.010], mean action: -0.064 [-1.115, 1.100], mean observation: 0.133 [-13.851, 15.456], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.513069\n",
      " 1593/2000: episode: 16, duration: 4.919s, episode steps: 100, steps per second: 20, episode reward: 0.428, mean reward: 0.004 [-0.001, 0.010], mean action: -0.058 [-1.185, 1.136], mean observation: 0.141 [-12.889, 15.207], loss: 0.000148, mean_squared_error: 0.000295, mean_q: 0.511038\n",
      " 1697/2000: episode: 17, duration: 5.321s, episode steps: 104, steps per second: 20, episode reward: 0.446, mean reward: 0.004 [-0.001, 0.010], mean action: -0.051 [-1.226, 1.101], mean observation: 0.137 [-27.614, 15.225], loss: 0.000492, mean_squared_error: 0.000983, mean_q: 0.508939\n",
      " 1794/2000: episode: 18, duration: 5.242s, episode steps: 97, steps per second: 19, episode reward: 0.412, mean reward: 0.004 [-0.001, 0.011], mean action: -0.046 [-1.172, 1.154], mean observation: 0.138 [-34.667, 14.986], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.516661\n",
      " 1899/2000: episode: 19, duration: 5.518s, episode steps: 105, steps per second: 19, episode reward: 0.462, mean reward: 0.004 [-0.001, 0.010], mean action: -0.099 [-1.226, 1.211], mean observation: 0.138 [-18.530, 16.086], loss: 0.000091, mean_squared_error: 0.000181, mean_q: 0.512214\n",
      " 2000/2000: episode: 20, duration: 4.952s, episode steps: 101, steps per second: 20, episode reward: 0.465, mean reward: 0.005 [-0.001, 0.010], mean action: -0.105 [-1.198, 1.187], mean observation: 0.140 [-16.924, 15.723], loss: 0.000233, mean_squared_error: 0.000466, mean_q: 0.501839\n",
      "done, took 93.954 seconds\n",
      "\n",
      "\n",
      "iteration: 120\n",
      "Training for 2000 steps ...\n",
      "  101/2000: episode: 1, duration: 4.149s, episode steps: 101, steps per second: 24, episode reward: 0.458, mean reward: 0.005 [-0.001, 0.010], mean action: -0.114 [-1.188, 1.155], mean observation: 0.140 [-14.724, 15.618], loss: --, mean_squared_error: --, mean_q: --\n",
      "  204/2000: episode: 2, duration: 4.173s, episode steps: 103, steps per second: 25, episode reward: 0.478, mean reward: 0.005 [-0.001, 0.010], mean action: -0.097 [-1.156, 1.184], mean observation: 0.142 [-13.842, 15.935], loss: --, mean_squared_error: --, mean_q: --\n",
      "  307/2000: episode: 3, duration: 4.142s, episode steps: 103, steps per second: 25, episode reward: 0.478, mean reward: 0.005 [-0.001, 0.010], mean action: -0.083 [-1.148, 1.134], mean observation: 0.142 [-13.166, 15.994], loss: --, mean_squared_error: --, mean_q: --\n",
      "  410/2000: episode: 4, duration: 4.197s, episode steps: 103, steps per second: 25, episode reward: 0.469, mean reward: 0.005 [-0.001, 0.010], mean action: -0.094 [-1.241, 1.204], mean observation: 0.140 [-13.819, 15.865], loss: --, mean_squared_error: --, mean_q: --\n",
      "  513/2000: episode: 5, duration: 4.192s, episode steps: 103, steps per second: 25, episode reward: 0.481, mean reward: 0.005 [-0.001, 0.010], mean action: -0.105 [-1.210, 1.130], mean observation: 0.141 [-14.216, 15.871], loss: --, mean_squared_error: --, mean_q: --\n",
      "  615/2000: episode: 6, duration: 4.130s, episode steps: 102, steps per second: 25, episode reward: 0.472, mean reward: 0.005 [-0.001, 0.010], mean action: -0.116 [-1.232, 1.107], mean observation: 0.140 [-19.666, 15.880], loss: --, mean_squared_error: --, mean_q: --\n",
      "  717/2000: episode: 7, duration: 4.101s, episode steps: 102, steps per second: 25, episode reward: 0.465, mean reward: 0.005 [-0.001, 0.010], mean action: -0.103 [-1.121, 1.109], mean observation: 0.140 [-14.337, 15.854], loss: --, mean_squared_error: --, mean_q: --\n",
      "  820/2000: episode: 8, duration: 4.143s, episode steps: 103, steps per second: 25, episode reward: 0.477, mean reward: 0.005 [-0.001, 0.010], mean action: -0.079 [-1.178, 1.246], mean observation: 0.140 [-15.293, 15.993], loss: --, mean_squared_error: --, mean_q: --\n",
      "  922/2000: episode: 9, duration: 4.129s, episode steps: 102, steps per second: 25, episode reward: 0.467, mean reward: 0.005 [-0.001, 0.010], mean action: -0.106 [-1.157, 1.213], mean observation: 0.141 [-14.038, 15.717], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1025/2000: episode: 10, duration: 4.415s, episode steps: 103, steps per second: 23, episode reward: 0.473, mean reward: 0.005 [-0.001, 0.010], mean action: -0.088 [-1.119, 1.171], mean observation: 0.141 [-13.828, 15.910], loss: 0.000107, mean_squared_error: 0.000215, mean_q: 0.518265\n",
      " 1128/2000: episode: 11, duration: 5.224s, episode steps: 103, steps per second: 20, episode reward: 0.473, mean reward: 0.005 [-0.001, 0.010], mean action: -0.118 [-1.156, 1.151], mean observation: 0.141 [-16.231, 15.789], loss: 0.000065, mean_squared_error: 0.000129, mean_q: 0.516881\n",
      " 1225/2000: episode: 12, duration: 4.763s, episode steps: 97, steps per second: 20, episode reward: 0.424, mean reward: 0.004 [-0.001, 0.010], mean action: -0.150 [-1.211, 1.092], mean observation: 0.137 [-28.292, 15.739], loss: 0.000158, mean_squared_error: 0.000315, mean_q: 0.511397\n",
      " 1334/2000: episode: 13, duration: 5.546s, episode steps: 109, steps per second: 20, episode reward: 0.525, mean reward: 0.005 [-0.001, 0.011], mean action: -0.097 [-1.346, 1.250], mean observation: 0.142 [-12.601, 15.825], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.521233\n",
      " 1434/2000: episode: 14, duration: 4.730s, episode steps: 100, steps per second: 21, episode reward: 0.422, mean reward: 0.004 [-0.001, 0.010], mean action: -0.139 [-1.176, 1.160], mean observation: 0.134 [-41.149, 15.935], loss: 0.000072, mean_squared_error: 0.000144, mean_q: 0.513963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1533/2000: episode: 15, duration: 4.790s, episode steps: 99, steps per second: 21, episode reward: 0.429, mean reward: 0.004 [-0.001, 0.010], mean action: -0.085 [-1.074, 1.174], mean observation: 0.135 [-10.754, 15.699], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.508624\n",
      " 1639/2000: episode: 16, duration: 5.475s, episode steps: 106, steps per second: 19, episode reward: 0.464, mean reward: 0.004 [-0.001, 0.010], mean action: -0.082 [-1.174, 1.180], mean observation: 0.137 [-34.501, 15.761], loss: 0.000256, mean_squared_error: 0.000513, mean_q: 0.508054\n",
      " 1749/2000: episode: 17, duration: 5.572s, episode steps: 110, steps per second: 20, episode reward: 0.480, mean reward: 0.004 [-0.001, 0.010], mean action: -0.102 [-1.227, 1.128], mean observation: 0.138 [-10.762, 15.748], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.505594\n",
      " 1859/2000: episode: 18, duration: 5.669s, episode steps: 110, steps per second: 19, episode reward: 0.492, mean reward: 0.004 [-0.001, 0.010], mean action: -0.127 [-1.321, 1.213], mean observation: 0.141 [-46.822, 15.148], loss: 0.000087, mean_squared_error: 0.000175, mean_q: 0.507313\n",
      " 1971/2000: episode: 19, duration: 5.582s, episode steps: 112, steps per second: 20, episode reward: 0.468, mean reward: 0.004 [-0.001, 0.010], mean action: -0.138 [-1.202, 1.217], mean observation: 0.137 [-10.314, 14.807], loss: 0.000066, mean_squared_error: 0.000131, mean_q: 0.514882\n",
      "done, took 91.023 seconds\n",
      "\n",
      "\n",
      "iteration: 121\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 4.605s, episode steps: 105, steps per second: 23, episode reward: 0.464, mean reward: 0.004 [-0.001, 0.010], mean action: -0.129 [-1.182, 1.178], mean observation: 0.144 [-44.380, 15.309], loss: --, mean_squared_error: --, mean_q: --\n",
      "  210/2000: episode: 2, duration: 4.400s, episode steps: 105, steps per second: 24, episode reward: 0.474, mean reward: 0.005 [-0.001, 0.010], mean action: -0.151 [-1.159, 1.143], mean observation: 0.144 [-46.174, 15.268], loss: --, mean_squared_error: --, mean_q: --\n",
      "  316/2000: episode: 3, duration: 4.554s, episode steps: 106, steps per second: 23, episode reward: 0.481, mean reward: 0.005 [-0.001, 0.010], mean action: -0.130 [-1.190, 1.175], mean observation: 0.142 [-44.682, 15.114], loss: --, mean_squared_error: --, mean_q: --\n",
      "  421/2000: episode: 4, duration: 4.499s, episode steps: 105, steps per second: 23, episode reward: 0.475, mean reward: 0.005 [-0.001, 0.010], mean action: -0.151 [-1.174, 1.201], mean observation: 0.139 [-43.894, 15.407], loss: --, mean_squared_error: --, mean_q: --\n",
      "  525/2000: episode: 5, duration: 4.316s, episode steps: 104, steps per second: 24, episode reward: 0.463, mean reward: 0.004 [-0.001, 0.010], mean action: -0.120 [-1.250, 1.233], mean observation: 0.139 [-10.432, 15.135], loss: --, mean_squared_error: --, mean_q: --\n",
      "  629/2000: episode: 6, duration: 4.385s, episode steps: 104, steps per second: 24, episode reward: 0.471, mean reward: 0.005 [-0.001, 0.010], mean action: -0.160 [-1.212, 1.155], mean observation: 0.133 [-41.142, 15.428], loss: --, mean_squared_error: --, mean_q: --\n",
      "  736/2000: episode: 7, duration: 4.672s, episode steps: 107, steps per second: 23, episode reward: 0.490, mean reward: 0.005 [-0.001, 0.010], mean action: -0.126 [-1.223, 1.138], mean observation: 0.140 [-39.189, 15.227], loss: --, mean_squared_error: --, mean_q: --\n",
      "  841/2000: episode: 8, duration: 4.404s, episode steps: 105, steps per second: 24, episode reward: 0.480, mean reward: 0.005 [-0.001, 0.010], mean action: -0.132 [-1.198, 1.173], mean observation: 0.142 [-44.246, 15.298], loss: --, mean_squared_error: --, mean_q: --\n",
      "  945/2000: episode: 9, duration: 4.534s, episode steps: 104, steps per second: 23, episode reward: 0.466, mean reward: 0.004 [-0.001, 0.010], mean action: -0.127 [-1.178, 1.142], mean observation: 0.140 [-45.004, 15.313], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1049/2000: episode: 10, duration: 5.048s, episode steps: 104, steps per second: 21, episode reward: 0.459, mean reward: 0.004 [-0.001, 0.010], mean action: -0.139 [-1.201, 1.241], mean observation: 0.137 [-38.064, 15.001], loss: 0.000107, mean_squared_error: 0.000213, mean_q: 0.527803\n",
      " 1147/2000: episode: 11, duration: 5.210s, episode steps: 98, steps per second: 19, episode reward: 0.435, mean reward: 0.004 [-0.001, 0.010], mean action: -0.158 [-1.215, 1.089], mean observation: 0.132 [-46.049, 14.913], loss: 0.000199, mean_squared_error: 0.000398, mean_q: 0.517784\n",
      " 1253/2000: episode: 12, duration: 5.511s, episode steps: 106, steps per second: 19, episode reward: 0.480, mean reward: 0.005 [-0.001, 0.011], mean action: -0.204 [-1.223, 1.186], mean observation: 0.135 [-33.199, 15.724], loss: 0.000213, mean_squared_error: 0.000426, mean_q: 0.506535\n",
      " 1356/2000: episode: 13, duration: 5.298s, episode steps: 103, steps per second: 19, episode reward: 0.446, mean reward: 0.004 [-0.001, 0.011], mean action: -0.202 [-1.236, 1.198], mean observation: 0.133 [-40.942, 15.518], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.504124\n",
      " 1457/2000: episode: 14, duration: 5.074s, episode steps: 101, steps per second: 20, episode reward: 0.444, mean reward: 0.004 [-0.001, 0.011], mean action: -0.219 [-1.204, 1.189], mean observation: 0.137 [-43.027, 15.247], loss: 0.000074, mean_squared_error: 0.000149, mean_q: 0.524225\n",
      " 1557/2000: episode: 15, duration: 4.995s, episode steps: 100, steps per second: 20, episode reward: 0.420, mean reward: 0.004 [-0.001, 0.011], mean action: -0.199 [-1.163, 1.200], mean observation: 0.131 [-39.865, 15.082], loss: 0.000134, mean_squared_error: 0.000267, mean_q: 0.518357\n",
      " 1662/2000: episode: 16, duration: 5.057s, episode steps: 105, steps per second: 21, episode reward: 0.438, mean reward: 0.004 [-0.001, 0.010], mean action: -0.192 [-1.219, 1.165], mean observation: 0.136 [-41.540, 15.739], loss: 0.000115, mean_squared_error: 0.000230, mean_q: 0.508196\n",
      " 1767/2000: episode: 17, duration: 5.252s, episode steps: 105, steps per second: 20, episode reward: 0.454, mean reward: 0.004 [-0.001, 0.010], mean action: -0.215 [-1.219, 1.164], mean observation: 0.133 [-41.398, 15.380], loss: 0.000124, mean_squared_error: 0.000249, mean_q: 0.518127\n",
      " 1873/2000: episode: 18, duration: 5.396s, episode steps: 106, steps per second: 20, episode reward: 0.446, mean reward: 0.004 [-0.001, 0.011], mean action: -0.188 [-1.197, 1.196], mean observation: 0.134 [-40.164, 15.477], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.524669\n",
      " 1979/2000: episode: 19, duration: 5.337s, episode steps: 106, steps per second: 20, episode reward: 0.458, mean reward: 0.004 [-0.001, 0.011], mean action: -0.184 [-1.106, 1.240], mean observation: 0.132 [-40.414, 15.597], loss: 0.000127, mean_squared_error: 0.000253, mean_q: 0.520413\n",
      "done, took 94.076 seconds\n",
      "\n",
      "\n",
      "iteration: 122\n",
      "Training for 2000 steps ...\n",
      "  105/2000: episode: 1, duration: 4.332s, episode steps: 105, steps per second: 24, episode reward: 0.440, mean reward: 0.004 [-0.001, 0.011], mean action: -0.201 [-1.241, 1.206], mean observation: 0.141 [-41.832, 15.655], loss: --, mean_squared_error: --, mean_q: --\n",
      "  208/2000: episode: 2, duration: 4.199s, episode steps: 103, steps per second: 25, episode reward: 0.426, mean reward: 0.004 [-0.001, 0.011], mean action: -0.195 [-1.254, 1.204], mean observation: 0.128 [-41.815, 15.777], loss: --, mean_squared_error: --, mean_q: --\n",
      "  311/2000: episode: 3, duration: 4.233s, episode steps: 103, steps per second: 24, episode reward: 0.424, mean reward: 0.004 [-0.001, 0.011], mean action: -0.222 [-1.183, 1.145], mean observation: 0.129 [-41.686, 15.539], loss: --, mean_squared_error: --, mean_q: --\n",
      "  416/2000: episode: 4, duration: 4.310s, episode steps: 105, steps per second: 24, episode reward: 0.445, mean reward: 0.004 [-0.001, 0.011], mean action: -0.209 [-1.201, 1.210], mean observation: 0.130 [-41.823, 15.747], loss: --, mean_squared_error: --, mean_q: --\n",
      "  521/2000: episode: 5, duration: 4.260s, episode steps: 105, steps per second: 25, episode reward: 0.437, mean reward: 0.004 [-0.001, 0.011], mean action: -0.198 [-1.196, 1.250], mean observation: 0.135 [-40.909, 15.696], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  625/2000: episode: 6, duration: 4.269s, episode steps: 104, steps per second: 24, episode reward: 0.431, mean reward: 0.004 [-0.001, 0.011], mean action: -0.194 [-1.186, 1.121], mean observation: 0.135 [-41.320, 15.852], loss: --, mean_squared_error: --, mean_q: --\n",
      "  728/2000: episode: 7, duration: 4.185s, episode steps: 103, steps per second: 25, episode reward: 0.428, mean reward: 0.004 [-0.001, 0.011], mean action: -0.189 [-1.152, 1.175], mean observation: 0.137 [-40.152, 15.616], loss: --, mean_squared_error: --, mean_q: --\n",
      "  832/2000: episode: 8, duration: 4.216s, episode steps: 104, steps per second: 25, episode reward: 0.435, mean reward: 0.004 [-0.001, 0.011], mean action: -0.221 [-1.274, 1.169], mean observation: 0.132 [-40.931, 15.897], loss: --, mean_squared_error: --, mean_q: --\n",
      "  940/2000: episode: 9, duration: 4.421s, episode steps: 108, steps per second: 24, episode reward: 0.476, mean reward: 0.004 [-0.001, 0.011], mean action: -0.226 [-1.196, 1.147], mean observation: 0.133 [-41.016, 15.862], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1044/2000: episode: 10, duration: 4.869s, episode steps: 104, steps per second: 21, episode reward: 0.434, mean reward: 0.004 [-0.001, 0.011], mean action: -0.205 [-1.170, 1.190], mean observation: 0.131 [-41.031, 15.550], loss: 0.000191, mean_squared_error: 0.000382, mean_q: 0.521776\n",
      " 1145/2000: episode: 11, duration: 5.117s, episode steps: 101, steps per second: 20, episode reward: 0.443, mean reward: 0.004 [-0.001, 0.011], mean action: -0.202 [-1.138, 1.123], mean observation: 0.135 [-39.214, 15.357], loss: 0.000066, mean_squared_error: 0.000132, mean_q: 0.519380\n",
      " 1247/2000: episode: 12, duration: 5.125s, episode steps: 102, steps per second: 20, episode reward: 0.433, mean reward: 0.004 [-0.001, 0.011], mean action: -0.199 [-1.156, 1.150], mean observation: 0.131 [-41.316, 15.696], loss: 0.000067, mean_squared_error: 0.000135, mean_q: 0.522430\n",
      " 1346/2000: episode: 13, duration: 4.961s, episode steps: 99, steps per second: 20, episode reward: 0.420, mean reward: 0.004 [-0.001, 0.011], mean action: -0.202 [-1.152, 1.157], mean observation: 0.136 [-41.636, 15.607], loss: 0.000222, mean_squared_error: 0.000443, mean_q: 0.520009\n",
      " 1444/2000: episode: 14, duration: 4.845s, episode steps: 98, steps per second: 20, episode reward: 0.419, mean reward: 0.004 [-0.001, 0.011], mean action: -0.222 [-1.156, 1.163], mean observation: 0.133 [-41.856, 15.478], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.520411\n",
      " 1548/2000: episode: 15, duration: 5.009s, episode steps: 104, steps per second: 21, episode reward: 0.440, mean reward: 0.004 [-0.001, 0.010], mean action: -0.198 [-1.174, 1.235], mean observation: 0.136 [-40.802, 15.635], loss: 0.000081, mean_squared_error: 0.000163, mean_q: 0.518697\n",
      " 1649/2000: episode: 16, duration: 5.043s, episode steps: 101, steps per second: 20, episode reward: 0.427, mean reward: 0.004 [-0.001, 0.010], mean action: -0.247 [-1.109, 1.171], mean observation: 0.129 [-41.725, 15.694], loss: 0.000401, mean_squared_error: 0.000801, mean_q: 0.528857\n",
      " 1747/2000: episode: 17, duration: 4.724s, episode steps: 98, steps per second: 21, episode reward: 0.416, mean reward: 0.004 [-0.001, 0.011], mean action: -0.303 [-1.163, 1.141], mean observation: 0.135 [-13.568, 15.607], loss: 0.000292, mean_squared_error: 0.000584, mean_q: 0.518050\n",
      " 1851/2000: episode: 18, duration: 5.220s, episode steps: 104, steps per second: 20, episode reward: 0.438, mean reward: 0.004 [-0.001, 0.011], mean action: -0.211 [-1.159, 1.211], mean observation: 0.136 [-40.460, 16.033], loss: 0.000093, mean_squared_error: 0.000185, mean_q: 0.518796\n",
      " 1958/2000: episode: 19, duration: 5.385s, episode steps: 107, steps per second: 20, episode reward: 0.463, mean reward: 0.004 [-0.001, 0.010], mean action: -0.187 [-1.165, 1.104], mean observation: 0.139 [-38.096, 15.758], loss: 0.000484, mean_squared_error: 0.000968, mean_q: 0.524581\n",
      "done, took 91.002 seconds\n",
      "\n",
      "\n",
      "iteration: 123\n",
      "Training for 2000 steps ...\n",
      "  104/2000: episode: 1, duration: 4.024s, episode steps: 104, steps per second: 26, episode reward: 0.465, mean reward: 0.004 [-0.001, 0.010], mean action: -0.161 [-1.162, 1.244], mean observation: 0.138 [-13.472, 16.281], loss: --, mean_squared_error: --, mean_q: --\n",
      "  209/2000: episode: 2, duration: 4.211s, episode steps: 105, steps per second: 25, episode reward: 0.471, mean reward: 0.004 [-0.001, 0.010], mean action: -0.141 [-1.182, 1.184], mean observation: 0.137 [-10.965, 15.702], loss: --, mean_squared_error: --, mean_q: --\n",
      "  309/2000: episode: 3, duration: 3.810s, episode steps: 100, steps per second: 26, episode reward: 0.445, mean reward: 0.004 [-0.001, 0.011], mean action: -0.178 [-1.145, 1.099], mean observation: 0.135 [-12.890, 15.931], loss: --, mean_squared_error: --, mean_q: --\n",
      "  412/2000: episode: 4, duration: 3.895s, episode steps: 103, steps per second: 26, episode reward: 0.464, mean reward: 0.005 [-0.001, 0.010], mean action: -0.148 [-1.297, 1.230], mean observation: 0.139 [-11.882, 15.741], loss: --, mean_squared_error: --, mean_q: --\n",
      "  517/2000: episode: 5, duration: 4.247s, episode steps: 105, steps per second: 25, episode reward: 0.471, mean reward: 0.004 [-0.001, 0.011], mean action: -0.151 [-1.147, 1.129], mean observation: 0.141 [-14.561, 15.780], loss: --, mean_squared_error: --, mean_q: --\n",
      "  623/2000: episode: 6, duration: 4.186s, episode steps: 106, steps per second: 25, episode reward: 0.475, mean reward: 0.004 [-0.001, 0.011], mean action: -0.165 [-1.143, 1.159], mean observation: 0.139 [-15.814, 15.653], loss: --, mean_squared_error: --, mean_q: --\n",
      "  727/2000: episode: 7, duration: 3.842s, episode steps: 104, steps per second: 27, episode reward: 0.473, mean reward: 0.005 [-0.001, 0.010], mean action: -0.170 [-1.134, 1.080], mean observation: 0.139 [-13.304, 15.248], loss: --, mean_squared_error: --, mean_q: --\n",
      "  832/2000: episode: 8, duration: 4.173s, episode steps: 105, steps per second: 25, episode reward: 0.473, mean reward: 0.005 [-0.001, 0.011], mean action: -0.151 [-1.145, 1.218], mean observation: 0.137 [-13.742, 16.772], loss: --, mean_squared_error: --, mean_q: --\n",
      "  929/2000: episode: 9, duration: 3.697s, episode steps: 97, steps per second: 26, episode reward: 0.416, mean reward: 0.004 [-0.001, 0.010], mean action: -0.150 [-1.161, 1.167], mean observation: 0.134 [-10.712, 15.562], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1034/2000: episode: 10, duration: 4.564s, episode steps: 105, steps per second: 23, episode reward: 0.475, mean reward: 0.005 [-0.001, 0.011], mean action: -0.165 [-1.245, 1.177], mean observation: 0.138 [-13.272, 15.797], loss: 0.000120, mean_squared_error: 0.000239, mean_q: 0.518520\n",
      " 1138/2000: episode: 11, duration: 4.967s, episode steps: 104, steps per second: 21, episode reward: 0.468, mean reward: 0.004 [-0.001, 0.011], mean action: -0.161 [-1.145, 1.134], mean observation: 0.137 [-17.551, 15.890], loss: 0.000191, mean_squared_error: 0.000383, mean_q: 0.517793\n",
      " 1245/2000: episode: 12, duration: 4.870s, episode steps: 107, steps per second: 22, episode reward: 0.476, mean reward: 0.004 [-0.001, 0.010], mean action: -0.156 [-1.169, 1.172], mean observation: 0.140 [-11.646, 15.310], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.521811\n",
      " 1363/2000: episode: 13, duration: 5.961s, episode steps: 118, steps per second: 20, episode reward: 0.561, mean reward: 0.005 [-0.001, 0.011], mean action: -0.093 [-1.153, 1.154], mean observation: 0.146 [-21.603, 15.831], loss: 0.000218, mean_squared_error: 0.000436, mean_q: 0.517784\n",
      " 1481/2000: episode: 14, duration: 6.215s, episode steps: 118, steps per second: 19, episode reward: 0.566, mean reward: 0.005 [-0.001, 0.010], mean action: -0.098 [-1.295, 1.256], mean observation: 0.147 [-18.874, 16.032], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.521803\n",
      " 1592/2000: episode: 15, duration: 5.455s, episode steps: 111, steps per second: 20, episode reward: 0.501, mean reward: 0.005 [-0.001, 0.011], mean action: -0.112 [-1.199, 1.286], mean observation: 0.142 [-39.355, 15.769], loss: 0.000323, mean_squared_error: 0.000646, mean_q: 0.527875\n",
      " 1716/2000: episode: 16, duration: 6.431s, episode steps: 124, steps per second: 19, episode reward: 0.515, mean reward: 0.004 [-0.001, 0.010], mean action: -0.044 [-1.156, 1.212], mean observation: 0.144 [-22.962, 15.937], loss: 0.000117, mean_squared_error: 0.000233, mean_q: 0.531552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1825/2000: episode: 17, duration: 5.744s, episode steps: 109, steps per second: 19, episode reward: 0.449, mean reward: 0.004 [-0.001, 0.010], mean action: -0.164 [-1.183, 1.113], mean observation: 0.141 [-41.970, 15.746], loss: 0.000295, mean_squared_error: 0.000591, mean_q: 0.518794\n",
      " 1937/2000: episode: 18, duration: 5.785s, episode steps: 112, steps per second: 19, episode reward: 0.462, mean reward: 0.004 [-0.002, 0.011], mean action: -0.170 [-1.184, 1.136], mean observation: 0.140 [-41.969, 15.794], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.510851\n",
      "done, took 89.465 seconds\n",
      "\n",
      "\n",
      "iteration: 124\n",
      "Training for 2000 steps ...\n",
      "  108/2000: episode: 1, duration: 4.742s, episode steps: 108, steps per second: 23, episode reward: 0.441, mean reward: 0.004 [-0.001, 0.010], mean action: -0.146 [-1.123, 1.152], mean observation: 0.138 [-42.184, 15.541], loss: --, mean_squared_error: --, mean_q: --\n",
      "  215/2000: episode: 2, duration: 4.633s, episode steps: 107, steps per second: 23, episode reward: 0.437, mean reward: 0.004 [-0.001, 0.010], mean action: -0.159 [-1.193, 1.126], mean observation: 0.136 [-38.574, 15.508], loss: --, mean_squared_error: --, mean_q: --\n",
      "  326/2000: episode: 3, duration: 4.687s, episode steps: 111, steps per second: 24, episode reward: 0.480, mean reward: 0.004 [-0.001, 0.010], mean action: -0.122 [-1.146, 1.258], mean observation: 0.145 [-42.109, 15.366], loss: --, mean_squared_error: --, mean_q: --\n",
      "  433/2000: episode: 4, duration: 4.766s, episode steps: 107, steps per second: 22, episode reward: 0.435, mean reward: 0.004 [-0.001, 0.010], mean action: -0.147 [-1.185, 1.163], mean observation: 0.137 [-41.492, 15.502], loss: --, mean_squared_error: --, mean_q: --\n",
      "  547/2000: episode: 5, duration: 4.663s, episode steps: 114, steps per second: 24, episode reward: 0.510, mean reward: 0.004 [-0.001, 0.010], mean action: -0.137 [-1.183, 1.210], mean observation: 0.145 [-40.632, 15.460], loss: --, mean_squared_error: --, mean_q: --\n",
      "  653/2000: episode: 6, duration: 4.697s, episode steps: 106, steps per second: 23, episode reward: 0.418, mean reward: 0.004 [-0.001, 0.010], mean action: -0.146 [-1.184, 1.182], mean observation: 0.134 [-42.599, 15.741], loss: --, mean_squared_error: --, mean_q: --\n",
      "  763/2000: episode: 7, duration: 4.682s, episode steps: 110, steps per second: 23, episode reward: 0.465, mean reward: 0.004 [-0.001, 0.011], mean action: -0.144 [-1.197, 1.123], mean observation: 0.137 [-41.876, 15.917], loss: --, mean_squared_error: --, mean_q: --\n",
      "  870/2000: episode: 8, duration: 4.629s, episode steps: 107, steps per second: 23, episode reward: 0.434, mean reward: 0.004 [-0.001, 0.010], mean action: -0.140 [-1.182, 1.205], mean observation: 0.134 [-42.334, 15.739], loss: --, mean_squared_error: --, mean_q: --\n",
      "  982/2000: episode: 9, duration: 4.674s, episode steps: 112, steps per second: 24, episode reward: 0.486, mean reward: 0.004 [-0.001, 0.011], mean action: -0.131 [-1.182, 1.121], mean observation: 0.145 [-39.311, 15.382], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1093/2000: episode: 10, duration: 5.564s, episode steps: 111, steps per second: 20, episode reward: 0.494, mean reward: 0.004 [-0.001, 0.010], mean action: -0.133 [-1.218, 1.216], mean observation: 0.143 [-42.431, 15.469], loss: 0.000183, mean_squared_error: 0.000366, mean_q: 0.522920\n",
      " 1204/2000: episode: 11, duration: 5.516s, episode steps: 111, steps per second: 20, episode reward: 0.506, mean reward: 0.005 [-0.001, 0.011], mean action: -0.225 [-1.221, 1.207], mean observation: 0.144 [-41.382, 15.814], loss: 0.000275, mean_squared_error: 0.000550, mean_q: 0.514223\n",
      " 1304/2000: episode: 12, duration: 4.985s, episode steps: 100, steps per second: 20, episode reward: 0.439, mean reward: 0.004 [-0.001, 0.011], mean action: -0.249 [-1.169, 1.209], mean observation: 0.131 [-39.846, 15.502], loss: 0.000297, mean_squared_error: 0.000595, mean_q: 0.511709\n",
      " 1405/2000: episode: 13, duration: 4.959s, episode steps: 101, steps per second: 20, episode reward: 0.442, mean reward: 0.004 [-0.001, 0.011], mean action: -0.246 [-1.095, 1.087], mean observation: 0.135 [-40.784, 15.815], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.522448\n",
      " 1505/2000: episode: 14, duration: 4.893s, episode steps: 100, steps per second: 20, episode reward: 0.438, mean reward: 0.004 [-0.001, 0.011], mean action: -0.201 [-1.193, 1.216], mean observation: 0.135 [-37.976, 15.587], loss: 0.000195, mean_squared_error: 0.000390, mean_q: 0.520989\n",
      " 1615/2000: episode: 15, duration: 5.116s, episode steps: 110, steps per second: 22, episode reward: 0.509, mean reward: 0.005 [-0.001, 0.011], mean action: -0.136 [-1.240, 1.206], mean observation: 0.144 [-15.061, 16.321], loss: 0.000074, mean_squared_error: 0.000148, mean_q: 0.520788\n",
      " 1717/2000: episode: 16, duration: 5.134s, episode steps: 102, steps per second: 20, episode reward: 0.443, mean reward: 0.004 [-0.001, 0.011], mean action: -0.139 [-1.188, 1.219], mean observation: 0.133 [-41.656, 15.783], loss: 0.000258, mean_squared_error: 0.000515, mean_q: 0.525388\n",
      " 1831/2000: episode: 17, duration: 5.494s, episode steps: 114, steps per second: 21, episode reward: 0.516, mean reward: 0.005 [-0.001, 0.010], mean action: -0.073 [-1.220, 1.118], mean observation: 0.140 [-38.044, 15.512], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.522877\n",
      " 1949/2000: episode: 18, duration: 5.674s, episode steps: 118, steps per second: 21, episode reward: 0.532, mean reward: 0.005 [-0.001, 0.010], mean action: -0.010 [-1.149, 1.265], mean observation: 0.144 [-15.256, 15.715], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.526514\n",
      "done, took 92.241 seconds\n",
      "\n",
      "\n",
      "iteration: 125\n",
      "Training for 2000 steps ...\n",
      "  112/2000: episode: 1, duration: 4.252s, episode steps: 112, steps per second: 26, episode reward: 0.486, mean reward: 0.004 [-0.001, 0.010], mean action: -0.101 [-1.149, 1.204], mean observation: 0.140 [-14.663, 15.697], loss: --, mean_squared_error: --, mean_q: --\n",
      "  224/2000: episode: 2, duration: 4.144s, episode steps: 112, steps per second: 27, episode reward: 0.491, mean reward: 0.004 [-0.001, 0.011], mean action: -0.089 [-1.189, 1.147], mean observation: 0.141 [-14.636, 15.634], loss: --, mean_squared_error: --, mean_q: --\n",
      "  333/2000: episode: 3, duration: 4.093s, episode steps: 109, steps per second: 27, episode reward: 0.473, mean reward: 0.004 [-0.001, 0.010], mean action: -0.084 [-1.101, 1.218], mean observation: 0.138 [-15.471, 15.352], loss: --, mean_squared_error: --, mean_q: --\n",
      "  443/2000: episode: 4, duration: 4.081s, episode steps: 110, steps per second: 27, episode reward: 0.479, mean reward: 0.004 [-0.001, 0.010], mean action: -0.078 [-1.255, 1.212], mean observation: 0.140 [-14.560, 15.508], loss: --, mean_squared_error: --, mean_q: --\n",
      "  550/2000: episode: 5, duration: 3.955s, episode steps: 107, steps per second: 27, episode reward: 0.468, mean reward: 0.004 [-0.001, 0.011], mean action: -0.102 [-1.235, 1.229], mean observation: 0.139 [-15.179, 15.395], loss: --, mean_squared_error: --, mean_q: --\n",
      "  663/2000: episode: 6, duration: 4.196s, episode steps: 113, steps per second: 27, episode reward: 0.499, mean reward: 0.004 [-0.001, 0.010], mean action: -0.064 [-1.165, 1.266], mean observation: 0.141 [-14.865, 15.713], loss: --, mean_squared_error: --, mean_q: --\n",
      "  777/2000: episode: 7, duration: 4.198s, episode steps: 114, steps per second: 27, episode reward: 0.506, mean reward: 0.004 [-0.001, 0.011], mean action: -0.053 [-1.090, 1.181], mean observation: 0.141 [-15.275, 15.824], loss: --, mean_squared_error: --, mean_q: --\n",
      "  889/2000: episode: 8, duration: 4.157s, episode steps: 112, steps per second: 27, episode reward: 0.495, mean reward: 0.004 [-0.001, 0.010], mean action: -0.063 [-1.134, 1.202], mean observation: 0.140 [-15.087, 15.324], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1000/2000: episode: 9, duration: 4.079s, episode steps: 111, steps per second: 27, episode reward: 0.482, mean reward: 0.004 [-0.001, 0.010], mean action: -0.103 [-1.182, 1.240], mean observation: 0.140 [-14.868, 15.714], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1113/2000: episode: 10, duration: 5.328s, episode steps: 113, steps per second: 21, episode reward: 0.497, mean reward: 0.004 [-0.001, 0.010], mean action: -0.121 [-1.225, 1.220], mean observation: 0.142 [-14.776, 15.628], loss: 0.000065, mean_squared_error: 0.000129, mean_q: 0.516100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1232/2000: episode: 11, duration: 5.939s, episode steps: 119, steps per second: 20, episode reward: 0.526, mean reward: 0.004 [-0.001, 0.011], mean action: -0.129 [-1.211, 1.151], mean observation: 0.146 [-16.424, 15.914], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.532940\n",
      " 1350/2000: episode: 12, duration: 5.614s, episode steps: 118, steps per second: 21, episode reward: 0.507, mean reward: 0.004 [-0.001, 0.010], mean action: -0.119 [-1.180, 1.262], mean observation: 0.143 [-15.418, 16.284], loss: 0.000179, mean_squared_error: 0.000357, mean_q: 0.521229\n",
      " 1461/2000: episode: 13, duration: 5.054s, episode steps: 111, steps per second: 22, episode reward: 0.465, mean reward: 0.004 [-0.001, 0.010], mean action: -0.123 [-1.189, 1.185], mean observation: 0.138 [-15.306, 15.546], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.514442\n",
      " 1585/2000: episode: 14, duration: 5.668s, episode steps: 124, steps per second: 22, episode reward: 0.532, mean reward: 0.004 [-0.001, 0.011], mean action: -0.144 [-1.194, 1.177], mean observation: 0.142 [-15.505, 15.891], loss: 0.000137, mean_squared_error: 0.000273, mean_q: 0.523228\n",
      " 1708/2000: episode: 15, duration: 5.736s, episode steps: 123, steps per second: 21, episode reward: 0.484, mean reward: 0.004 [-0.003, 0.011], mean action: -0.134 [-1.108, 1.146], mean observation: 0.137 [-23.148, 15.324], loss: 0.000127, mean_squared_error: 0.000253, mean_q: 0.509644\n",
      " 1846/2000: episode: 16, duration: 6.644s, episode steps: 138, steps per second: 21, episode reward: 0.502, mean reward: 0.004 [-0.003, 0.012], mean action: -0.150 [-1.236, 1.186], mean observation: 0.142 [-21.185, 16.096], loss: 0.000109, mean_squared_error: 0.000218, mean_q: 0.522129\n",
      " 1986/2000: episode: 17, duration: 7.256s, episode steps: 140, steps per second: 19, episode reward: 0.554, mean reward: 0.004 [-0.003, 0.012], mean action: -0.095 [-1.172, 1.146], mean observation: 0.145 [-17.788, 15.832], loss: 0.000115, mean_squared_error: 0.000230, mean_q: 0.512708\n",
      "done, took 85.522 seconds\n",
      "\n",
      "\n",
      "iteration: 126\n",
      "Training for 2000 steps ...\n",
      "  147/2000: episode: 1, duration: 5.722s, episode steps: 147, steps per second: 26, episode reward: 0.541, mean reward: 0.004 [-0.003, 0.012], mean action: -0.110 [-1.239, 1.174], mean observation: 0.142 [-21.924, 15.491], loss: --, mean_squared_error: --, mean_q: --\n",
      "  289/2000: episode: 2, duration: 5.245s, episode steps: 142, steps per second: 27, episode reward: 0.509, mean reward: 0.004 [-0.003, 0.012], mean action: -0.130 [-1.172, 1.101], mean observation: 0.141 [-19.915, 16.156], loss: --, mean_squared_error: --, mean_q: --\n",
      "  430/2000: episode: 3, duration: 5.366s, episode steps: 141, steps per second: 26, episode reward: 0.514, mean reward: 0.004 [-0.003, 0.012], mean action: -0.120 [-1.191, 1.098], mean observation: 0.141 [-25.738, 16.324], loss: --, mean_squared_error: --, mean_q: --\n",
      "  571/2000: episode: 4, duration: 5.339s, episode steps: 141, steps per second: 26, episode reward: 0.511, mean reward: 0.004 [-0.003, 0.012], mean action: -0.069 [-1.175, 1.195], mean observation: 0.140 [-27.350, 16.212], loss: --, mean_squared_error: --, mean_q: --\n",
      "  716/2000: episode: 5, duration: 5.799s, episode steps: 145, steps per second: 25, episode reward: 0.556, mean reward: 0.004 [-0.003, 0.012], mean action: -0.069 [-1.260, 1.187], mean observation: 0.144 [-18.716, 16.440], loss: --, mean_squared_error: --, mean_q: --\n",
      "  860/2000: episode: 6, duration: 5.568s, episode steps: 144, steps per second: 26, episode reward: 0.551, mean reward: 0.004 [-0.003, 0.012], mean action: -0.100 [-1.260, 1.193], mean observation: 0.144 [-18.450, 15.562], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1000/2000: episode: 7, duration: 5.107s, episode steps: 140, steps per second: 27, episode reward: 0.505, mean reward: 0.004 [-0.003, 0.012], mean action: -0.093 [-1.209, 1.226], mean observation: 0.139 [-27.363, 15.737], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1145/2000: episode: 8, duration: 6.803s, episode steps: 145, steps per second: 21, episode reward: 0.516, mean reward: 0.004 [-0.003, 0.012], mean action: -0.061 [-1.185, 1.232], mean observation: 0.140 [-21.266, 15.735], loss: 0.000066, mean_squared_error: 0.000131, mean_q: 0.519911\n",
      " 1289/2000: episode: 9, duration: 6.705s, episode steps: 144, steps per second: 21, episode reward: 0.481, mean reward: 0.003 [-0.003, 0.011], mean action: -0.085 [-1.200, 1.180], mean observation: 0.138 [-21.386, 16.364], loss: 0.000062, mean_squared_error: 0.000125, mean_q: 0.533169\n",
      " 1431/2000: episode: 10, duration: 6.740s, episode steps: 142, steps per second: 21, episode reward: 0.475, mean reward: 0.003 [-0.003, 0.011], mean action: -0.056 [-1.195, 1.264], mean observation: 0.138 [-27.262, 16.192], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.517872\n",
      " 1561/2000: episode: 11, duration: 6.706s, episode steps: 130, steps per second: 19, episode reward: 0.470, mean reward: 0.004 [-0.002, 0.011], mean action: -0.113 [-1.181, 1.163], mean observation: 0.138 [-27.542, 15.398], loss: 0.000214, mean_squared_error: 0.000427, mean_q: 0.521635\n",
      " 1683/2000: episode: 12, duration: 6.339s, episode steps: 122, steps per second: 19, episode reward: 0.488, mean reward: 0.004 [-0.002, 0.011], mean action: -0.121 [-1.208, 1.209], mean observation: 0.145 [-27.693, 15.720], loss: 0.000135, mean_squared_error: 0.000270, mean_q: 0.518907\n",
      " 1816/2000: episode: 13, duration: 7.119s, episode steps: 133, steps per second: 19, episode reward: 0.542, mean reward: 0.004 [-0.001, 0.010], mean action: -0.036 [-1.179, 1.135], mean observation: 0.144 [-26.127, 16.286], loss: 0.000302, mean_squared_error: 0.000603, mean_q: 0.516249\n",
      " 1948/2000: episode: 14, duration: 7.345s, episode steps: 132, steps per second: 18, episode reward: 0.517, mean reward: 0.004 [-0.002, 0.010], mean action: -0.076 [-1.207, 1.230], mean observation: 0.144 [-24.950, 16.039], loss: 0.000096, mean_squared_error: 0.000191, mean_q: 0.528222\n",
      "done, took 88.951 seconds\n",
      "\n",
      "\n",
      "iteration: 127\n",
      "Training for 2000 steps ...\n",
      "  140/2000: episode: 1, duration: 6.152s, episode steps: 140, steps per second: 23, episode reward: 0.523, mean reward: 0.004 [-0.001, 0.011], mean action: -0.073 [-1.224, 1.102], mean observation: 0.143 [-21.473, 15.927], loss: --, mean_squared_error: --, mean_q: --\n",
      "  272/2000: episode: 2, duration: 5.949s, episode steps: 132, steps per second: 22, episode reward: 0.454, mean reward: 0.003 [-0.002, 0.011], mean action: -0.065 [-1.202, 1.221], mean observation: 0.138 [-21.560, 16.014], loss: --, mean_squared_error: --, mean_q: --\n",
      "  413/2000: episode: 3, duration: 6.339s, episode steps: 141, steps per second: 22, episode reward: 0.500, mean reward: 0.004 [-0.001, 0.011], mean action: -0.031 [-1.176, 1.301], mean observation: 0.142 [-22.725, 15.833], loss: --, mean_squared_error: --, mean_q: --\n",
      "  546/2000: episode: 4, duration: 5.756s, episode steps: 133, steps per second: 23, episode reward: 0.526, mean reward: 0.004 [-0.002, 0.011], mean action: -0.023 [-1.146, 1.256], mean observation: 0.143 [-27.379, 15.867], loss: --, mean_squared_error: --, mean_q: --\n",
      "  676/2000: episode: 5, duration: 5.541s, episode steps: 130, steps per second: 23, episode reward: 0.488, mean reward: 0.004 [-0.001, 0.011], mean action: -0.104 [-1.234, 1.109], mean observation: 0.138 [-20.070, 15.923], loss: --, mean_squared_error: --, mean_q: --\n",
      "  811/2000: episode: 6, duration: 5.944s, episode steps: 135, steps per second: 23, episode reward: 0.477, mean reward: 0.004 [-0.002, 0.011], mean action: -0.100 [-1.280, 1.168], mean observation: 0.138 [-21.177, 16.180], loss: --, mean_squared_error: --, mean_q: --\n",
      "  942/2000: episode: 7, duration: 5.619s, episode steps: 131, steps per second: 23, episode reward: 0.494, mean reward: 0.004 [-0.001, 0.011], mean action: -0.101 [-1.247, 1.191], mean observation: 0.139 [-25.259, 16.201], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1070/2000: episode: 8, duration: 6.408s, episode steps: 128, steps per second: 20, episode reward: 0.453, mean reward: 0.004 [-0.002, 0.011], mean action: -0.098 [-1.169, 1.170], mean observation: 0.139 [-25.117, 15.861], loss: 0.000085, mean_squared_error: 0.000169, mean_q: 0.511504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1220/2000: episode: 9, duration: 7.933s, episode steps: 150, steps per second: 19, episode reward: 0.508, mean reward: 0.003 [-0.002, 0.011], mean action: -0.010 [-1.234, 1.220], mean observation: 0.143 [-18.373, 15.878], loss: 0.000109, mean_squared_error: 0.000217, mean_q: 0.515414\n",
      " 1393/2000: episode: 10, duration: 9.187s, episode steps: 173, steps per second: 19, episode reward: 0.495, mean reward: 0.003 [-0.003, 0.011], mean action: 0.039 [-1.257, 1.336], mean observation: 0.138 [-21.754, 15.775], loss: 0.000269, mean_squared_error: 0.000539, mean_q: 0.518877\n",
      " 1518/2000: episode: 11, duration: 6.266s, episode steps: 125, steps per second: 20, episode reward: 0.486, mean reward: 0.004 [-0.001, 0.010], mean action: -0.056 [-1.159, 1.159], mean observation: 0.139 [-32.079, 17.958], loss: 0.000197, mean_squared_error: 0.000393, mean_q: 0.527357\n",
      " 1657/2000: episode: 12, duration: 7.523s, episode steps: 139, steps per second: 18, episode reward: 0.430, mean reward: 0.003 [-0.001, 0.010], mean action: -0.050 [-1.286, 1.136], mean observation: 0.135 [-32.740, 17.949], loss: 0.000231, mean_squared_error: 0.000462, mean_q: 0.523336\n",
      " 1813/2000: episode: 13, duration: 9.076s, episode steps: 156, steps per second: 17, episode reward: 0.487, mean reward: 0.003 [-0.002, 0.010], mean action: -0.055 [-1.255, 1.280], mean observation: 0.139 [-33.050, 18.004], loss: 0.000378, mean_squared_error: 0.000756, mean_q: 0.514394\n",
      " 1948/2000: episode: 14, duration: 6.833s, episode steps: 135, steps per second: 20, episode reward: 0.492, mean reward: 0.004 [-0.002, 0.010], mean action: -0.020 [-1.203, 1.232], mean observation: 0.137 [-33.301, 18.031], loss: 0.000109, mean_squared_error: 0.000219, mean_q: 0.522164\n",
      "done, took 97.610 seconds\n",
      "\n",
      "\n",
      "iteration: 128\n",
      "Training for 2000 steps ...\n",
      "  169/2000: episode: 1, duration: 7.995s, episode steps: 169, steps per second: 21, episode reward: 0.456, mean reward: 0.003 [-0.002, 0.011], mean action: 0.048 [-1.252, 1.378], mean observation: 0.132 [-31.926, 17.761], loss: --, mean_squared_error: --, mean_q: --\n",
      "  326/2000: episode: 2, duration: 7.351s, episode steps: 157, steps per second: 21, episode reward: 0.486, mean reward: 0.003 [-0.002, 0.010], mean action: 0.021 [-1.179, 1.207], mean observation: 0.136 [-30.853, 17.453], loss: --, mean_squared_error: --, mean_q: --\n",
      "  490/2000: episode: 3, duration: 7.696s, episode steps: 164, steps per second: 21, episode reward: 0.474, mean reward: 0.003 [-0.002, 0.010], mean action: 0.015 [-1.247, 1.174], mean observation: 0.134 [-30.426, 17.319], loss: --, mean_squared_error: --, mean_q: --\n",
      "  648/2000: episode: 4, duration: 7.339s, episode steps: 158, steps per second: 22, episode reward: 0.438, mean reward: 0.003 [-0.002, 0.010], mean action: 0.007 [-1.239, 1.218], mean observation: 0.133 [-30.367, 17.248], loss: --, mean_squared_error: --, mean_q: --\n",
      "  852/2000: episode: 5, duration: 9.293s, episode steps: 204, steps per second: 22, episode reward: 0.451, mean reward: 0.002 [-0.002, 0.011], mean action: 0.063 [-1.244, 1.275], mean observation: 0.129 [-31.011, 17.289], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1416/2000: episode: 6, duration: 28.807s, episode steps: 564, steps per second: 20, episode reward: 0.479, mean reward: 0.001 [-0.002, 0.010], mean action: 0.080 [-1.406, 1.449], mean observation: 0.124 [-30.011, 17.098], loss: 0.000181, mean_squared_error: 0.000361, mean_q: 0.523432\n",
      " 1575/2000: episode: 7, duration: 9.420s, episode steps: 159, steps per second: 17, episode reward: 0.486, mean reward: 0.003 [-0.002, 0.010], mean action: 0.036 [-1.169, 1.192], mean observation: 0.132 [-31.689, 17.593], loss: 0.000316, mean_squared_error: 0.000633, mean_q: 0.524281\n",
      " 1769/2000: episode: 8, duration: 11.675s, episode steps: 194, steps per second: 17, episode reward: 0.619, mean reward: 0.003 [-0.002, 0.015], mean action: 0.059 [-1.308, 1.277], mean observation: 0.133 [-31.340, 17.723], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.521417\n",
      "done, took 101.738 seconds\n",
      "\n",
      "\n",
      "iteration: 129\n",
      "Training for 2000 steps ...\n",
      "  213/2000: episode: 1, duration: 10.279s, episode steps: 213, steps per second: 21, episode reward: 0.681, mean reward: 0.003 [-0.002, 0.016], mean action: 0.005 [-1.151, 1.268], mean observation: 0.132 [-30.946, 17.707], loss: --, mean_squared_error: --, mean_q: --\n",
      "  419/2000: episode: 2, duration: 10.164s, episode steps: 206, steps per second: 20, episode reward: 0.640, mean reward: 0.003 [-0.002, 0.013], mean action: 0.009 [-1.233, 1.202], mean observation: 0.134 [-31.041, 17.733], loss: --, mean_squared_error: --, mean_q: --\n",
      "  629/2000: episode: 3, duration: 10.619s, episode steps: 210, steps per second: 20, episode reward: 0.684, mean reward: 0.003 [-0.002, 0.014], mean action: 0.010 [-1.174, 1.224], mean observation: 0.134 [-29.358, 17.258], loss: --, mean_squared_error: --, mean_q: --\n",
      "  835/2000: episode: 4, duration: 9.901s, episode steps: 206, steps per second: 21, episode reward: 0.649, mean reward: 0.003 [-0.002, 0.014], mean action: -0.012 [-1.316, 1.199], mean observation: 0.133 [-30.019, 17.519], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1053/2000: episode: 5, duration: 11.376s, episode steps: 218, steps per second: 19, episode reward: 0.696, mean reward: 0.003 [-0.002, 0.015], mean action: 0.001 [-1.275, 1.290], mean observation: 0.133 [-29.773, 17.358], loss: 0.000142, mean_squared_error: 0.000285, mean_q: 0.530886\n",
      " 1220/2000: episode: 6, duration: 10.050s, episode steps: 167, steps per second: 17, episode reward: 0.709, mean reward: 0.004 [-0.002, 0.015], mean action: -0.002 [-1.185, 1.237], mean observation: 0.133 [-31.333, 17.726], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.527765\n",
      " 1369/2000: episode: 7, duration: 8.158s, episode steps: 149, steps per second: 18, episode reward: 0.694, mean reward: 0.005 [-0.002, 0.015], mean action: 0.037 [-1.182, 1.114], mean observation: 0.129 [-32.024, 17.666], loss: 0.000166, mean_squared_error: 0.000331, mean_q: 0.525370\n",
      " 1517/2000: episode: 8, duration: 8.548s, episode steps: 148, steps per second: 17, episode reward: 0.719, mean reward: 0.005 [-0.003, 0.014], mean action: 0.017 [-1.249, 1.180], mean observation: 0.131 [-31.364, 17.629], loss: 0.000174, mean_squared_error: 0.000349, mean_q: 0.533635\n",
      " 1665/2000: episode: 9, duration: 9.033s, episode steps: 148, steps per second: 16, episode reward: 0.708, mean reward: 0.005 [-0.003, 0.014], mean action: 0.029 [-1.211, 1.205], mean observation: 0.135 [-32.702, 17.957], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.528645\n",
      " 1839/2000: episode: 10, duration: 10.671s, episode steps: 174, steps per second: 16, episode reward: 0.717, mean reward: 0.004 [-0.003, 0.013], mean action: 0.027 [-1.259, 1.168], mean observation: 0.131 [-31.462, 17.668], loss: 0.000165, mean_squared_error: 0.000330, mean_q: 0.528884\n",
      "done, took 108.458 seconds\n",
      "\n",
      "\n",
      "iteration: 130\n",
      "Training for 2000 steps ...\n",
      "  178/2000: episode: 1, duration: 9.086s, episode steps: 178, steps per second: 20, episode reward: 0.723, mean reward: 0.004 [-0.003, 0.014], mean action: 0.049 [-1.170, 1.261], mean observation: 0.133 [-30.472, 17.356], loss: --, mean_squared_error: --, mean_q: --\n",
      "  365/2000: episode: 2, duration: 9.448s, episode steps: 187, steps per second: 20, episode reward: 0.746, mean reward: 0.004 [-0.003, 0.014], mean action: 0.011 [-1.257, 1.260], mean observation: 0.134 [-31.371, 17.610], loss: --, mean_squared_error: --, mean_q: --\n",
      "  544/2000: episode: 3, duration: 9.028s, episode steps: 179, steps per second: 20, episode reward: 0.710, mean reward: 0.004 [-0.003, 0.015], mean action: -0.015 [-1.276, 1.274], mean observation: 0.133 [-31.418, 17.583], loss: --, mean_squared_error: --, mean_q: --\n",
      "  718/2000: episode: 4, duration: 8.960s, episode steps: 174, steps per second: 19, episode reward: 0.712, mean reward: 0.004 [-0.003, 0.014], mean action: -0.027 [-1.265, 1.195], mean observation: 0.131 [-31.185, 17.570], loss: --, mean_squared_error: --, mean_q: --\n",
      "  895/2000: episode: 5, duration: 8.984s, episode steps: 177, steps per second: 20, episode reward: 0.712, mean reward: 0.004 [-0.003, 0.014], mean action: 0.005 [-1.179, 1.158], mean observation: 0.133 [-31.599, 17.737], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1071/2000: episode: 6, duration: 9.555s, episode steps: 176, steps per second: 18, episode reward: 0.698, mean reward: 0.004 [-0.003, 0.015], mean action: 0.006 [-1.192, 1.244], mean observation: 0.133 [-31.532, 17.692], loss: 0.000185, mean_squared_error: 0.000369, mean_q: 0.535754\n",
      " 1235/2000: episode: 7, duration: 9.295s, episode steps: 164, steps per second: 18, episode reward: 0.720, mean reward: 0.004 [-0.003, 0.014], mean action: -0.023 [-1.346, 1.237], mean observation: 0.135 [-31.436, 17.762], loss: 0.000250, mean_squared_error: 0.000501, mean_q: 0.531270\n",
      " 1398/2000: episode: 8, duration: 8.828s, episode steps: 163, steps per second: 18, episode reward: 0.720, mean reward: 0.004 [-0.003, 0.014], mean action: 0.028 [-1.328, 1.273], mean observation: 0.132 [-31.954, 17.987], loss: 0.000192, mean_squared_error: 0.000385, mean_q: 0.534768\n",
      " 1554/2000: episode: 9, duration: 8.772s, episode steps: 156, steps per second: 18, episode reward: 0.700, mean reward: 0.004 [-0.003, 0.014], mean action: 0.036 [-1.253, 1.192], mean observation: 0.132 [-31.905, 17.547], loss: 0.000332, mean_squared_error: 0.000664, mean_q: 0.529617\n",
      " 1710/2000: episode: 10, duration: 8.928s, episode steps: 156, steps per second: 17, episode reward: 0.724, mean reward: 0.005 [-0.003, 0.014], mean action: 0.040 [-1.230, 1.185], mean observation: 0.133 [-31.331, 17.655], loss: 0.000340, mean_squared_error: 0.000680, mean_q: 0.522650\n",
      " 1860/2000: episode: 11, duration: 8.238s, episode steps: 150, steps per second: 18, episode reward: 0.683, mean reward: 0.005 [-0.003, 0.013], mean action: 0.124 [-1.178, 1.169], mean observation: 0.127 [-28.633, 17.159], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.532368\n",
      "done, took 107.281 seconds\n",
      "\n",
      "\n",
      "iteration: 131\n",
      "Training for 2000 steps ...\n",
      "  213/2000: episode: 1, duration: 10.468s, episode steps: 213, steps per second: 20, episode reward: 0.694, mean reward: 0.003 [-0.003, 0.015], mean action: 0.025 [-1.293, 1.186], mean observation: 0.127 [-22.243, 16.978], loss: --, mean_squared_error: --, mean_q: --\n",
      "  428/2000: episode: 2, duration: 10.660s, episode steps: 215, steps per second: 20, episode reward: 0.717, mean reward: 0.003 [-0.003, 0.014], mean action: 0.049 [-1.197, 1.241], mean observation: 0.126 [-21.765, 16.776], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 3, duration: 10.557s, episode steps: 214, steps per second: 20, episode reward: 0.709, mean reward: 0.003 [-0.003, 0.014], mean action: 0.039 [-1.212, 1.191], mean observation: 0.128 [-20.955, 16.631], loss: --, mean_squared_error: --, mean_q: --\n",
      "  853/2000: episode: 4, duration: 10.488s, episode steps: 211, steps per second: 20, episode reward: 0.720, mean reward: 0.003 [-0.003, 0.014], mean action: 0.071 [-1.373, 1.289], mean observation: 0.127 [-22.694, 17.145], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1055/2000: episode: 5, duration: 10.755s, episode steps: 202, steps per second: 19, episode reward: 0.703, mean reward: 0.003 [-0.003, 0.014], mean action: 0.026 [-1.242, 1.184], mean observation: 0.128 [-21.232, 16.828], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.525751\n",
      " 1281/2000: episode: 6, duration: 13.440s, episode steps: 226, steps per second: 17, episode reward: 0.706, mean reward: 0.003 [-0.003, 0.014], mean action: 0.057 [-1.202, 1.224], mean observation: 0.126 [-22.181, 17.009], loss: 0.000082, mean_squared_error: 0.000163, mean_q: 0.536138\n",
      " 1448/2000: episode: 7, duration: 10.228s, episode steps: 167, steps per second: 16, episode reward: 0.713, mean reward: 0.004 [-0.003, 0.014], mean action: -0.016 [-1.326, 1.228], mean observation: 0.129 [-24.166, 17.470], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.537264\n",
      " 1605/2000: episode: 8, duration: 9.343s, episode steps: 157, steps per second: 17, episode reward: 0.713, mean reward: 0.005 [-0.004, 0.015], mean action: -0.004 [-1.326, 1.173], mean observation: 0.129 [-24.734, 17.461], loss: 0.000261, mean_squared_error: 0.000522, mean_q: 0.531532\n",
      " 1751/2000: episode: 9, duration: 8.670s, episode steps: 146, steps per second: 17, episode reward: 0.702, mean reward: 0.005 [-0.003, 0.014], mean action: 0.066 [-1.274, 1.285], mean observation: 0.131 [-24.263, 17.304], loss: 0.000157, mean_squared_error: 0.000315, mean_q: 0.526697\n",
      " 1907/2000: episode: 10, duration: 8.677s, episode steps: 156, steps per second: 18, episode reward: 0.797, mean reward: 0.005 [-0.003, 0.014], mean action: 0.095 [-1.229, 1.365], mean observation: 0.128 [-23.553, 17.214], loss: 0.000239, mean_squared_error: 0.000479, mean_q: 0.530559\n",
      "done, took 108.167 seconds\n",
      "\n",
      "\n",
      "iteration: 132\n",
      "Training for 2000 steps ...\n",
      "  157/2000: episode: 1, duration: 7.287s, episode steps: 157, steps per second: 22, episode reward: 0.773, mean reward: 0.005 [-0.003, 0.014], mean action: 0.044 [-1.116, 1.247], mean observation: 0.116 [-23.898, 17.281], loss: --, mean_squared_error: --, mean_q: --\n",
      "  305/2000: episode: 2, duration: 6.569s, episode steps: 148, steps per second: 23, episode reward: 0.758, mean reward: 0.005 [-0.003, 0.015], mean action: 0.040 [-1.245, 1.190], mean observation: 0.121 [-24.823, 17.559], loss: --, mean_squared_error: --, mean_q: --\n",
      "  465/2000: episode: 3, duration: 7.091s, episode steps: 160, steps per second: 23, episode reward: 0.793, mean reward: 0.005 [-0.003, 0.013], mean action: 0.064 [-1.170, 1.226], mean observation: 0.120 [-25.107, 17.576], loss: --, mean_squared_error: --, mean_q: --\n",
      "  626/2000: episode: 4, duration: 7.307s, episode steps: 161, steps per second: 22, episode reward: 0.786, mean reward: 0.005 [-0.003, 0.014], mean action: 0.021 [-1.132, 1.185], mean observation: 0.119 [-25.263, 17.723], loss: --, mean_squared_error: --, mean_q: --\n",
      "  785/2000: episode: 5, duration: 7.168s, episode steps: 159, steps per second: 22, episode reward: 0.780, mean reward: 0.005 [-0.003, 0.014], mean action: 0.039 [-1.267, 1.201], mean observation: 0.116 [-24.985, 17.475], loss: --, mean_squared_error: --, mean_q: --\n",
      "  943/2000: episode: 6, duration: 6.966s, episode steps: 158, steps per second: 23, episode reward: 0.787, mean reward: 0.005 [-0.003, 0.014], mean action: 0.062 [-1.169, 1.298], mean observation: 0.119 [-23.978, 17.238], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1101/2000: episode: 7, duration: 7.826s, episode steps: 158, steps per second: 20, episode reward: 0.775, mean reward: 0.005 [-0.003, 0.014], mean action: 0.053 [-1.298, 1.246], mean observation: 0.116 [-24.739, 17.525], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.532345\n",
      " 1255/2000: episode: 8, duration: 8.548s, episode steps: 154, steps per second: 18, episode reward: 0.821, mean reward: 0.005 [-0.003, 0.015], mean action: 0.060 [-1.187, 1.151], mean observation: 0.125 [-23.674, 17.143], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.530773\n",
      " 1419/2000: episode: 9, duration: 8.988s, episode steps: 164, steps per second: 18, episode reward: 0.785, mean reward: 0.005 [-0.003, 0.013], mean action: 0.055 [-1.134, 1.228], mean observation: 0.123 [-24.713, 17.526], loss: 0.000094, mean_squared_error: 0.000187, mean_q: 0.539657\n",
      " 1591/2000: episode: 10, duration: 10.046s, episode steps: 172, steps per second: 17, episode reward: 0.762, mean reward: 0.004 [-0.003, 0.013], mean action: 0.036 [-1.233, 1.141], mean observation: 0.116 [-24.653, 17.415], loss: 0.000285, mean_squared_error: 0.000570, mean_q: 0.530976\n",
      " 1812/2000: episode: 11, duration: 13.166s, episode steps: 221, steps per second: 17, episode reward: 0.752, mean reward: 0.003 [-0.003, 0.013], mean action: -0.012 [-1.263, 1.236], mean observation: 0.112 [-24.540, 17.485], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.539554\n",
      "done, took 102.103 seconds\n",
      "\n",
      "\n",
      "iteration: 133\n",
      "Training for 2000 steps ...\n",
      "  221/2000: episode: 1, duration: 11.121s, episode steps: 221, steps per second: 20, episode reward: 0.746, mean reward: 0.003 [-0.003, 0.013], mean action: -0.008 [-1.153, 1.193], mean observation: 0.115 [-25.194, 17.609], loss: --, mean_squared_error: --, mean_q: --\n",
      "  461/2000: episode: 2, duration: 12.064s, episode steps: 240, steps per second: 20, episode reward: 0.757, mean reward: 0.003 [-0.004, 0.013], mean action: -0.013 [-1.256, 1.333], mean observation: 0.116 [-25.138, 17.742], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  667/2000: episode: 3, duration: 10.426s, episode steps: 206, steps per second: 20, episode reward: 0.758, mean reward: 0.004 [-0.003, 0.013], mean action: 0.028 [-1.143, 1.352], mean observation: 0.116 [-24.446, 17.433], loss: --, mean_squared_error: --, mean_q: --\n",
      "  867/2000: episode: 4, duration: 9.855s, episode steps: 200, steps per second: 20, episode reward: 0.789, mean reward: 0.004 [-0.003, 0.013], mean action: -0.009 [-1.336, 1.277], mean observation: 0.120 [-25.012, 17.610], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1070/2000: episode: 5, duration: 10.790s, episode steps: 203, steps per second: 19, episode reward: 0.782, mean reward: 0.004 [-0.003, 0.013], mean action: -0.034 [-1.293, 1.249], mean observation: 0.121 [-24.731, 17.533], loss: 0.000085, mean_squared_error: 0.000169, mean_q: 0.541746\n",
      " 1253/2000: episode: 6, duration: 10.334s, episode steps: 183, steps per second: 18, episode reward: 0.774, mean reward: 0.004 [-0.003, 0.013], mean action: 0.000 [-1.185, 1.244], mean observation: 0.121 [-24.861, 17.407], loss: 0.000261, mean_squared_error: 0.000522, mean_q: 0.538525\n",
      " 1514/2000: episode: 7, duration: 15.060s, episode steps: 261, steps per second: 17, episode reward: 0.775, mean reward: 0.003 [-0.003, 0.014], mean action: -0.055 [-1.194, 1.274], mean observation: 0.121 [-21.858, 16.645], loss: 0.000196, mean_squared_error: 0.000391, mean_q: 0.540249\n",
      " 1742/2000: episode: 8, duration: 13.428s, episode steps: 228, steps per second: 17, episode reward: 0.802, mean reward: 0.004 [-0.003, 0.013], mean action: -0.053 [-1.184, 1.300], mean observation: 0.123 [-21.914, 16.709], loss: 0.000126, mean_squared_error: 0.000251, mean_q: 0.553658\n",
      " 1999/2000: episode: 9, duration: 15.207s, episode steps: 257, steps per second: 17, episode reward: 0.763, mean reward: 0.003 [-0.004, 0.013], mean action: -0.006 [-1.308, 1.251], mean observation: 0.117 [-28.140, 16.840], loss: 0.000249, mean_squared_error: 0.000497, mean_q: 0.545495\n",
      "done, took 108.404 seconds\n",
      "\n",
      "\n",
      "iteration: 134\n",
      "Training for 2000 steps ...\n",
      "  260/2000: episode: 1, duration: 13.243s, episode steps: 260, steps per second: 20, episode reward: 0.790, mean reward: 0.003 [-0.003, 0.013], mean action: -0.036 [-1.357, 1.313], mean observation: 0.122 [-24.101, 17.486], loss: --, mean_squared_error: --, mean_q: --\n",
      "  599/2000: episode: 2, duration: 17.896s, episode steps: 339, steps per second: 19, episode reward: 0.776, mean reward: 0.002 [-0.003, 0.014], mean action: -0.080 [-1.381, 1.305], mean observation: 0.124 [-24.596, 17.662], loss: --, mean_squared_error: --, mean_q: --\n",
      "  855/2000: episode: 3, duration: 12.981s, episode steps: 256, steps per second: 20, episode reward: 0.809, mean reward: 0.003 [-0.003, 0.012], mean action: 0.018 [-1.226, 1.326], mean observation: 0.128 [-24.538, 17.616], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1280/2000: episode: 4, duration: 19.663s, episode steps: 425, steps per second: 22, episode reward: -0.833, mean reward: -0.002 [-0.019, 0.009], mean action: 0.098 [-1.259, 1.366], mean observation: 0.086 [-25.438, 17.888], loss: 0.000207, mean_squared_error: 0.000414, mean_q: 0.544997\n",
      " 1686/2000: episode: 5, duration: 23.703s, episode steps: 406, steps per second: 17, episode reward: 0.765, mean reward: 0.002 [-0.003, 0.014], mean action: -0.101 [-1.252, 1.231], mean observation: 0.118 [-22.647, 16.823], loss: 0.000199, mean_squared_error: 0.000398, mean_q: 0.544495\n",
      "done, took 105.588 seconds\n",
      "\n",
      "\n",
      "iteration: 135\n",
      "Training for 2000 steps ...\n",
      "  320/2000: episode: 1, duration: 15.836s, episode steps: 320, steps per second: 20, episode reward: 0.791, mean reward: 0.002 [-0.003, 0.013], mean action: -0.070 [-1.178, 1.388], mean observation: 0.120 [-23.934, 17.201], loss: --, mean_squared_error: --, mean_q: --\n",
      "  714/2000: episode: 2, duration: 19.911s, episode steps: 394, steps per second: 20, episode reward: 0.746, mean reward: 0.002 [-0.003, 0.013], mean action: -0.090 [-1.314, 1.253], mean observation: 0.119 [-24.835, 17.495], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1048/2000: episode: 3, duration: 16.860s, episode steps: 334, steps per second: 20, episode reward: 0.746, mean reward: 0.002 [-0.003, 0.014], mean action: -0.116 [-1.238, 1.164], mean observation: 0.118 [-23.754, 17.073], loss: 0.000427, mean_squared_error: 0.000854, mean_q: 0.533239\n",
      " 1353/2000: episode: 4, duration: 17.788s, episode steps: 305, steps per second: 17, episode reward: 0.741, mean reward: 0.002 [-0.003, 0.013], mean action: -0.172 [-1.233, 1.158], mean observation: 0.116 [-22.995, 16.693], loss: 0.000206, mean_squared_error: 0.000411, mean_q: 0.556433\n",
      " 1666/2000: episode: 5, duration: 14.566s, episode steps: 313, steps per second: 21, episode reward: -0.839, mean reward: -0.003 [-0.019, 0.008], mean action: -0.086 [-1.271, 1.143], mean observation: 0.069 [-23.101, 17.022], loss: 0.000189, mean_squared_error: 0.000377, mean_q: 0.546131\n",
      " 1929/2000: episode: 6, duration: 11.787s, episode steps: 263, steps per second: 22, episode reward: -0.833, mean reward: -0.003 [-0.019, 0.008], mean action: 0.034 [-1.267, 1.465], mean observation: 0.064 [-23.322, 16.847], loss: 0.000141, mean_squared_error: 0.000283, mean_q: 0.546698\n",
      "done, took 100.972 seconds\n",
      "\n",
      "\n",
      "iteration: 136\n",
      "Training for 2000 steps ...\n",
      "  349/2000: episode: 1, duration: 12.446s, episode steps: 349, steps per second: 28, episode reward: -0.840, mean reward: -0.002 [-0.020, 0.010], mean action: 0.080 [-1.209, 1.276], mean observation: 0.075 [-21.847, 16.610], loss: --, mean_squared_error: --, mean_q: --\n",
      "  625/2000: episode: 2, duration: 10.157s, episode steps: 276, steps per second: 27, episode reward: -0.799, mean reward: -0.003 [-0.019, 0.010], mean action: 0.080 [-1.162, 1.358], mean observation: 0.067 [-23.267, 16.853], loss: --, mean_squared_error: --, mean_q: --\n",
      "  904/2000: episode: 3, duration: 9.861s, episode steps: 279, steps per second: 28, episode reward: -0.823, mean reward: -0.003 [-0.019, 0.010], mean action: 0.056 [-1.191, 1.167], mean observation: 0.064 [-21.868, 16.553], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1483/2000: episode: 4, duration: 25.749s, episode steps: 579, steps per second: 22, episode reward: -0.847, mean reward: -0.001 [-0.020, 0.010], mean action: 0.044 [-1.379, 1.301], mean observation: 0.090 [-21.962, 16.630], loss: 0.000197, mean_squared_error: 0.000393, mean_q: 0.551398\n",
      " 1752/2000: episode: 5, duration: 11.672s, episode steps: 269, steps per second: 23, episode reward: -0.819, mean reward: -0.003 [-0.020, 0.011], mean action: 0.047 [-1.185, 1.200], mean observation: 0.064 [-22.567, 16.840], loss: 0.000172, mean_squared_error: 0.000343, mean_q: 0.561334\n",
      " 1998/2000: episode: 6, duration: 10.873s, episode steps: 246, steps per second: 23, episode reward: -0.837, mean reward: -0.003 [-0.020, 0.010], mean action: 0.156 [-1.206, 1.282], mean observation: 0.058 [-22.493, 16.817], loss: 0.000253, mean_squared_error: 0.000506, mean_q: 0.549578\n",
      "done, took 80.921 seconds\n",
      "\n",
      "\n",
      "iteration: 137\n",
      "Training for 2000 steps ...\n",
      "  722/2000: episode: 1, duration: 24.551s, episode steps: 722, steps per second: 29, episode reward: -0.871, mean reward: -0.001 [-0.020, 0.010], mean action: 0.051 [-1.375, 1.314], mean observation: 0.096 [-23.010, 17.004], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1237/2000: episode: 2, duration: 18.952s, episode steps: 515, steps per second: 27, episode reward: -0.834, mean reward: -0.002 [-0.020, 0.010], mean action: 0.086 [-1.356, 1.296], mean observation: 0.087 [-22.486, 16.938], loss: 0.000128, mean_squared_error: 0.000256, mean_q: 0.560463\n",
      " 1549/2000: episode: 3, duration: 12.736s, episode steps: 312, steps per second: 24, episode reward: -0.829, mean reward: -0.003 [-0.020, 0.010], mean action: 0.139 [-1.371, 1.306], mean observation: 0.073 [-21.968, 16.741], loss: 0.000184, mean_squared_error: 0.000367, mean_q: 0.561710\n",
      "done, took 77.564 seconds\n",
      "\n",
      "\n",
      "iteration: 138\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 34.070s, episode steps: 1000, steps per second: 29, episode reward: -0.011, mean reward: -0.000 [-0.002, 0.011], mean action: 0.055 [-1.327, 1.283], mean observation: 0.107 [-22.309, 16.795], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1430/2000: episode: 2, duration: 17.360s, episode steps: 430, steps per second: 25, episode reward: -0.801, mean reward: -0.002 [-0.020, 0.011], mean action: 0.071 [-1.354, 1.319], mean observation: 0.082 [-22.367, 16.832], loss: 0.000121, mean_squared_error: 0.000243, mean_q: 0.563833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1774/2000: episode: 3, duration: 15.315s, episode steps: 344, steps per second: 22, episode reward: -0.790, mean reward: -0.002 [-0.020, 0.011], mean action: 0.145 [-1.414, 1.419], mean observation: 0.075 [-15.088, 14.999], loss: 0.000217, mean_squared_error: 0.000435, mean_q: 0.568039\n",
      "done, took 76.753 seconds\n",
      "\n",
      "\n",
      "iteration: 139\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 39.514s, episode steps: 1000, steps per second: 25, episode reward: 0.004, mean reward: 0.000 [-0.003, 0.011], mean action: 0.117 [-1.360, 1.326], mean observation: 0.107 [-16.419, 15.104], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1266/2000: episode: 2, duration: 10.198s, episode steps: 266, steps per second: 26, episode reward: -0.832, mean reward: -0.003 [-0.020, 0.011], mean action: 0.166 [-1.257, 1.241], mean observation: 0.060 [-16.123, 14.955], loss: 0.000165, mean_squared_error: 0.000330, mean_q: 0.579192\n",
      "done, took 84.035 seconds\n",
      "\n",
      "\n",
      "iteration: 140\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 38.842s, episode steps: 1000, steps per second: 26, episode reward: 0.071, mean reward: 0.000 [-0.002, 0.011], mean action: -0.121 [-1.232, 1.393], mean observation: 0.112 [-15.190, 15.119], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 44.675s, episode steps: 1000, steps per second: 22, episode reward: 0.075, mean reward: 0.000 [-0.003, 0.011], mean action: -0.090 [-1.429, 1.309], mean observation: 0.110 [-15.172, 14.746], loss: 0.000337, mean_squared_error: 0.000674, mean_q: 0.581100\n",
      "done, took 83.534 seconds\n",
      "\n",
      "\n",
      "iteration: 141\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 34.826s, episode steps: 1000, steps per second: 29, episode reward: 0.101, mean reward: 0.000 [-0.003, 0.011], mean action: -0.185 [-1.393, 1.403], mean observation: 0.117 [-15.327, 15.033], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 42.226s, episode steps: 1000, steps per second: 24, episode reward: 0.119, mean reward: 0.000 [-0.003, 0.011], mean action: -0.018 [-1.536, 1.339], mean observation: 0.116 [-14.818, 14.936], loss: 0.000253, mean_squared_error: 0.000505, mean_q: 0.585151\n",
      "done, took 77.068 seconds\n",
      "\n",
      "\n",
      "iteration: 142\n",
      "Training for 2000 steps ...\n",
      "  270/2000: episode: 1, duration: 10.385s, episode steps: 270, steps per second: 26, episode reward: 0.780, mean reward: 0.003 [-0.003, 0.013], mean action: -0.001 [-1.186, 1.233], mean observation: 0.116 [-15.165, 15.096], loss: --, mean_squared_error: --, mean_q: --\n",
      "  517/2000: episode: 2, duration: 9.638s, episode steps: 247, steps per second: 26, episode reward: 0.749, mean reward: 0.003 [-0.003, 0.013], mean action: -0.004 [-1.342, 1.255], mean observation: 0.113 [-14.984, 14.687], loss: --, mean_squared_error: --, mean_q: --\n",
      "  760/2000: episode: 3, duration: 9.709s, episode steps: 243, steps per second: 25, episode reward: 0.752, mean reward: 0.003 [-0.003, 0.013], mean action: -0.030 [-1.301, 1.183], mean observation: 0.113 [-15.242, 14.982], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1005/2000: episode: 4, duration: 9.666s, episode steps: 245, steps per second: 25, episode reward: 0.750, mean reward: 0.003 [-0.003, 0.013], mean action: 0.005 [-1.203, 1.365], mean observation: 0.114 [-15.062, 14.553], loss: 0.000303, mean_squared_error: 0.000606, mean_q: 0.587480\n",
      " 1575/2000: episode: 5, duration: 27.984s, episode steps: 570, steps per second: 20, episode reward: 0.729, mean reward: 0.001 [-0.003, 0.014], mean action: -0.002 [-1.330, 1.257], mean observation: 0.123 [-15.249, 15.070], loss: 0.000249, mean_squared_error: 0.000498, mean_q: 0.586684\n",
      " 1739/2000: episode: 6, duration: 8.852s, episode steps: 164, steps per second: 19, episode reward: 0.702, mean reward: 0.004 [-0.003, 0.015], mean action: -0.122 [-1.172, 1.197], mean observation: 0.104 [-19.367, 15.111], loss: 0.000094, mean_squared_error: 0.000188, mean_q: 0.593990\n",
      " 1876/2000: episode: 7, duration: 7.156s, episode steps: 137, steps per second: 19, episode reward: 0.679, mean reward: 0.005 [-0.002, 0.016], mean action: -0.160 [-1.182, 1.160], mean observation: 0.098 [-25.122, 15.003], loss: 0.000132, mean_squared_error: 0.000265, mean_q: 0.585913\n",
      "done, took 90.230 seconds\n",
      "\n",
      "\n",
      "iteration: 143\n",
      "Training for 2000 steps ...\n",
      "  140/2000: episode: 1, duration: 6.068s, episode steps: 140, steps per second: 23, episode reward: 0.717, mean reward: 0.005 [-0.003, 0.015], mean action: -0.182 [-1.193, 1.240], mean observation: 0.099 [-17.667, 14.940], loss: --, mean_squared_error: --, mean_q: --\n",
      "  279/2000: episode: 2, duration: 5.953s, episode steps: 139, steps per second: 23, episode reward: 0.704, mean reward: 0.005 [-0.003, 0.015], mean action: -0.166 [-1.191, 1.285], mean observation: 0.100 [-19.722, 14.798], loss: --, mean_squared_error: --, mean_q: --\n",
      "  418/2000: episode: 3, duration: 6.055s, episode steps: 139, steps per second: 23, episode reward: 0.688, mean reward: 0.005 [-0.003, 0.015], mean action: -0.198 [-1.208, 1.175], mean observation: 0.103 [-21.156, 14.651], loss: --, mean_squared_error: --, mean_q: --\n",
      "  570/2000: episode: 4, duration: 7.282s, episode steps: 152, steps per second: 21, episode reward: 0.686, mean reward: 0.005 [-0.003, 0.013], mean action: -0.166 [-1.217, 1.271], mean observation: 0.107 [-15.256, 15.176], loss: --, mean_squared_error: --, mean_q: --\n",
      "  712/2000: episode: 5, duration: 6.074s, episode steps: 142, steps per second: 23, episode reward: 0.722, mean reward: 0.005 [-0.003, 0.015], mean action: -0.189 [-1.212, 1.241], mean observation: 0.103 [-15.483, 14.981], loss: --, mean_squared_error: --, mean_q: --\n",
      "  867/2000: episode: 6, duration: 7.518s, episode steps: 155, steps per second: 21, episode reward: 0.677, mean reward: 0.004 [-0.003, 0.013], mean action: -0.126 [-1.221, 1.283], mean observation: 0.107 [-23.022, 14.918], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1006/2000: episode: 7, duration: 6.097s, episode steps: 139, steps per second: 23, episode reward: 0.702, mean reward: 0.005 [-0.003, 0.015], mean action: -0.176 [-1.148, 1.169], mean observation: 0.101 [-14.871, 14.868], loss: 0.000099, mean_squared_error: 0.000199, mean_q: 0.563877\n",
      " 1163/2000: episode: 8, duration: 9.310s, episode steps: 157, steps per second: 17, episode reward: 0.672, mean reward: 0.004 [-0.003, 0.013], mean action: -0.141 [-1.248, 1.190], mean observation: 0.107 [-17.136, 14.677], loss: 0.000243, mean_squared_error: 0.000486, mean_q: 0.592432\n",
      " 1326/2000: episode: 9, duration: 9.626s, episode steps: 163, steps per second: 17, episode reward: 0.691, mean reward: 0.004 [-0.003, 0.014], mean action: -0.156 [-1.281, 1.276], mean observation: 0.107 [-15.143, 14.869], loss: 0.000448, mean_squared_error: 0.000897, mean_q: 0.585874\n",
      " 1499/2000: episode: 10, duration: 9.954s, episode steps: 173, steps per second: 17, episode reward: 0.724, mean reward: 0.004 [-0.003, 0.013], mean action: -0.121 [-1.241, 1.112], mean observation: 0.107 [-19.679, 14.796], loss: 0.000138, mean_squared_error: 0.000275, mean_q: 0.582866\n",
      " 1699/2000: episode: 11, duration: 11.418s, episode steps: 200, steps per second: 18, episode reward: 0.776, mean reward: 0.004 [-0.003, 0.013], mean action: -0.056 [-1.269, 1.328], mean observation: 0.117 [-15.293, 14.994], loss: 0.000141, mean_squared_error: 0.000283, mean_q: 0.589172\n",
      " 1947/2000: episode: 12, duration: 13.647s, episode steps: 248, steps per second: 18, episode reward: 0.750, mean reward: 0.003 [-0.003, 0.013], mean action: 0.029 [-1.297, 1.263], mean observation: 0.116 [-15.280, 14.843], loss: 0.000215, mean_squared_error: 0.000429, mean_q: 0.587091\n",
      "done, took 102.544 seconds\n",
      "\n",
      "\n",
      "iteration: 144\n",
      "Training for 2000 steps ...\n",
      "  212/2000: episode: 1, duration: 11.134s, episode steps: 212, steps per second: 19, episode reward: 0.704, mean reward: 0.003 [-0.003, 0.013], mean action: -0.023 [-1.348, 1.175], mean observation: 0.115 [-37.684, 15.050], loss: --, mean_squared_error: --, mean_q: --\n",
      "  422/2000: episode: 2, duration: 10.582s, episode steps: 210, steps per second: 20, episode reward: 0.710, mean reward: 0.003 [-0.003, 0.013], mean action: 0.021 [-1.136, 1.200], mean observation: 0.112 [-37.463, 15.000], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  628/2000: episode: 3, duration: 10.808s, episode steps: 206, steps per second: 19, episode reward: 0.725, mean reward: 0.004 [-0.003, 0.013], mean action: -0.003 [-1.204, 1.283], mean observation: 0.111 [-38.485, 15.100], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1433/2000: episode: 4, duration: 33.779s, episode steps: 805, steps per second: 24, episode reward: 0.755, mean reward: 0.001 [-0.003, 0.013], mean action: 0.011 [-1.400, 1.391], mean observation: 0.118 [-15.328, 14.675], loss: 0.000183, mean_squared_error: 0.000365, mean_q: 0.590711\n",
      "done, took 95.370 seconds\n",
      "\n",
      "\n",
      "iteration: 145\n",
      "Training for 2000 steps ...\n",
      "  322/2000: episode: 1, duration: 13.370s, episode steps: 322, steps per second: 24, episode reward: 0.736, mean reward: 0.002 [-0.002, 0.013], mean action: 0.004 [-1.309, 1.352], mean observation: 0.116 [-15.529, 15.051], loss: --, mean_squared_error: --, mean_q: --\n",
      "  569/2000: episode: 2, duration: 10.916s, episode steps: 247, steps per second: 23, episode reward: 0.731, mean reward: 0.003 [-0.002, 0.013], mean action: 0.017 [-1.210, 1.298], mean observation: 0.113 [-15.211, 14.683], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 3, duration: 12.605s, episode steps: 293, steps per second: 23, episode reward: 0.747, mean reward: 0.003 [-0.002, 0.013], mean action: -0.000 [-1.381, 1.267], mean observation: 0.116 [-15.103, 14.689], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1141/2000: episode: 4, duration: 13.553s, episode steps: 279, steps per second: 21, episode reward: 0.730, mean reward: 0.003 [-0.002, 0.013], mean action: -0.003 [-1.255, 1.223], mean observation: 0.115 [-15.140, 15.369], loss: 0.000103, mean_squared_error: 0.000206, mean_q: 0.590848\n",
      " 1553/2000: episode: 5, duration: 22.577s, episode steps: 412, steps per second: 18, episode reward: 0.770, mean reward: 0.002 [-0.002, 0.013], mean action: -0.058 [-1.261, 1.276], mean observation: 0.119 [-15.047, 14.926], loss: 0.000203, mean_squared_error: 0.000407, mean_q: 0.597380\n",
      "done, took 95.572 seconds\n",
      "\n",
      "\n",
      "iteration: 146\n",
      "Training for 2000 steps ...\n",
      "  449/2000: episode: 1, duration: 19.951s, episode steps: 449, steps per second: 23, episode reward: 0.742, mean reward: 0.002 [-0.002, 0.013], mean action: -0.067 [-1.337, 1.329], mean observation: 0.115 [-14.971, 15.515], loss: --, mean_squared_error: --, mean_q: --\n",
      "  889/2000: episode: 2, duration: 16.979s, episode steps: 440, steps per second: 26, episode reward: 0.790, mean reward: 0.002 [-0.002, 0.013], mean action: -0.126 [-1.523, 1.361], mean observation: 0.119 [-22.750, 14.967], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1294/2000: episode: 3, duration: 21.734s, episode steps: 405, steps per second: 19, episode reward: 0.731, mean reward: 0.002 [-0.002, 0.012], mean action: -0.028 [-1.188, 1.473], mean observation: 0.117 [-15.193, 14.771], loss: 0.000197, mean_squared_error: 0.000393, mean_q: 0.598488\n",
      " 1566/2000: episode: 4, duration: 12.953s, episode steps: 272, steps per second: 21, episode reward: 0.725, mean reward: 0.003 [-0.002, 0.013], mean action: -0.005 [-1.234, 1.308], mean observation: 0.108 [-15.784, 14.982], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.603498\n",
      " 1810/2000: episode: 5, duration: 13.054s, episode steps: 244, steps per second: 19, episode reward: 0.733, mean reward: 0.003 [-0.002, 0.013], mean action: -0.054 [-1.208, 1.308], mean observation: 0.110 [-15.145, 15.158], loss: 0.000224, mean_squared_error: 0.000447, mean_q: 0.598937\n",
      "done, took 93.573 seconds\n",
      "\n",
      "\n",
      "iteration: 147\n",
      "Training for 2000 steps ...\n",
      "  278/2000: episode: 1, duration: 10.754s, episode steps: 278, steps per second: 26, episode reward: 0.735, mean reward: 0.003 [-0.002, 0.013], mean action: -0.007 [-1.291, 1.272], mean observation: 0.112 [-15.106, 14.806], loss: --, mean_squared_error: --, mean_q: --\n",
      "  565/2000: episode: 2, duration: 10.880s, episode steps: 287, steps per second: 26, episode reward: 0.727, mean reward: 0.003 [-0.002, 0.013], mean action: 0.006 [-1.272, 1.251], mean observation: 0.112 [-15.053, 15.089], loss: --, mean_squared_error: --, mean_q: --\n",
      "  828/2000: episode: 3, duration: 10.458s, episode steps: 263, steps per second: 25, episode reward: 0.742, mean reward: 0.003 [-0.002, 0.013], mean action: 0.008 [-1.217, 1.329], mean observation: 0.112 [-15.253, 14.946], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1151/2000: episode: 4, duration: 13.996s, episode steps: 323, steps per second: 23, episode reward: 0.717, mean reward: 0.002 [-0.002, 0.013], mean action: -0.045 [-1.235, 1.234], mean observation: 0.113 [-15.043, 14.991], loss: 0.000263, mean_squared_error: 0.000527, mean_q: 0.601980\n",
      " 1317/2000: episode: 5, duration: 8.599s, episode steps: 166, steps per second: 19, episode reward: 0.761, mean reward: 0.005 [-0.002, 0.013], mean action: -0.102 [-1.311, 1.244], mean observation: 0.113 [-15.243, 15.494], loss: 0.000386, mean_squared_error: 0.000773, mean_q: 0.604203\n",
      " 1492/2000: episode: 6, duration: 9.313s, episode steps: 175, steps per second: 19, episode reward: 0.736, mean reward: 0.004 [-0.002, 0.013], mean action: -0.087 [-1.385, 1.195], mean observation: 0.112 [-14.696, 14.938], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.604793\n",
      " 1674/2000: episode: 7, duration: 9.367s, episode steps: 182, steps per second: 19, episode reward: 0.739, mean reward: 0.004 [-0.002, 0.013], mean action: -0.094 [-1.141, 1.248], mean observation: 0.112 [-15.013, 15.560], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.598930\n",
      " 1907/2000: episode: 8, duration: 12.447s, episode steps: 233, steps per second: 19, episode reward: 0.794, mean reward: 0.003 [-0.002, 0.013], mean action: -0.168 [-1.302, 1.177], mean observation: 0.121 [-23.159, 16.975], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.601336\n",
      "done, took 90.204 seconds\n",
      "\n",
      "\n",
      "iteration: 148\n",
      "Training for 2000 steps ...\n",
      "  263/2000: episode: 1, duration: 11.143s, episode steps: 263, steps per second: 24, episode reward: 0.740, mean reward: 0.003 [-0.002, 0.013], mean action: -0.094 [-1.259, 1.261], mean observation: 0.112 [-24.502, 17.263], loss: --, mean_squared_error: --, mean_q: --\n",
      "  516/2000: episode: 2, duration: 11.092s, episode steps: 253, steps per second: 23, episode reward: 0.743, mean reward: 0.003 [-0.002, 0.013], mean action: -0.103 [-1.266, 1.199], mean observation: 0.111 [-24.280, 17.223], loss: --, mean_squared_error: --, mean_q: --\n",
      "  772/2000: episode: 3, duration: 11.438s, episode steps: 256, steps per second: 22, episode reward: 0.752, mean reward: 0.003 [-0.002, 0.013], mean action: -0.129 [-1.297, 1.270], mean observation: 0.114 [-24.245, 17.717], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1024/2000: episode: 4, duration: 11.653s, episode steps: 252, steps per second: 22, episode reward: 0.750, mean reward: 0.003 [-0.002, 0.013], mean action: -0.087 [-1.167, 1.379], mean observation: 0.113 [-24.452, 17.734], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.609027\n",
      " 1391/2000: episode: 5, duration: 18.689s, episode steps: 367, steps per second: 20, episode reward: 0.741, mean reward: 0.002 [-0.002, 0.013], mean action: -0.030 [-1.241, 1.319], mean observation: 0.112 [-23.668, 17.596], loss: 0.000288, mean_squared_error: 0.000576, mean_q: 0.611183\n",
      " 1641/2000: episode: 6, duration: 12.484s, episode steps: 250, steps per second: 20, episode reward: 0.713, mean reward: 0.003 [-0.002, 0.013], mean action: -0.080 [-1.321, 1.199], mean observation: 0.108 [-24.011, 17.184], loss: 0.000327, mean_squared_error: 0.000655, mean_q: 0.604188\n",
      " 1881/2000: episode: 7, duration: 11.404s, episode steps: 240, steps per second: 21, episode reward: 0.732, mean reward: 0.003 [-0.002, 0.014], mean action: -0.085 [-1.295, 1.282], mean observation: 0.110 [-23.850, 17.222], loss: 0.000249, mean_squared_error: 0.000499, mean_q: 0.605847\n",
      "done, took 94.193 seconds\n",
      "\n",
      "\n",
      "iteration: 149\n",
      "Training for 2000 steps ...\n",
      "  258/2000: episode: 1, duration: 10.185s, episode steps: 258, steps per second: 25, episode reward: 0.738, mean reward: 0.003 [-0.001, 0.013], mean action: -0.096 [-1.140, 1.296], mean observation: 0.111 [-23.011, 17.139], loss: --, mean_squared_error: --, mean_q: --\n",
      "  543/2000: episode: 2, duration: 11.787s, episode steps: 285, steps per second: 24, episode reward: 0.752, mean reward: 0.003 [-0.002, 0.013], mean action: -0.124 [-1.225, 1.224], mean observation: 0.111 [-23.096, 17.004], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  771/2000: episode: 3, duration: 8.767s, episode steps: 228, steps per second: 26, episode reward: 0.744, mean reward: 0.003 [-0.001, 0.014], mean action: -0.105 [-1.185, 1.470], mean observation: 0.113 [-24.183, 17.643], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1021/2000: episode: 4, duration: 9.888s, episode steps: 250, steps per second: 25, episode reward: 0.756, mean reward: 0.003 [-0.001, 0.013], mean action: -0.141 [-1.227, 1.195], mean observation: 0.111 [-22.844, 17.148], loss: 0.000124, mean_squared_error: 0.000248, mean_q: 0.615747\n",
      " 1340/2000: episode: 5, duration: 14.750s, episode steps: 319, steps per second: 22, episode reward: 0.734, mean reward: 0.002 [-0.002, 0.014], mean action: -0.124 [-1.358, 1.165], mean observation: 0.111 [-23.351, 17.215], loss: 0.000292, mean_squared_error: 0.000583, mean_q: 0.614296\n",
      " 1536/2000: episode: 6, duration: 10.757s, episode steps: 196, steps per second: 18, episode reward: 0.748, mean reward: 0.004 [-0.002, 0.013], mean action: -0.083 [-1.107, 1.237], mean observation: 0.112 [-24.313, 17.467], loss: 0.000151, mean_squared_error: 0.000303, mean_q: 0.616081\n",
      " 1841/2000: episode: 7, duration: 15.066s, episode steps: 305, steps per second: 20, episode reward: 0.748, mean reward: 0.002 [-0.002, 0.013], mean action: -0.033 [-1.317, 1.284], mean observation: 0.118 [-23.942, 17.242], loss: 0.000162, mean_squared_error: 0.000323, mean_q: 0.603692\n",
      "done, took 90.249 seconds\n",
      "\n",
      "\n",
      "iteration: 150\n",
      "Training for 2000 steps ...\n",
      "  291/2000: episode: 1, duration: 12.180s, episode steps: 291, steps per second: 24, episode reward: 0.778, mean reward: 0.003 [-0.002, 0.013], mean action: 0.096 [-1.240, 1.316], mean observation: 0.116 [-23.931, 17.389], loss: --, mean_squared_error: --, mean_q: --\n",
      "  569/2000: episode: 2, duration: 11.921s, episode steps: 278, steps per second: 23, episode reward: 0.769, mean reward: 0.003 [-0.002, 0.013], mean action: 0.030 [-1.195, 1.208], mean observation: 0.116 [-24.669, 17.475], loss: --, mean_squared_error: --, mean_q: --\n",
      "  852/2000: episode: 3, duration: 12.286s, episode steps: 283, steps per second: 23, episode reward: 0.763, mean reward: 0.003 [-0.002, 0.013], mean action: 0.040 [-1.427, 1.336], mean observation: 0.117 [-24.337, 17.638], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1098/2000: episode: 4, duration: 10.913s, episode steps: 246, steps per second: 23, episode reward: 0.775, mean reward: 0.003 [-0.002, 0.013], mean action: -0.016 [-1.188, 1.389], mean observation: 0.113 [-40.535, 17.365], loss: 0.000255, mean_squared_error: 0.000511, mean_q: 0.607378\n",
      " 1479/2000: episode: 5, duration: 18.591s, episode steps: 381, steps per second: 20, episode reward: 0.743, mean reward: 0.002 [-0.002, 0.013], mean action: -0.010 [-1.270, 1.278], mean observation: 0.120 [-23.669, 17.529], loss: 0.000166, mean_squared_error: 0.000332, mean_q: 0.603930\n",
      " 1734/2000: episode: 6, duration: 13.840s, episode steps: 255, steps per second: 18, episode reward: 0.821, mean reward: 0.003 [-0.002, 0.012], mean action: -0.070 [-1.225, 1.253], mean observation: 0.124 [-25.252, 18.354], loss: 0.000158, mean_squared_error: 0.000316, mean_q: 0.616075\n",
      " 1926/2000: episode: 7, duration: 10.503s, episode steps: 192, steps per second: 18, episode reward: 0.620, mean reward: 0.003 [-0.001, 0.013], mean action: -0.032 [-1.150, 1.163], mean observation: 0.107 [-40.638, 18.400], loss: 0.000284, mean_squared_error: 0.000569, mean_q: 0.612736\n",
      "done, took 94.251 seconds\n",
      "\n",
      "\n",
      "iteration: 151\n",
      "Training for 2000 steps ...\n",
      "  185/2000: episode: 1, duration: 8.557s, episode steps: 185, steps per second: 22, episode reward: 0.771, mean reward: 0.004 [-0.001, 0.013], mean action: 0.014 [-1.178, 1.214], mean observation: 0.112 [-23.953, 17.598], loss: --, mean_squared_error: --, mean_q: --\n",
      "  354/2000: episode: 2, duration: 7.784s, episode steps: 169, steps per second: 22, episode reward: 0.704, mean reward: 0.004 [-0.001, 0.013], mean action: -0.069 [-1.297, 1.123], mean observation: 0.106 [-48.185, 17.177], loss: --, mean_squared_error: --, mean_q: --\n",
      "  546/2000: episode: 3, duration: 8.726s, episode steps: 192, steps per second: 22, episode reward: 0.777, mean reward: 0.004 [-0.001, 0.013], mean action: 0.018 [-1.274, 1.186], mean observation: 0.112 [-23.155, 16.989], loss: --, mean_squared_error: --, mean_q: --\n",
      "  713/2000: episode: 4, duration: 7.482s, episode steps: 167, steps per second: 22, episode reward: 0.689, mean reward: 0.004 [-0.001, 0.013], mean action: -0.054 [-1.191, 1.165], mean observation: 0.107 [-47.325, 17.402], loss: --, mean_squared_error: --, mean_q: --\n",
      "  875/2000: episode: 5, duration: 7.749s, episode steps: 162, steps per second: 21, episode reward: 0.689, mean reward: 0.004 [-0.001, 0.013], mean action: -0.049 [-1.220, 1.228], mean observation: 0.100 [-46.943, 17.080], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1067/2000: episode: 6, duration: 9.629s, episode steps: 192, steps per second: 20, episode reward: 0.778, mean reward: 0.004 [-0.001, 0.013], mean action: 0.015 [-1.241, 1.221], mean observation: 0.116 [-23.518, 17.407], loss: 0.000190, mean_squared_error: 0.000380, mean_q: 0.615466\n",
      " 1239/2000: episode: 7, duration: 9.426s, episode steps: 172, steps per second: 18, episode reward: 0.685, mean reward: 0.004 [-0.001, 0.013], mean action: -0.011 [-1.217, 1.234], mean observation: 0.107 [-49.871, 17.164], loss: 0.000277, mean_squared_error: 0.000555, mean_q: 0.609008\n",
      " 1403/2000: episode: 8, duration: 9.036s, episode steps: 164, steps per second: 18, episode reward: 0.648, mean reward: 0.004 [-0.001, 0.013], mean action: -0.010 [-1.238, 1.188], mean observation: 0.093 [-46.995, 17.465], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.609881\n",
      " 1580/2000: episode: 9, duration: 9.262s, episode steps: 177, steps per second: 19, episode reward: 0.762, mean reward: 0.004 [-0.001, 0.013], mean action: 0.012 [-1.170, 1.241], mean observation: 0.112 [-23.163, 16.946], loss: 0.000187, mean_squared_error: 0.000375, mean_q: 0.603481\n",
      " 1777/2000: episode: 10, duration: 10.762s, episode steps: 197, steps per second: 18, episode reward: 0.784, mean reward: 0.004 [-0.001, 0.013], mean action: 0.127 [-1.319, 1.321], mean observation: 0.116 [-20.368, 17.142], loss: 0.000199, mean_squared_error: 0.000399, mean_q: 0.611998\n",
      " 1946/2000: episode: 11, duration: 8.850s, episode steps: 169, steps per second: 19, episode reward: 0.780, mean reward: 0.005 [-0.001, 0.013], mean action: 0.029 [-1.241, 1.224], mean observation: 0.116 [-23.518, 17.221], loss: 0.000251, mean_squared_error: 0.000502, mean_q: 0.618437\n",
      "done, took 100.167 seconds\n",
      "\n",
      "\n",
      "iteration: 152\n",
      "Training for 2000 steps ...\n",
      "  194/2000: episode: 1, duration: 8.100s, episode steps: 194, steps per second: 24, episode reward: 0.757, mean reward: 0.004 [-0.000, 0.012], mean action: 0.055 [-1.249, 1.260], mean observation: 0.109 [-23.562, 17.362], loss: --, mean_squared_error: --, mean_q: --\n",
      "  412/2000: episode: 2, duration: 9.234s, episode steps: 218, steps per second: 24, episode reward: 0.808, mean reward: 0.004 [-0.001, 0.012], mean action: 0.042 [-1.291, 1.182], mean observation: 0.118 [-23.403, 17.090], loss: --, mean_squared_error: --, mean_q: --\n",
      "  625/2000: episode: 3, duration: 9.274s, episode steps: 213, steps per second: 23, episode reward: 0.791, mean reward: 0.004 [-0.000, 0.012], mean action: 0.068 [-1.304, 1.171], mean observation: 0.119 [-23.648, 17.413], loss: --, mean_squared_error: --, mean_q: --\n",
      "  824/2000: episode: 4, duration: 8.371s, episode steps: 199, steps per second: 24, episode reward: 0.772, mean reward: 0.004 [-0.000, 0.012], mean action: 0.060 [-1.227, 1.198], mean observation: 0.113 [-23.160, 17.023], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1037/2000: episode: 5, duration: 9.509s, episode steps: 213, steps per second: 22, episode reward: 0.786, mean reward: 0.004 [-0.000, 0.012], mean action: 0.073 [-1.182, 1.262], mean observation: 0.117 [-25.638, 17.203], loss: 0.000107, mean_squared_error: 0.000215, mean_q: 0.615876\n",
      " 1225/2000: episode: 6, duration: 10.409s, episode steps: 188, steps per second: 18, episode reward: 0.776, mean reward: 0.004 [-0.001, 0.013], mean action: 0.080 [-1.175, 1.219], mean observation: 0.112 [-23.187, 17.097], loss: 0.000360, mean_squared_error: 0.000719, mean_q: 0.611487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1438/2000: episode: 7, duration: 11.319s, episode steps: 213, steps per second: 19, episode reward: 0.777, mean reward: 0.004 [-0.000, 0.013], mean action: 0.146 [-1.227, 1.170], mean observation: 0.113 [-22.351, 16.886], loss: 0.000195, mean_squared_error: 0.000390, mean_q: 0.613777\n",
      " 1678/2000: episode: 8, duration: 13.459s, episode steps: 240, steps per second: 18, episode reward: 0.802, mean reward: 0.003 [-0.000, 0.012], mean action: 0.198 [-1.180, 1.341], mean observation: 0.120 [-19.949, 16.635], loss: 0.000297, mean_squared_error: 0.000595, mean_q: 0.615715\n",
      " 1886/2000: episode: 9, duration: 11.407s, episode steps: 208, steps per second: 18, episode reward: 0.815, mean reward: 0.004 [-0.000, 0.012], mean action: 0.153 [-1.141, 1.266], mean observation: 0.125 [-19.946, 16.263], loss: 0.000312, mean_squared_error: 0.000625, mean_q: 0.607687\n",
      "done, took 97.082 seconds\n",
      "\n",
      "\n",
      "iteration: 153\n",
      "Training for 2000 steps ...\n",
      "  229/2000: episode: 1, duration: 9.959s, episode steps: 229, steps per second: 23, episode reward: 0.797, mean reward: 0.003 [-0.000, 0.012], mean action: 0.185 [-1.180, 1.259], mean observation: 0.121 [-22.198, 16.664], loss: --, mean_squared_error: --, mean_q: --\n",
      "  474/2000: episode: 2, duration: 10.539s, episode steps: 245, steps per second: 23, episode reward: 0.835, mean reward: 0.003 [-0.000, 0.012], mean action: 0.131 [-1.364, 1.378], mean observation: 0.128 [-22.865, 17.109], loss: --, mean_squared_error: --, mean_q: --\n",
      "  694/2000: episode: 3, duration: 9.584s, episode steps: 220, steps per second: 23, episode reward: 0.812, mean reward: 0.004 [-0.000, 0.012], mean action: 0.146 [-1.180, 1.255], mean observation: 0.123 [-21.824, 16.574], loss: --, mean_squared_error: --, mean_q: --\n",
      "  928/2000: episode: 4, duration: 10.312s, episode steps: 234, steps per second: 23, episode reward: 0.839, mean reward: 0.004 [-0.000, 0.012], mean action: 0.113 [-1.275, 1.224], mean observation: 0.129 [-21.854, 16.651], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1169/2000: episode: 5, duration: 11.972s, episode steps: 241, steps per second: 20, episode reward: 0.839, mean reward: 0.003 [-0.000, 0.012], mean action: 0.127 [-1.288, 1.216], mean observation: 0.129 [-22.283, 16.799], loss: 0.000092, mean_squared_error: 0.000184, mean_q: 0.605437\n",
      " 1354/2000: episode: 6, duration: 10.260s, episode steps: 185, steps per second: 18, episode reward: 0.742, mean reward: 0.004 [-0.000, 0.013], mean action: 0.109 [-1.155, 1.255], mean observation: 0.106 [-21.391, 16.414], loss: 0.000361, mean_squared_error: 0.000721, mean_q: 0.615003\n",
      " 1530/2000: episode: 7, duration: 9.624s, episode steps: 176, steps per second: 18, episode reward: 0.774, mean reward: 0.004 [-0.000, 0.013], mean action: 0.126 [-1.251, 1.226], mean observation: 0.112 [-19.437, 15.913], loss: 0.000122, mean_squared_error: 0.000245, mean_q: 0.618902\n",
      " 1715/2000: episode: 8, duration: 10.294s, episode steps: 185, steps per second: 18, episode reward: 0.805, mean reward: 0.004 [-0.000, 0.013], mean action: 0.058 [-1.215, 1.226], mean observation: 0.120 [-19.772, 16.239], loss: 0.000220, mean_squared_error: 0.000439, mean_q: 0.609388\n",
      " 1905/2000: episode: 9, duration: 10.411s, episode steps: 190, steps per second: 18, episode reward: 0.755, mean reward: 0.004 [-0.000, 0.013], mean action: 0.083 [-1.152, 1.183], mean observation: 0.113 [-20.570, 16.892], loss: 0.000286, mean_squared_error: 0.000573, mean_q: 0.613623\n",
      "done, took 97.527 seconds\n",
      "\n",
      "\n",
      "iteration: 154\n",
      "Training for 2000 steps ...\n",
      "  192/2000: episode: 1, duration: 7.662s, episode steps: 192, steps per second: 25, episode reward: 0.819, mean reward: 0.004 [-0.001, 0.013], mean action: 0.013 [-1.230, 1.156], mean observation: 0.117 [-25.859, 16.811], loss: --, mean_squared_error: --, mean_q: --\n",
      "  384/2000: episode: 2, duration: 8.091s, episode steps: 192, steps per second: 24, episode reward: 0.780, mean reward: 0.004 [-0.001, 0.013], mean action: 0.012 [-1.171, 1.285], mean observation: 0.114 [-42.104, 17.018], loss: --, mean_squared_error: --, mean_q: --\n",
      "  577/2000: episode: 3, duration: 7.608s, episode steps: 193, steps per second: 25, episode reward: 0.819, mean reward: 0.004 [-0.001, 0.013], mean action: 0.019 [-1.187, 1.201], mean observation: 0.118 [-22.503, 16.286], loss: --, mean_squared_error: --, mean_q: --\n",
      "  767/2000: episode: 4, duration: 7.628s, episode steps: 190, steps per second: 25, episode reward: 0.792, mean reward: 0.004 [-0.001, 0.014], mean action: 0.003 [-1.235, 1.211], mean observation: 0.114 [-35.021, 16.475], loss: --, mean_squared_error: --, mean_q: --\n",
      "  958/2000: episode: 5, duration: 7.530s, episode steps: 191, steps per second: 25, episode reward: 0.806, mean reward: 0.004 [-0.001, 0.014], mean action: 0.020 [-1.203, 1.187], mean observation: 0.118 [-22.215, 16.313], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1135/2000: episode: 6, duration: 8.662s, episode steps: 177, steps per second: 20, episode reward: 0.678, mean reward: 0.004 [-0.001, 0.013], mean action: 0.002 [-1.184, 1.225], mean observation: 0.108 [-44.851, 16.548], loss: 0.000172, mean_squared_error: 0.000344, mean_q: 0.618716\n",
      " 1298/2000: episode: 7, duration: 8.392s, episode steps: 163, steps per second: 19, episode reward: 0.685, mean reward: 0.004 [-0.001, 0.013], mean action: -0.002 [-1.143, 1.207], mean observation: 0.098 [-46.265, 16.743], loss: 0.000311, mean_squared_error: 0.000622, mean_q: 0.609096\n",
      " 1481/2000: episode: 8, duration: 9.090s, episode steps: 183, steps per second: 20, episode reward: 0.752, mean reward: 0.004 [-0.001, 0.013], mean action: 0.012 [-1.134, 1.205], mean observation: 0.106 [-23.634, 16.699], loss: 0.000090, mean_squared_error: 0.000180, mean_q: 0.611874\n",
      " 1661/2000: episode: 9, duration: 8.862s, episode steps: 180, steps per second: 20, episode reward: 0.764, mean reward: 0.004 [-0.000, 0.013], mean action: -0.001 [-1.326, 1.199], mean observation: 0.115 [-22.423, 16.268], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.618314\n",
      " 1843/2000: episode: 10, duration: 9.163s, episode steps: 182, steps per second: 20, episode reward: 0.779, mean reward: 0.004 [-0.001, 0.013], mean action: 0.019 [-1.186, 1.216], mean observation: 0.116 [-23.902, 16.854], loss: 0.000181, mean_squared_error: 0.000363, mean_q: 0.616197\n",
      "done, took 90.310 seconds\n",
      "\n",
      "\n",
      "iteration: 155\n",
      "Training for 2000 steps ...\n",
      "  172/2000: episode: 1, duration: 6.719s, episode steps: 172, steps per second: 26, episode reward: 0.748, mean reward: 0.004 [-0.001, 0.013], mean action: 0.045 [-1.159, 1.182], mean observation: 0.111 [-21.578, 16.214], loss: --, mean_squared_error: --, mean_q: --\n",
      "  347/2000: episode: 2, duration: 7.020s, episode steps: 175, steps per second: 25, episode reward: 0.763, mean reward: 0.004 [-0.001, 0.013], mean action: 0.023 [-1.246, 1.172], mean observation: 0.113 [-22.731, 16.602], loss: --, mean_squared_error: --, mean_q: --\n",
      "  520/2000: episode: 3, duration: 6.904s, episode steps: 173, steps per second: 25, episode reward: 0.759, mean reward: 0.004 [-0.001, 0.013], mean action: 0.028 [-1.360, 1.263], mean observation: 0.111 [-22.198, 16.435], loss: --, mean_squared_error: --, mean_q: --\n",
      "  693/2000: episode: 4, duration: 6.799s, episode steps: 173, steps per second: 25, episode reward: 0.770, mean reward: 0.004 [-0.001, 0.013], mean action: 0.037 [-1.188, 1.243], mean observation: 0.111 [-22.370, 16.559], loss: --, mean_squared_error: --, mean_q: --\n",
      "  867/2000: episode: 5, duration: 6.851s, episode steps: 174, steps per second: 25, episode reward: 0.759, mean reward: 0.004 [-0.001, 0.013], mean action: 0.034 [-1.195, 1.157], mean observation: 0.111 [-22.646, 16.686], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1037/2000: episode: 6, duration: 7.018s, episode steps: 170, steps per second: 24, episode reward: 0.754, mean reward: 0.004 [-0.001, 0.013], mean action: 0.003 [-1.223, 1.185], mean observation: 0.110 [-23.318, 17.003], loss: 0.000067, mean_squared_error: 0.000135, mean_q: 0.634524\n",
      " 1215/2000: episode: 7, duration: 9.212s, episode steps: 178, steps per second: 19, episode reward: 0.795, mean reward: 0.004 [-0.001, 0.013], mean action: 0.058 [-1.298, 1.261], mean observation: 0.115 [-19.474, 16.345], loss: 0.000120, mean_squared_error: 0.000239, mean_q: 0.608227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1382/2000: episode: 8, duration: 8.846s, episode steps: 167, steps per second: 19, episode reward: 0.780, mean reward: 0.005 [-0.001, 0.013], mean action: 0.055 [-1.117, 1.156], mean observation: 0.108 [-22.016, 15.988], loss: 0.000261, mean_squared_error: 0.000523, mean_q: 0.616082\n",
      " 1541/2000: episode: 9, duration: 8.520s, episode steps: 159, steps per second: 19, episode reward: 0.770, mean reward: 0.005 [-0.001, 0.013], mean action: 0.024 [-1.241, 1.253], mean observation: 0.103 [-23.295, 16.523], loss: 0.000279, mean_squared_error: 0.000558, mean_q: 0.606445\n",
      " 1695/2000: episode: 10, duration: 7.916s, episode steps: 154, steps per second: 19, episode reward: 0.771, mean reward: 0.005 [-0.001, 0.013], mean action: -0.062 [-1.180, 1.140], mean observation: 0.114 [-32.961, 16.250], loss: 0.000219, mean_squared_error: 0.000438, mean_q: 0.606458\n",
      " 1855/2000: episode: 11, duration: 8.582s, episode steps: 160, steps per second: 19, episode reward: 0.788, mean reward: 0.005 [-0.002, 0.013], mean action: 0.006 [-1.149, 1.233], mean observation: 0.112 [-23.208, 16.595], loss: 0.000338, mean_squared_error: 0.000676, mean_q: 0.607868\n",
      "done, took 91.962 seconds\n",
      "\n",
      "\n",
      "iteration: 156\n",
      "Training for 2000 steps ...\n",
      "  172/2000: episode: 1, duration: 7.592s, episode steps: 172, steps per second: 23, episode reward: 0.810, mean reward: 0.005 [-0.001, 0.013], mean action: 0.040 [-1.173, 1.173], mean observation: 0.118 [-23.377, 16.609], loss: --, mean_squared_error: --, mean_q: --\n",
      "  334/2000: episode: 2, duration: 6.604s, episode steps: 162, steps per second: 25, episode reward: 0.788, mean reward: 0.005 [-0.001, 0.013], mean action: 0.009 [-1.251, 1.161], mean observation: 0.108 [-23.402, 16.519], loss: --, mean_squared_error: --, mean_q: --\n",
      "  499/2000: episode: 3, duration: 7.039s, episode steps: 165, steps per second: 23, episode reward: 0.788, mean reward: 0.005 [-0.000, 0.013], mean action: 0.026 [-1.212, 1.251], mean observation: 0.112 [-23.732, 16.742], loss: --, mean_squared_error: --, mean_q: --\n",
      "  663/2000: episode: 4, duration: 6.896s, episode steps: 164, steps per second: 24, episode reward: 0.803, mean reward: 0.005 [-0.001, 0.013], mean action: 0.028 [-1.168, 1.204], mean observation: 0.112 [-23.879, 16.790], loss: --, mean_squared_error: --, mean_q: --\n",
      "  837/2000: episode: 5, duration: 7.317s, episode steps: 174, steps per second: 24, episode reward: 0.807, mean reward: 0.005 [-0.001, 0.013], mean action: 0.029 [-1.122, 1.175], mean observation: 0.116 [-23.413, 16.657], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1001/2000: episode: 6, duration: 6.881s, episode steps: 164, steps per second: 24, episode reward: 0.796, mean reward: 0.005 [-0.001, 0.013], mean action: 0.039 [-1.174, 1.148], mean observation: 0.111 [-22.695, 16.284], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1164/2000: episode: 7, duration: 8.840s, episode steps: 163, steps per second: 18, episode reward: 0.784, mean reward: 0.005 [-0.001, 0.013], mean action: 0.011 [-1.210, 1.255], mean observation: 0.113 [-23.377, 16.529], loss: 0.000284, mean_squared_error: 0.000567, mean_q: 0.608407\n",
      " 1343/2000: episode: 8, duration: 9.521s, episode steps: 179, steps per second: 19, episode reward: 0.792, mean reward: 0.004 [-0.001, 0.013], mean action: 0.017 [-1.287, 1.232], mean observation: 0.118 [-23.555, 16.721], loss: 0.000441, mean_squared_error: 0.000881, mean_q: 0.614896\n",
      " 1517/2000: episode: 9, duration: 9.001s, episode steps: 174, steps per second: 19, episode reward: 0.788, mean reward: 0.005 [-0.001, 0.013], mean action: 0.034 [-1.237, 1.294], mean observation: 0.114 [-19.967, 16.293], loss: 0.000268, mean_squared_error: 0.000537, mean_q: 0.611852\n",
      " 1675/2000: episode: 10, duration: 8.298s, episode steps: 158, steps per second: 19, episode reward: 0.776, mean reward: 0.005 [-0.001, 0.013], mean action: 0.005 [-1.201, 1.230], mean observation: 0.107 [-13.026, 16.021], loss: 0.000318, mean_squared_error: 0.000636, mean_q: 0.616243\n",
      " 1835/2000: episode: 11, duration: 8.652s, episode steps: 160, steps per second: 18, episode reward: 0.784, mean reward: 0.005 [-0.000, 0.013], mean action: 0.037 [-1.256, 1.245], mean observation: 0.111 [-21.455, 16.605], loss: 0.000320, mean_squared_error: 0.000639, mean_q: 0.609346\n",
      " 1984/2000: episode: 12, duration: 7.737s, episode steps: 149, steps per second: 19, episode reward: 0.780, mean reward: 0.005 [-0.002, 0.013], mean action: 0.004 [-1.134, 1.189], mean observation: 0.115 [-13.275, 16.168], loss: 0.000201, mean_squared_error: 0.000401, mean_q: 0.609843\n",
      "done, took 95.430 seconds\n",
      "\n",
      "\n",
      "iteration: 157\n",
      "Training for 2000 steps ...\n",
      "  151/2000: episode: 1, duration: 6.503s, episode steps: 151, steps per second: 23, episode reward: 0.772, mean reward: 0.005 [-0.001, 0.013], mean action: -0.033 [-1.171, 1.179], mean observation: 0.111 [-43.508, 16.097], loss: --, mean_squared_error: --, mean_q: --\n",
      "  300/2000: episode: 2, duration: 6.279s, episode steps: 149, steps per second: 24, episode reward: 0.762, mean reward: 0.005 [-0.001, 0.013], mean action: -0.020 [-1.129, 1.198], mean observation: 0.108 [-43.187, 16.150], loss: --, mean_squared_error: --, mean_q: --\n",
      "  449/2000: episode: 3, duration: 6.522s, episode steps: 149, steps per second: 23, episode reward: 0.750, mean reward: 0.005 [-0.001, 0.013], mean action: -0.003 [-1.210, 1.190], mean observation: 0.106 [-46.295, 16.186], loss: --, mean_squared_error: --, mean_q: --\n",
      "  599/2000: episode: 4, duration: 6.441s, episode steps: 150, steps per second: 23, episode reward: 0.763, mean reward: 0.005 [-0.001, 0.013], mean action: 0.001 [-1.183, 1.206], mean observation: 0.108 [-44.079, 16.699], loss: --, mean_squared_error: --, mean_q: --\n",
      "  748/2000: episode: 5, duration: 6.444s, episode steps: 149, steps per second: 23, episode reward: 0.761, mean reward: 0.005 [-0.001, 0.013], mean action: -0.020 [-1.167, 1.181], mean observation: 0.111 [-47.039, 16.209], loss: --, mean_squared_error: --, mean_q: --\n",
      "  899/2000: episode: 6, duration: 6.548s, episode steps: 151, steps per second: 23, episode reward: 0.766, mean reward: 0.005 [-0.001, 0.013], mean action: -0.010 [-1.134, 1.272], mean observation: 0.117 [-32.267, 16.669], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1049/2000: episode: 7, duration: 6.955s, episode steps: 150, steps per second: 22, episode reward: 0.763, mean reward: 0.005 [-0.001, 0.013], mean action: -0.007 [-1.269, 1.169], mean observation: 0.108 [-43.239, 16.493], loss: 0.000118, mean_squared_error: 0.000236, mean_q: 0.606969\n",
      " 1196/2000: episode: 8, duration: 7.556s, episode steps: 147, steps per second: 19, episode reward: 0.762, mean reward: 0.005 [-0.001, 0.013], mean action: -0.037 [-1.134, 1.210], mean observation: 0.112 [-39.544, 16.914], loss: 0.000426, mean_squared_error: 0.000852, mean_q: 0.615214\n",
      " 1347/2000: episode: 9, duration: 8.467s, episode steps: 151, steps per second: 18, episode reward: 0.777, mean reward: 0.005 [-0.002, 0.013], mean action: -0.017 [-1.123, 1.341], mean observation: 0.112 [-40.047, 15.979], loss: 0.000116, mean_squared_error: 0.000233, mean_q: 0.616737\n",
      " 1503/2000: episode: 10, duration: 8.528s, episode steps: 156, steps per second: 18, episode reward: 0.771, mean reward: 0.005 [-0.002, 0.013], mean action: 0.021 [-1.138, 1.144], mean observation: 0.108 [-33.251, 16.086], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.603312\n",
      " 1678/2000: episode: 11, duration: 9.358s, episode steps: 175, steps per second: 19, episode reward: 0.818, mean reward: 0.005 [-0.001, 0.013], mean action: 0.009 [-1.151, 1.174], mean observation: 0.121 [-13.622, 16.450], loss: 0.000157, mean_squared_error: 0.000315, mean_q: 0.604877\n",
      " 1852/2000: episode: 12, duration: 9.467s, episode steps: 174, steps per second: 18, episode reward: 0.825, mean reward: 0.005 [-0.000, 0.013], mean action: 0.005 [-1.259, 1.200], mean observation: 0.121 [-13.044, 16.565], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.612628\n",
      "done, took 97.353 seconds\n",
      "\n",
      "\n",
      "iteration: 158\n",
      "Training for 2000 steps ...\n",
      "  168/2000: episode: 1, duration: 7.574s, episode steps: 168, steps per second: 22, episode reward: 0.785, mean reward: 0.005 [-0.001, 0.013], mean action: 0.023 [-1.176, 1.157], mean observation: 0.116 [-12.092, 15.982], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  337/2000: episode: 2, duration: 7.507s, episode steps: 169, steps per second: 23, episode reward: 0.787, mean reward: 0.005 [-0.001, 0.013], mean action: 0.051 [-1.131, 1.197], mean observation: 0.113 [-12.571, 16.246], loss: --, mean_squared_error: --, mean_q: --\n",
      "  504/2000: episode: 3, duration: 7.264s, episode steps: 167, steps per second: 23, episode reward: 0.781, mean reward: 0.005 [-0.000, 0.013], mean action: 0.049 [-1.134, 1.284], mean observation: 0.111 [-11.878, 15.822], loss: --, mean_squared_error: --, mean_q: --\n",
      "  680/2000: episode: 4, duration: 7.982s, episode steps: 176, steps per second: 22, episode reward: 0.808, mean reward: 0.005 [-0.001, 0.013], mean action: 0.050 [-1.224, 1.243], mean observation: 0.121 [-12.278, 16.043], loss: --, mean_squared_error: --, mean_q: --\n",
      "  846/2000: episode: 5, duration: 7.389s, episode steps: 166, steps per second: 22, episode reward: 0.783, mean reward: 0.005 [-0.001, 0.013], mean action: 0.023 [-1.155, 1.126], mean observation: 0.113 [-12.656, 16.297], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1008/2000: episode: 6, duration: 7.473s, episode steps: 162, steps per second: 22, episode reward: 0.775, mean reward: 0.005 [-0.000, 0.013], mean action: 0.043 [-1.322, 1.289], mean observation: 0.108 [-37.303, 16.150], loss: 0.000099, mean_squared_error: 0.000199, mean_q: 0.610513\n",
      " 1190/2000: episode: 7, duration: 9.826s, episode steps: 182, steps per second: 19, episode reward: 0.822, mean reward: 0.005 [-0.001, 0.013], mean action: -0.017 [-1.243, 1.127], mean observation: 0.121 [-12.570, 16.239], loss: 0.000147, mean_squared_error: 0.000295, mean_q: 0.613486\n",
      " 1347/2000: episode: 8, duration: 8.653s, episode steps: 157, steps per second: 18, episode reward: 0.783, mean reward: 0.005 [-0.001, 0.013], mean action: 0.001 [-1.214, 1.245], mean observation: 0.114 [-27.046, 16.624], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.608758\n",
      " 1505/2000: episode: 9, duration: 8.731s, episode steps: 158, steps per second: 18, episode reward: 0.772, mean reward: 0.005 [-0.000, 0.013], mean action: -0.017 [-1.138, 1.144], mean observation: 0.113 [-24.776, 16.068], loss: 0.000255, mean_squared_error: 0.000510, mean_q: 0.603777\n",
      " 1659/2000: episode: 10, duration: 8.646s, episode steps: 154, steps per second: 18, episode reward: 0.777, mean reward: 0.005 [-0.001, 0.013], mean action: 0.040 [-1.177, 1.195], mean observation: 0.113 [-48.260, 15.685], loss: 0.000192, mean_squared_error: 0.000385, mean_q: 0.610369\n",
      " 1825/2000: episode: 11, duration: 9.509s, episode steps: 166, steps per second: 17, episode reward: 0.802, mean reward: 0.005 [-0.000, 0.013], mean action: -0.025 [-1.276, 1.179], mean observation: 0.120 [-13.243, 16.392], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.604346\n",
      " 1980/2000: episode: 12, duration: 8.556s, episode steps: 155, steps per second: 18, episode reward: 0.796, mean reward: 0.005 [-0.001, 0.013], mean action: -0.030 [-1.087, 1.157], mean observation: 0.119 [-24.412, 16.416], loss: 0.000088, mean_squared_error: 0.000177, mean_q: 0.605295\n",
      "done, took 100.410 seconds\n",
      "\n",
      "\n",
      "iteration: 159\n",
      "Training for 2000 steps ...\n",
      "  156/2000: episode: 1, duration: 6.876s, episode steps: 156, steps per second: 23, episode reward: 0.816, mean reward: 0.005 [-0.001, 0.013], mean action: -0.057 [-1.214, 1.206], mean observation: 0.121 [-19.985, 17.085], loss: --, mean_squared_error: --, mean_q: --\n",
      "  311/2000: episode: 2, duration: 6.969s, episode steps: 155, steps per second: 22, episode reward: 0.808, mean reward: 0.005 [-0.001, 0.013], mean action: -0.029 [-1.205, 1.197], mean observation: 0.120 [-24.076, 16.390], loss: --, mean_squared_error: --, mean_q: --\n",
      "  465/2000: episode: 3, duration: 7.179s, episode steps: 154, steps per second: 21, episode reward: 0.794, mean reward: 0.005 [-0.001, 0.013], mean action: -0.033 [-1.247, 1.204], mean observation: 0.114 [-30.550, 16.195], loss: --, mean_squared_error: --, mean_q: --\n",
      "  620/2000: episode: 4, duration: 7.059s, episode steps: 155, steps per second: 22, episode reward: 0.816, mean reward: 0.005 [-0.001, 0.013], mean action: -0.084 [-1.227, 1.185], mean observation: 0.127 [-13.820, 16.302], loss: --, mean_squared_error: --, mean_q: --\n",
      "  777/2000: episode: 5, duration: 7.166s, episode steps: 157, steps per second: 22, episode reward: 0.817, mean reward: 0.005 [-0.001, 0.013], mean action: -0.036 [-1.407, 1.230], mean observation: 0.118 [-24.574, 16.706], loss: --, mean_squared_error: --, mean_q: --\n",
      "  934/2000: episode: 6, duration: 7.217s, episode steps: 157, steps per second: 22, episode reward: 0.811, mean reward: 0.005 [-0.001, 0.013], mean action: -0.035 [-1.141, 1.334], mean observation: 0.122 [-22.481, 16.475], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1092/2000: episode: 7, duration: 7.943s, episode steps: 158, steps per second: 20, episode reward: 0.830, mean reward: 0.005 [-0.001, 0.013], mean action: -0.057 [-1.255, 1.139], mean observation: 0.129 [-14.286, 16.455], loss: 0.000286, mean_squared_error: 0.000572, mean_q: 0.609155\n",
      " 1251/2000: episode: 8, duration: 8.814s, episode steps: 159, steps per second: 18, episode reward: 0.790, mean reward: 0.005 [-0.001, 0.013], mean action: -0.009 [-1.179, 1.123], mean observation: 0.117 [-29.211, 16.164], loss: 0.000084, mean_squared_error: 0.000168, mean_q: 0.597487\n",
      " 1414/2000: episode: 9, duration: 9.314s, episode steps: 163, steps per second: 18, episode reward: 0.786, mean reward: 0.005 [-0.002, 0.013], mean action: 0.025 [-1.133, 1.220], mean observation: 0.117 [-30.295, 16.658], loss: 0.000077, mean_squared_error: 0.000154, mean_q: 0.596442\n",
      " 1584/2000: episode: 10, duration: 9.220s, episode steps: 170, steps per second: 18, episode reward: 0.841, mean reward: 0.005 [-0.002, 0.013], mean action: -0.067 [-1.203, 1.208], mean observation: 0.130 [-13.616, 16.427], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.596695\n",
      " 1738/2000: episode: 11, duration: 8.283s, episode steps: 154, steps per second: 19, episode reward: 0.814, mean reward: 0.005 [-0.002, 0.013], mean action: -0.119 [-1.236, 1.158], mean observation: 0.119 [-20.622, 16.664], loss: 0.000215, mean_squared_error: 0.000431, mean_q: 0.598988\n",
      " 1892/2000: episode: 12, duration: 8.784s, episode steps: 154, steps per second: 18, episode reward: 0.782, mean reward: 0.005 [-0.002, 0.013], mean action: -0.033 [-1.193, 1.211], mean observation: 0.117 [-43.536, 16.670], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.597564\n",
      "done, took 100.474 seconds\n",
      "\n",
      "\n",
      "iteration: 160\n",
      "Training for 2000 steps ...\n",
      "  154/2000: episode: 1, duration: 7.192s, episode steps: 154, steps per second: 21, episode reward: 0.790, mean reward: 0.005 [-0.002, 0.013], mean action: -0.083 [-1.288, 1.240], mean observation: 0.119 [-30.544, 16.633], loss: --, mean_squared_error: --, mean_q: --\n",
      "  308/2000: episode: 2, duration: 7.158s, episode steps: 154, steps per second: 22, episode reward: 0.782, mean reward: 0.005 [-0.002, 0.013], mean action: -0.065 [-1.312, 1.172], mean observation: 0.120 [-26.731, 16.327], loss: --, mean_squared_error: --, mean_q: --\n",
      "  463/2000: episode: 3, duration: 7.119s, episode steps: 155, steps per second: 22, episode reward: 0.792, mean reward: 0.005 [-0.002, 0.013], mean action: -0.058 [-1.370, 1.187], mean observation: 0.118 [-23.992, 16.860], loss: --, mean_squared_error: --, mean_q: --\n",
      "  619/2000: episode: 4, duration: 7.423s, episode steps: 156, steps per second: 21, episode reward: 0.807, mean reward: 0.005 [-0.002, 0.013], mean action: -0.063 [-1.214, 1.170], mean observation: 0.128 [-12.438, 16.160], loss: --, mean_squared_error: --, mean_q: --\n",
      "  773/2000: episode: 5, duration: 7.247s, episode steps: 154, steps per second: 21, episode reward: 0.782, mean reward: 0.005 [-0.002, 0.013], mean action: -0.050 [-1.295, 1.169], mean observation: 0.116 [-42.993, 16.614], loss: --, mean_squared_error: --, mean_q: --\n",
      "  924/2000: episode: 6, duration: 7.112s, episode steps: 151, steps per second: 21, episode reward: 0.768, mean reward: 0.005 [-0.002, 0.013], mean action: -0.039 [-1.199, 1.219], mean observation: 0.115 [-49.883, 16.417], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1079/2000: episode: 7, duration: 7.954s, episode steps: 155, steps per second: 19, episode reward: 0.800, mean reward: 0.005 [-0.002, 0.013], mean action: -0.069 [-1.191, 1.193], mean observation: 0.117 [-32.413, 16.764], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.611840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1228/2000: episode: 8, duration: 8.466s, episode steps: 149, steps per second: 18, episode reward: 0.766, mean reward: 0.005 [-0.002, 0.013], mean action: -0.064 [-1.216, 1.194], mean observation: 0.111 [-48.948, 16.429], loss: 0.000438, mean_squared_error: 0.000876, mean_q: 0.604735\n",
      " 1397/2000: episode: 9, duration: 9.416s, episode steps: 169, steps per second: 18, episode reward: 0.821, mean reward: 0.005 [-0.002, 0.013], mean action: -0.017 [-1.205, 1.271], mean observation: 0.127 [-15.202, 15.538], loss: 0.000151, mean_squared_error: 0.000303, mean_q: 0.616677\n",
      " 1574/2000: episode: 10, duration: 7.820s, episode steps: 177, steps per second: 23, episode reward: -0.725, mean reward: -0.004 [-0.021, 0.010], mean action: -0.188 [-1.174, 1.160], mean observation: 0.083 [-8.200, 17.717], loss: 0.000244, mean_squared_error: 0.000487, mean_q: 0.599019\n",
      " 1730/2000: episode: 11, duration: 8.870s, episode steps: 156, steps per second: 18, episode reward: 0.774, mean reward: 0.005 [-0.001, 0.013], mean action: -0.094 [-1.196, 1.247], mean observation: 0.120 [-51.061, 17.196], loss: 0.000207, mean_squared_error: 0.000415, mean_q: 0.600983\n",
      " 1880/2000: episode: 12, duration: 8.687s, episode steps: 150, steps per second: 17, episode reward: 0.771, mean reward: 0.005 [-0.002, 0.013], mean action: -0.069 [-1.257, 1.203], mean observation: 0.108 [-49.686, 15.173], loss: 0.000176, mean_squared_error: 0.000352, mean_q: 0.597905\n",
      "done, took 100.645 seconds\n",
      "\n",
      "\n",
      "iteration: 161\n",
      "Training for 2000 steps ...\n",
      "  178/2000: episode: 1, duration: 8.357s, episode steps: 178, steps per second: 21, episode reward: 0.806, mean reward: 0.005 [-0.002, 0.013], mean action: -0.034 [-1.242, 1.275], mean observation: 0.122 [-13.616, 19.288], loss: --, mean_squared_error: --, mean_q: --\n",
      "  358/2000: episode: 2, duration: 8.637s, episode steps: 180, steps per second: 21, episode reward: 0.809, mean reward: 0.004 [-0.002, 0.013], mean action: -0.003 [-1.160, 1.246], mean observation: 0.124 [-14.099, 18.719], loss: --, mean_squared_error: --, mean_q: --\n",
      "  538/2000: episode: 3, duration: 8.568s, episode steps: 180, steps per second: 21, episode reward: 0.815, mean reward: 0.005 [-0.002, 0.013], mean action: -0.030 [-1.195, 1.297], mean observation: 0.123 [-13.833, 18.956], loss: --, mean_squared_error: --, mean_q: --\n",
      "  716/2000: episode: 4, duration: 8.335s, episode steps: 178, steps per second: 21, episode reward: 0.821, mean reward: 0.005 [-0.002, 0.013], mean action: -0.038 [-1.238, 1.147], mean observation: 0.125 [-10.344, 19.245], loss: --, mean_squared_error: --, mean_q: --\n",
      "  893/2000: episode: 5, duration: 8.379s, episode steps: 177, steps per second: 21, episode reward: 0.811, mean reward: 0.005 [-0.002, 0.013], mean action: -0.019 [-1.167, 1.222], mean observation: 0.125 [-11.483, 18.716], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1085/2000: episode: 6, duration: 9.816s, episode steps: 192, steps per second: 20, episode reward: 0.814, mean reward: 0.004 [-0.002, 0.013], mean action: -0.008 [-1.236, 1.366], mean observation: 0.125 [-8.705, 18.930], loss: 0.000167, mean_squared_error: 0.000333, mean_q: 0.603304\n",
      " 1273/2000: episode: 7, duration: 10.515s, episode steps: 188, steps per second: 18, episode reward: 0.823, mean reward: 0.004 [-0.002, 0.013], mean action: -0.059 [-1.201, 1.188], mean observation: 0.129 [-13.263, 17.250], loss: 0.000304, mean_squared_error: 0.000608, mean_q: 0.603329\n",
      " 1469/2000: episode: 8, duration: 8.397s, episode steps: 196, steps per second: 23, episode reward: -0.736, mean reward: -0.004 [-0.020, 0.010], mean action: -0.192 [-1.270, 1.182], mean observation: 0.088 [-11.335, 19.556], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.594363\n",
      " 1705/2000: episode: 9, duration: 13.379s, episode steps: 236, steps per second: 18, episode reward: 0.797, mean reward: 0.003 [-0.002, 0.013], mean action: -0.086 [-1.164, 1.290], mean observation: 0.128 [-11.992, 19.754], loss: 0.000202, mean_squared_error: 0.000404, mean_q: 0.598450\n",
      " 1931/2000: episode: 10, duration: 12.490s, episode steps: 226, steps per second: 18, episode reward: 0.848, mean reward: 0.004 [-0.002, 0.013], mean action: -0.039 [-1.167, 1.318], mean observation: 0.135 [-12.217, 19.692], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.603503\n",
      "done, took 100.503 seconds\n",
      "\n",
      "\n",
      "iteration: 162\n",
      "Training for 2000 steps ...\n",
      "  200/2000: episode: 1, duration: 10.830s, episode steps: 200, steps per second: 18, episode reward: 0.786, mean reward: 0.004 [-0.001, 0.013], mean action: -0.126 [-1.196, 1.242], mean observation: 0.134 [-12.440, 19.797], loss: --, mean_squared_error: --, mean_q: --\n",
      "  399/2000: episode: 2, duration: 10.788s, episode steps: 199, steps per second: 18, episode reward: 0.771, mean reward: 0.004 [-0.001, 0.014], mean action: -0.119 [-1.190, 1.194], mean observation: 0.134 [-11.841, 19.920], loss: --, mean_squared_error: --, mean_q: --\n",
      "  590/2000: episode: 3, duration: 9.870s, episode steps: 191, steps per second: 19, episode reward: 0.818, mean reward: 0.004 [-0.001, 0.014], mean action: -0.125 [-1.178, 1.168], mean observation: 0.137 [-12.351, 19.499], loss: --, mean_squared_error: --, mean_q: --\n",
      "  791/2000: episode: 4, duration: 10.806s, episode steps: 201, steps per second: 19, episode reward: 0.788, mean reward: 0.004 [-0.001, 0.014], mean action: -0.123 [-1.228, 1.246], mean observation: 0.133 [-12.686, 19.714], loss: --, mean_squared_error: --, mean_q: --\n",
      "  991/2000: episode: 5, duration: 11.000s, episode steps: 200, steps per second: 18, episode reward: 0.771, mean reward: 0.004 [-0.001, 0.013], mean action: -0.105 [-1.299, 1.446], mean observation: 0.135 [-12.456, 19.737], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1171/2000: episode: 6, duration: 11.156s, episode steps: 180, steps per second: 16, episode reward: 0.791, mean reward: 0.004 [-0.001, 0.013], mean action: -0.152 [-1.191, 1.190], mean observation: 0.136 [-11.779, 19.789], loss: 0.000161, mean_squared_error: 0.000323, mean_q: 0.608521\n",
      " 1358/2000: episode: 7, duration: 11.324s, episode steps: 187, steps per second: 17, episode reward: 0.773, mean reward: 0.004 [-0.002, 0.013], mean action: -0.113 [-1.200, 1.292], mean observation: 0.122 [-12.219, 16.843], loss: 0.000351, mean_squared_error: 0.000702, mean_q: 0.598932\n",
      " 1550/2000: episode: 8, duration: 12.290s, episode steps: 192, steps per second: 16, episode reward: 0.792, mean reward: 0.004 [-0.002, 0.014], mean action: -0.177 [-1.171, 1.218], mean observation: 0.136 [-11.448, 16.366], loss: 0.000251, mean_squared_error: 0.000502, mean_q: 0.605255\n",
      " 1734/2000: episode: 9, duration: 11.320s, episode steps: 184, steps per second: 16, episode reward: 0.788, mean reward: 0.004 [-0.002, 0.013], mean action: -0.050 [-1.193, 1.151], mean observation: 0.126 [-15.084, 17.198], loss: 0.000071, mean_squared_error: 0.000141, mean_q: 0.604013\n",
      " 1958/2000: episode: 10, duration: 12.510s, episode steps: 224, steps per second: 18, episode reward: 0.840, mean reward: 0.004 [-0.003, 0.013], mean action: -0.148 [-1.202, 1.219], mean observation: 0.127 [-12.878, 17.144], loss: 0.000199, mean_squared_error: 0.000399, mean_q: 0.599134\n",
      "done, took 114.340 seconds\n",
      "\n",
      "\n",
      "iteration: 163\n",
      "Training for 2000 steps ...\n",
      "  239/2000: episode: 1, duration: 11.029s, episode steps: 239, steps per second: 22, episode reward: 0.855, mean reward: 0.004 [-0.002, 0.014], mean action: -0.057 [-1.193, 1.509], mean observation: 0.123 [-12.447, 16.883], loss: --, mean_squared_error: --, mean_q: --\n",
      "  517/2000: episode: 2, duration: 12.486s, episode steps: 278, steps per second: 22, episode reward: 0.843, mean reward: 0.003 [-0.002, 0.013], mean action: -0.075 [-1.239, 1.244], mean observation: 0.124 [-12.957, 17.213], loss: --, mean_squared_error: --, mean_q: --\n",
      "  782/2000: episode: 3, duration: 12.238s, episode steps: 265, steps per second: 22, episode reward: 0.848, mean reward: 0.003 [-0.002, 0.013], mean action: -0.115 [-1.292, 1.276], mean observation: 0.122 [-12.714, 16.712], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1023/2000: episode: 4, duration: 11.420s, episode steps: 241, steps per second: 21, episode reward: 0.825, mean reward: 0.003 [-0.002, 0.013], mean action: -0.106 [-1.216, 1.229], mean observation: 0.122 [-19.619, 17.200], loss: 0.000832, mean_squared_error: 0.001665, mean_q: 0.623163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1257/2000: episode: 5, duration: 13.034s, episode steps: 234, steps per second: 18, episode reward: 0.844, mean reward: 0.004 [-0.001, 0.013], mean action: -0.068 [-1.254, 1.252], mean observation: 0.121 [-12.537, 16.893], loss: 0.000214, mean_squared_error: 0.000429, mean_q: 0.613262\n",
      " 1483/2000: episode: 6, duration: 12.205s, episode steps: 226, steps per second: 19, episode reward: 0.830, mean reward: 0.004 [-0.002, 0.013], mean action: -0.157 [-1.355, 1.364], mean observation: 0.124 [-12.604, 16.996], loss: 0.000170, mean_squared_error: 0.000340, mean_q: 0.617657\n",
      " 1670/2000: episode: 7, duration: 10.394s, episode steps: 187, steps per second: 18, episode reward: 0.831, mean reward: 0.004 [-0.002, 0.013], mean action: -0.093 [-1.201, 1.256], mean observation: 0.119 [-12.270, 16.765], loss: 0.000109, mean_squared_error: 0.000218, mean_q: 0.618543\n",
      " 1886/2000: episode: 8, duration: 11.771s, episode steps: 216, steps per second: 18, episode reward: 0.830, mean reward: 0.004 [-0.002, 0.013], mean action: -0.104 [-1.263, 1.279], mean observation: 0.123 [-14.710, 16.908], loss: 0.000123, mean_squared_error: 0.000245, mean_q: 0.618558\n",
      "done, took 100.529 seconds\n",
      "\n",
      "\n",
      "iteration: 164\n",
      "Training for 2000 steps ...\n",
      "  229/2000: episode: 1, duration: 10.734s, episode steps: 229, steps per second: 21, episode reward: 0.911, mean reward: 0.004 [-0.002, 0.014], mean action: -0.029 [-1.194, 1.250], mean observation: 0.130 [-13.713, 16.970], loss: --, mean_squared_error: --, mean_q: --\n",
      "  471/2000: episode: 2, duration: 11.157s, episode steps: 242, steps per second: 22, episode reward: 0.878, mean reward: 0.004 [-0.002, 0.014], mean action: -0.059 [-1.230, 1.290], mean observation: 0.127 [-13.815, 17.034], loss: --, mean_squared_error: --, mean_q: --\n",
      "  714/2000: episode: 3, duration: 10.477s, episode steps: 243, steps per second: 23, episode reward: 0.816, mean reward: 0.003 [-0.002, 0.013], mean action: -0.076 [-1.254, 1.288], mean observation: 0.123 [-13.782, 16.973], loss: --, mean_squared_error: --, mean_q: --\n",
      "  930/2000: episode: 4, duration: 10.001s, episode steps: 216, steps per second: 22, episode reward: 0.830, mean reward: 0.004 [-0.002, 0.013], mean action: -0.014 [-1.133, 1.265], mean observation: 0.120 [-14.763, 16.609], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1147/2000: episode: 5, duration: 11.514s, episode steps: 217, steps per second: 19, episode reward: 0.834, mean reward: 0.004 [-0.002, 0.013], mean action: -0.007 [-1.176, 1.356], mean observation: 0.121 [-13.834, 17.072], loss: 0.000158, mean_squared_error: 0.000315, mean_q: 0.622661\n",
      " 1423/2000: episode: 6, duration: 14.835s, episode steps: 276, steps per second: 19, episode reward: 0.818, mean reward: 0.003 [-0.002, 0.013], mean action: 0.025 [-1.202, 1.212], mean observation: 0.122 [-13.772, 16.977], loss: 0.000175, mean_squared_error: 0.000349, mean_q: 0.623379\n",
      " 1663/2000: episode: 7, duration: 13.375s, episode steps: 240, steps per second: 18, episode reward: 0.921, mean reward: 0.004 [-0.002, 0.013], mean action: 0.084 [-1.235, 1.321], mean observation: 0.131 [-14.067, 17.177], loss: 0.000148, mean_squared_error: 0.000297, mean_q: 0.624296\n",
      " 1951/2000: episode: 8, duration: 16.112s, episode steps: 288, steps per second: 18, episode reward: 0.946, mean reward: 0.003 [-0.002, 0.014], mean action: 0.044 [-1.454, 1.312], mean observation: 0.134 [-14.156, 17.292], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.626102\n",
      "done, took 101.129 seconds\n",
      "\n",
      "\n",
      "iteration: 165\n",
      "Training for 2000 steps ...\n",
      "  266/2000: episode: 1, duration: 12.764s, episode steps: 266, steps per second: 21, episode reward: 0.804, mean reward: 0.003 [-0.002, 0.013], mean action: -0.035 [-1.348, 1.199], mean observation: 0.118 [-34.927, 19.055], loss: --, mean_squared_error: --, mean_q: --\n",
      "  530/2000: episode: 2, duration: 12.624s, episode steps: 264, steps per second: 21, episode reward: 0.817, mean reward: 0.003 [-0.002, 0.013], mean action: -0.044 [-1.442, 1.264], mean observation: 0.120 [-34.886, 19.047], loss: --, mean_squared_error: --, mean_q: --\n",
      "  801/2000: episode: 3, duration: 12.636s, episode steps: 271, steps per second: 21, episode reward: 0.823, mean reward: 0.003 [-0.003, 0.013], mean action: -0.029 [-1.186, 1.422], mean observation: 0.118 [-34.296, 18.727], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1067/2000: episode: 4, duration: 13.484s, episode steps: 266, steps per second: 20, episode reward: 0.841, mean reward: 0.003 [-0.003, 0.013], mean action: -0.043 [-1.248, 1.214], mean observation: 0.121 [-34.608, 18.718], loss: 0.000412, mean_squared_error: 0.000825, mean_q: 0.632294\n",
      " 1376/2000: episode: 5, duration: 16.219s, episode steps: 309, steps per second: 19, episode reward: 0.824, mean reward: 0.003 [-0.002, 0.013], mean action: 0.033 [-1.381, 1.408], mean observation: 0.123 [-35.299, 18.998], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.633554\n",
      " 1502/2000: episode: 6, duration: 5.937s, episode steps: 126, steps per second: 21, episode reward: -0.759, mean reward: -0.006 [-0.021, 0.009], mean action: 0.016 [-1.162, 1.149], mean observation: 0.015 [-36.360, 19.134], loss: 0.000054, mean_squared_error: 0.000108, mean_q: 0.638419\n",
      " 1622/2000: episode: 7, duration: 5.556s, episode steps: 120, steps per second: 22, episode reward: -0.750, mean reward: -0.006 [-0.021, 0.009], mean action: -0.036 [-1.135, 1.112], mean observation: 0.014 [-36.203, 19.231], loss: 0.000119, mean_squared_error: 0.000239, mean_q: 0.630262\n",
      " 1749/2000: episode: 8, duration: 5.985s, episode steps: 127, steps per second: 21, episode reward: -0.752, mean reward: -0.006 [-0.021, 0.009], mean action: -0.029 [-1.115, 1.141], mean observation: 0.019 [-35.758, 19.168], loss: 0.000186, mean_squared_error: 0.000371, mean_q: 0.626647\n",
      " 1876/2000: episode: 9, duration: 5.948s, episode steps: 127, steps per second: 21, episode reward: -0.733, mean reward: -0.006 [-0.020, 0.009], mean action: -0.008 [-1.282, 1.185], mean observation: 0.021 [-35.477, 18.871], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.625963\n",
      "done, took 96.419 seconds\n",
      "\n",
      "\n",
      "iteration: 166\n",
      "Training for 2000 steps ...\n",
      "  303/2000: episode: 1, duration: 11.765s, episode steps: 303, steps per second: 26, episode reward: 0.868, mean reward: 0.003 [-0.002, 0.013], mean action: 0.075 [-1.146, 1.218], mean observation: 0.126 [-34.852, 18.819], loss: --, mean_squared_error: --, mean_q: --\n",
      "  607/2000: episode: 2, duration: 11.882s, episode steps: 304, steps per second: 26, episode reward: 0.857, mean reward: 0.003 [-0.002, 0.013], mean action: 0.054 [-1.195, 1.195], mean observation: 0.126 [-34.817, 18.968], loss: --, mean_squared_error: --, mean_q: --\n",
      "  884/2000: episode: 3, duration: 10.811s, episode steps: 277, steps per second: 26, episode reward: 0.879, mean reward: 0.003 [-0.002, 0.013], mean action: 0.042 [-1.291, 1.301], mean observation: 0.126 [-34.656, 18.907], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1174/2000: episode: 4, duration: 13.558s, episode steps: 290, steps per second: 21, episode reward: 0.837, mean reward: 0.003 [-0.002, 0.013], mean action: 0.091 [-1.231, 1.305], mean observation: 0.125 [-34.814, 18.812], loss: 0.000065, mean_squared_error: 0.000129, mean_q: 0.638707\n",
      " 1447/2000: episode: 5, duration: 14.706s, episode steps: 273, steps per second: 19, episode reward: 0.820, mean reward: 0.003 [-0.002, 0.013], mean action: -0.009 [-1.295, 1.240], mean observation: 0.125 [-34.925, 19.042], loss: 0.000076, mean_squared_error: 0.000153, mean_q: 0.633898\n",
      " 1726/2000: episode: 6, duration: 15.111s, episode steps: 279, steps per second: 18, episode reward: 0.819, mean reward: 0.003 [-0.002, 0.013], mean action: 0.037 [-1.152, 1.255], mean observation: 0.125 [-35.663, 19.342], loss: 0.000214, mean_squared_error: 0.000428, mean_q: 0.631226\n",
      " 2000/2000: episode: 7, duration: 15.313s, episode steps: 274, steps per second: 18, episode reward: 0.787, mean reward: 0.003 [-0.003, 0.013], mean action: 0.010 [-1.290, 1.215], mean observation: 0.122 [-35.845, 19.201], loss: 0.000173, mean_squared_error: 0.000347, mean_q: 0.627533\n",
      "done, took 93.163 seconds\n",
      "\n",
      "\n",
      "iteration: 167\n",
      "Training for 2000 steps ...\n",
      "  268/2000: episode: 1, duration: 13.455s, episode steps: 268, steps per second: 20, episode reward: 0.782, mean reward: 0.003 [-0.003, 0.013], mean action: 0.059 [-1.191, 1.374], mean observation: 0.120 [-35.394, 19.111], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  536/2000: episode: 2, duration: 13.430s, episode steps: 268, steps per second: 20, episode reward: 0.777, mean reward: 0.003 [-0.003, 0.013], mean action: 0.022 [-1.249, 1.356], mean observation: 0.118 [-34.454, 18.867], loss: --, mean_squared_error: --, mean_q: --\n",
      "  804/2000: episode: 3, duration: 13.143s, episode steps: 268, steps per second: 20, episode reward: 0.785, mean reward: 0.003 [-0.003, 0.013], mean action: -0.008 [-1.296, 1.189], mean observation: 0.119 [-34.519, 19.009], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1069/2000: episode: 4, duration: 13.673s, episode steps: 265, steps per second: 19, episode reward: 0.772, mean reward: 0.003 [-0.003, 0.013], mean action: 0.022 [-1.237, 1.266], mean observation: 0.114 [-34.945, 19.064], loss: 0.000299, mean_squared_error: 0.000598, mean_q: 0.632893\n",
      " 1275/2000: episode: 5, duration: 13.263s, episode steps: 206, steps per second: 16, episode reward: 0.771, mean reward: 0.004 [-0.003, 0.013], mean action: 0.051 [-1.151, 1.349], mean observation: 0.112 [-35.230, 19.059], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.629862\n",
      " 1491/2000: episode: 6, duration: 13.873s, episode steps: 216, steps per second: 16, episode reward: 0.778, mean reward: 0.004 [-0.002, 0.013], mean action: 0.089 [-1.214, 1.270], mean observation: 0.113 [-34.737, 18.918], loss: 0.000146, mean_squared_error: 0.000291, mean_q: 0.627045\n",
      " 1717/2000: episode: 7, duration: 14.441s, episode steps: 226, steps per second: 16, episode reward: 0.765, mean reward: 0.003 [-0.002, 0.013], mean action: 0.102 [-1.257, 1.227], mean observation: 0.112 [-34.548, 18.813], loss: 0.000119, mean_squared_error: 0.000237, mean_q: 0.628888\n",
      " 1926/2000: episode: 8, duration: 13.448s, episode steps: 209, steps per second: 16, episode reward: 0.774, mean reward: 0.004 [-0.002, 0.013], mean action: 0.105 [-1.280, 1.256], mean observation: 0.111 [-29.198, 17.949], loss: 0.000085, mean_squared_error: 0.000171, mean_q: 0.628410\n",
      "done, took 113.155 seconds\n",
      "\n",
      "\n",
      "iteration: 168\n",
      "Training for 2000 steps ...\n",
      "  229/2000: episode: 1, duration: 12.378s, episode steps: 229, steps per second: 19, episode reward: 0.770, mean reward: 0.003 [-0.003, 0.014], mean action: 0.095 [-1.399, 1.188], mean observation: 0.112 [-22.077, 16.419], loss: --, mean_squared_error: --, mean_q: --\n",
      "  446/2000: episode: 2, duration: 11.762s, episode steps: 217, steps per second: 18, episode reward: 0.759, mean reward: 0.003 [-0.003, 0.013], mean action: 0.109 [-1.246, 1.130], mean observation: 0.113 [-22.840, 16.777], loss: --, mean_squared_error: --, mean_q: --\n",
      "  664/2000: episode: 3, duration: 12.064s, episode steps: 218, steps per second: 18, episode reward: 0.757, mean reward: 0.003 [-0.003, 0.013], mean action: 0.124 [-1.240, 1.168], mean observation: 0.111 [-23.291, 16.886], loss: --, mean_squared_error: --, mean_q: --\n",
      "  885/2000: episode: 4, duration: 11.970s, episode steps: 221, steps per second: 18, episode reward: 0.746, mean reward: 0.003 [-0.003, 0.014], mean action: 0.115 [-1.231, 1.258], mean observation: 0.112 [-20.579, 16.020], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1115/2000: episode: 5, duration: 13.566s, episode steps: 230, steps per second: 17, episode reward: 0.760, mean reward: 0.003 [-0.003, 0.013], mean action: 0.132 [-1.179, 1.294], mean observation: 0.113 [-22.028, 16.456], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.626923\n",
      " 1321/2000: episode: 6, duration: 13.145s, episode steps: 206, steps per second: 16, episode reward: 0.747, mean reward: 0.004 [-0.002, 0.013], mean action: 0.109 [-1.252, 1.226], mean observation: 0.113 [-22.680, 16.785], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.627209\n",
      " 1531/2000: episode: 7, duration: 12.703s, episode steps: 210, steps per second: 17, episode reward: 0.765, mean reward: 0.004 [-0.002, 0.013], mean action: 0.070 [-1.306, 1.226], mean observation: 0.115 [-23.848, 17.249], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.623079\n",
      " 1774/2000: episode: 8, duration: 14.729s, episode steps: 243, steps per second: 16, episode reward: 0.761, mean reward: 0.003 [-0.003, 0.014], mean action: 0.079 [-1.179, 1.428], mean observation: 0.116 [-24.505, 17.717], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.621277\n",
      "done, took 115.035 seconds\n",
      "\n",
      "\n",
      "iteration: 169\n",
      "Training for 2000 steps ...\n",
      "  223/2000: episode: 1, duration: 11.026s, episode steps: 223, steps per second: 20, episode reward: 0.716, mean reward: 0.003 [-0.003, 0.013], mean action: 0.060 [-1.316, 1.155], mean observation: 0.116 [-21.675, 16.425], loss: --, mean_squared_error: --, mean_q: --\n",
      "  463/2000: episode: 2, duration: 11.499s, episode steps: 240, steps per second: 21, episode reward: 0.710, mean reward: 0.003 [-0.003, 0.014], mean action: 0.113 [-1.242, 1.434], mean observation: 0.116 [-23.111, 16.992], loss: --, mean_squared_error: --, mean_q: --\n",
      "  698/2000: episode: 3, duration: 11.538s, episode steps: 235, steps per second: 20, episode reward: 0.712, mean reward: 0.003 [-0.003, 0.013], mean action: 0.060 [-1.279, 1.207], mean observation: 0.116 [-22.142, 16.593], loss: --, mean_squared_error: --, mean_q: --\n",
      "  922/2000: episode: 4, duration: 10.939s, episode steps: 224, steps per second: 20, episode reward: 0.720, mean reward: 0.003 [-0.003, 0.013], mean action: 0.086 [-1.207, 1.261], mean observation: 0.117 [-23.375, 17.077], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1159/2000: episode: 5, duration: 13.122s, episode steps: 237, steps per second: 18, episode reward: 0.721, mean reward: 0.003 [-0.003, 0.014], mean action: 0.066 [-1.195, 1.165], mean observation: 0.116 [-22.552, 16.611], loss: 0.000370, mean_squared_error: 0.000739, mean_q: 0.634343\n",
      " 1354/2000: episode: 6, duration: 11.513s, episode steps: 195, steps per second: 17, episode reward: 0.714, mean reward: 0.004 [-0.003, 0.013], mean action: 0.058 [-1.225, 1.247], mean observation: 0.112 [-24.809, 17.776], loss: 0.000082, mean_squared_error: 0.000165, mean_q: 0.616645\n",
      " 1565/2000: episode: 7, duration: 12.532s, episode steps: 211, steps per second: 17, episode reward: 0.717, mean reward: 0.003 [-0.003, 0.013], mean action: 0.090 [-1.184, 1.313], mean observation: 0.113 [-25.075, 18.036], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.616545\n",
      " 1872/2000: episode: 8, duration: 16.941s, episode steps: 307, steps per second: 18, episode reward: 0.696, mean reward: 0.002 [-0.003, 0.013], mean action: 0.074 [-1.343, 1.235], mean observation: 0.118 [-24.788, 17.660], loss: 0.000167, mean_squared_error: 0.000334, mean_q: 0.626427\n",
      "done, took 105.607 seconds\n",
      "\n",
      "\n",
      "iteration: 170\n",
      "Training for 2000 steps ...\n",
      "  238/2000: episode: 1, duration: 11.691s, episode steps: 238, steps per second: 20, episode reward: 0.716, mean reward: 0.003 [-0.003, 0.013], mean action: 0.077 [-1.327, 1.354], mean observation: 0.114 [-24.958, 17.714], loss: --, mean_squared_error: --, mean_q: --\n",
      "  458/2000: episode: 2, duration: 10.476s, episode steps: 220, steps per second: 21, episode reward: 0.715, mean reward: 0.003 [-0.003, 0.013], mean action: 0.066 [-1.206, 1.200], mean observation: 0.113 [-24.823, 17.923], loss: --, mean_squared_error: --, mean_q: --\n",
      "  751/2000: episode: 3, duration: 13.995s, episode steps: 293, steps per second: 21, episode reward: 0.712, mean reward: 0.002 [-0.003, 0.013], mean action: 0.070 [-1.221, 1.228], mean observation: 0.116 [-24.680, 17.362], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1127/2000: episode: 4, duration: 18.738s, episode steps: 376, steps per second: 20, episode reward: 0.705, mean reward: 0.002 [-0.003, 0.013], mean action: 0.046 [-1.367, 1.319], mean observation: 0.117 [-25.285, 17.705], loss: 0.000110, mean_squared_error: 0.000221, mean_q: 0.618409\n",
      " 1344/2000: episode: 5, duration: 12.759s, episode steps: 217, steps per second: 17, episode reward: 0.724, mean reward: 0.003 [-0.003, 0.013], mean action: 0.024 [-1.231, 1.224], mean observation: 0.114 [-24.464, 17.520], loss: 0.000104, mean_squared_error: 0.000207, mean_q: 0.621004\n",
      " 1648/2000: episode: 6, duration: 17.082s, episode steps: 304, steps per second: 18, episode reward: 0.713, mean reward: 0.002 [-0.003, 0.013], mean action: 0.029 [-1.191, 1.267], mean observation: 0.114 [-24.917, 17.578], loss: 0.000150, mean_squared_error: 0.000300, mean_q: 0.620063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1955/2000: episode: 7, duration: 17.121s, episode steps: 307, steps per second: 18, episode reward: 0.765, mean reward: 0.002 [-0.002, 0.013], mean action: 0.130 [-1.257, 1.290], mean observation: 0.118 [-24.748, 17.436], loss: 0.000211, mean_squared_error: 0.000422, mean_q: 0.630831\n",
      "done, took 104.675 seconds\n",
      "\n",
      "\n",
      "iteration: 171\n",
      "Training for 2000 steps ...\n",
      "  295/2000: episode: 1, duration: 13.465s, episode steps: 295, steps per second: 22, episode reward: 0.736, mean reward: 0.002 [-0.002, 0.013], mean action: 0.166 [-1.138, 1.328], mean observation: 0.116 [-24.998, 17.582], loss: --, mean_squared_error: --, mean_q: --\n",
      "  583/2000: episode: 2, duration: 13.200s, episode steps: 288, steps per second: 22, episode reward: 0.724, mean reward: 0.003 [-0.003, 0.013], mean action: 0.152 [-1.271, 1.274], mean observation: 0.115 [-25.123, 17.701], loss: --, mean_squared_error: --, mean_q: --\n",
      "  846/2000: episode: 3, duration: 12.274s, episode steps: 263, steps per second: 21, episode reward: 0.738, mean reward: 0.003 [-0.002, 0.013], mean action: 0.152 [-1.347, 1.375], mean observation: 0.116 [-25.025, 17.631], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1109/2000: episode: 4, duration: 13.576s, episode steps: 263, steps per second: 19, episode reward: 0.766, mean reward: 0.003 [-0.002, 0.013], mean action: 0.177 [-1.187, 1.259], mean observation: 0.119 [-25.013, 17.620], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.619577\n",
      " 1330/2000: episode: 5, duration: 12.637s, episode steps: 221, steps per second: 17, episode reward: 0.754, mean reward: 0.003 [-0.002, 0.013], mean action: 0.164 [-1.331, 1.299], mean observation: 0.115 [-23.670, 17.184], loss: 0.000236, mean_squared_error: 0.000473, mean_q: 0.617482\n",
      " 1645/2000: episode: 6, duration: 17.651s, episode steps: 315, steps per second: 18, episode reward: 0.706, mean reward: 0.002 [-0.002, 0.013], mean action: 0.085 [-1.234, 1.269], mean observation: 0.116 [-24.731, 17.568], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.622244\n",
      " 1932/2000: episode: 7, duration: 15.989s, episode steps: 287, steps per second: 18, episode reward: 0.717, mean reward: 0.002 [-0.002, 0.013], mean action: 0.085 [-1.190, 1.234], mean observation: 0.115 [-24.415, 17.880], loss: 0.000088, mean_squared_error: 0.000176, mean_q: 0.616228\n",
      "done, took 102.933 seconds\n",
      "\n",
      "\n",
      "iteration: 172\n",
      "Training for 2000 steps ...\n",
      "  265/2000: episode: 1, duration: 12.039s, episode steps: 265, steps per second: 22, episode reward: 0.728, mean reward: 0.003 [-0.002, 0.013], mean action: 0.012 [-1.305, 1.350], mean observation: 0.117 [-25.572, 18.476], loss: --, mean_squared_error: --, mean_q: --\n",
      "  528/2000: episode: 2, duration: 11.759s, episode steps: 263, steps per second: 22, episode reward: 0.766, mean reward: 0.003 [-0.002, 0.013], mean action: 0.021 [-1.224, 1.283], mean observation: 0.119 [-25.926, 18.761], loss: --, mean_squared_error: --, mean_q: --\n",
      "  797/2000: episode: 3, duration: 12.319s, episode steps: 269, steps per second: 22, episode reward: 0.736, mean reward: 0.003 [-0.002, 0.013], mean action: 0.015 [-1.236, 1.251], mean observation: 0.118 [-24.825, 18.139], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1058/2000: episode: 4, duration: 12.525s, episode steps: 261, steps per second: 21, episode reward: 0.755, mean reward: 0.003 [-0.002, 0.013], mean action: 0.020 [-1.287, 1.380], mean observation: 0.121 [-25.814, 18.638], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.633818\n",
      " 1229/2000: episode: 5, duration: 10.201s, episode steps: 171, steps per second: 17, episode reward: 0.713, mean reward: 0.004 [-0.002, 0.013], mean action: 0.160 [-1.204, 1.236], mean observation: 0.104 [-24.656, 17.356], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.615800\n",
      " 1374/2000: episode: 6, duration: 9.381s, episode steps: 145, steps per second: 15, episode reward: 0.632, mean reward: 0.004 [-0.003, 0.013], mean action: 0.093 [-1.198, 1.174], mean observation: 0.093 [-44.165, 17.481], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.619547\n",
      " 1613/2000: episode: 7, duration: 13.934s, episode steps: 239, steps per second: 17, episode reward: 0.709, mean reward: 0.003 [-0.002, 0.013], mean action: 0.028 [-1.222, 1.299], mean observation: 0.116 [-25.948, 18.337], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.614360\n",
      " 1832/2000: episode: 8, duration: 12.895s, episode steps: 219, steps per second: 17, episode reward: 0.711, mean reward: 0.003 [-0.002, 0.013], mean action: 0.090 [-1.246, 1.251], mean observation: 0.113 [-26.071, 18.507], loss: 0.000123, mean_squared_error: 0.000245, mean_q: 0.614328\n",
      " 1998/2000: episode: 9, duration: 10.015s, episode steps: 166, steps per second: 17, episode reward: 0.695, mean reward: 0.004 [-0.002, 0.013], mean action: 0.053 [-1.162, 1.215], mean observation: 0.106 [-23.995, 17.561], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.611507\n",
      "done, took 105.235 seconds\n",
      "\n",
      "\n",
      "iteration: 173\n",
      "Training for 2000 steps ...\n",
      "  186/2000: episode: 1, duration: 8.926s, episode steps: 186, steps per second: 21, episode reward: 0.727, mean reward: 0.004 [-0.002, 0.013], mean action: 0.049 [-1.228, 1.307], mean observation: 0.110 [-24.144, 17.609], loss: --, mean_squared_error: --, mean_q: --\n",
      "  360/2000: episode: 2, duration: 8.681s, episode steps: 174, steps per second: 20, episode reward: 0.706, mean reward: 0.004 [-0.002, 0.013], mean action: 0.019 [-1.174, 1.368], mean observation: 0.108 [-23.692, 17.274], loss: --, mean_squared_error: --, mean_q: --\n",
      "  541/2000: episode: 3, duration: 8.796s, episode steps: 181, steps per second: 21, episode reward: 0.709, mean reward: 0.004 [-0.002, 0.013], mean action: 0.029 [-1.219, 1.136], mean observation: 0.110 [-23.934, 17.527], loss: --, mean_squared_error: --, mean_q: --\n",
      "  732/2000: episode: 4, duration: 9.180s, episode steps: 191, steps per second: 21, episode reward: 0.705, mean reward: 0.004 [-0.002, 0.013], mean action: 0.050 [-1.140, 1.222], mean observation: 0.109 [-24.806, 18.029], loss: --, mean_squared_error: --, mean_q: --\n",
      "  910/2000: episode: 5, duration: 8.867s, episode steps: 178, steps per second: 20, episode reward: 0.704, mean reward: 0.004 [-0.002, 0.013], mean action: 0.043 [-1.149, 1.131], mean observation: 0.109 [-24.009, 17.568], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1097/2000: episode: 6, duration: 10.218s, episode steps: 187, steps per second: 18, episode reward: 0.714, mean reward: 0.004 [-0.002, 0.014], mean action: 0.075 [-1.286, 1.202], mean observation: 0.108 [-25.292, 18.082], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.618468\n",
      " 1320/2000: episode: 7, duration: 12.707s, episode steps: 223, steps per second: 18, episode reward: 0.725, mean reward: 0.003 [-0.002, 0.013], mean action: 0.085 [-1.111, 1.204], mean observation: 0.114 [-23.600, 17.187], loss: 0.000173, mean_squared_error: 0.000347, mean_q: 0.619549\n",
      " 1504/2000: episode: 8, duration: 11.123s, episode steps: 184, steps per second: 17, episode reward: 0.707, mean reward: 0.004 [-0.002, 0.014], mean action: -0.007 [-1.293, 1.324], mean observation: 0.110 [-26.541, 18.703], loss: 0.000134, mean_squared_error: 0.000269, mean_q: 0.615997\n",
      " 1726/2000: episode: 9, duration: 13.542s, episode steps: 222, steps per second: 16, episode reward: 0.716, mean reward: 0.003 [-0.002, 0.013], mean action: 0.052 [-1.188, 1.222], mean observation: 0.112 [-25.763, 18.443], loss: 0.000202, mean_squared_error: 0.000405, mean_q: 0.610487\n",
      " 1929/2000: episode: 10, duration: 12.020s, episode steps: 203, steps per second: 17, episode reward: 0.711, mean reward: 0.004 [-0.002, 0.013], mean action: 0.017 [-1.158, 1.244], mean observation: 0.111 [-25.795, 18.364], loss: 0.000132, mean_squared_error: 0.000265, mean_q: 0.611653\n",
      "done, took 108.125 seconds\n",
      "\n",
      "\n",
      "iteration: 174\n",
      "Training for 2000 steps ...\n",
      "  187/2000: episode: 1, duration: 8.836s, episode steps: 187, steps per second: 21, episode reward: 0.683, mean reward: 0.004 [-0.002, 0.013], mean action: 0.034 [-1.253, 1.159], mean observation: 0.109 [-23.060, 17.122], loss: --, mean_squared_error: --, mean_q: --\n",
      "  386/2000: episode: 2, duration: 9.737s, episode steps: 199, steps per second: 20, episode reward: 0.710, mean reward: 0.004 [-0.002, 0.013], mean action: 0.038 [-1.469, 1.206], mean observation: 0.109 [-23.751, 17.471], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  584/2000: episode: 3, duration: 9.677s, episode steps: 198, steps per second: 20, episode reward: 0.704, mean reward: 0.004 [-0.002, 0.013], mean action: 0.036 [-1.211, 1.278], mean observation: 0.110 [-24.542, 17.715], loss: --, mean_squared_error: --, mean_q: --\n",
      "  785/2000: episode: 4, duration: 9.968s, episode steps: 201, steps per second: 20, episode reward: 0.705, mean reward: 0.004 [-0.001, 0.013], mean action: 0.066 [-1.151, 1.211], mean observation: 0.110 [-24.752, 17.740], loss: --, mean_squared_error: --, mean_q: --\n",
      "  980/2000: episode: 5, duration: 9.603s, episode steps: 195, steps per second: 20, episode reward: 0.706, mean reward: 0.004 [-0.002, 0.013], mean action: 0.052 [-1.214, 1.205], mean observation: 0.108 [-23.342, 17.137], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1176/2000: episode: 6, duration: 11.297s, episode steps: 196, steps per second: 17, episode reward: 0.709, mean reward: 0.004 [-0.002, 0.014], mean action: 0.067 [-1.171, 1.156], mean observation: 0.111 [-23.836, 17.667], loss: 0.000133, mean_squared_error: 0.000266, mean_q: 0.610109\n",
      " 1383/2000: episode: 7, duration: 12.325s, episode steps: 207, steps per second: 17, episode reward: 0.704, mean reward: 0.003 [-0.002, 0.013], mean action: 0.067 [-1.280, 1.280], mean observation: 0.114 [-25.115, 18.321], loss: 0.000208, mean_squared_error: 0.000417, mean_q: 0.615759\n",
      " 1609/2000: episode: 8, duration: 13.158s, episode steps: 226, steps per second: 17, episode reward: 0.696, mean reward: 0.003 [-0.002, 0.013], mean action: -0.004 [-1.445, 1.132], mean observation: 0.114 [-24.174, 17.395], loss: 0.000253, mean_squared_error: 0.000506, mean_q: 0.607324\n",
      " 1794/2000: episode: 9, duration: 10.756s, episode steps: 185, steps per second: 17, episode reward: 0.712, mean reward: 0.004 [-0.003, 0.013], mean action: 0.031 [-1.164, 1.322], mean observation: 0.111 [-24.816, 17.443], loss: 0.000375, mean_squared_error: 0.000750, mean_q: 0.612712\n",
      " 1960/2000: episode: 10, duration: 8.801s, episode steps: 166, steps per second: 19, episode reward: 0.553, mean reward: 0.003 [-0.002, 0.011], mean action: 0.080 [-1.193, 1.196], mean observation: 0.126 [-25.178, 17.639], loss: 0.000258, mean_squared_error: 0.000517, mean_q: 0.605398\n",
      "done, took 107.107 seconds\n",
      "\n",
      "\n",
      "iteration: 175\n",
      "Training for 2000 steps ...\n",
      "  133/2000: episode: 1, duration: 6.094s, episode steps: 133, steps per second: 22, episode reward: 0.563, mean reward: 0.004 [-0.003, 0.011], mean action: 0.008 [-1.204, 1.162], mean observation: 0.116 [-44.578, 18.556], loss: --, mean_squared_error: --, mean_q: --\n",
      "  266/2000: episode: 2, duration: 6.049s, episode steps: 133, steps per second: 22, episode reward: 0.542, mean reward: 0.004 [-0.003, 0.011], mean action: 0.015 [-1.170, 1.183], mean observation: 0.119 [-45.316, 18.512], loss: --, mean_squared_error: --, mean_q: --\n",
      "  397/2000: episode: 3, duration: 6.371s, episode steps: 131, steps per second: 21, episode reward: 0.510, mean reward: 0.004 [-0.003, 0.011], mean action: 0.025 [-1.119, 1.246], mean observation: 0.114 [-42.991, 18.575], loss: --, mean_squared_error: --, mean_q: --\n",
      "  527/2000: episode: 4, duration: 6.107s, episode steps: 130, steps per second: 21, episode reward: 0.551, mean reward: 0.004 [-0.003, 0.011], mean action: 0.015 [-1.132, 1.171], mean observation: 0.112 [-43.377, 18.329], loss: --, mean_squared_error: --, mean_q: --\n",
      "  656/2000: episode: 5, duration: 6.043s, episode steps: 129, steps per second: 21, episode reward: 0.531, mean reward: 0.004 [-0.003, 0.011], mean action: 0.023 [-1.205, 1.197], mean observation: 0.113 [-43.571, 18.504], loss: --, mean_squared_error: --, mean_q: --\n",
      "  786/2000: episode: 6, duration: 6.182s, episode steps: 130, steps per second: 21, episode reward: 0.542, mean reward: 0.004 [-0.003, 0.011], mean action: 0.036 [-1.171, 1.190], mean observation: 0.116 [-41.449, 18.458], loss: --, mean_squared_error: --, mean_q: --\n",
      "  913/2000: episode: 7, duration: 5.873s, episode steps: 127, steps per second: 22, episode reward: 0.531, mean reward: 0.004 [-0.003, 0.011], mean action: 0.036 [-1.150, 1.264], mean observation: 0.115 [-43.006, 18.280], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1044/2000: episode: 8, duration: 6.505s, episode steps: 131, steps per second: 20, episode reward: 0.521, mean reward: 0.004 [-0.003, 0.011], mean action: 0.018 [-1.123, 1.137], mean observation: 0.118 [-44.053, 18.212], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.614213\n",
      " 1155/2000: episode: 9, duration: 6.269s, episode steps: 111, steps per second: 18, episode reward: 0.476, mean reward: 0.004 [-0.002, 0.011], mean action: 0.061 [-1.110, 1.162], mean observation: 0.101 [-38.539, 17.634], loss: 0.000171, mean_squared_error: 0.000343, mean_q: 0.611031\n",
      " 1251/2000: episode: 10, duration: 5.593s, episode steps: 96, steps per second: 17, episode reward: 0.404, mean reward: 0.004 [-0.002, 0.011], mean action: 0.010 [-1.121, 1.112], mean observation: 0.100 [-44.517, 18.027], loss: 0.000185, mean_squared_error: 0.000369, mean_q: 0.608045\n",
      " 1345/2000: episode: 11, duration: 4.920s, episode steps: 94, steps per second: 19, episode reward: 0.422, mean reward: 0.004 [-0.001, 0.011], mean action: 0.001 [-1.163, 1.173], mean observation: 0.099 [-24.593, 17.829], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.603379\n",
      " 1444/2000: episode: 12, duration: 5.553s, episode steps: 99, steps per second: 18, episode reward: 0.418, mean reward: 0.004 [-0.002, 0.011], mean action: 0.001 [-1.130, 1.199], mean observation: 0.095 [-43.546, 18.163], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.602523\n",
      " 1547/2000: episode: 13, duration: 5.686s, episode steps: 103, steps per second: 18, episode reward: 0.455, mean reward: 0.004 [-0.002, 0.011], mean action: -0.002 [-1.093, 1.195], mean observation: 0.102 [-34.028, 17.960], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.603420\n",
      " 1647/2000: episode: 14, duration: 4.975s, episode steps: 100, steps per second: 20, episode reward: 0.466, mean reward: 0.005 [-0.001, 0.011], mean action: -0.027 [-1.126, 1.151], mean observation: 0.106 [-25.040, 17.805], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.602098\n",
      " 1744/2000: episode: 15, duration: 4.707s, episode steps: 97, steps per second: 21, episode reward: 0.460, mean reward: 0.005 [-0.001, 0.011], mean action: -0.070 [-1.088, 1.106], mean observation: 0.121 [-24.622, 17.875], loss: 0.000080, mean_squared_error: 0.000159, mean_q: 0.604678\n",
      " 1929/2000: episode: 16, duration: 9.945s, episode steps: 185, steps per second: 19, episode reward: 0.449, mean reward: 0.002 [-0.003, 0.011], mean action: -0.049 [-1.150, 1.179], mean observation: 0.129 [-24.350, 18.095], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.601130\n",
      "done, took 100.745 seconds\n",
      "\n",
      "\n",
      "iteration: 176\n",
      "Training for 2000 steps ...\n",
      "  286/2000: episode: 1, duration: 11.702s, episode steps: 286, steps per second: 24, episode reward: 0.454, mean reward: 0.002 [-0.002, 0.011], mean action: 0.041 [-1.192, 1.184], mean observation: 0.127 [-25.132, 17.754], loss: --, mean_squared_error: --, mean_q: --\n",
      "  553/2000: episode: 2, duration: 11.224s, episode steps: 267, steps per second: 24, episode reward: 0.453, mean reward: 0.002 [-0.002, 0.011], mean action: 0.034 [-1.236, 1.312], mean observation: 0.127 [-24.653, 18.046], loss: --, mean_squared_error: --, mean_q: --\n",
      "  842/2000: episode: 3, duration: 12.219s, episode steps: 289, steps per second: 24, episode reward: 0.452, mean reward: 0.002 [-0.002, 0.011], mean action: 0.007 [-1.274, 1.244], mean observation: 0.127 [-24.954, 17.824], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1099/2000: episode: 4, duration: 11.773s, episode steps: 257, steps per second: 22, episode reward: 0.457, mean reward: 0.002 [-0.002, 0.011], mean action: -0.000 [-1.598, 1.260], mean observation: 0.128 [-24.339, 17.845], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.604688\n",
      " 1283/2000: episode: 5, duration: 9.506s, episode steps: 184, steps per second: 19, episode reward: 0.456, mean reward: 0.002 [-0.002, 0.012], mean action: -0.010 [-1.144, 1.277], mean observation: 0.133 [-25.044, 17.926], loss: 0.000105, mean_squared_error: 0.000211, mean_q: 0.605776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1471/2000: episode: 6, duration: 9.001s, episode steps: 188, steps per second: 21, episode reward: 0.479, mean reward: 0.003 [-0.002, 0.012], mean action: -0.044 [-1.240, 1.300], mean observation: 0.136 [-22.785, 17.009], loss: 0.000227, mean_squared_error: 0.000453, mean_q: 0.602196\n",
      " 1659/2000: episode: 7, duration: 9.630s, episode steps: 188, steps per second: 20, episode reward: 0.462, mean reward: 0.002 [-0.002, 0.012], mean action: -0.048 [-1.270, 1.169], mean observation: 0.134 [-23.273, 17.193], loss: 0.000223, mean_squared_error: 0.000446, mean_q: 0.605073\n",
      " 1841/2000: episode: 8, duration: 9.324s, episode steps: 182, steps per second: 20, episode reward: 0.458, mean reward: 0.003 [-0.002, 0.012], mean action: -0.052 [-1.223, 1.221], mean observation: 0.132 [-22.205, 16.787], loss: 0.000148, mean_squared_error: 0.000296, mean_q: 0.610768\n",
      "done, took 92.446 seconds\n",
      "\n",
      "\n",
      "iteration: 177\n",
      "Training for 2000 steps ...\n",
      "  145/2000: episode: 1, duration: 5.626s, episode steps: 145, steps per second: 26, episode reward: 0.460, mean reward: 0.003 [-0.002, 0.011], mean action: -0.076 [-1.314, 1.198], mean observation: 0.132 [-21.724, 16.636], loss: --, mean_squared_error: --, mean_q: --\n",
      "  286/2000: episode: 2, duration: 5.424s, episode steps: 141, steps per second: 26, episode reward: 0.463, mean reward: 0.003 [-0.002, 0.011], mean action: -0.079 [-1.170, 1.253], mean observation: 0.132 [-21.611, 16.656], loss: --, mean_squared_error: --, mean_q: --\n",
      "  432/2000: episode: 3, duration: 5.684s, episode steps: 146, steps per second: 26, episode reward: 0.458, mean reward: 0.003 [-0.002, 0.011], mean action: -0.088 [-1.154, 1.124], mean observation: 0.131 [-22.245, 16.770], loss: --, mean_squared_error: --, mean_q: --\n",
      "  578/2000: episode: 4, duration: 5.622s, episode steps: 146, steps per second: 26, episode reward: 0.461, mean reward: 0.003 [-0.002, 0.011], mean action: -0.102 [-1.194, 1.164], mean observation: 0.131 [-22.283, 16.825], loss: --, mean_squared_error: --, mean_q: --\n",
      "  724/2000: episode: 5, duration: 5.586s, episode steps: 146, steps per second: 26, episode reward: 0.460, mean reward: 0.003 [-0.002, 0.011], mean action: -0.097 [-1.250, 1.184], mean observation: 0.131 [-21.888, 16.697], loss: --, mean_squared_error: --, mean_q: --\n",
      "  868/2000: episode: 6, duration: 5.541s, episode steps: 144, steps per second: 26, episode reward: 0.455, mean reward: 0.003 [-0.002, 0.011], mean action: -0.078 [-1.315, 1.231], mean observation: 0.132 [-22.774, 17.083], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1010/2000: episode: 7, duration: 5.505s, episode steps: 142, steps per second: 26, episode reward: 0.467, mean reward: 0.003 [-0.002, 0.011], mean action: -0.083 [-1.158, 1.231], mean observation: 0.131 [-23.199, 17.116], loss: 0.000046, mean_squared_error: 0.000093, mean_q: 0.615465\n",
      " 1159/2000: episode: 8, duration: 7.069s, episode steps: 149, steps per second: 21, episode reward: 0.462, mean reward: 0.003 [-0.002, 0.011], mean action: -0.080 [-1.233, 1.159], mean observation: 0.130 [-22.379, 16.865], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.606153\n",
      " 1286/2000: episode: 9, duration: 6.025s, episode steps: 127, steps per second: 21, episode reward: 0.463, mean reward: 0.004 [-0.002, 0.011], mean action: -0.093 [-1.284, 1.121], mean observation: 0.131 [-25.886, 17.827], loss: 0.000185, mean_squared_error: 0.000369, mean_q: 0.610871\n",
      " 1429/2000: episode: 10, duration: 6.883s, episode steps: 143, steps per second: 21, episode reward: 0.442, mean reward: 0.003 [-0.002, 0.011], mean action: -0.115 [-1.189, 1.183], mean observation: 0.125 [-25.144, 17.876], loss: 0.000187, mean_squared_error: 0.000375, mean_q: 0.604666\n",
      " 1603/2000: episode: 11, duration: 8.296s, episode steps: 174, steps per second: 21, episode reward: 0.450, mean reward: 0.003 [-0.002, 0.011], mean action: -0.151 [-1.198, 1.315], mean observation: 0.127 [-23.046, 17.833], loss: 0.000103, mean_squared_error: 0.000207, mean_q: 0.597948\n",
      " 1775/2000: episode: 12, duration: 8.183s, episode steps: 172, steps per second: 21, episode reward: 0.461, mean reward: 0.003 [-0.002, 0.011], mean action: -0.102 [-1.219, 1.194], mean observation: 0.125 [-24.946, 18.340], loss: 0.000042, mean_squared_error: 0.000085, mean_q: 0.598925\n",
      " 1945/2000: episode: 13, duration: 8.556s, episode steps: 170, steps per second: 20, episode reward: 0.451, mean reward: 0.003 [-0.002, 0.011], mean action: -0.044 [-1.172, 1.191], mean observation: 0.126 [-24.901, 18.143], loss: 0.000140, mean_squared_error: 0.000280, mean_q: 0.596577\n",
      "done, took 87.196 seconds\n",
      "\n",
      "\n",
      "iteration: 178\n",
      "Training for 2000 steps ...\n",
      "  182/2000: episode: 1, duration: 7.144s, episode steps: 182, steps per second: 25, episode reward: 0.463, mean reward: 0.003 [-0.002, 0.011], mean action: -0.027 [-1.225, 1.346], mean observation: 0.129 [-22.565, 17.582], loss: --, mean_squared_error: --, mean_q: --\n",
      "  364/2000: episode: 2, duration: 7.245s, episode steps: 182, steps per second: 25, episode reward: 0.472, mean reward: 0.003 [-0.002, 0.011], mean action: -0.043 [-1.270, 1.207], mean observation: 0.128 [-22.808, 17.680], loss: --, mean_squared_error: --, mean_q: --\n",
      "  542/2000: episode: 3, duration: 7.159s, episode steps: 178, steps per second: 25, episode reward: 0.473, mean reward: 0.003 [-0.002, 0.011], mean action: -0.054 [-1.166, 1.169], mean observation: 0.130 [-23.428, 17.940], loss: --, mean_squared_error: --, mean_q: --\n",
      "  719/2000: episode: 4, duration: 7.045s, episode steps: 177, steps per second: 25, episode reward: 0.460, mean reward: 0.003 [-0.002, 0.011], mean action: -0.032 [-1.291, 1.231], mean observation: 0.129 [-23.301, 17.932], loss: --, mean_squared_error: --, mean_q: --\n",
      "  900/2000: episode: 5, duration: 7.293s, episode steps: 181, steps per second: 25, episode reward: 0.470, mean reward: 0.003 [-0.002, 0.011], mean action: -0.036 [-1.227, 1.257], mean observation: 0.129 [-22.283, 17.546], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1082/2000: episode: 6, duration: 7.989s, episode steps: 182, steps per second: 23, episode reward: 0.468, mean reward: 0.003 [-0.002, 0.011], mean action: -0.054 [-1.281, 1.211], mean observation: 0.129 [-23.530, 18.006], loss: 0.000043, mean_squared_error: 0.000087, mean_q: 0.603494\n",
      " 1247/2000: episode: 7, duration: 8.161s, episode steps: 165, steps per second: 20, episode reward: 0.462, mean reward: 0.003 [-0.002, 0.011], mean action: -0.030 [-1.205, 1.227], mean observation: 0.130 [-25.765, 17.721], loss: 0.000049, mean_squared_error: 0.000099, mean_q: 0.604906\n",
      " 1377/2000: episode: 8, duration: 6.268s, episode steps: 130, steps per second: 21, episode reward: 0.455, mean reward: 0.003 [-0.002, 0.011], mean action: -0.049 [-1.099, 1.166], mean observation: 0.128 [-25.846, 17.984], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.592108\n",
      " 1513/2000: episode: 9, duration: 6.475s, episode steps: 136, steps per second: 21, episode reward: 0.461, mean reward: 0.003 [-0.002, 0.011], mean action: -0.036 [-1.144, 1.196], mean observation: 0.130 [-23.364, 17.261], loss: 0.000081, mean_squared_error: 0.000161, mean_q: 0.598904\n",
      " 1665/2000: episode: 10, duration: 7.302s, episode steps: 152, steps per second: 21, episode reward: 0.454, mean reward: 0.003 [-0.002, 0.011], mean action: -0.139 [-1.168, 1.269], mean observation: 0.128 [-24.812, 17.405], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.606150\n",
      " 1776/2000: episode: 11, duration: 5.187s, episode steps: 111, steps per second: 21, episode reward: 0.433, mean reward: 0.004 [-0.002, 0.011], mean action: -0.142 [-1.243, 1.133], mean observation: 0.125 [-25.138, 18.105], loss: 0.000103, mean_squared_error: 0.000206, mean_q: 0.602966\n",
      " 1905/2000: episode: 12, duration: 6.260s, episode steps: 129, steps per second: 21, episode reward: 0.426, mean reward: 0.003 [-0.002, 0.011], mean action: -0.149 [-1.180, 1.132], mean observation: 0.124 [-25.080, 18.381], loss: 0.000206, mean_squared_error: 0.000412, mean_q: 0.596962\n",
      "done, took 88.747 seconds\n",
      "\n",
      "\n",
      "iteration: 179\n",
      "Training for 2000 steps ...\n",
      "  179/2000: episode: 1, duration: 7.171s, episode steps: 179, steps per second: 25, episode reward: 0.457, mean reward: 0.003 [-0.002, 0.011], mean action: -0.096 [-1.246, 1.258], mean observation: 0.124 [-24.825, 17.989], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  351/2000: episode: 2, duration: 6.661s, episode steps: 172, steps per second: 26, episode reward: 0.460, mean reward: 0.003 [-0.002, 0.011], mean action: -0.090 [-1.155, 1.168], mean observation: 0.124 [-24.701, 18.116], loss: --, mean_squared_error: --, mean_q: --\n",
      "  531/2000: episode: 3, duration: 7.126s, episode steps: 180, steps per second: 25, episode reward: 0.457, mean reward: 0.003 [-0.002, 0.011], mean action: -0.131 [-1.249, 1.190], mean observation: 0.123 [-24.592, 17.858], loss: --, mean_squared_error: --, mean_q: --\n",
      "  709/2000: episode: 4, duration: 6.975s, episode steps: 178, steps per second: 26, episode reward: 0.463, mean reward: 0.003 [-0.002, 0.011], mean action: -0.095 [-1.335, 1.229], mean observation: 0.124 [-24.625, 17.933], loss: --, mean_squared_error: --, mean_q: --\n",
      "  887/2000: episode: 5, duration: 6.843s, episode steps: 178, steps per second: 26, episode reward: 0.458, mean reward: 0.003 [-0.002, 0.011], mean action: -0.104 [-1.363, 1.266], mean observation: 0.124 [-25.804, 18.505], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1064/2000: episode: 6, duration: 7.479s, episode steps: 177, steps per second: 24, episode reward: 0.455, mean reward: 0.003 [-0.002, 0.011], mean action: -0.085 [-1.217, 1.237], mean observation: 0.124 [-25.026, 18.287], loss: 0.000045, mean_squared_error: 0.000090, mean_q: 0.601090\n",
      " 1235/2000: episode: 7, duration: 8.607s, episode steps: 171, steps per second: 20, episode reward: 0.454, mean reward: 0.003 [-0.002, 0.011], mean action: -0.142 [-1.234, 1.226], mean observation: 0.124 [-24.422, 17.974], loss: 0.000166, mean_squared_error: 0.000331, mean_q: 0.600375\n",
      " 1403/2000: episode: 8, duration: 8.325s, episode steps: 168, steps per second: 20, episode reward: 0.460, mean reward: 0.003 [-0.002, 0.011], mean action: -0.144 [-1.385, 1.146], mean observation: 0.126 [-25.235, 17.863], loss: 0.000182, mean_squared_error: 0.000364, mean_q: 0.599750\n",
      " 1536/2000: episode: 9, duration: 6.669s, episode steps: 133, steps per second: 20, episode reward: 0.429, mean reward: 0.003 [-0.002, 0.011], mean action: -0.094 [-1.158, 1.207], mean observation: 0.126 [-24.642, 17.980], loss: 0.000161, mean_squared_error: 0.000322, mean_q: 0.596764\n",
      " 1723/2000: episode: 10, duration: 9.429s, episode steps: 187, steps per second: 20, episode reward: 0.455, mean reward: 0.002 [-0.002, 0.011], mean action: -0.138 [-1.224, 1.206], mean observation: 0.129 [-23.740, 18.070], loss: 0.000087, mean_squared_error: 0.000174, mean_q: 0.598546\n",
      " 1925/2000: episode: 11, duration: 10.509s, episode steps: 202, steps per second: 19, episode reward: 0.436, mean reward: 0.002 [-0.002, 0.011], mean action: -0.077 [-1.355, 1.217], mean observation: 0.126 [-24.566, 17.693], loss: 0.000060, mean_squared_error: 0.000121, mean_q: 0.603879\n",
      "done, took 89.683 seconds\n",
      "\n",
      "\n",
      "iteration: 180\n",
      "Training for 2000 steps ...\n",
      "  182/2000: episode: 1, duration: 6.783s, episode steps: 182, steps per second: 27, episode reward: 0.462, mean reward: 0.003 [-0.002, 0.011], mean action: 0.003 [-1.210, 1.201], mean observation: 0.129 [-24.213, 18.075], loss: --, mean_squared_error: --, mean_q: --\n",
      "  368/2000: episode: 2, duration: 7.041s, episode steps: 186, steps per second: 26, episode reward: 0.459, mean reward: 0.002 [-0.002, 0.011], mean action: -0.011 [-1.235, 1.317], mean observation: 0.128 [-23.972, 17.770], loss: --, mean_squared_error: --, mean_q: --\n",
      "  547/2000: episode: 3, duration: 6.580s, episode steps: 179, steps per second: 27, episode reward: 0.466, mean reward: 0.003 [-0.002, 0.011], mean action: 0.011 [-1.217, 1.189], mean observation: 0.130 [-24.751, 18.163], loss: --, mean_squared_error: --, mean_q: --\n",
      "  728/2000: episode: 4, duration: 6.794s, episode steps: 181, steps per second: 27, episode reward: 0.463, mean reward: 0.003 [-0.002, 0.011], mean action: 0.008 [-1.210, 1.167], mean observation: 0.129 [-24.201, 17.956], loss: --, mean_squared_error: --, mean_q: --\n",
      "  907/2000: episode: 5, duration: 6.611s, episode steps: 179, steps per second: 27, episode reward: 0.456, mean reward: 0.003 [-0.002, 0.011], mean action: -0.001 [-1.237, 1.193], mean observation: 0.128 [-25.806, 18.425], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1090/2000: episode: 6, duration: 7.446s, episode steps: 183, steps per second: 25, episode reward: 0.465, mean reward: 0.003 [-0.002, 0.011], mean action: 0.029 [-1.161, 1.191], mean observation: 0.130 [-23.986, 17.697], loss: 0.000054, mean_squared_error: 0.000108, mean_q: 0.594946\n",
      " 1271/2000: episode: 7, duration: 8.964s, episode steps: 181, steps per second: 20, episode reward: 0.458, mean reward: 0.003 [-0.002, 0.011], mean action: -0.123 [-1.289, 1.333], mean observation: 0.125 [-23.247, 17.631], loss: 0.000124, mean_squared_error: 0.000248, mean_q: 0.601489\n",
      " 1459/2000: episode: 8, duration: 8.795s, episode steps: 188, steps per second: 21, episode reward: 0.452, mean reward: 0.002 [-0.002, 0.011], mean action: -0.201 [-1.212, 1.160], mean observation: 0.127 [-23.142, 17.910], loss: 0.000234, mean_squared_error: 0.000468, mean_q: 0.604307\n",
      " 1644/2000: episode: 9, duration: 8.359s, episode steps: 185, steps per second: 22, episode reward: 0.473, mean reward: 0.003 [-0.002, 0.011], mean action: -0.182 [-1.189, 1.219], mean observation: 0.131 [-23.572, 18.074], loss: 0.000074, mean_squared_error: 0.000148, mean_q: 0.593069\n",
      " 1823/2000: episode: 10, duration: 8.726s, episode steps: 179, steps per second: 21, episode reward: 0.463, mean reward: 0.003 [-0.003, 0.011], mean action: -0.161 [-1.247, 1.157], mean observation: 0.133 [-23.444, 18.205], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.593073\n",
      "done, took 85.102 seconds\n",
      "\n",
      "\n",
      "iteration: 181\n",
      "Training for 2000 steps ...\n",
      "  197/2000: episode: 1, duration: 6.203s, episode steps: 197, steps per second: 32, episode reward: 0.452, mean reward: 0.002 [-0.002, 0.011], mean action: -0.147 [-1.218, 1.382], mean observation: 0.125 [-24.448, 20.018], loss: --, mean_squared_error: --, mean_q: --\n",
      "  397/2000: episode: 2, duration: 6.389s, episode steps: 200, steps per second: 31, episode reward: 0.453, mean reward: 0.002 [-0.002, 0.011], mean action: -0.171 [-1.170, 1.192], mean observation: 0.125 [-23.692, 20.648], loss: --, mean_squared_error: --, mean_q: --\n",
      "  597/2000: episode: 3, duration: 6.428s, episode steps: 200, steps per second: 31, episode reward: 0.458, mean reward: 0.002 [-0.002, 0.011], mean action: -0.187 [-1.179, 1.122], mean observation: 0.124 [-24.584, 19.708], loss: --, mean_squared_error: --, mean_q: --\n",
      "  796/2000: episode: 4, duration: 6.285s, episode steps: 199, steps per second: 32, episode reward: 0.451, mean reward: 0.002 [-0.002, 0.011], mean action: -0.164 [-1.109, 1.158], mean observation: 0.125 [-24.003, 19.671], loss: --, mean_squared_error: --, mean_q: --\n",
      "  994/2000: episode: 5, duration: 6.372s, episode steps: 198, steps per second: 31, episode reward: 0.459, mean reward: 0.002 [-0.002, 0.011], mean action: -0.201 [-1.252, 1.249], mean observation: 0.125 [-24.995, 20.408], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1189/2000: episode: 6, duration: 9.176s, episode steps: 195, steps per second: 21, episode reward: 0.453, mean reward: 0.002 [-0.002, 0.011], mean action: -0.182 [-1.247, 1.160], mean observation: 0.127 [-24.435, 20.098], loss: 0.000189, mean_squared_error: 0.000378, mean_q: 0.593939\n",
      " 1397/2000: episode: 7, duration: 10.172s, episode steps: 208, steps per second: 20, episode reward: 0.444, mean reward: 0.002 [-0.003, 0.011], mean action: -0.167 [-1.228, 1.246], mean observation: 0.128 [-23.097, 19.934], loss: 0.000119, mean_squared_error: 0.000239, mean_q: 0.599446\n",
      " 1598/2000: episode: 8, duration: 9.593s, episode steps: 201, steps per second: 21, episode reward: 0.492, mean reward: 0.002 [-0.002, 0.011], mean action: -0.131 [-1.215, 1.245], mean observation: 0.136 [-22.673, 20.163], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.593716\n",
      " 1801/2000: episode: 9, duration: 10.000s, episode steps: 203, steps per second: 20, episode reward: 0.483, mean reward: 0.002 [-0.003, 0.011], mean action: -0.124 [-1.197, 1.151], mean observation: 0.134 [-23.000, 20.481], loss: 0.000240, mean_squared_error: 0.000479, mean_q: 0.592526\n",
      "done, took 79.758 seconds\n",
      "\n",
      "\n",
      "iteration: 182\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  194/2000: episode: 1, duration: 7.581s, episode steps: 194, steps per second: 26, episode reward: 0.458, mean reward: 0.002 [-0.003, 0.011], mean action: -0.144 [-1.150, 1.193], mean observation: 0.132 [-22.316, 20.377], loss: --, mean_squared_error: --, mean_q: --\n",
      "  391/2000: episode: 2, duration: 7.529s, episode steps: 197, steps per second: 26, episode reward: 0.455, mean reward: 0.002 [-0.003, 0.011], mean action: -0.177 [-1.247, 1.194], mean observation: 0.132 [-23.027, 20.392], loss: --, mean_squared_error: --, mean_q: --\n",
      "  590/2000: episode: 3, duration: 7.659s, episode steps: 199, steps per second: 26, episode reward: 0.465, mean reward: 0.002 [-0.003, 0.011], mean action: -0.178 [-1.235, 1.105], mean observation: 0.131 [-23.864, 19.818], loss: --, mean_squared_error: --, mean_q: --\n",
      "  787/2000: episode: 4, duration: 7.594s, episode steps: 197, steps per second: 26, episode reward: 0.458, mean reward: 0.002 [-0.003, 0.011], mean action: -0.142 [-1.180, 1.240], mean observation: 0.132 [-23.808, 20.155], loss: --, mean_squared_error: --, mean_q: --\n",
      "  984/2000: episode: 5, duration: 7.478s, episode steps: 197, steps per second: 26, episode reward: 0.464, mean reward: 0.002 [-0.003, 0.011], mean action: -0.162 [-1.181, 1.197], mean observation: 0.132 [-23.643, 20.192], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1176/2000: episode: 6, duration: 8.982s, episode steps: 192, steps per second: 21, episode reward: 0.463, mean reward: 0.002 [-0.003, 0.011], mean action: -0.124 [-1.211, 1.158], mean observation: 0.132 [-22.116, 20.271], loss: 0.000056, mean_squared_error: 0.000113, mean_q: 0.595377\n",
      " 1364/2000: episode: 7, duration: 9.273s, episode steps: 188, steps per second: 20, episode reward: 0.457, mean reward: 0.002 [-0.002, 0.011], mean action: -0.152 [-1.258, 1.277], mean observation: 0.130 [-22.164, 20.232], loss: 0.000237, mean_squared_error: 0.000473, mean_q: 0.599289\n",
      " 1567/2000: episode: 8, duration: 10.065s, episode steps: 203, steps per second: 20, episode reward: 0.448, mean reward: 0.002 [-0.003, 0.011], mean action: -0.123 [-1.391, 1.229], mean observation: 0.129 [-22.550, 20.248], loss: 0.000146, mean_squared_error: 0.000293, mean_q: 0.594903\n",
      " 1759/2000: episode: 9, duration: 9.266s, episode steps: 192, steps per second: 21, episode reward: 0.456, mean reward: 0.002 [-0.002, 0.011], mean action: -0.114 [-1.234, 1.242], mean observation: 0.132 [-22.354, 20.540], loss: 0.000089, mean_squared_error: 0.000179, mean_q: 0.592973\n",
      " 1917/2000: episode: 10, duration: 7.852s, episode steps: 158, steps per second: 20, episode reward: 0.462, mean reward: 0.003 [-0.002, 0.011], mean action: -0.082 [-1.205, 1.338], mean observation: 0.132 [-22.693, 20.909], loss: 0.000256, mean_squared_error: 0.000511, mean_q: 0.601808\n",
      "done, took 87.969 seconds\n",
      "\n",
      "\n",
      "iteration: 183\n",
      "Training for 2000 steps ...\n",
      "  178/2000: episode: 1, duration: 6.971s, episode steps: 178, steps per second: 26, episode reward: 0.446, mean reward: 0.003 [-0.002, 0.011], mean action: -0.034 [-1.290, 1.267], mean observation: 0.131 [-22.123, 20.495], loss: --, mean_squared_error: --, mean_q: --\n",
      "  355/2000: episode: 2, duration: 6.829s, episode steps: 177, steps per second: 26, episode reward: 0.441, mean reward: 0.002 [-0.002, 0.011], mean action: -0.055 [-1.171, 1.268], mean observation: 0.130 [-22.610, 20.679], loss: --, mean_squared_error: --, mean_q: --\n",
      "  535/2000: episode: 3, duration: 7.059s, episode steps: 180, steps per second: 25, episode reward: 0.447, mean reward: 0.002 [-0.002, 0.011], mean action: -0.060 [-1.226, 1.202], mean observation: 0.132 [-23.189, 20.441], loss: --, mean_squared_error: --, mean_q: --\n",
      "  712/2000: episode: 4, duration: 6.854s, episode steps: 177, steps per second: 26, episode reward: 0.437, mean reward: 0.002 [-0.002, 0.011], mean action: -0.038 [-1.224, 1.328], mean observation: 0.130 [-21.220, 20.724], loss: --, mean_squared_error: --, mean_q: --\n",
      "  888/2000: episode: 5, duration: 6.835s, episode steps: 176, steps per second: 26, episode reward: 0.448, mean reward: 0.003 [-0.002, 0.011], mean action: -0.048 [-1.229, 1.175], mean observation: 0.132 [-23.899, 20.660], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1067/2000: episode: 6, duration: 7.620s, episode steps: 179, steps per second: 23, episode reward: 0.448, mean reward: 0.003 [-0.002, 0.011], mean action: -0.043 [-1.165, 1.199], mean observation: 0.131 [-22.492, 20.422], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.586618\n",
      " 1221/2000: episode: 7, duration: 7.577s, episode steps: 154, steps per second: 20, episode reward: 0.434, mean reward: 0.003 [-0.002, 0.011], mean action: -0.117 [-1.175, 1.191], mean observation: 0.128 [-23.590, 20.287], loss: 0.000213, mean_squared_error: 0.000426, mean_q: 0.597372\n",
      " 1293/2000: episode: 8, duration: 3.231s, episode steps: 72, steps per second: 22, episode reward: 0.367, mean reward: 0.005 [-0.000, 0.012], mean action: -0.164 [-1.146, 1.152], mean observation: 0.129 [-22.412, 19.206], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.584848\n",
      " 1368/2000: episode: 9, duration: 3.517s, episode steps: 75, steps per second: 21, episode reward: 0.373, mean reward: 0.005 [-0.001, 0.012], mean action: -0.145 [-1.170, 1.137], mean observation: 0.126 [-22.545, 18.941], loss: 0.000103, mean_squared_error: 0.000206, mean_q: 0.590081\n",
      " 1520/2000: episode: 10, duration: 7.345s, episode steps: 152, steps per second: 21, episode reward: 0.500, mean reward: 0.003 [-0.002, 0.011], mean action: -0.093 [-1.244, 1.217], mean observation: 0.128 [-21.685, 18.992], loss: 0.000105, mean_squared_error: 0.000210, mean_q: 0.592245\n",
      " 1665/2000: episode: 11, duration: 6.746s, episode steps: 145, steps per second: 21, episode reward: 0.499, mean reward: 0.003 [-0.002, 0.012], mean action: -0.031 [-1.189, 1.146], mean observation: 0.132 [-22.045, 18.839], loss: 0.000039, mean_squared_error: 0.000077, mean_q: 0.582082\n",
      " 1839/2000: episode: 12, duration: 8.518s, episode steps: 174, steps per second: 20, episode reward: 0.488, mean reward: 0.003 [-0.002, 0.012], mean action: -0.046 [-1.188, 1.270], mean observation: 0.131 [-22.926, 20.230], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.588992\n",
      "done, took 87.464 seconds\n",
      "\n",
      "\n",
      "iteration: 184\n",
      "Training for 2000 steps ...\n",
      "  215/2000: episode: 1, duration: 8.525s, episode steps: 215, steps per second: 25, episode reward: 0.512, mean reward: 0.002 [-0.002, 0.011], mean action: -0.055 [-1.285, 1.219], mean observation: 0.129 [-22.036, 20.050], loss: --, mean_squared_error: --, mean_q: --\n",
      "  433/2000: episode: 2, duration: 8.829s, episode steps: 218, steps per second: 25, episode reward: 0.484, mean reward: 0.002 [-0.002, 0.011], mean action: -0.033 [-1.254, 1.392], mean observation: 0.128 [-22.534, 20.604], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 3, duration: 8.636s, episode steps: 216, steps per second: 25, episode reward: 0.479, mean reward: 0.002 [-0.002, 0.011], mean action: -0.040 [-1.292, 1.268], mean observation: 0.128 [-23.032, 20.224], loss: --, mean_squared_error: --, mean_q: --\n",
      "  868/2000: episode: 4, duration: 8.839s, episode steps: 219, steps per second: 25, episode reward: 0.510, mean reward: 0.002 [-0.002, 0.011], mean action: -0.044 [-1.195, 1.221], mean observation: 0.129 [-22.878, 20.429], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1087/2000: episode: 5, duration: 9.669s, episode steps: 219, steps per second: 23, episode reward: 0.513, mean reward: 0.002 [-0.002, 0.011], mean action: -0.050 [-1.287, 1.154], mean observation: 0.128 [-22.398, 20.632], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.588380\n",
      " 1288/2000: episode: 6, duration: 10.349s, episode steps: 201, steps per second: 19, episode reward: 0.490, mean reward: 0.002 [-0.002, 0.011], mean action: 0.035 [-1.133, 1.414], mean observation: 0.130 [-23.523, 19.141], loss: 0.000140, mean_squared_error: 0.000279, mean_q: 0.589763\n",
      " 1486/2000: episode: 7, duration: 9.860s, episode steps: 198, steps per second: 20, episode reward: 0.470, mean reward: 0.002 [-0.002, 0.011], mean action: -0.006 [-1.267, 1.249], mean observation: 0.131 [-23.020, 19.229], loss: 0.000230, mean_squared_error: 0.000461, mean_q: 0.595934\n",
      " 1707/2000: episode: 8, duration: 10.394s, episode steps: 221, steps per second: 21, episode reward: 0.505, mean reward: 0.002 [-0.002, 0.011], mean action: -0.180 [-1.392, 1.264], mean observation: 0.128 [-22.440, 19.324], loss: 0.000046, mean_squared_error: 0.000093, mean_q: 0.586511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1877/2000: episode: 9, duration: 8.003s, episode steps: 170, steps per second: 21, episode reward: 0.495, mean reward: 0.003 [-0.002, 0.011], mean action: -0.070 [-1.244, 1.213], mean observation: 0.130 [-23.891, 18.882], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.591629\n",
      "done, took 89.850 seconds\n",
      "\n",
      "\n",
      "iteration: 185\n",
      "Training for 2000 steps ...\n",
      "  215/2000: episode: 1, duration: 8.121s, episode steps: 215, steps per second: 26, episode reward: 0.520, mean reward: 0.002 [-0.002, 0.010], mean action: -0.070 [-1.238, 1.179], mean observation: 0.127 [-24.111, 18.770], loss: --, mean_squared_error: --, mean_q: --\n",
      "  457/2000: episode: 2, duration: 9.135s, episode steps: 242, steps per second: 26, episode reward: 0.491, mean reward: 0.002 [-0.002, 0.010], mean action: -0.091 [-1.276, 1.215], mean observation: 0.124 [-23.477, 18.934], loss: --, mean_squared_error: --, mean_q: --\n",
      "  681/2000: episode: 3, duration: 8.401s, episode steps: 224, steps per second: 27, episode reward: 0.514, mean reward: 0.002 [-0.002, 0.010], mean action: -0.067 [-1.151, 1.271], mean observation: 0.125 [-22.386, 18.882], loss: --, mean_squared_error: --, mean_q: --\n",
      "  909/2000: episode: 4, duration: 8.526s, episode steps: 228, steps per second: 27, episode reward: 0.504, mean reward: 0.002 [-0.002, 0.010], mean action: -0.085 [-1.156, 1.244], mean observation: 0.125 [-22.377, 18.920], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1098/2000: episode: 5, duration: 8.035s, episode steps: 189, steps per second: 24, episode reward: 0.502, mean reward: 0.003 [-0.002, 0.010], mean action: -0.144 [-1.267, 1.179], mean observation: 0.129 [-22.141, 19.016], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.586184\n",
      " 1303/2000: episode: 6, duration: 10.165s, episode steps: 205, steps per second: 20, episode reward: 0.520, mean reward: 0.003 [-0.003, 0.010], mean action: -0.196 [-1.254, 1.181], mean observation: 0.130 [-23.382, 18.542], loss: 0.000180, mean_squared_error: 0.000360, mean_q: 0.579956\n",
      " 1485/2000: episode: 7, duration: 8.975s, episode steps: 182, steps per second: 20, episode reward: 0.510, mean reward: 0.003 [-0.001, 0.010], mean action: -0.182 [-1.209, 1.165], mean observation: 0.137 [-22.711, 19.108], loss: 0.000108, mean_squared_error: 0.000216, mean_q: 0.586441\n",
      " 1649/2000: episode: 8, duration: 7.970s, episode steps: 164, steps per second: 21, episode reward: 0.517, mean reward: 0.003 [-0.001, 0.010], mean action: -0.149 [-1.162, 1.267], mean observation: 0.140 [-22.010, 18.063], loss: 0.000313, mean_squared_error: 0.000626, mean_q: 0.581831\n",
      " 1860/2000: episode: 9, duration: 10.317s, episode steps: 211, steps per second: 20, episode reward: 0.499, mean reward: 0.002 [-0.001, 0.010], mean action: -0.104 [-1.277, 1.327], mean observation: 0.130 [-22.762, 18.097], loss: 0.000055, mean_squared_error: 0.000110, mean_q: 0.589233\n",
      "done, took 87.289 seconds\n",
      "\n",
      "\n",
      "iteration: 186\n",
      "Training for 2000 steps ...\n",
      "  223/2000: episode: 1, duration: 9.073s, episode steps: 223, steps per second: 25, episode reward: 0.537, mean reward: 0.002 [-0.002, 0.010], mean action: -0.229 [-1.173, 1.187], mean observation: 0.133 [-20.647, 17.377], loss: --, mean_squared_error: --, mean_q: --\n",
      "  435/2000: episode: 2, duration: 8.710s, episode steps: 212, steps per second: 24, episode reward: 0.537, mean reward: 0.003 [-0.002, 0.010], mean action: -0.217 [-1.239, 1.209], mean observation: 0.134 [-19.842, 16.984], loss: --, mean_squared_error: --, mean_q: --\n",
      "  646/2000: episode: 3, duration: 8.542s, episode steps: 211, steps per second: 25, episode reward: 0.531, mean reward: 0.003 [-0.002, 0.010], mean action: -0.218 [-1.270, 1.231], mean observation: 0.133 [-19.122, 16.721], loss: --, mean_squared_error: --, mean_q: --\n",
      "  865/2000: episode: 4, duration: 8.960s, episode steps: 219, steps per second: 24, episode reward: 0.530, mean reward: 0.002 [-0.002, 0.010], mean action: -0.203 [-1.238, 1.335], mean observation: 0.133 [-20.339, 17.606], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1108/2000: episode: 5, duration: 10.956s, episode steps: 243, steps per second: 22, episode reward: 0.539, mean reward: 0.002 [-0.002, 0.010], mean action: -0.190 [-1.198, 1.371], mean observation: 0.131 [-21.554, 17.732], loss: 0.000161, mean_squared_error: 0.000321, mean_q: 0.591847\n",
      " 1272/2000: episode: 6, duration: 8.047s, episode steps: 164, steps per second: 20, episode reward: 0.502, mean reward: 0.003 [-0.003, 0.010], mean action: -0.201 [-1.253, 1.289], mean observation: 0.137 [-20.599, 17.309], loss: 0.000048, mean_squared_error: 0.000096, mean_q: 0.585896\n",
      " 1473/2000: episode: 7, duration: 10.186s, episode steps: 201, steps per second: 20, episode reward: 0.553, mean reward: 0.003 [-0.002, 0.010], mean action: -0.129 [-1.182, 1.246], mean observation: 0.135 [-21.788, 16.924], loss: 0.000062, mean_squared_error: 0.000125, mean_q: 0.583547\n",
      " 1682/2000: episode: 8, duration: 10.268s, episode steps: 209, steps per second: 20, episode reward: 0.533, mean reward: 0.003 [-0.002, 0.010], mean action: -0.148 [-1.264, 1.230], mean observation: 0.133 [-19.316, 16.819], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.587761\n",
      " 1875/2000: episode: 9, duration: 9.482s, episode steps: 193, steps per second: 20, episode reward: 0.509, mean reward: 0.003 [-0.002, 0.010], mean action: -0.125 [-1.298, 1.204], mean observation: 0.135 [-22.895, 17.301], loss: 0.000084, mean_squared_error: 0.000167, mean_q: 0.586751\n",
      "done, took 90.737 seconds\n",
      "\n",
      "\n",
      "iteration: 187\n",
      "Training for 2000 steps ...\n",
      "  152/2000: episode: 1, duration: 6.098s, episode steps: 152, steps per second: 25, episode reward: 0.541, mean reward: 0.004 [-0.002, 0.010], mean action: -0.146 [-1.246, 1.225], mean observation: 0.142 [-18.579, 16.535], loss: --, mean_squared_error: --, mean_q: --\n",
      "  303/2000: episode: 2, duration: 5.807s, episode steps: 151, steps per second: 26, episode reward: 0.513, mean reward: 0.003 [-0.002, 0.010], mean action: -0.138 [-1.207, 1.197], mean observation: 0.141 [-19.795, 17.883], loss: --, mean_squared_error: --, mean_q: --\n",
      "  457/2000: episode: 3, duration: 5.734s, episode steps: 154, steps per second: 27, episode reward: 0.521, mean reward: 0.003 [-0.002, 0.010], mean action: -0.145 [-1.192, 1.277], mean observation: 0.141 [-19.568, 19.034], loss: --, mean_squared_error: --, mean_q: --\n",
      "  610/2000: episode: 4, duration: 5.955s, episode steps: 153, steps per second: 26, episode reward: 0.538, mean reward: 0.004 [-0.002, 0.010], mean action: -0.156 [-1.267, 1.286], mean observation: 0.142 [-19.960, 17.007], loss: --, mean_squared_error: --, mean_q: --\n",
      "  763/2000: episode: 5, duration: 5.896s, episode steps: 153, steps per second: 26, episode reward: 0.526, mean reward: 0.003 [-0.002, 0.010], mean action: -0.148 [-1.129, 1.194], mean observation: 0.143 [-21.479, 18.690], loss: --, mean_squared_error: --, mean_q: --\n",
      "  916/2000: episode: 6, duration: 6.006s, episode steps: 153, steps per second: 25, episode reward: 0.531, mean reward: 0.003 [-0.002, 0.010], mean action: -0.158 [-1.229, 1.236], mean observation: 0.142 [-19.473, 18.426], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1069/2000: episode: 7, duration: 6.676s, episode steps: 153, steps per second: 23, episode reward: 0.526, mean reward: 0.003 [-0.002, 0.010], mean action: -0.144 [-1.177, 1.185], mean observation: 0.144 [-20.898, 17.327], loss: 0.000046, mean_squared_error: 0.000092, mean_q: 0.579144\n",
      " 1240/2000: episode: 8, duration: 8.488s, episode steps: 171, steps per second: 20, episode reward: 0.571, mean reward: 0.003 [-0.003, 0.010], mean action: -0.155 [-1.204, 1.228], mean observation: 0.147 [-21.326, 17.536], loss: 0.000111, mean_squared_error: 0.000221, mean_q: 0.577719\n",
      " 1444/2000: episode: 9, duration: 9.913s, episode steps: 204, steps per second: 21, episode reward: 0.579, mean reward: 0.003 [-0.002, 0.010], mean action: -0.086 [-1.271, 1.307], mean observation: 0.134 [-22.134, 18.740], loss: 0.000112, mean_squared_error: 0.000225, mean_q: 0.586004\n",
      " 1757/2000: episode: 10, duration: 14.345s, episode steps: 313, steps per second: 22, episode reward: 0.675, mean reward: 0.002 [-0.004, 0.012], mean action: -0.013 [-1.327, 1.314], mean observation: 0.112 [-24.167, 17.455], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.580883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1932/2000: episode: 11, duration: 5.804s, episode steps: 175, steps per second: 30, episode reward: -0.810, mean reward: -0.005 [-0.020, 0.008], mean action: -0.165 [-1.317, 1.193], mean observation: 0.062 [-32.989, 18.274], loss: 0.000174, mean_squared_error: 0.000349, mean_q: 0.576842\n",
      "done, took 84.251 seconds\n",
      "\n",
      "\n",
      "iteration: 188\n",
      "Training for 2000 steps ...\n",
      "  257/2000: episode: 1, duration: 6.757s, episode steps: 257, steps per second: 38, episode reward: -0.855, mean reward: -0.003 [-0.019, 0.011], mean action: -0.121 [-1.234, 1.203], mean observation: 0.073 [-32.580, 18.221], loss: --, mean_squared_error: --, mean_q: --\n",
      "  520/2000: episode: 2, duration: 6.904s, episode steps: 263, steps per second: 38, episode reward: -0.862, mean reward: -0.003 [-0.019, 0.011], mean action: -0.120 [-1.158, 1.125], mean observation: 0.073 [-32.623, 18.171], loss: --, mean_squared_error: --, mean_q: --\n",
      "  770/2000: episode: 3, duration: 6.603s, episode steps: 250, steps per second: 38, episode reward: -0.867, mean reward: -0.003 [-0.019, 0.011], mean action: -0.107 [-1.221, 1.330], mean observation: 0.070 [-32.975, 18.103], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1041/2000: episode: 4, duration: 7.060s, episode steps: 271, steps per second: 38, episode reward: -0.848, mean reward: -0.003 [-0.019, 0.011], mean action: -0.149 [-1.265, 1.256], mean observation: 0.075 [-32.460, 18.212], loss: 0.000061, mean_squared_error: 0.000123, mean_q: 0.571184\n",
      " 1254/2000: episode: 5, duration: 10.690s, episode steps: 213, steps per second: 20, episode reward: 0.651, mean reward: 0.003 [-0.005, 0.012], mean action: 0.003 [-1.310, 1.198], mean observation: 0.105 [-32.773, 18.150], loss: 0.000060, mean_squared_error: 0.000119, mean_q: 0.581574\n",
      " 1510/2000: episode: 6, duration: 12.269s, episode steps: 256, steps per second: 21, episode reward: 0.680, mean reward: 0.003 [-0.004, 0.012], mean action: -0.020 [-1.284, 1.207], mean observation: 0.114 [-33.590, 18.528], loss: 0.000129, mean_squared_error: 0.000257, mean_q: 0.570165\n",
      " 1698/2000: episode: 7, duration: 8.414s, episode steps: 188, steps per second: 22, episode reward: 0.528, mean reward: 0.003 [-0.004, 0.010], mean action: 0.007 [-1.206, 1.209], mean observation: 0.131 [-32.503, 18.211], loss: 0.000206, mean_squared_error: 0.000411, mean_q: 0.570160\n",
      " 1873/2000: episode: 8, duration: 8.706s, episode steps: 175, steps per second: 20, episode reward: 0.537, mean reward: 0.003 [-0.004, 0.010], mean action: 0.015 [-1.217, 1.243], mean observation: 0.132 [-24.328, 17.486], loss: 0.000044, mean_squared_error: 0.000089, mean_q: 0.569327\n",
      "done, took 74.788 seconds\n",
      "\n",
      "\n",
      "iteration: 189\n",
      "Training for 2000 steps ...\n",
      "  171/2000: episode: 1, duration: 7.165s, episode steps: 171, steps per second: 24, episode reward: 0.507, mean reward: 0.003 [-0.003, 0.011], mean action: -0.014 [-1.244, 1.309], mean observation: 0.133 [-22.622, 16.803], loss: --, mean_squared_error: --, mean_q: --\n",
      "  340/2000: episode: 2, duration: 7.025s, episode steps: 169, steps per second: 24, episode reward: 0.509, mean reward: 0.003 [-0.003, 0.011], mean action: 0.004 [-1.207, 1.228], mean observation: 0.134 [-22.483, 16.750], loss: --, mean_squared_error: --, mean_q: --\n",
      "  513/2000: episode: 3, duration: 7.099s, episode steps: 173, steps per second: 24, episode reward: 0.494, mean reward: 0.003 [-0.003, 0.011], mean action: 0.001 [-1.212, 1.173], mean observation: 0.133 [-22.025, 16.633], loss: --, mean_squared_error: --, mean_q: --\n",
      "  679/2000: episode: 4, duration: 6.718s, episode steps: 166, steps per second: 25, episode reward: 0.490, mean reward: 0.003 [-0.003, 0.011], mean action: 0.035 [-1.193, 1.288], mean observation: 0.134 [-22.373, 16.804], loss: --, mean_squared_error: --, mean_q: --\n",
      "  853/2000: episode: 5, duration: 7.159s, episode steps: 174, steps per second: 24, episode reward: 0.525, mean reward: 0.003 [-0.003, 0.011], mean action: -0.011 [-1.143, 1.199], mean observation: 0.134 [-22.189, 16.626], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1027/2000: episode: 6, duration: 7.402s, episode steps: 174, steps per second: 24, episode reward: 0.523, mean reward: 0.003 [-0.003, 0.011], mean action: -0.032 [-1.361, 1.189], mean observation: 0.132 [-23.138, 17.009], loss: 0.000065, mean_squared_error: 0.000131, mean_q: 0.563309\n",
      " 1199/2000: episode: 7, duration: 8.541s, episode steps: 172, steps per second: 20, episode reward: 0.505, mean reward: 0.003 [-0.003, 0.011], mean action: -0.007 [-1.183, 1.255], mean observation: 0.132 [-21.964, 16.695], loss: 0.000125, mean_squared_error: 0.000251, mean_q: 0.575061\n",
      " 1361/2000: episode: 8, duration: 8.248s, episode steps: 162, steps per second: 20, episode reward: 0.528, mean reward: 0.003 [-0.002, 0.011], mean action: 0.012 [-1.238, 1.172], mean observation: 0.135 [-22.671, 19.493], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.570198\n",
      " 1511/2000: episode: 9, duration: 7.340s, episode steps: 150, steps per second: 20, episode reward: 0.510, mean reward: 0.003 [-0.003, 0.011], mean action: 0.053 [-1.171, 1.199], mean observation: 0.135 [-26.849, 15.102], loss: 0.000239, mean_squared_error: 0.000478, mean_q: 0.569224\n",
      " 1758/2000: episode: 10, duration: 13.027s, episode steps: 247, steps per second: 19, episode reward: 0.801, mean reward: 0.003 [-0.002, 0.013], mean action: 0.021 [-1.282, 1.272], mean observation: 0.124 [-23.661, 16.711], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.564948\n",
      " 1954/2000: episode: 11, duration: 10.716s, episode steps: 196, steps per second: 18, episode reward: 0.796, mean reward: 0.004 [-0.002, 0.013], mean action: -0.016 [-1.161, 1.192], mean observation: 0.123 [-24.271, 16.581], loss: 0.000153, mean_squared_error: 0.000307, mean_q: 0.567807\n",
      "done, took 93.117 seconds\n",
      "\n",
      "\n",
      "iteration: 190\n",
      "Training for 2000 steps ...\n",
      "  257/2000: episode: 1, duration: 11.132s, episode steps: 257, steps per second: 23, episode reward: 0.742, mean reward: 0.003 [-0.003, 0.013], mean action: -0.049 [-1.151, 1.201], mean observation: 0.115 [-23.570, 16.813], loss: --, mean_squared_error: --, mean_q: --\n",
      "  521/2000: episode: 2, duration: 11.536s, episode steps: 264, steps per second: 23, episode reward: 0.791, mean reward: 0.003 [-0.003, 0.013], mean action: -0.073 [-1.291, 1.227], mean observation: 0.119 [-23.388, 16.192], loss: --, mean_squared_error: --, mean_q: --\n",
      "  782/2000: episode: 3, duration: 11.554s, episode steps: 261, steps per second: 23, episode reward: 0.788, mean reward: 0.003 [-0.003, 0.013], mean action: -0.061 [-1.182, 1.250], mean observation: 0.118 [-24.504, 16.676], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1037/2000: episode: 4, duration: 11.326s, episode steps: 255, steps per second: 23, episode reward: 0.759, mean reward: 0.003 [-0.003, 0.013], mean action: -0.040 [-1.165, 1.276], mean observation: 0.114 [-23.964, 16.745], loss: 0.000060, mean_squared_error: 0.000120, mean_q: 0.580112\n",
      " 1272/2000: episode: 5, duration: 12.949s, episode steps: 235, steps per second: 18, episode reward: 0.796, mean reward: 0.003 [-0.003, 0.013], mean action: -0.086 [-1.286, 1.198], mean observation: 0.123 [-24.457, 16.767], loss: 0.000049, mean_squared_error: 0.000099, mean_q: 0.560653\n",
      " 1467/2000: episode: 6, duration: 11.019s, episode steps: 195, steps per second: 18, episode reward: 0.766, mean reward: 0.004 [-0.003, 0.012], mean action: -0.082 [-1.216, 1.131], mean observation: 0.119 [-23.803, 15.932], loss: 0.000145, mean_squared_error: 0.000290, mean_q: 0.560787\n",
      " 1704/2000: episode: 7, duration: 12.569s, episode steps: 237, steps per second: 19, episode reward: 0.770, mean reward: 0.003 [-0.002, 0.013], mean action: -0.073 [-1.331, 1.333], mean observation: 0.116 [-23.254, 16.853], loss: 0.000212, mean_squared_error: 0.000423, mean_q: 0.562831\n",
      " 1914/2000: episode: 8, duration: 11.301s, episode steps: 210, steps per second: 19, episode reward: 0.751, mean reward: 0.004 [-0.003, 0.013], mean action: -0.137 [-1.207, 1.158], mean observation: 0.115 [-23.678, 16.532], loss: 0.000105, mean_squared_error: 0.000209, mean_q: 0.551038\n",
      "done, took 98.361 seconds\n",
      "\n",
      "\n",
      "iteration: 191\n",
      "Training for 2000 steps ...\n",
      "  199/2000: episode: 1, duration: 8.707s, episode steps: 199, steps per second: 23, episode reward: 0.732, mean reward: 0.004 [-0.002, 0.013], mean action: -0.046 [-1.204, 1.280], mean observation: 0.111 [-23.752, 17.056], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  403/2000: episode: 2, duration: 8.776s, episode steps: 204, steps per second: 23, episode reward: 0.750, mean reward: 0.004 [-0.002, 0.013], mean action: -0.082 [-1.283, 1.249], mean observation: 0.112 [-24.279, 16.957], loss: --, mean_squared_error: --, mean_q: --\n",
      "  605/2000: episode: 3, duration: 8.761s, episode steps: 202, steps per second: 23, episode reward: 0.745, mean reward: 0.004 [-0.002, 0.013], mean action: -0.091 [-1.249, 1.273], mean observation: 0.111 [-24.951, 17.077], loss: --, mean_squared_error: --, mean_q: --\n",
      "  807/2000: episode: 4, duration: 8.773s, episode steps: 202, steps per second: 23, episode reward: 0.737, mean reward: 0.004 [-0.002, 0.013], mean action: -0.057 [-1.187, 1.305], mean observation: 0.112 [-24.549, 16.823], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1008/2000: episode: 5, duration: 8.868s, episode steps: 201, steps per second: 23, episode reward: 0.744, mean reward: 0.004 [-0.002, 0.013], mean action: -0.120 [-1.305, 1.130], mean observation: 0.111 [-24.070, 17.130], loss: 0.000039, mean_squared_error: 0.000077, mean_q: 0.530412\n",
      " 1188/2000: episode: 6, duration: 9.462s, episode steps: 180, steps per second: 19, episode reward: 0.743, mean reward: 0.004 [-0.002, 0.013], mean action: -0.119 [-1.247, 1.129], mean observation: 0.111 [-23.799, 17.039], loss: 0.000090, mean_squared_error: 0.000180, mean_q: 0.553218\n",
      " 1361/2000: episode: 7, duration: 8.914s, episode steps: 173, steps per second: 19, episode reward: 0.783, mean reward: 0.005 [-0.003, 0.013], mean action: -0.118 [-1.185, 1.168], mean observation: 0.116 [-23.690, 15.655], loss: 0.000202, mean_squared_error: 0.000403, mean_q: 0.554504\n",
      " 1539/2000: episode: 8, duration: 9.215s, episode steps: 178, steps per second: 19, episode reward: 0.783, mean reward: 0.004 [-0.003, 0.012], mean action: -0.121 [-1.212, 1.190], mean observation: 0.114 [-23.804, 16.897], loss: 0.000113, mean_squared_error: 0.000227, mean_q: 0.548602\n",
      " 1719/2000: episode: 9, duration: 9.399s, episode steps: 180, steps per second: 19, episode reward: 0.778, mean reward: 0.004 [-0.002, 0.012], mean action: -0.113 [-1.293, 1.236], mean observation: 0.114 [-25.086, 16.927], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.547946\n",
      " 1945/2000: episode: 10, duration: 11.537s, episode steps: 226, steps per second: 20, episode reward: 0.771, mean reward: 0.003 [-0.004, 0.013], mean action: -0.102 [-1.162, 1.299], mean observation: 0.122 [-24.213, 17.139], loss: 0.000098, mean_squared_error: 0.000197, mean_q: 0.549948\n",
      "done, took 95.596 seconds\n",
      "\n",
      "\n",
      "iteration: 192\n",
      "Training for 2000 steps ...\n",
      "  223/2000: episode: 1, duration: 8.938s, episode steps: 223, steps per second: 25, episode reward: 0.812, mean reward: 0.004 [-0.002, 0.012], mean action: -0.126 [-1.137, 1.189], mean observation: 0.132 [-24.282, 17.061], loss: --, mean_squared_error: --, mean_q: --\n",
      "  440/2000: episode: 2, duration: 8.944s, episode steps: 217, steps per second: 24, episode reward: 0.811, mean reward: 0.004 [-0.002, 0.013], mean action: -0.121 [-1.256, 1.214], mean observation: 0.131 [-24.070, 16.567], loss: --, mean_squared_error: --, mean_q: --\n",
      "  658/2000: episode: 3, duration: 8.699s, episode steps: 218, steps per second: 25, episode reward: 0.819, mean reward: 0.004 [-0.002, 0.013], mean action: -0.128 [-1.201, 1.183], mean observation: 0.132 [-24.486, 17.055], loss: --, mean_squared_error: --, mean_q: --\n",
      "  871/2000: episode: 4, duration: 8.568s, episode steps: 213, steps per second: 25, episode reward: 0.816, mean reward: 0.004 [-0.002, 0.013], mean action: -0.104 [-1.360, 1.272], mean observation: 0.133 [-24.104, 16.842], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1083/2000: episode: 5, duration: 9.599s, episode steps: 212, steps per second: 22, episode reward: 0.814, mean reward: 0.004 [-0.002, 0.012], mean action: -0.125 [-1.187, 1.247], mean observation: 0.130 [-24.753, 16.987], loss: 0.000076, mean_squared_error: 0.000153, mean_q: 0.549106\n",
      " 1280/2000: episode: 6, duration: 10.385s, episode steps: 197, steps per second: 19, episode reward: 0.799, mean reward: 0.004 [-0.003, 0.012], mean action: -0.135 [-1.220, 1.145], mean observation: 0.122 [-23.640, 16.875], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.544945\n",
      " 1478/2000: episode: 7, duration: 10.362s, episode steps: 198, steps per second: 19, episode reward: 0.813, mean reward: 0.004 [-0.004, 0.013], mean action: -0.112 [-1.178, 1.287], mean observation: 0.118 [-24.084, 16.627], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.540037\n",
      " 1634/2000: episode: 8, duration: 8.691s, episode steps: 156, steps per second: 18, episode reward: 0.754, mean reward: 0.005 [-0.004, 0.013], mean action: -0.126 [-1.210, 1.190], mean observation: 0.102 [-38.245, 18.137], loss: 0.000084, mean_squared_error: 0.000167, mean_q: 0.548658\n",
      " 1875/2000: episode: 9, duration: 12.798s, episode steps: 241, steps per second: 19, episode reward: 0.816, mean reward: 0.003 [-0.003, 0.013], mean action: -0.153 [-1.169, 1.220], mean observation: 0.119 [-24.222, 16.924], loss: 0.000068, mean_squared_error: 0.000135, mean_q: 0.542386\n",
      "done, took 93.803 seconds\n",
      "\n",
      "\n",
      "iteration: 193\n",
      "Training for 2000 steps ...\n",
      "  208/2000: episode: 1, duration: 9.124s, episode steps: 208, steps per second: 23, episode reward: 0.811, mean reward: 0.004 [-0.002, 0.013], mean action: -0.111 [-1.225, 1.201], mean observation: 0.117 [-24.793, 16.836], loss: --, mean_squared_error: --, mean_q: --\n",
      "  413/2000: episode: 2, duration: 8.920s, episode steps: 205, steps per second: 23, episode reward: 0.802, mean reward: 0.004 [-0.002, 0.013], mean action: -0.080 [-1.200, 1.375], mean observation: 0.115 [-24.246, 17.100], loss: --, mean_squared_error: --, mean_q: --\n",
      "  695/2000: episode: 3, duration: 11.926s, episode steps: 282, steps per second: 24, episode reward: 0.878, mean reward: 0.003 [-0.002, 0.015], mean action: -0.144 [-1.214, 1.279], mean observation: 0.128 [-24.262, 16.762], loss: --, mean_squared_error: --, mean_q: --\n",
      "  902/2000: episode: 4, duration: 9.121s, episode steps: 207, steps per second: 23, episode reward: 0.808, mean reward: 0.004 [-0.002, 0.013], mean action: -0.088 [-1.165, 1.261], mean observation: 0.114 [-24.437, 16.869], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1183/2000: episode: 5, duration: 13.606s, episode steps: 281, steps per second: 21, episode reward: 0.861, mean reward: 0.003 [-0.002, 0.015], mean action: -0.134 [-1.361, 1.379], mean observation: 0.127 [-24.481, 16.951], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.539711\n",
      " 1368/2000: episode: 6, duration: 9.521s, episode steps: 185, steps per second: 19, episode reward: 0.800, mean reward: 0.004 [-0.002, 0.014], mean action: -0.080 [-1.188, 1.215], mean observation: 0.111 [-22.490, 16.829], loss: 0.000071, mean_squared_error: 0.000143, mean_q: 0.539031\n",
      " 1570/2000: episode: 7, duration: 10.387s, episode steps: 202, steps per second: 19, episode reward: 0.818, mean reward: 0.004 [-0.002, 0.013], mean action: -0.112 [-1.253, 1.282], mean observation: 0.112 [-24.193, 16.815], loss: 0.000049, mean_squared_error: 0.000098, mean_q: 0.538773\n",
      " 1749/2000: episode: 8, duration: 9.197s, episode steps: 179, steps per second: 19, episode reward: 0.771, mean reward: 0.004 [-0.002, 0.013], mean action: -0.081 [-1.224, 1.178], mean observation: 0.104 [-24.436, 16.808], loss: 0.000053, mean_squared_error: 0.000107, mean_q: 0.537282\n",
      " 1942/2000: episode: 9, duration: 9.695s, episode steps: 193, steps per second: 20, episode reward: 0.822, mean reward: 0.004 [-0.002, 0.013], mean action: -0.138 [-1.241, 1.112], mean observation: 0.117 [-24.023, 16.780], loss: 0.000135, mean_squared_error: 0.000271, mean_q: 0.538721\n",
      "done, took 94.465 seconds\n",
      "\n",
      "\n",
      "iteration: 194\n",
      "Training for 2000 steps ...\n",
      "  181/2000: episode: 1, duration: 7.729s, episode steps: 181, steps per second: 23, episode reward: 0.800, mean reward: 0.004 [-0.001, 0.013], mean action: -0.063 [-1.289, 1.233], mean observation: 0.115 [-23.313, 17.058], loss: --, mean_squared_error: --, mean_q: --\n",
      "  355/2000: episode: 2, duration: 6.862s, episode steps: 174, steps per second: 25, episode reward: 0.758, mean reward: 0.004 [-0.001, 0.013], mean action: -0.009 [-1.191, 1.259], mean observation: 0.112 [-21.692, 16.613], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  533/2000: episode: 3, duration: 7.139s, episode steps: 178, steps per second: 25, episode reward: 0.773, mean reward: 0.004 [-0.001, 0.013], mean action: -0.043 [-1.285, 1.145], mean observation: 0.113 [-23.049, 17.048], loss: --, mean_squared_error: --, mean_q: --\n",
      "  707/2000: episode: 4, duration: 6.802s, episode steps: 174, steps per second: 26, episode reward: 0.748, mean reward: 0.004 [-0.001, 0.013], mean action: 0.030 [-1.082, 1.270], mean observation: 0.110 [-23.153, 17.075], loss: --, mean_squared_error: --, mean_q: --\n",
      "  878/2000: episode: 5, duration: 7.013s, episode steps: 171, steps per second: 24, episode reward: 0.747, mean reward: 0.004 [-0.001, 0.013], mean action: -0.015 [-1.221, 1.117], mean observation: 0.110 [-22.812, 16.975], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1054/2000: episode: 6, duration: 7.080s, episode steps: 176, steps per second: 25, episode reward: 0.731, mean reward: 0.004 [-0.001, 0.012], mean action: 0.014 [-1.258, 1.322], mean observation: 0.121 [-21.602, 16.680], loss: 0.000048, mean_squared_error: 0.000097, mean_q: 0.540037\n",
      " 1227/2000: episode: 7, duration: 8.016s, episode steps: 173, steps per second: 22, episode reward: 0.769, mean reward: 0.004 [-0.001, 0.013], mean action: -0.004 [-1.244, 1.323], mean observation: 0.124 [-22.724, 16.957], loss: 0.000039, mean_squared_error: 0.000078, mean_q: 0.529728\n",
      " 1355/2000: episode: 8, duration: 4.528s, episode steps: 128, steps per second: 28, episode reward: -0.756, mean reward: -0.006 [-0.021, 0.010], mean action: 0.014 [-1.209, 1.197], mean observation: 0.030 [-23.346, 17.181], loss: 0.000061, mean_squared_error: 0.000121, mean_q: 0.534874\n",
      " 1473/2000: episode: 9, duration: 4.550s, episode steps: 118, steps per second: 26, episode reward: -0.755, mean reward: -0.006 [-0.020, 0.010], mean action: 0.034 [-1.178, 1.162], mean observation: 0.025 [-22.407, 16.718], loss: 0.000038, mean_squared_error: 0.000076, mean_q: 0.530939\n",
      " 1613/2000: episode: 10, duration: 4.769s, episode steps: 140, steps per second: 29, episode reward: -0.797, mean reward: -0.006 [-0.020, 0.010], mean action: -0.013 [-1.178, 1.163], mean observation: 0.029 [-24.396, 16.617], loss: 0.000125, mean_squared_error: 0.000249, mean_q: 0.525955\n",
      " 1744/2000: episode: 11, duration: 4.933s, episode steps: 131, steps per second: 27, episode reward: -0.775, mean reward: -0.006 [-0.019, 0.010], mean action: 0.039 [-1.190, 1.261], mean observation: 0.024 [-24.502, 16.817], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.528382\n",
      " 1858/2000: episode: 12, duration: 4.846s, episode steps: 114, steps per second: 24, episode reward: -0.789, mean reward: -0.007 [-0.022, 0.010], mean action: 0.042 [-1.257, 1.133], mean observation: 0.012 [-23.781, 16.743], loss: 0.000252, mean_squared_error: 0.000504, mean_q: 0.529462\n",
      " 1986/2000: episode: 13, duration: 5.105s, episode steps: 128, steps per second: 25, episode reward: -0.800, mean reward: -0.006 [-0.020, 0.010], mean action: -0.039 [-1.113, 1.126], mean observation: 0.025 [-23.728, 17.069], loss: 0.000227, mean_squared_error: 0.000455, mean_q: 0.526446\n",
      "done, took 80.431 seconds\n",
      "\n",
      "\n",
      "iteration: 195\n",
      "Training for 2000 steps ...\n",
      "  123/2000: episode: 1, duration: 3.507s, episode steps: 123, steps per second: 35, episode reward: -0.749, mean reward: -0.006 [-0.020, 0.009], mean action: -0.119 [-1.231, 1.114], mean observation: 0.030 [-22.195, 16.675], loss: --, mean_squared_error: --, mean_q: --\n",
      "  248/2000: episode: 2, duration: 3.442s, episode steps: 125, steps per second: 36, episode reward: -0.742, mean reward: -0.006 [-0.020, 0.009], mean action: -0.133 [-1.184, 1.182], mean observation: 0.034 [-21.205, 16.451], loss: --, mean_squared_error: --, mean_q: --\n",
      "  373/2000: episode: 3, duration: 3.400s, episode steps: 125, steps per second: 37, episode reward: -0.747, mean reward: -0.006 [-0.020, 0.009], mean action: -0.093 [-1.155, 1.207], mean observation: 0.034 [-21.455, 16.586], loss: --, mean_squared_error: --, mean_q: --\n",
      "  496/2000: episode: 4, duration: 3.361s, episode steps: 123, steps per second: 37, episode reward: -0.753, mean reward: -0.006 [-0.020, 0.009], mean action: -0.102 [-1.253, 1.160], mean observation: 0.031 [-21.932, 16.776], loss: --, mean_squared_error: --, mean_q: --\n",
      "  619/2000: episode: 5, duration: 3.475s, episode steps: 123, steps per second: 35, episode reward: -0.748, mean reward: -0.006 [-0.020, 0.009], mean action: -0.111 [-1.192, 1.211], mean observation: 0.031 [-22.843, 16.939], loss: --, mean_squared_error: --, mean_q: --\n",
      "  742/2000: episode: 6, duration: 3.347s, episode steps: 123, steps per second: 37, episode reward: -0.746, mean reward: -0.006 [-0.020, 0.009], mean action: -0.105 [-1.183, 1.148], mean observation: 0.030 [-22.710, 16.995], loss: --, mean_squared_error: --, mean_q: --\n",
      "  866/2000: episode: 7, duration: 3.373s, episode steps: 124, steps per second: 37, episode reward: -0.747, mean reward: -0.006 [-0.020, 0.009], mean action: -0.096 [-1.145, 1.232], mean observation: 0.032 [-23.439, 17.161], loss: --, mean_squared_error: --, mean_q: --\n",
      "  989/2000: episode: 8, duration: 3.445s, episode steps: 123, steps per second: 36, episode reward: -0.747, mean reward: -0.006 [-0.020, 0.009], mean action: -0.107 [-1.123, 1.184], mean observation: 0.032 [-22.823, 16.985], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1114/2000: episode: 9, duration: 4.130s, episode steps: 125, steps per second: 30, episode reward: -0.736, mean reward: -0.006 [-0.019, 0.008], mean action: -0.146 [-1.190, 1.184], mean observation: 0.031 [-22.351, 16.760], loss: 0.000047, mean_squared_error: 0.000094, mean_q: 0.511031\n",
      " 1253/2000: episode: 10, duration: 4.530s, episode steps: 139, steps per second: 31, episode reward: -0.757, mean reward: -0.005 [-0.019, 0.010], mean action: -0.083 [-1.161, 1.188], mean observation: 0.037 [-22.514, 16.961], loss: 0.000048, mean_squared_error: 0.000096, mean_q: 0.518293\n",
      " 1402/2000: episode: 11, duration: 7.226s, episode steps: 149, steps per second: 21, episode reward: 0.518, mean reward: 0.003 [-0.002, 0.011], mean action: -0.039 [-1.245, 1.097], mean observation: 0.130 [-22.554, 16.918], loss: 0.000104, mean_squared_error: 0.000208, mean_q: 0.512980\n",
      " 1548/2000: episode: 12, duration: 7.235s, episode steps: 146, steps per second: 20, episode reward: 0.507, mean reward: 0.003 [-0.002, 0.010], mean action: -0.018 [-1.204, 1.291], mean observation: 0.130 [-23.713, 17.309], loss: 0.000160, mean_squared_error: 0.000319, mean_q: 0.512417\n",
      " 1838/2000: episode: 13, duration: 8.839s, episode steps: 290, steps per second: 33, episode reward: -0.809, mean reward: -0.003 [-0.019, 0.009], mean action: -0.137 [-1.284, 1.296], mean observation: 0.090 [-22.549, 16.852], loss: 0.000060, mean_squared_error: 0.000120, mean_q: 0.506539\n",
      " 1972/2000: episode: 14, duration: 6.395s, episode steps: 134, steps per second: 21, episode reward: 0.447, mean reward: 0.003 [-0.002, 0.012], mean action: -0.014 [-1.177, 1.165], mean observation: 0.124 [-22.271, 16.892], loss: 0.000088, mean_squared_error: 0.000175, mean_q: 0.509949\n",
      "done, took 67.490 seconds\n",
      "\n",
      "\n",
      "iteration: 196\n",
      "Training for 2000 steps ...\n",
      "  135/2000: episode: 1, duration: 5.086s, episode steps: 135, steps per second: 27, episode reward: 0.461, mean reward: 0.003 [-0.002, 0.012], mean action: -0.049 [-1.211, 1.214], mean observation: 0.122 [-23.482, 17.186], loss: --, mean_squared_error: --, mean_q: --\n",
      "  260/2000: episode: 2, duration: 4.512s, episode steps: 125, steps per second: 28, episode reward: 0.427, mean reward: 0.003 [-0.002, 0.012], mean action: -0.074 [-1.173, 1.101], mean observation: 0.122 [-22.394, 16.860], loss: --, mean_squared_error: --, mean_q: --\n",
      "  392/2000: episode: 3, duration: 4.875s, episode steps: 132, steps per second: 27, episode reward: 0.458, mean reward: 0.003 [-0.002, 0.012], mean action: -0.060 [-1.265, 1.133], mean observation: 0.122 [-22.101, 16.856], loss: --, mean_squared_error: --, mean_q: --\n",
      "  524/2000: episode: 4, duration: 4.820s, episode steps: 132, steps per second: 27, episode reward: 0.456, mean reward: 0.003 [-0.002, 0.012], mean action: -0.048 [-1.207, 1.175], mean observation: 0.123 [-21.839, 16.688], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  663/2000: episode: 5, duration: 5.439s, episode steps: 139, steps per second: 26, episode reward: 0.481, mean reward: 0.003 [-0.002, 0.011], mean action: -0.050 [-1.327, 1.244], mean observation: 0.122 [-22.107, 16.719], loss: --, mean_squared_error: --, mean_q: --\n",
      "  801/2000: episode: 6, duration: 5.353s, episode steps: 138, steps per second: 26, episode reward: 0.473, mean reward: 0.003 [-0.002, 0.012], mean action: -0.041 [-1.141, 1.199], mean observation: 0.122 [-23.025, 17.122], loss: --, mean_squared_error: --, mean_q: --\n",
      "  928/2000: episode: 7, duration: 4.579s, episode steps: 127, steps per second: 28, episode reward: 0.424, mean reward: 0.003 [-0.002, 0.012], mean action: -0.046 [-1.150, 1.209], mean observation: 0.122 [-21.966, 16.730], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1056/2000: episode: 8, duration: 5.157s, episode steps: 128, steps per second: 25, episode reward: 0.446, mean reward: 0.003 [-0.002, 0.012], mean action: -0.071 [-1.312, 1.200], mean observation: 0.123 [-22.427, 16.836], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.503230\n",
      " 1177/2000: episode: 9, duration: 5.601s, episode steps: 121, steps per second: 22, episode reward: 0.405, mean reward: 0.003 [-0.002, 0.011], mean action: -0.007 [-1.249, 1.246], mean observation: 0.123 [-21.359, 16.589], loss: 0.000226, mean_squared_error: 0.000452, mean_q: 0.510191\n",
      " 1293/2000: episode: 10, duration: 5.423s, episode steps: 116, steps per second: 21, episode reward: 0.405, mean reward: 0.003 [-0.002, 0.012], mean action: 0.009 [-1.201, 1.296], mean observation: 0.123 [-21.453, 16.602], loss: 0.000152, mean_squared_error: 0.000304, mean_q: 0.506389\n",
      " 1413/2000: episode: 11, duration: 5.722s, episode steps: 120, steps per second: 21, episode reward: 0.462, mean reward: 0.004 [-0.002, 0.012], mean action: -0.040 [-1.201, 1.084], mean observation: 0.120 [-22.566, 16.919], loss: 0.000049, mean_squared_error: 0.000098, mean_q: 0.503969\n",
      " 1533/2000: episode: 12, duration: 6.393s, episode steps: 120, steps per second: 19, episode reward: 0.474, mean reward: 0.004 [-0.001, 0.011], mean action: -0.021 [-1.170, 1.188], mean observation: 0.124 [-18.313, 16.263], loss: 0.000054, mean_squared_error: 0.000107, mean_q: 0.503108\n",
      " 1654/2000: episode: 13, duration: 6.353s, episode steps: 121, steps per second: 19, episode reward: 0.443, mean reward: 0.004 [-0.001, 0.012], mean action: -0.055 [-1.144, 1.174], mean observation: 0.120 [-15.526, 14.570], loss: 0.000106, mean_squared_error: 0.000212, mean_q: 0.496914\n",
      " 1772/2000: episode: 14, duration: 6.317s, episode steps: 118, steps per second: 19, episode reward: 0.438, mean reward: 0.004 [-0.001, 0.013], mean action: -0.030 [-1.142, 1.254], mean observation: 0.118 [-21.814, 14.867], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.498805\n",
      " 1895/2000: episode: 15, duration: 6.298s, episode steps: 123, steps per second: 20, episode reward: 0.459, mean reward: 0.004 [-0.002, 0.011], mean action: -0.095 [-1.225, 1.173], mean observation: 0.120 [-20.595, 14.771], loss: 0.000226, mean_squared_error: 0.000453, mean_q: 0.496333\n",
      "done, took 87.561 seconds\n",
      "\n",
      "\n",
      "iteration: 197\n",
      "Training for 2000 steps ...\n",
      "  123/2000: episode: 1, duration: 5.328s, episode steps: 123, steps per second: 23, episode reward: 0.454, mean reward: 0.004 [-0.002, 0.013], mean action: -0.044 [-1.127, 1.140], mean observation: 0.119 [-21.494, 14.828], loss: --, mean_squared_error: --, mean_q: --\n",
      "  245/2000: episode: 2, duration: 5.347s, episode steps: 122, steps per second: 23, episode reward: 0.448, mean reward: 0.004 [-0.002, 0.013], mean action: -0.021 [-1.165, 1.172], mean observation: 0.120 [-21.732, 14.878], loss: --, mean_squared_error: --, mean_q: --\n",
      "  368/2000: episode: 3, duration: 5.417s, episode steps: 123, steps per second: 23, episode reward: 0.455, mean reward: 0.004 [-0.002, 0.012], mean action: -0.022 [-1.211, 1.180], mean observation: 0.119 [-21.454, 14.936], loss: --, mean_squared_error: --, mean_q: --\n",
      "  491/2000: episode: 4, duration: 5.343s, episode steps: 123, steps per second: 23, episode reward: 0.446, mean reward: 0.004 [-0.002, 0.013], mean action: -0.033 [-1.328, 1.216], mean observation: 0.119 [-21.578, 14.985], loss: --, mean_squared_error: --, mean_q: --\n",
      "  612/2000: episode: 5, duration: 5.253s, episode steps: 121, steps per second: 23, episode reward: 0.445, mean reward: 0.004 [-0.002, 0.013], mean action: -0.037 [-1.170, 1.184], mean observation: 0.118 [-21.730, 14.803], loss: --, mean_squared_error: --, mean_q: --\n",
      "  733/2000: episode: 6, duration: 5.147s, episode steps: 121, steps per second: 24, episode reward: 0.437, mean reward: 0.004 [-0.002, 0.013], mean action: -0.007 [-1.227, 1.227], mean observation: 0.119 [-21.460, 15.016], loss: --, mean_squared_error: --, mean_q: --\n",
      "  848/2000: episode: 7, duration: 4.800s, episode steps: 115, steps per second: 24, episode reward: 0.392, mean reward: 0.003 [-0.002, 0.012], mean action: -0.041 [-1.296, 1.192], mean observation: 0.120 [-21.509, 14.672], loss: --, mean_squared_error: --, mean_q: --\n",
      "  966/2000: episode: 8, duration: 4.937s, episode steps: 118, steps per second: 24, episode reward: 0.398, mean reward: 0.003 [-0.002, 0.012], mean action: -0.007 [-1.135, 1.212], mean observation: 0.119 [-21.642, 14.669], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1087/2000: episode: 9, duration: 5.955s, episode steps: 121, steps per second: 20, episode reward: 0.437, mean reward: 0.004 [-0.002, 0.013], mean action: -0.007 [-1.196, 1.254], mean observation: 0.119 [-21.539, 14.828], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.501210\n",
      " 1203/2000: episode: 10, duration: 5.919s, episode steps: 116, steps per second: 20, episode reward: 0.465, mean reward: 0.004 [-0.002, 0.011], mean action: -0.003 [-1.260, 1.202], mean observation: 0.126 [-13.488, 14.820], loss: 0.000058, mean_squared_error: 0.000115, mean_q: 0.493989\n",
      " 1312/2000: episode: 11, duration: 5.566s, episode steps: 109, steps per second: 20, episode reward: 0.441, mean reward: 0.004 [-0.001, 0.013], mean action: -0.037 [-1.236, 1.205], mean observation: 0.117 [-15.556, 14.734], loss: 0.000092, mean_squared_error: 0.000183, mean_q: 0.495169\n",
      " 1420/2000: episode: 12, duration: 5.739s, episode steps: 108, steps per second: 19, episode reward: 0.388, mean reward: 0.004 [-0.001, 0.012], mean action: -0.004 [-1.243, 1.231], mean observation: 0.117 [-10.425, 14.878], loss: 0.000192, mean_squared_error: 0.000384, mean_q: 0.501719\n",
      " 1532/2000: episode: 13, duration: 5.644s, episode steps: 112, steps per second: 20, episode reward: 0.419, mean reward: 0.004 [-0.001, 0.012], mean action: -0.012 [-1.188, 1.244], mean observation: 0.123 [-12.320, 15.095], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.496320\n",
      " 1646/2000: episode: 14, duration: 5.505s, episode steps: 114, steps per second: 21, episode reward: 0.426, mean reward: 0.004 [-0.002, 0.011], mean action: -0.030 [-1.152, 1.216], mean observation: 0.122 [-13.089, 15.077], loss: 0.000092, mean_squared_error: 0.000185, mean_q: 0.509972\n",
      " 1758/2000: episode: 15, duration: 5.818s, episode steps: 112, steps per second: 19, episode reward: 0.455, mean reward: 0.004 [-0.002, 0.012], mean action: -0.028 [-1.221, 1.174], mean observation: 0.122 [-10.480, 15.208], loss: 0.000094, mean_squared_error: 0.000187, mean_q: 0.494062\n",
      " 1863/2000: episode: 16, duration: 5.377s, episode steps: 105, steps per second: 20, episode reward: 0.445, mean reward: 0.004 [-0.001, 0.011], mean action: -0.016 [-1.162, 1.214], mean observation: 0.123 [-10.434, 14.997], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.501992\n",
      " 1958/2000: episode: 17, duration: 4.272s, episode steps: 95, steps per second: 22, episode reward: 0.423, mean reward: 0.004 [-0.002, 0.012], mean action: -0.085 [-1.175, 1.194], mean observation: 0.127 [-10.408, 15.001], loss: 0.000145, mean_squared_error: 0.000290, mean_q: 0.506315\n",
      "done, took 93.789 seconds\n",
      "\n",
      "\n",
      "iteration: 198\n",
      "Training for 2000 steps ...\n",
      "  100/2000: episode: 1, duration: 3.632s, episode steps: 100, steps per second: 28, episode reward: 0.441, mean reward: 0.004 [-0.001, 0.011], mean action: -0.048 [-1.208, 1.163], mean observation: 0.127 [-10.391, 14.914], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  207/2000: episode: 2, duration: 4.211s, episode steps: 107, steps per second: 25, episode reward: 0.445, mean reward: 0.004 [-0.001, 0.013], mean action: -0.044 [-1.132, 1.156], mean observation: 0.125 [-10.389, 14.975], loss: --, mean_squared_error: --, mean_q: --\n",
      "  312/2000: episode: 3, duration: 3.941s, episode steps: 105, steps per second: 27, episode reward: 0.428, mean reward: 0.004 [-0.001, 0.013], mean action: -0.033 [-1.087, 1.104], mean observation: 0.126 [-10.479, 15.133], loss: --, mean_squared_error: --, mean_q: --\n",
      "  419/2000: episode: 4, duration: 4.251s, episode steps: 107, steps per second: 25, episode reward: 0.424, mean reward: 0.004 [-0.001, 0.012], mean action: -0.044 [-1.178, 1.171], mean observation: 0.125 [-10.304, 14.663], loss: --, mean_squared_error: --, mean_q: --\n",
      "  524/2000: episode: 5, duration: 4.090s, episode steps: 105, steps per second: 26, episode reward: 0.429, mean reward: 0.004 [-0.001, 0.013], mean action: -0.041 [-1.221, 1.202], mean observation: 0.124 [-10.329, 14.704], loss: --, mean_squared_error: --, mean_q: --\n",
      "  629/2000: episode: 6, duration: 4.257s, episode steps: 105, steps per second: 25, episode reward: 0.426, mean reward: 0.004 [-0.002, 0.013], mean action: -0.051 [-1.152, 1.266], mean observation: 0.127 [-10.404, 15.012], loss: --, mean_squared_error: --, mean_q: --\n",
      "  737/2000: episode: 7, duration: 4.361s, episode steps: 108, steps per second: 25, episode reward: 0.433, mean reward: 0.004 [-0.002, 0.013], mean action: -0.050 [-1.261, 1.155], mean observation: 0.124 [-10.359, 14.803], loss: --, mean_squared_error: --, mean_q: --\n",
      "  847/2000: episode: 8, duration: 4.471s, episode steps: 110, steps per second: 25, episode reward: 0.495, mean reward: 0.004 [-0.002, 0.013], mean action: -0.050 [-1.133, 1.258], mean observation: 0.125 [-10.434, 15.025], loss: --, mean_squared_error: --, mean_q: --\n",
      "  955/2000: episode: 9, duration: 4.345s, episode steps: 108, steps per second: 25, episode reward: 0.446, mean reward: 0.004 [-0.001, 0.013], mean action: -0.042 [-1.200, 1.209], mean observation: 0.124 [-10.329, 14.690], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1062/2000: episode: 10, duration: 4.857s, episode steps: 107, steps per second: 22, episode reward: 0.446, mean reward: 0.004 [-0.002, 0.013], mean action: -0.067 [-1.185, 1.160], mean observation: 0.125 [-10.432, 15.002], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.486831\n",
      " 1162/2000: episode: 11, duration: 4.720s, episode steps: 100, steps per second: 21, episode reward: 0.429, mean reward: 0.004 [-0.002, 0.012], mean action: -0.063 [-1.232, 1.194], mean observation: 0.126 [-10.231, 17.890], loss: 0.000201, mean_squared_error: 0.000401, mean_q: 0.493960\n",
      " 1262/2000: episode: 12, duration: 4.799s, episode steps: 100, steps per second: 21, episode reward: 0.407, mean reward: 0.004 [-0.002, 0.012], mean action: -0.135 [-1.176, 1.197], mean observation: 0.127 [-17.410, 16.972], loss: 0.000149, mean_squared_error: 0.000298, mean_q: 0.490505\n",
      " 1361/2000: episode: 13, duration: 4.590s, episode steps: 99, steps per second: 22, episode reward: 0.411, mean reward: 0.004 [-0.002, 0.012], mean action: -0.137 [-1.163, 1.158], mean observation: 0.127 [-16.664, 16.442], loss: 0.000209, mean_squared_error: 0.000419, mean_q: 0.497156\n",
      " 1465/2000: episode: 14, duration: 4.918s, episode steps: 104, steps per second: 21, episode reward: 0.414, mean reward: 0.004 [-0.002, 0.011], mean action: -0.181 [-1.141, 1.117], mean observation: 0.131 [-16.809, 16.330], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.499770\n",
      " 1568/2000: episode: 15, duration: 4.779s, episode steps: 103, steps per second: 22, episode reward: 0.466, mean reward: 0.005 [-0.002, 0.011], mean action: -0.189 [-1.154, 1.141], mean observation: 0.133 [-16.686, 15.052], loss: 0.000054, mean_squared_error: 0.000108, mean_q: 0.500115\n",
      " 1677/2000: episode: 16, duration: 4.843s, episode steps: 109, steps per second: 23, episode reward: 0.472, mean reward: 0.004 [-0.002, 0.011], mean action: -0.190 [-1.217, 1.184], mean observation: 0.133 [-16.322, 14.979], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.488364\n",
      " 1782/2000: episode: 17, duration: 4.474s, episode steps: 105, steps per second: 23, episode reward: 0.450, mean reward: 0.004 [-0.002, 0.011], mean action: -0.198 [-1.196, 1.118], mean observation: 0.139 [-16.794, 15.091], loss: 0.000053, mean_squared_error: 0.000107, mean_q: 0.488184\n",
      " 1887/2000: episode: 18, duration: 4.599s, episode steps: 105, steps per second: 23, episode reward: 0.443, mean reward: 0.004 [-0.002, 0.012], mean action: -0.189 [-1.149, 1.134], mean observation: 0.138 [-16.947, 17.281], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.499508\n",
      " 1991/2000: episode: 19, duration: 4.442s, episode steps: 104, steps per second: 23, episode reward: 0.444, mean reward: 0.004 [-0.002, 0.011], mean action: -0.203 [-1.250, 1.201], mean observation: 0.137 [-17.183, 16.692], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.485865\n",
      "done, took 85.269 seconds\n",
      "\n",
      "\n",
      "iteration: 199\n",
      "Training for 2000 steps ...\n",
      "  102/2000: episode: 1, duration: 3.393s, episode steps: 102, steps per second: 30, episode reward: 0.439, mean reward: 0.004 [-0.002, 0.011], mean action: -0.214 [-1.234, 1.228], mean observation: 0.137 [-16.497, 15.891], loss: --, mean_squared_error: --, mean_q: --\n",
      "  206/2000: episode: 2, duration: 3.464s, episode steps: 104, steps per second: 30, episode reward: 0.447, mean reward: 0.004 [-0.002, 0.011], mean action: -0.181 [-1.121, 1.186], mean observation: 0.137 [-16.384, 17.750], loss: --, mean_squared_error: --, mean_q: --\n",
      "  309/2000: episode: 3, duration: 3.502s, episode steps: 103, steps per second: 29, episode reward: 0.446, mean reward: 0.004 [-0.002, 0.011], mean action: -0.205 [-1.227, 1.155], mean observation: 0.137 [-16.667, 15.263], loss: --, mean_squared_error: --, mean_q: --\n",
      "  411/2000: episode: 4, duration: 3.450s, episode steps: 102, steps per second: 30, episode reward: 0.442, mean reward: 0.004 [-0.002, 0.011], mean action: -0.192 [-1.196, 1.144], mean observation: 0.136 [-17.035, 17.460], loss: --, mean_squared_error: --, mean_q: --\n",
      "  515/2000: episode: 5, duration: 3.604s, episode steps: 104, steps per second: 29, episode reward: 0.444, mean reward: 0.004 [-0.002, 0.011], mean action: -0.190 [-1.165, 1.146], mean observation: 0.136 [-16.583, 15.692], loss: --, mean_squared_error: --, mean_q: --\n",
      "  617/2000: episode: 6, duration: 3.357s, episode steps: 102, steps per second: 30, episode reward: 0.441, mean reward: 0.004 [-0.002, 0.011], mean action: -0.209 [-1.143, 1.118], mean observation: 0.138 [-16.884, 16.895], loss: --, mean_squared_error: --, mean_q: --\n",
      "  719/2000: episode: 7, duration: 3.379s, episode steps: 102, steps per second: 30, episode reward: 0.439, mean reward: 0.004 [-0.002, 0.011], mean action: -0.207 [-1.216, 1.133], mean observation: 0.136 [-16.693, 15.190], loss: --, mean_squared_error: --, mean_q: --\n",
      "  820/2000: episode: 8, duration: 3.408s, episode steps: 101, steps per second: 30, episode reward: 0.444, mean reward: 0.004 [-0.002, 0.011], mean action: -0.212 [-1.193, 1.094], mean observation: 0.136 [-16.808, 15.168], loss: --, mean_squared_error: --, mean_q: --\n",
      "  923/2000: episode: 9, duration: 3.446s, episode steps: 103, steps per second: 30, episode reward: 0.441, mean reward: 0.004 [-0.002, 0.011], mean action: -0.215 [-1.175, 1.133], mean observation: 0.137 [-17.182, 17.713], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1024/2000: episode: 10, duration: 3.583s, episode steps: 101, steps per second: 28, episode reward: 0.442, mean reward: 0.004 [-0.002, 0.011], mean action: -0.208 [-1.232, 1.143], mean observation: 0.138 [-16.898, 18.439], loss: 0.000197, mean_squared_error: 0.000395, mean_q: 0.482741\n",
      " 1127/2000: episode: 11, duration: 4.290s, episode steps: 103, steps per second: 24, episode reward: 0.445, mean reward: 0.004 [-0.002, 0.011], mean action: -0.214 [-1.227, 1.201], mean observation: 0.138 [-16.756, 15.092], loss: 0.000184, mean_squared_error: 0.000369, mean_q: 0.494699\n",
      " 1234/2000: episode: 12, duration: 4.510s, episode steps: 107, steps per second: 24, episode reward: 0.449, mean reward: 0.004 [-0.002, 0.011], mean action: -0.176 [-1.257, 1.256], mean observation: 0.137 [-16.371, 15.638], loss: 0.000044, mean_squared_error: 0.000089, mean_q: 0.491973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1339/2000: episode: 13, duration: 4.311s, episode steps: 105, steps per second: 24, episode reward: 0.439, mean reward: 0.004 [-0.002, 0.011], mean action: -0.183 [-1.197, 1.257], mean observation: 0.139 [-17.200, 15.333], loss: 0.000053, mean_squared_error: 0.000106, mean_q: 0.486861\n",
      " 1443/2000: episode: 14, duration: 4.471s, episode steps: 104, steps per second: 23, episode reward: 0.435, mean reward: 0.004 [-0.002, 0.010], mean action: -0.201 [-1.110, 1.113], mean observation: 0.138 [-16.928, 14.785], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.489129\n",
      " 1548/2000: episode: 15, duration: 4.458s, episode steps: 105, steps per second: 24, episode reward: 0.434, mean reward: 0.004 [-0.002, 0.011], mean action: -0.229 [-1.179, 1.072], mean observation: 0.138 [-16.461, 15.151], loss: 0.000058, mean_squared_error: 0.000117, mean_q: 0.487902\n",
      " 1648/2000: episode: 16, duration: 4.382s, episode steps: 100, steps per second: 23, episode reward: 0.426, mean reward: 0.004 [-0.002, 0.012], mean action: -0.194 [-1.120, 1.181], mean observation: 0.136 [-17.046, 15.240], loss: 0.000083, mean_squared_error: 0.000167, mean_q: 0.491447\n",
      " 1745/2000: episode: 17, duration: 4.196s, episode steps: 97, steps per second: 23, episode reward: 0.439, mean reward: 0.005 [-0.001, 0.011], mean action: -0.163 [-1.143, 1.145], mean observation: 0.139 [-16.360, 15.731], loss: 0.000159, mean_squared_error: 0.000318, mean_q: 0.486914\n",
      " 1844/2000: episode: 18, duration: 4.441s, episode steps: 99, steps per second: 22, episode reward: 0.448, mean reward: 0.005 [-0.001, 0.012], mean action: -0.181 [-1.229, 1.124], mean observation: 0.139 [-16.882, 16.914], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.482201\n",
      " 1947/2000: episode: 19, duration: 4.580s, episode steps: 103, steps per second: 22, episode reward: 0.448, mean reward: 0.004 [-0.001, 0.011], mean action: -0.141 [-1.271, 1.117], mean observation: 0.136 [-16.837, 15.273], loss: 0.000166, mean_squared_error: 0.000331, mean_q: 0.488716\n",
      "done, took 77.003 seconds\n",
      "\n",
      "\n",
      "iteration: 200\n",
      "Training for 2000 steps ...\n",
      "   97/2000: episode: 1, duration: 3.158s, episode steps: 97, steps per second: 31, episode reward: 0.434, mean reward: 0.004 [-0.002, 0.011], mean action: -0.217 [-1.136, 1.166], mean observation: 0.136 [-16.909, 14.973], loss: --, mean_squared_error: --, mean_q: --\n",
      "  194/2000: episode: 2, duration: 3.086s, episode steps: 97, steps per second: 31, episode reward: 0.433, mean reward: 0.004 [-0.002, 0.011], mean action: -0.232 [-1.206, 1.077], mean observation: 0.137 [-16.221, 14.784], loss: --, mean_squared_error: --, mean_q: --\n",
      "  292/2000: episode: 3, duration: 3.206s, episode steps: 98, steps per second: 31, episode reward: 0.442, mean reward: 0.005 [-0.002, 0.011], mean action: -0.202 [-1.220, 1.236], mean observation: 0.136 [-16.504, 14.755], loss: --, mean_squared_error: --, mean_q: --\n",
      "  390/2000: episode: 4, duration: 3.129s, episode steps: 98, steps per second: 31, episode reward: 0.443, mean reward: 0.005 [-0.002, 0.011], mean action: -0.218 [-1.179, 1.118], mean observation: 0.137 [-16.462, 15.414], loss: --, mean_squared_error: --, mean_q: --\n",
      "  488/2000: episode: 5, duration: 3.179s, episode steps: 98, steps per second: 31, episode reward: 0.436, mean reward: 0.004 [-0.002, 0.011], mean action: -0.190 [-1.177, 1.131], mean observation: 0.137 [-16.663, 15.174], loss: --, mean_squared_error: --, mean_q: --\n",
      "  585/2000: episode: 6, duration: 3.091s, episode steps: 97, steps per second: 31, episode reward: 0.436, mean reward: 0.004 [-0.002, 0.011], mean action: -0.221 [-1.172, 1.177], mean observation: 0.136 [-16.730, 15.006], loss: --, mean_squared_error: --, mean_q: --\n",
      "  683/2000: episode: 7, duration: 3.228s, episode steps: 98, steps per second: 30, episode reward: 0.438, mean reward: 0.004 [-0.002, 0.011], mean action: -0.210 [-1.240, 1.232], mean observation: 0.136 [-16.461, 15.202], loss: --, mean_squared_error: --, mean_q: --\n",
      "  782/2000: episode: 8, duration: 3.227s, episode steps: 99, steps per second: 31, episode reward: 0.444, mean reward: 0.004 [-0.002, 0.011], mean action: -0.207 [-1.195, 1.187], mean observation: 0.137 [-16.566, 15.050], loss: --, mean_squared_error: --, mean_q: --\n",
      "  879/2000: episode: 9, duration: 3.053s, episode steps: 97, steps per second: 32, episode reward: 0.437, mean reward: 0.005 [-0.002, 0.011], mean action: -0.189 [-1.117, 1.111], mean observation: 0.137 [-16.676, 15.252], loss: --, mean_squared_error: --, mean_q: --\n",
      "  977/2000: episode: 10, duration: 3.248s, episode steps: 98, steps per second: 30, episode reward: 0.437, mean reward: 0.004 [-0.002, 0.011], mean action: -0.205 [-1.166, 1.167], mean observation: 0.137 [-16.858, 14.639], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1079/2000: episode: 11, duration: 4.193s, episode steps: 102, steps per second: 24, episode reward: 0.453, mean reward: 0.004 [-0.002, 0.011], mean action: -0.237 [-1.164, 1.113], mean observation: 0.137 [-16.498, 14.504], loss: 0.000063, mean_squared_error: 0.000126, mean_q: 0.495743\n",
      " 1179/2000: episode: 12, duration: 4.298s, episode steps: 100, steps per second: 23, episode reward: 0.441, mean reward: 0.004 [-0.001, 0.012], mean action: -0.258 [-1.185, 1.215], mean observation: 0.134 [-16.646, 15.226], loss: 0.000053, mean_squared_error: 0.000105, mean_q: 0.489151\n",
      " 1278/2000: episode: 13, duration: 4.208s, episode steps: 99, steps per second: 24, episode reward: 0.438, mean reward: 0.004 [-0.001, 0.011], mean action: -0.264 [-1.222, 1.165], mean observation: 0.132 [-17.194, 14.736], loss: 0.000070, mean_squared_error: 0.000140, mean_q: 0.489926\n",
      " 1375/2000: episode: 14, duration: 3.973s, episode steps: 97, steps per second: 24, episode reward: 0.432, mean reward: 0.004 [-0.001, 0.011], mean action: -0.236 [-1.155, 1.123], mean observation: 0.135 [-14.329, 14.737], loss: 0.000105, mean_squared_error: 0.000210, mean_q: 0.484387\n",
      " 1474/2000: episode: 15, duration: 4.170s, episode steps: 99, steps per second: 24, episode reward: 0.438, mean reward: 0.004 [-0.002, 0.011], mean action: -0.232 [-1.182, 1.117], mean observation: 0.136 [-15.995, 15.008], loss: 0.000112, mean_squared_error: 0.000223, mean_q: 0.488658\n",
      " 1574/2000: episode: 16, duration: 3.991s, episode steps: 100, steps per second: 25, episode reward: 0.440, mean reward: 0.004 [-0.002, 0.011], mean action: -0.223 [-1.194, 1.117], mean observation: 0.135 [-16.869, 14.888], loss: 0.000145, mean_squared_error: 0.000289, mean_q: 0.488088\n",
      " 1679/2000: episode: 17, duration: 4.036s, episode steps: 105, steps per second: 26, episode reward: 0.440, mean reward: 0.004 [-0.002, 0.011], mean action: -0.217 [-1.122, 1.174], mean observation: 0.135 [-15.849, 15.132], loss: 0.000136, mean_squared_error: 0.000271, mean_q: 0.486206\n",
      " 1776/2000: episode: 18, duration: 3.855s, episode steps: 97, steps per second: 25, episode reward: 0.425, mean reward: 0.004 [-0.002, 0.011], mean action: -0.235 [-1.179, 1.146], mean observation: 0.134 [-16.531, 14.684], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.479595\n",
      " 1873/2000: episode: 19, duration: 3.876s, episode steps: 97, steps per second: 25, episode reward: 0.431, mean reward: 0.004 [-0.002, 0.012], mean action: -0.212 [-1.105, 1.108], mean observation: 0.135 [-16.701, 14.760], loss: 0.000151, mean_squared_error: 0.000302, mean_q: 0.483029\n",
      " 1969/2000: episode: 20, duration: 3.911s, episode steps: 96, steps per second: 25, episode reward: 0.430, mean reward: 0.004 [-0.002, 0.012], mean action: -0.241 [-1.237, 1.119], mean observation: 0.135 [-16.590, 15.118], loss: 0.000082, mean_squared_error: 0.000163, mean_q: 0.487571\n",
      "done, took 73.854 seconds\n",
      "\n",
      "\n",
      "iteration: 201\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 3.038s, episode steps: 95, steps per second: 31, episode reward: 0.421, mean reward: 0.004 [-0.002, 0.012], mean action: -0.227 [-1.163, 1.118], mean observation: 0.135 [-16.341, 14.753], loss: --, mean_squared_error: --, mean_q: --\n",
      "  191/2000: episode: 2, duration: 2.972s, episode steps: 96, steps per second: 32, episode reward: 0.427, mean reward: 0.004 [-0.002, 0.012], mean action: -0.243 [-1.210, 1.189], mean observation: 0.135 [-16.978, 14.498], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  287/2000: episode: 3, duration: 3.068s, episode steps: 96, steps per second: 31, episode reward: 0.423, mean reward: 0.004 [-0.002, 0.012], mean action: -0.237 [-1.152, 1.095], mean observation: 0.135 [-16.704, 15.105], loss: --, mean_squared_error: --, mean_q: --\n",
      "  382/2000: episode: 4, duration: 3.042s, episode steps: 95, steps per second: 31, episode reward: 0.423, mean reward: 0.004 [-0.002, 0.012], mean action: -0.219 [-1.174, 1.180], mean observation: 0.134 [-16.489, 14.851], loss: --, mean_squared_error: --, mean_q: --\n",
      "  477/2000: episode: 5, duration: 3.076s, episode steps: 95, steps per second: 31, episode reward: 0.425, mean reward: 0.004 [-0.002, 0.012], mean action: -0.234 [-1.151, 1.221], mean observation: 0.134 [-16.796, 14.765], loss: --, mean_squared_error: --, mean_q: --\n",
      "  573/2000: episode: 6, duration: 2.957s, episode steps: 96, steps per second: 32, episode reward: 0.429, mean reward: 0.004 [-0.002, 0.012], mean action: -0.243 [-1.140, 1.134], mean observation: 0.134 [-16.885, 14.903], loss: --, mean_squared_error: --, mean_q: --\n",
      "  669/2000: episode: 7, duration: 2.945s, episode steps: 96, steps per second: 33, episode reward: 0.426, mean reward: 0.004 [-0.002, 0.011], mean action: -0.228 [-1.122, 1.184], mean observation: 0.134 [-16.506, 15.028], loss: --, mean_squared_error: --, mean_q: --\n",
      "  766/2000: episode: 8, duration: 3.026s, episode steps: 97, steps per second: 32, episode reward: 0.434, mean reward: 0.004 [-0.002, 0.012], mean action: -0.223 [-1.172, 1.217], mean observation: 0.135 [-16.170, 15.201], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 9, duration: 2.945s, episode steps: 96, steps per second: 33, episode reward: 0.427, mean reward: 0.004 [-0.002, 0.012], mean action: -0.230 [-1.217, 1.155], mean observation: 0.135 [-16.815, 14.844], loss: --, mean_squared_error: --, mean_q: --\n",
      "  958/2000: episode: 10, duration: 3.021s, episode steps: 96, steps per second: 32, episode reward: 0.428, mean reward: 0.004 [-0.002, 0.012], mean action: -0.230 [-1.162, 1.156], mean observation: 0.134 [-16.537, 15.004], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1054/2000: episode: 11, duration: 3.565s, episode steps: 96, steps per second: 27, episode reward: 0.428, mean reward: 0.004 [-0.002, 0.012], mean action: -0.231 [-1.114, 1.115], mean observation: 0.135 [-16.616, 14.835], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.483757\n",
      " 1151/2000: episode: 12, duration: 3.798s, episode steps: 97, steps per second: 26, episode reward: 0.431, mean reward: 0.004 [-0.002, 0.012], mean action: -0.269 [-1.194, 1.242], mean observation: 0.134 [-17.416, 15.233], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.481999\n",
      " 1249/2000: episode: 13, duration: 3.884s, episode steps: 98, steps per second: 25, episode reward: 0.431, mean reward: 0.004 [-0.002, 0.012], mean action: -0.266 [-1.114, 1.097], mean observation: 0.134 [-16.819, 15.260], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.477420\n",
      " 1348/2000: episode: 14, duration: 3.816s, episode steps: 99, steps per second: 26, episode reward: 0.439, mean reward: 0.004 [-0.002, 0.012], mean action: -0.295 [-1.168, 1.101], mean observation: 0.132 [-17.013, 15.134], loss: 0.000125, mean_squared_error: 0.000251, mean_q: 0.485926\n",
      " 1447/2000: episode: 15, duration: 3.795s, episode steps: 99, steps per second: 26, episode reward: 0.439, mean reward: 0.004 [-0.002, 0.012], mean action: -0.289 [-1.145, 1.127], mean observation: 0.132 [-16.168, 15.048], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.477323\n",
      " 1549/2000: episode: 16, duration: 3.994s, episode steps: 102, steps per second: 26, episode reward: 0.450, mean reward: 0.004 [-0.002, 0.012], mean action: -0.283 [-1.137, 1.218], mean observation: 0.132 [-17.157, 14.870], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.481988\n",
      " 1650/2000: episode: 17, duration: 3.999s, episode steps: 101, steps per second: 25, episode reward: 0.442, mean reward: 0.004 [-0.002, 0.011], mean action: -0.300 [-1.186, 1.072], mean observation: 0.132 [-16.095, 14.636], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.481782\n",
      " 1749/2000: episode: 18, duration: 3.771s, episode steps: 99, steps per second: 26, episode reward: 0.432, mean reward: 0.004 [-0.002, 0.012], mean action: -0.301 [-1.154, 1.204], mean observation: 0.132 [-16.960, 14.744], loss: 0.000131, mean_squared_error: 0.000262, mean_q: 0.492482\n",
      " 1850/2000: episode: 19, duration: 3.923s, episode steps: 101, steps per second: 26, episode reward: 0.451, mean reward: 0.004 [-0.002, 0.011], mean action: -0.310 [-1.143, 1.153], mean observation: 0.132 [-17.104, 14.987], loss: 0.000074, mean_squared_error: 0.000147, mean_q: 0.481037\n",
      " 1941/2000: episode: 20, duration: 3.528s, episode steps: 91, steps per second: 26, episode reward: 0.422, mean reward: 0.005 [-0.001, 0.012], mean action: -0.298 [-1.185, 1.209], mean observation: 0.132 [-16.863, 15.081], loss: 0.000084, mean_squared_error: 0.000167, mean_q: 0.472842\n",
      "done, took 71.182 seconds\n",
      "\n",
      "\n",
      "iteration: 202\n",
      "Training for 2000 steps ...\n",
      "   96/2000: episode: 1, duration: 2.982s, episode steps: 96, steps per second: 32, episode reward: 0.431, mean reward: 0.004 [-0.002, 0.012], mean action: -0.298 [-1.158, 1.152], mean observation: 0.131 [-16.875, 14.600], loss: --, mean_squared_error: --, mean_q: --\n",
      "  193/2000: episode: 2, duration: 3.039s, episode steps: 97, steps per second: 32, episode reward: 0.425, mean reward: 0.004 [-0.002, 0.012], mean action: -0.302 [-1.186, 1.191], mean observation: 0.131 [-16.729, 14.514], loss: --, mean_squared_error: --, mean_q: --\n",
      "  293/2000: episode: 3, duration: 3.212s, episode steps: 100, steps per second: 31, episode reward: 0.444, mean reward: 0.004 [-0.002, 0.012], mean action: -0.296 [-1.264, 1.251], mean observation: 0.131 [-16.405, 14.953], loss: --, mean_squared_error: --, mean_q: --\n",
      "  393/2000: episode: 4, duration: 3.244s, episode steps: 100, steps per second: 31, episode reward: 0.443, mean reward: 0.004 [-0.002, 0.012], mean action: -0.288 [-1.139, 1.235], mean observation: 0.131 [-16.205, 15.124], loss: --, mean_squared_error: --, mean_q: --\n",
      "  494/2000: episode: 5, duration: 3.147s, episode steps: 101, steps per second: 32, episode reward: 0.438, mean reward: 0.004 [-0.002, 0.012], mean action: -0.303 [-1.157, 1.186], mean observation: 0.133 [-17.617, 14.816], loss: --, mean_squared_error: --, mean_q: --\n",
      "  593/2000: episode: 6, duration: 3.162s, episode steps: 99, steps per second: 31, episode reward: 0.435, mean reward: 0.004 [-0.002, 0.012], mean action: -0.291 [-1.174, 1.178], mean observation: 0.131 [-16.760, 14.745], loss: --, mean_squared_error: --, mean_q: --\n",
      "  693/2000: episode: 7, duration: 3.171s, episode steps: 100, steps per second: 32, episode reward: 0.441, mean reward: 0.004 [-0.002, 0.012], mean action: -0.299 [-1.172, 1.161], mean observation: 0.132 [-16.881, 15.102], loss: --, mean_squared_error: --, mean_q: --\n",
      "  792/2000: episode: 8, duration: 3.072s, episode steps: 99, steps per second: 32, episode reward: 0.442, mean reward: 0.004 [-0.002, 0.012], mean action: -0.333 [-1.239, 1.150], mean observation: 0.131 [-16.821, 15.093], loss: --, mean_squared_error: --, mean_q: --\n",
      "  891/2000: episode: 9, duration: 3.060s, episode steps: 99, steps per second: 32, episode reward: 0.440, mean reward: 0.004 [-0.002, 0.012], mean action: -0.297 [-1.148, 1.144], mean observation: 0.131 [-16.304, 14.748], loss: --, mean_squared_error: --, mean_q: --\n",
      "  991/2000: episode: 10, duration: 3.182s, episode steps: 100, steps per second: 31, episode reward: 0.445, mean reward: 0.004 [-0.002, 0.012], mean action: -0.304 [-1.213, 1.246], mean observation: 0.131 [-16.706, 15.101], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1087/2000: episode: 11, duration: 3.741s, episode steps: 96, steps per second: 26, episode reward: 0.426, mean reward: 0.004 [-0.002, 0.012], mean action: -0.303 [-1.191, 1.135], mean observation: 0.131 [-16.974, 15.288], loss: 0.000045, mean_squared_error: 0.000090, mean_q: 0.474105\n",
      " 1182/2000: episode: 12, duration: 3.908s, episode steps: 95, steps per second: 24, episode reward: 0.430, mean reward: 0.005 [-0.002, 0.012], mean action: -0.293 [-1.213, 1.176], mean observation: 0.131 [-17.072, 14.735], loss: 0.000080, mean_squared_error: 0.000161, mean_q: 0.473337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1280/2000: episode: 13, duration: 4.096s, episode steps: 98, steps per second: 24, episode reward: 0.435, mean reward: 0.004 [-0.002, 0.012], mean action: -0.292 [-1.129, 1.148], mean observation: 0.131 [-16.302, 15.094], loss: 0.000084, mean_squared_error: 0.000167, mean_q: 0.476662\n",
      " 1378/2000: episode: 14, duration: 3.838s, episode steps: 98, steps per second: 26, episode reward: 0.438, mean reward: 0.004 [-0.002, 0.012], mean action: -0.318 [-1.197, 1.107], mean observation: 0.131 [-17.004, 14.742], loss: 0.000138, mean_squared_error: 0.000277, mean_q: 0.475024\n",
      " 1480/2000: episode: 15, duration: 3.939s, episode steps: 102, steps per second: 26, episode reward: 0.436, mean reward: 0.004 [-0.002, 0.011], mean action: -0.304 [-1.158, 1.150], mean observation: 0.132 [-16.185, 14.827], loss: 0.000052, mean_squared_error: 0.000105, mean_q: 0.470479\n",
      " 1581/2000: episode: 16, duration: 3.878s, episode steps: 101, steps per second: 26, episode reward: 0.434, mean reward: 0.004 [-0.002, 0.012], mean action: -0.311 [-1.166, 1.143], mean observation: 0.134 [-16.880, 14.882], loss: 0.000047, mean_squared_error: 0.000093, mean_q: 0.475182\n",
      " 1678/2000: episode: 17, duration: 3.853s, episode steps: 97, steps per second: 25, episode reward: 0.423, mean reward: 0.004 [-0.002, 0.012], mean action: -0.270 [-1.219, 1.159], mean observation: 0.134 [-16.464, 14.908], loss: 0.000053, mean_squared_error: 0.000106, mean_q: 0.473510\n",
      " 1774/2000: episode: 18, duration: 3.816s, episode steps: 96, steps per second: 25, episode reward: 0.428, mean reward: 0.004 [-0.002, 0.012], mean action: -0.286 [-1.179, 1.185], mean observation: 0.134 [-17.264, 14.622], loss: 0.000094, mean_squared_error: 0.000189, mean_q: 0.472325\n",
      " 1870/2000: episode: 19, duration: 3.797s, episode steps: 96, steps per second: 25, episode reward: 0.430, mean reward: 0.004 [-0.002, 0.011], mean action: -0.293 [-1.184, 1.142], mean observation: 0.134 [-16.936, 14.612], loss: 0.000131, mean_squared_error: 0.000261, mean_q: 0.473716\n",
      " 1965/2000: episode: 20, duration: 3.828s, episode steps: 95, steps per second: 25, episode reward: 0.427, mean reward: 0.004 [-0.002, 0.011], mean action: -0.283 [-1.135, 1.141], mean observation: 0.135 [-16.301, 14.602], loss: 0.000110, mean_squared_error: 0.000220, mean_q: 0.472190\n",
      "done, took 71.863 seconds\n",
      "\n",
      "\n",
      "iteration: 203\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 3.024s, episode steps: 95, steps per second: 31, episode reward: 0.424, mean reward: 0.004 [-0.002, 0.012], mean action: -0.280 [-1.195, 1.133], mean observation: 0.135 [-16.694, 14.670], loss: --, mean_squared_error: --, mean_q: --\n",
      "  191/2000: episode: 2, duration: 2.985s, episode steps: 96, steps per second: 32, episode reward: 0.432, mean reward: 0.005 [-0.002, 0.012], mean action: -0.293 [-1.230, 1.098], mean observation: 0.135 [-16.772, 14.526], loss: --, mean_squared_error: --, mean_q: --\n",
      "  289/2000: episode: 3, duration: 2.980s, episode steps: 98, steps per second: 33, episode reward: 0.434, mean reward: 0.004 [-0.002, 0.012], mean action: -0.266 [-1.162, 1.182], mean observation: 0.135 [-16.929, 14.982], loss: --, mean_squared_error: --, mean_q: --\n",
      "  385/2000: episode: 4, duration: 3.062s, episode steps: 96, steps per second: 31, episode reward: 0.427, mean reward: 0.004 [-0.002, 0.011], mean action: -0.241 [-1.185, 1.208], mean observation: 0.135 [-16.180, 14.940], loss: --, mean_squared_error: --, mean_q: --\n",
      "  481/2000: episode: 5, duration: 2.999s, episode steps: 96, steps per second: 32, episode reward: 0.424, mean reward: 0.004 [-0.002, 0.012], mean action: -0.283 [-1.184, 1.091], mean observation: 0.135 [-16.791, 14.625], loss: --, mean_squared_error: --, mean_q: --\n",
      "  578/2000: episode: 6, duration: 2.997s, episode steps: 97, steps per second: 32, episode reward: 0.433, mean reward: 0.004 [-0.002, 0.012], mean action: -0.268 [-1.187, 1.150], mean observation: 0.135 [-16.833, 15.232], loss: --, mean_squared_error: --, mean_q: --\n",
      "  675/2000: episode: 7, duration: 3.027s, episode steps: 97, steps per second: 32, episode reward: 0.434, mean reward: 0.004 [-0.002, 0.012], mean action: -0.275 [-1.187, 1.137], mean observation: 0.135 [-16.666, 14.949], loss: --, mean_squared_error: --, mean_q: --\n",
      "  771/2000: episode: 8, duration: 3.025s, episode steps: 96, steps per second: 32, episode reward: 0.425, mean reward: 0.004 [-0.002, 0.012], mean action: -0.267 [-1.215, 1.129], mean observation: 0.135 [-16.691, 14.846], loss: --, mean_squared_error: --, mean_q: --\n",
      "  867/2000: episode: 9, duration: 2.990s, episode steps: 96, steps per second: 32, episode reward: 0.426, mean reward: 0.004 [-0.002, 0.011], mean action: -0.283 [-1.277, 1.119], mean observation: 0.135 [-16.508, 15.004], loss: --, mean_squared_error: --, mean_q: --\n",
      "  966/2000: episode: 10, duration: 3.114s, episode steps: 99, steps per second: 32, episode reward: 0.441, mean reward: 0.004 [-0.002, 0.012], mean action: -0.288 [-1.174, 1.151], mean observation: 0.136 [-16.820, 14.789], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1063/2000: episode: 11, duration: 3.559s, episode steps: 97, steps per second: 27, episode reward: 0.432, mean reward: 0.004 [-0.002, 0.012], mean action: -0.301 [-1.298, 1.159], mean observation: 0.135 [-16.692, 14.881], loss: 0.000295, mean_squared_error: 0.000590, mean_q: 0.459998\n",
      " 1158/2000: episode: 12, duration: 3.798s, episode steps: 95, steps per second: 25, episode reward: 0.431, mean reward: 0.005 [-0.002, 0.011], mean action: -0.247 [-1.087, 1.134], mean observation: 0.136 [-16.011, 14.948], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.468929\n",
      " 1252/2000: episode: 13, duration: 3.833s, episode steps: 94, steps per second: 25, episode reward: 0.428, mean reward: 0.005 [-0.002, 0.011], mean action: -0.329 [-1.203, 1.195], mean observation: 0.136 [-16.052, 14.840], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.466375\n",
      " 1347/2000: episode: 14, duration: 3.850s, episode steps: 95, steps per second: 25, episode reward: 0.433, mean reward: 0.005 [-0.002, 0.012], mean action: -0.323 [-1.139, 1.117], mean observation: 0.135 [-16.453, 14.901], loss: 0.000045, mean_squared_error: 0.000090, mean_q: 0.468113\n",
      " 1442/2000: episode: 15, duration: 3.731s, episode steps: 95, steps per second: 25, episode reward: 0.429, mean reward: 0.005 [-0.002, 0.011], mean action: -0.302 [-1.280, 1.129], mean observation: 0.136 [-18.872, 14.936], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.464848\n",
      " 1540/2000: episode: 16, duration: 4.091s, episode steps: 98, steps per second: 24, episode reward: 0.439, mean reward: 0.004 [-0.002, 0.011], mean action: -0.294 [-1.181, 1.143], mean observation: 0.137 [-16.626, 17.815], loss: 0.000133, mean_squared_error: 0.000265, mean_q: 0.466179\n",
      " 1632/2000: episode: 17, duration: 3.738s, episode steps: 92, steps per second: 25, episode reward: 0.432, mean reward: 0.005 [-0.002, 0.011], mean action: -0.275 [-1.186, 1.233], mean observation: 0.136 [-16.097, 15.009], loss: 0.000055, mean_squared_error: 0.000111, mean_q: 0.471483\n",
      " 1727/2000: episode: 18, duration: 3.850s, episode steps: 95, steps per second: 25, episode reward: 0.443, mean reward: 0.005 [-0.001, 0.011], mean action: -0.290 [-1.173, 1.135], mean observation: 0.139 [-16.732, 14.947], loss: 0.000151, mean_squared_error: 0.000302, mean_q: 0.451517\n",
      " 1823/2000: episode: 19, duration: 3.914s, episode steps: 96, steps per second: 25, episode reward: 0.445, mean reward: 0.005 [-0.002, 0.011], mean action: -0.201 [-1.137, 1.184], mean observation: 0.139 [-16.776, 14.893], loss: 0.000187, mean_squared_error: 0.000374, mean_q: 0.471308\n",
      " 1919/2000: episode: 20, duration: 3.861s, episode steps: 96, steps per second: 25, episode reward: 0.452, mean reward: 0.005 [-0.002, 0.011], mean action: -0.247 [-1.170, 1.134], mean observation: 0.140 [-16.850, 14.961], loss: 0.000105, mean_squared_error: 0.000209, mean_q: 0.466145\n",
      "done, took 71.840 seconds\n",
      "\n",
      "\n",
      "iteration: 204\n",
      "Training for 2000 steps ...\n",
      "   89/2000: episode: 1, duration: 2.857s, episode steps: 89, steps per second: 31, episode reward: 0.431, mean reward: 0.005 [-0.002, 0.012], mean action: -0.200 [-1.129, 1.143], mean observation: 0.141 [-10.488, 15.315], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  177/2000: episode: 2, duration: 2.810s, episode steps: 88, steps per second: 31, episode reward: 0.428, mean reward: 0.005 [-0.002, 0.012], mean action: -0.223 [-1.127, 1.085], mean observation: 0.139 [-10.110, 14.183], loss: --, mean_squared_error: --, mean_q: --\n",
      "  266/2000: episode: 3, duration: 2.844s, episode steps: 89, steps per second: 31, episode reward: 0.435, mean reward: 0.005 [-0.002, 0.012], mean action: -0.218 [-1.210, 1.110], mean observation: 0.139 [-10.328, 14.829], loss: --, mean_squared_error: --, mean_q: --\n",
      "  356/2000: episode: 4, duration: 2.854s, episode steps: 90, steps per second: 32, episode reward: 0.438, mean reward: 0.005 [-0.002, 0.012], mean action: -0.213 [-1.182, 1.108], mean observation: 0.140 [-10.448, 15.131], loss: --, mean_squared_error: --, mean_q: --\n",
      "  446/2000: episode: 5, duration: 2.869s, episode steps: 90, steps per second: 31, episode reward: 0.437, mean reward: 0.005 [-0.002, 0.012], mean action: -0.210 [-1.180, 1.110], mean observation: 0.140 [-10.310, 14.743], loss: --, mean_squared_error: --, mean_q: --\n",
      "  536/2000: episode: 6, duration: 2.835s, episode steps: 90, steps per second: 32, episode reward: 0.436, mean reward: 0.005 [-0.002, 0.011], mean action: -0.217 [-1.237, 1.147], mean observation: 0.141 [-10.382, 15.032], loss: --, mean_squared_error: --, mean_q: --\n",
      "  624/2000: episode: 7, duration: 2.776s, episode steps: 88, steps per second: 32, episode reward: 0.426, mean reward: 0.005 [-0.002, 0.012], mean action: -0.202 [-1.124, 1.157], mean observation: 0.139 [-10.235, 14.377], loss: --, mean_squared_error: --, mean_q: --\n",
      "  713/2000: episode: 8, duration: 2.786s, episode steps: 89, steps per second: 32, episode reward: 0.434, mean reward: 0.005 [-0.002, 0.012], mean action: -0.222 [-1.189, 1.115], mean observation: 0.139 [-10.328, 14.878], loss: --, mean_squared_error: --, mean_q: --\n",
      "  802/2000: episode: 9, duration: 2.770s, episode steps: 89, steps per second: 32, episode reward: 0.433, mean reward: 0.005 [-0.002, 0.012], mean action: -0.214 [-1.263, 1.187], mean observation: 0.140 [-10.323, 14.827], loss: --, mean_squared_error: --, mean_q: --\n",
      "  891/2000: episode: 10, duration: 2.777s, episode steps: 89, steps per second: 32, episode reward: 0.432, mean reward: 0.005 [-0.002, 0.012], mean action: -0.190 [-1.117, 1.132], mean observation: 0.140 [-10.300, 14.749], loss: --, mean_squared_error: --, mean_q: --\n",
      "  981/2000: episode: 11, duration: 2.782s, episode steps: 90, steps per second: 32, episode reward: 0.438, mean reward: 0.005 [-0.002, 0.012], mean action: -0.210 [-1.125, 1.082], mean observation: 0.139 [-10.386, 14.992], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1070/2000: episode: 12, duration: 3.426s, episode steps: 89, steps per second: 26, episode reward: 0.434, mean reward: 0.005 [-0.002, 0.011], mean action: -0.202 [-1.150, 1.160], mean observation: 0.141 [-10.279, 14.788], loss: 0.000042, mean_squared_error: 0.000085, mean_q: 0.457420\n",
      " 1159/2000: episode: 13, duration: 3.715s, episode steps: 89, steps per second: 24, episode reward: 0.436, mean reward: 0.005 [-0.002, 0.011], mean action: -0.187 [-1.100, 1.150], mean observation: 0.140 [-10.295, 14.681], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.472807\n",
      " 1250/2000: episode: 14, duration: 3.694s, episode steps: 91, steps per second: 25, episode reward: 0.436, mean reward: 0.005 [-0.002, 0.011], mean action: -0.211 [-1.134, 1.147], mean observation: 0.140 [-10.325, 14.833], loss: 0.000072, mean_squared_error: 0.000143, mean_q: 0.463604\n",
      " 1340/2000: episode: 15, duration: 3.575s, episode steps: 90, steps per second: 25, episode reward: 0.435, mean reward: 0.005 [-0.002, 0.011], mean action: -0.209 [-1.197, 1.135], mean observation: 0.140 [-10.304, 14.842], loss: 0.000071, mean_squared_error: 0.000141, mean_q: 0.465759\n",
      " 1431/2000: episode: 16, duration: 3.732s, episode steps: 91, steps per second: 24, episode reward: 0.440, mean reward: 0.005 [-0.002, 0.011], mean action: -0.220 [-1.175, 1.105], mean observation: 0.139 [-16.086, 15.159], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.465819\n",
      " 1521/2000: episode: 17, duration: 3.711s, episode steps: 90, steps per second: 24, episode reward: 0.431, mean reward: 0.005 [-0.002, 0.011], mean action: -0.214 [-1.111, 1.127], mean observation: 0.137 [-16.759, 14.436], loss: 0.000097, mean_squared_error: 0.000193, mean_q: 0.462177\n",
      " 1618/2000: episode: 18, duration: 3.902s, episode steps: 97, steps per second: 25, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.011], mean action: -0.157 [-1.145, 1.132], mean observation: 0.140 [-16.718, 14.780], loss: 0.000087, mean_squared_error: 0.000173, mean_q: 0.453905\n",
      " 1714/2000: episode: 19, duration: 3.672s, episode steps: 96, steps per second: 26, episode reward: 0.451, mean reward: 0.005 [-0.001, 0.011], mean action: -0.122 [-1.114, 1.129], mean observation: 0.140 [-19.586, 15.359], loss: 0.000192, mean_squared_error: 0.000384, mean_q: 0.461227\n",
      " 1807/2000: episode: 20, duration: 3.669s, episode steps: 93, steps per second: 25, episode reward: 0.440, mean reward: 0.005 [-0.001, 0.011], mean action: -0.183 [-1.219, 1.112], mean observation: 0.140 [-10.407, 17.381], loss: 0.000188, mean_squared_error: 0.000376, mean_q: 0.459738\n",
      " 1900/2000: episode: 21, duration: 3.680s, episode steps: 93, steps per second: 25, episode reward: 0.439, mean reward: 0.005 [-0.001, 0.011], mean action: -0.193 [-1.201, 1.194], mean observation: 0.140 [-10.333, 14.718], loss: 0.000051, mean_squared_error: 0.000101, mean_q: 0.453895\n",
      " 1996/2000: episode: 22, duration: 3.930s, episode steps: 96, steps per second: 24, episode reward: 0.449, mean reward: 0.005 [-0.001, 0.011], mean action: -0.157 [-1.158, 1.184], mean observation: 0.140 [-16.817, 14.993], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.464184\n",
      "done, took 72.066 seconds\n",
      "\n",
      "\n",
      "iteration: 205\n",
      "Training for 2000 steps ...\n",
      "   96/2000: episode: 1, duration: 2.949s, episode steps: 96, steps per second: 33, episode reward: 0.446, mean reward: 0.005 [-0.001, 0.011], mean action: -0.168 [-1.180, 1.151], mean observation: 0.140 [-16.554, 14.789], loss: --, mean_squared_error: --, mean_q: --\n",
      "  192/2000: episode: 2, duration: 2.917s, episode steps: 96, steps per second: 33, episode reward: 0.452, mean reward: 0.005 [-0.002, 0.010], mean action: -0.180 [-1.141, 1.151], mean observation: 0.140 [-16.875, 14.929], loss: --, mean_squared_error: --, mean_q: --\n",
      "  288/2000: episode: 3, duration: 2.944s, episode steps: 96, steps per second: 33, episode reward: 0.449, mean reward: 0.005 [-0.002, 0.010], mean action: -0.186 [-1.158, 1.186], mean observation: 0.141 [-17.393, 16.374], loss: --, mean_squared_error: --, mean_q: --\n",
      "  384/2000: episode: 4, duration: 2.796s, episode steps: 96, steps per second: 34, episode reward: 0.446, mean reward: 0.005 [-0.001, 0.011], mean action: -0.172 [-1.198, 1.208], mean observation: 0.140 [-16.710, 17.237], loss: --, mean_squared_error: --, mean_q: --\n",
      "  479/2000: episode: 5, duration: 2.854s, episode steps: 95, steps per second: 33, episode reward: 0.445, mean reward: 0.005 [-0.001, 0.011], mean action: -0.148 [-1.130, 1.152], mean observation: 0.140 [-16.854, 14.345], loss: --, mean_squared_error: --, mean_q: --\n",
      "  576/2000: episode: 6, duration: 2.905s, episode steps: 97, steps per second: 33, episode reward: 0.456, mean reward: 0.005 [-0.002, 0.011], mean action: -0.175 [-1.174, 1.187], mean observation: 0.140 [-16.590, 15.144], loss: --, mean_squared_error: --, mean_q: --\n",
      "  671/2000: episode: 7, duration: 2.924s, episode steps: 95, steps per second: 32, episode reward: 0.448, mean reward: 0.005 [-0.002, 0.010], mean action: -0.181 [-1.187, 1.102], mean observation: 0.141 [-17.220, 15.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  767/2000: episode: 8, duration: 2.936s, episode steps: 96, steps per second: 33, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.011], mean action: -0.173 [-1.158, 1.139], mean observation: 0.141 [-15.973, 14.844], loss: --, mean_squared_error: --, mean_q: --\n",
      "  863/2000: episode: 9, duration: 2.848s, episode steps: 96, steps per second: 34, episode reward: 0.451, mean reward: 0.005 [-0.001, 0.010], mean action: -0.190 [-1.216, 1.146], mean observation: 0.140 [-16.693, 15.535], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  960/2000: episode: 10, duration: 2.991s, episode steps: 97, steps per second: 32, episode reward: 0.456, mean reward: 0.005 [-0.002, 0.010], mean action: -0.170 [-1.163, 1.185], mean observation: 0.141 [-16.527, 14.700], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1057/2000: episode: 11, duration: 3.400s, episode steps: 97, steps per second: 29, episode reward: 0.451, mean reward: 0.005 [-0.002, 0.011], mean action: -0.180 [-1.155, 1.181], mean observation: 0.140 [-16.454, 15.223], loss: 0.000070, mean_squared_error: 0.000140, mean_q: 0.456551\n",
      " 1152/2000: episode: 12, duration: 3.826s, episode steps: 95, steps per second: 25, episode reward: 0.437, mean reward: 0.005 [-0.001, 0.011], mean action: -0.191 [-1.163, 1.232], mean observation: 0.143 [-10.309, 17.831], loss: 0.000248, mean_squared_error: 0.000496, mean_q: 0.452012\n",
      " 1248/2000: episode: 13, duration: 3.893s, episode steps: 96, steps per second: 25, episode reward: 0.441, mean reward: 0.005 [-0.002, 0.011], mean action: -0.205 [-1.142, 1.169], mean observation: 0.144 [-10.327, 17.398], loss: 0.000098, mean_squared_error: 0.000196, mean_q: 0.457334\n",
      " 1339/2000: episode: 14, duration: 3.730s, episode steps: 91, steps per second: 24, episode reward: 0.432, mean reward: 0.005 [-0.001, 0.012], mean action: -0.236 [-1.171, 1.160], mean observation: 0.142 [-10.520, 15.757], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.453801\n",
      " 1435/2000: episode: 15, duration: 3.871s, episode steps: 96, steps per second: 25, episode reward: 0.440, mean reward: 0.005 [-0.001, 0.011], mean action: -0.208 [-1.176, 1.119], mean observation: 0.142 [-10.292, 15.501], loss: 0.000048, mean_squared_error: 0.000096, mean_q: 0.462921\n",
      " 1532/2000: episode: 16, duration: 3.910s, episode steps: 97, steps per second: 25, episode reward: 0.442, mean reward: 0.005 [-0.001, 0.010], mean action: -0.192 [-1.145, 1.115], mean observation: 0.141 [-10.372, 14.922], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.444916\n",
      " 1630/2000: episode: 17, duration: 3.911s, episode steps: 98, steps per second: 25, episode reward: 0.446, mean reward: 0.005 [-0.001, 0.010], mean action: -0.182 [-1.202, 1.219], mean observation: 0.142 [-10.474, 16.162], loss: 0.000095, mean_squared_error: 0.000190, mean_q: 0.455223\n",
      " 1726/2000: episode: 18, duration: 3.829s, episode steps: 96, steps per second: 25, episode reward: 0.443, mean reward: 0.005 [-0.001, 0.010], mean action: -0.204 [-1.175, 1.150], mean observation: 0.142 [-10.414, 15.714], loss: 0.000075, mean_squared_error: 0.000150, mean_q: 0.450843\n",
      " 1823/2000: episode: 19, duration: 3.957s, episode steps: 97, steps per second: 25, episode reward: 0.443, mean reward: 0.005 [-0.001, 0.011], mean action: -0.226 [-1.086, 1.109], mean observation: 0.142 [-10.338, 17.084], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.450358\n",
      " 1920/2000: episode: 20, duration: 3.864s, episode steps: 97, steps per second: 25, episode reward: 0.438, mean reward: 0.005 [-0.001, 0.010], mean action: -0.227 [-1.175, 1.171], mean observation: 0.141 [-10.364, 15.451], loss: 0.000106, mean_squared_error: 0.000212, mean_q: 0.460998\n",
      "done, took 70.815 seconds\n",
      "\n",
      "\n",
      "iteration: 206\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 2.943s, episode steps: 95, steps per second: 32, episode reward: 0.449, mean reward: 0.005 [-0.000, 0.010], mean action: -0.228 [-1.157, 1.123], mean observation: 0.142 [-10.221, 19.953], loss: --, mean_squared_error: --, mean_q: --\n",
      "  192/2000: episode: 2, duration: 2.900s, episode steps: 97, steps per second: 33, episode reward: 0.448, mean reward: 0.005 [-0.000, 0.010], mean action: -0.228 [-1.154, 1.174], mean observation: 0.142 [-10.381, 19.904], loss: --, mean_squared_error: --, mean_q: --\n",
      "  288/2000: episode: 3, duration: 3.065s, episode steps: 96, steps per second: 31, episode reward: 0.448, mean reward: 0.005 [-0.000, 0.010], mean action: -0.245 [-1.263, 1.099], mean observation: 0.143 [-10.378, 20.062], loss: --, mean_squared_error: --, mean_q: --\n",
      "  384/2000: episode: 4, duration: 2.967s, episode steps: 96, steps per second: 32, episode reward: 0.443, mean reward: 0.005 [-0.000, 0.010], mean action: -0.219 [-1.144, 1.144], mean observation: 0.143 [-10.497, 20.076], loss: --, mean_squared_error: --, mean_q: --\n",
      "  480/2000: episode: 5, duration: 2.927s, episode steps: 96, steps per second: 33, episode reward: 0.445, mean reward: 0.005 [-0.000, 0.010], mean action: -0.220 [-1.177, 1.156], mean observation: 0.142 [-10.399, 20.024], loss: --, mean_squared_error: --, mean_q: --\n",
      "  577/2000: episode: 6, duration: 2.983s, episode steps: 97, steps per second: 33, episode reward: 0.455, mean reward: 0.005 [-0.000, 0.010], mean action: -0.233 [-1.156, 1.166], mean observation: 0.142 [-10.500, 19.806], loss: --, mean_squared_error: --, mean_q: --\n",
      "  673/2000: episode: 7, duration: 2.910s, episode steps: 96, steps per second: 33, episode reward: 0.443, mean reward: 0.005 [-0.000, 0.010], mean action: -0.235 [-1.180, 1.115], mean observation: 0.143 [-10.610, 19.812], loss: --, mean_squared_error: --, mean_q: --\n",
      "  770/2000: episode: 8, duration: 3.055s, episode steps: 97, steps per second: 32, episode reward: 0.453, mean reward: 0.005 [-0.000, 0.010], mean action: -0.222 [-1.130, 1.169], mean observation: 0.143 [-10.365, 19.769], loss: --, mean_squared_error: --, mean_q: --\n",
      "  866/2000: episode: 9, duration: 2.942s, episode steps: 96, steps per second: 33, episode reward: 0.448, mean reward: 0.005 [-0.000, 0.010], mean action: -0.219 [-1.131, 1.161], mean observation: 0.142 [-10.431, 20.057], loss: --, mean_squared_error: --, mean_q: --\n",
      "  963/2000: episode: 10, duration: 2.950s, episode steps: 97, steps per second: 33, episode reward: 0.450, mean reward: 0.005 [-0.000, 0.010], mean action: -0.241 [-1.220, 1.129], mean observation: 0.142 [-10.552, 19.884], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1059/2000: episode: 11, duration: 3.469s, episode steps: 96, steps per second: 28, episode reward: 0.443, mean reward: 0.005 [-0.000, 0.010], mean action: -0.240 [-1.210, 1.161], mean observation: 0.142 [-10.428, 19.862], loss: 0.000040, mean_squared_error: 0.000080, mean_q: 0.455720\n",
      " 1155/2000: episode: 12, duration: 3.825s, episode steps: 96, steps per second: 25, episode reward: 0.448, mean reward: 0.005 [-0.000, 0.010], mean action: -0.221 [-1.157, 1.138], mean observation: 0.143 [-10.342, 19.930], loss: 0.000051, mean_squared_error: 0.000103, mean_q: 0.449195\n",
      " 1255/2000: episode: 13, duration: 3.992s, episode steps: 100, steps per second: 25, episode reward: 0.442, mean reward: 0.004 [-0.001, 0.010], mean action: -0.266 [-1.194, 1.199], mean observation: 0.139 [-15.604, 17.490], loss: 0.000047, mean_squared_error: 0.000095, mean_q: 0.454293\n",
      " 1352/2000: episode: 14, duration: 3.954s, episode steps: 97, steps per second: 25, episode reward: 0.448, mean reward: 0.005 [-0.000, 0.010], mean action: -0.206 [-1.134, 1.116], mean observation: 0.142 [-10.508, 19.869], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.449915\n",
      " 1447/2000: episode: 15, duration: 3.844s, episode steps: 95, steps per second: 25, episode reward: 0.443, mean reward: 0.005 [-0.000, 0.011], mean action: -0.197 [-1.184, 1.105], mean observation: 0.143 [-10.426, 19.992], loss: 0.000059, mean_squared_error: 0.000117, mean_q: 0.458120\n",
      " 1543/2000: episode: 16, duration: 3.753s, episode steps: 96, steps per second: 26, episode reward: 0.438, mean reward: 0.005 [-0.000, 0.010], mean action: -0.197 [-1.196, 1.088], mean observation: 0.142 [-10.473, 19.880], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.445659\n",
      " 1639/2000: episode: 17, duration: 3.633s, episode steps: 96, steps per second: 26, episode reward: 0.442, mean reward: 0.005 [-0.000, 0.010], mean action: -0.189 [-1.129, 1.232], mean observation: 0.142 [-10.483, 20.406], loss: 0.000164, mean_squared_error: 0.000328, mean_q: 0.448373\n",
      " 1736/2000: episode: 18, duration: 3.830s, episode steps: 97, steps per second: 25, episode reward: 0.444, mean reward: 0.005 [-0.000, 0.010], mean action: -0.190 [-1.212, 1.139], mean observation: 0.142 [-10.390, 19.783], loss: 0.000152, mean_squared_error: 0.000304, mean_q: 0.448626\n",
      " 1830/2000: episode: 19, duration: 3.898s, episode steps: 94, steps per second: 24, episode reward: 0.383, mean reward: 0.004 [-0.000, 0.010], mean action: -0.222 [-1.157, 1.152], mean observation: 0.136 [-10.388, 20.053], loss: 0.000211, mean_squared_error: 0.000421, mean_q: 0.449527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1925/2000: episode: 20, duration: 4.125s, episode steps: 95, steps per second: 23, episode reward: 0.408, mean reward: 0.004 [-0.000, 0.010], mean action: -0.236 [-1.276, 1.094], mean observation: 0.135 [-10.320, 19.932], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.438230\n",
      "done, took 71.373 seconds\n",
      "\n",
      "\n",
      "iteration: 207\n",
      "Training for 2000 steps ...\n",
      "   95/2000: episode: 1, duration: 3.279s, episode steps: 95, steps per second: 29, episode reward: 0.387, mean reward: 0.004 [-0.000, 0.011], mean action: -0.251 [-1.223, 1.226], mean observation: 0.134 [-10.281, 19.937], loss: --, mean_squared_error: --, mean_q: --\n",
      "  195/2000: episode: 2, duration: 3.303s, episode steps: 100, steps per second: 30, episode reward: 0.452, mean reward: 0.005 [-0.000, 0.010], mean action: -0.257 [-1.174, 1.173], mean observation: 0.141 [-10.311, 19.525], loss: --, mean_squared_error: --, mean_q: --\n",
      "  292/2000: episode: 3, duration: 3.184s, episode steps: 97, steps per second: 30, episode reward: 0.413, mean reward: 0.004 [-0.000, 0.011], mean action: -0.247 [-1.228, 1.210], mean observation: 0.136 [-10.411, 19.887], loss: --, mean_squared_error: --, mean_q: --\n",
      "  389/2000: episode: 4, duration: 3.257s, episode steps: 97, steps per second: 30, episode reward: 0.405, mean reward: 0.004 [-0.000, 0.011], mean action: -0.235 [-1.222, 1.134], mean observation: 0.134 [-12.860, 19.906], loss: --, mean_squared_error: --, mean_q: --\n",
      "  485/2000: episode: 5, duration: 3.306s, episode steps: 96, steps per second: 29, episode reward: 0.389, mean reward: 0.004 [-0.000, 0.011], mean action: -0.228 [-1.166, 1.101], mean observation: 0.135 [-11.091, 19.800], loss: --, mean_squared_error: --, mean_q: --\n",
      "  581/2000: episode: 6, duration: 3.262s, episode steps: 96, steps per second: 29, episode reward: 0.389, mean reward: 0.004 [-0.000, 0.011], mean action: -0.224 [-1.092, 1.145], mean observation: 0.133 [-11.225, 19.873], loss: --, mean_squared_error: --, mean_q: --\n",
      "  680/2000: episode: 7, duration: 3.268s, episode steps: 99, steps per second: 30, episode reward: 0.449, mean reward: 0.005 [-0.000, 0.010], mean action: -0.242 [-1.222, 1.157], mean observation: 0.141 [-10.431, 20.085], loss: --, mean_squared_error: --, mean_q: --\n",
      "  776/2000: episode: 8, duration: 3.307s, episode steps: 96, steps per second: 29, episode reward: 0.388, mean reward: 0.004 [-0.000, 0.011], mean action: -0.240 [-1.133, 1.146], mean observation: 0.133 [-10.580, 19.869], loss: --, mean_squared_error: --, mean_q: --\n",
      "  876/2000: episode: 9, duration: 3.282s, episode steps: 100, steps per second: 30, episode reward: 0.445, mean reward: 0.004 [-0.000, 0.010], mean action: -0.244 [-1.220, 1.192], mean observation: 0.139 [-10.558, 19.632], loss: --, mean_squared_error: --, mean_q: --\n",
      "  973/2000: episode: 10, duration: 3.353s, episode steps: 97, steps per second: 29, episode reward: 0.395, mean reward: 0.004 [-0.000, 0.011], mean action: -0.248 [-1.220, 1.147], mean observation: 0.134 [-12.566, 19.797], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1071/2000: episode: 11, duration: 3.792s, episode steps: 98, steps per second: 26, episode reward: 0.408, mean reward: 0.004 [-0.000, 0.010], mean action: -0.237 [-1.138, 1.133], mean observation: 0.135 [-10.556, 19.809], loss: 0.000157, mean_squared_error: 0.000313, mean_q: 0.441505\n",
      " 1170/2000: episode: 12, duration: 4.115s, episode steps: 99, steps per second: 24, episode reward: 0.423, mean reward: 0.004 [-0.000, 0.011], mean action: -0.234 [-1.182, 1.203], mean observation: 0.138 [-10.502, 19.823], loss: 0.000211, mean_squared_error: 0.000422, mean_q: 0.450789\n",
      " 1275/2000: episode: 13, duration: 4.389s, episode steps: 105, steps per second: 24, episode reward: 0.448, mean reward: 0.004 [-0.001, 0.011], mean action: -0.216 [-1.182, 1.192], mean observation: 0.140 [-10.347, 19.779], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.447940\n",
      " 1383/2000: episode: 14, duration: 4.780s, episode steps: 108, steps per second: 23, episode reward: 0.448, mean reward: 0.004 [-0.001, 0.010], mean action: -0.191 [-1.118, 1.147], mean observation: 0.137 [-13.722, 19.918], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.437359\n",
      " 1489/2000: episode: 15, duration: 5.327s, episode steps: 106, steps per second: 20, episode reward: 0.419, mean reward: 0.004 [-0.001, 0.011], mean action: -0.228 [-1.179, 1.149], mean observation: 0.135 [-25.032, 19.975], loss: 0.000049, mean_squared_error: 0.000098, mean_q: 0.440433\n",
      " 1604/2000: episode: 16, duration: 5.339s, episode steps: 115, steps per second: 22, episode reward: 0.570, mean reward: 0.005 [-0.001, 0.012], mean action: -0.196 [-1.152, 1.143], mean observation: 0.120 [-10.827, 19.932], loss: 0.000055, mean_squared_error: 0.000110, mean_q: 0.443695\n",
      " 1725/2000: episode: 17, duration: 6.602s, episode steps: 121, steps per second: 18, episode reward: 0.622, mean reward: 0.005 [-0.001, 0.012], mean action: -0.257 [-1.150, 1.113], mean observation: 0.107 [-11.571, 19.850], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.437727\n",
      " 1845/2000: episode: 18, duration: 6.643s, episode steps: 120, steps per second: 18, episode reward: 0.631, mean reward: 0.005 [-0.001, 0.014], mean action: -0.232 [-1.220, 1.165], mean observation: 0.109 [-12.017, 19.646], loss: 0.000050, mean_squared_error: 0.000100, mean_q: 0.443193\n",
      " 1969/2000: episode: 19, duration: 6.841s, episode steps: 124, steps per second: 18, episode reward: 0.676, mean reward: 0.005 [-0.001, 0.014], mean action: -0.251 [-1.112, 1.200], mean observation: 0.101 [-14.639, 20.095], loss: 0.000069, mean_squared_error: 0.000139, mean_q: 0.436942\n",
      "done, took 82.284 seconds\n",
      "\n",
      "\n",
      "iteration: 208\n",
      "Training for 2000 steps ...\n",
      "  126/2000: episode: 1, duration: 6.020s, episode steps: 126, steps per second: 21, episode reward: 0.671, mean reward: 0.005 [-0.001, 0.013], mean action: -0.217 [-1.168, 1.243], mean observation: 0.097 [-13.302, 19.836], loss: --, mean_squared_error: --, mean_q: --\n",
      "  250/2000: episode: 2, duration: 5.954s, episode steps: 124, steps per second: 21, episode reward: 0.654, mean reward: 0.005 [-0.001, 0.013], mean action: -0.225 [-1.179, 1.177], mean observation: 0.093 [-14.190, 19.958], loss: --, mean_squared_error: --, mean_q: --\n",
      "  375/2000: episode: 3, duration: 6.068s, episode steps: 125, steps per second: 21, episode reward: 0.656, mean reward: 0.005 [-0.001, 0.013], mean action: -0.227 [-1.189, 1.233], mean observation: 0.098 [-10.490, 19.734], loss: --, mean_squared_error: --, mean_q: --\n",
      "  500/2000: episode: 4, duration: 5.921s, episode steps: 125, steps per second: 21, episode reward: 0.657, mean reward: 0.005 [-0.001, 0.013], mean action: -0.224 [-1.170, 1.305], mean observation: 0.097 [-12.261, 19.756], loss: --, mean_squared_error: --, mean_q: --\n",
      "  626/2000: episode: 5, duration: 6.085s, episode steps: 126, steps per second: 21, episode reward: 0.667, mean reward: 0.005 [-0.001, 0.013], mean action: -0.228 [-1.154, 1.155], mean observation: 0.099 [-11.954, 19.733], loss: --, mean_squared_error: --, mean_q: --\n",
      "  751/2000: episode: 6, duration: 6.034s, episode steps: 125, steps per second: 21, episode reward: 0.666, mean reward: 0.005 [-0.001, 0.013], mean action: -0.227 [-1.150, 1.096], mean observation: 0.095 [-13.097, 20.004], loss: --, mean_squared_error: --, mean_q: --\n",
      "  875/2000: episode: 7, duration: 5.939s, episode steps: 124, steps per second: 21, episode reward: 0.654, mean reward: 0.005 [-0.001, 0.013], mean action: -0.240 [-1.201, 1.099], mean observation: 0.096 [-11.724, 19.823], loss: --, mean_squared_error: --, mean_q: --\n",
      "  999/2000: episode: 8, duration: 5.940s, episode steps: 124, steps per second: 21, episode reward: 0.658, mean reward: 0.005 [-0.001, 0.013], mean action: -0.233 [-1.152, 1.148], mean observation: 0.096 [-13.817, 20.063], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1125/2000: episode: 9, duration: 7.410s, episode steps: 126, steps per second: 17, episode reward: 0.671, mean reward: 0.005 [-0.001, 0.013], mean action: -0.204 [-1.113, 1.176], mean observation: 0.098 [-16.281, 19.959], loss: 0.000229, mean_squared_error: 0.000458, mean_q: 0.436157\n",
      " 1256/2000: episode: 10, duration: 6.648s, episode steps: 131, steps per second: 20, episode reward: 0.715, mean reward: 0.005 [-0.001, 0.013], mean action: -0.162 [-1.157, 1.111], mean observation: 0.095 [-17.093, 19.847], loss: 0.000136, mean_squared_error: 0.000272, mean_q: 0.440075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1391/2000: episode: 11, duration: 6.941s, episode steps: 135, steps per second: 19, episode reward: 0.731, mean reward: 0.005 [-0.001, 0.013], mean action: -0.150 [-1.173, 1.195], mean observation: 0.098 [-23.054, 19.956], loss: 0.000104, mean_squared_error: 0.000209, mean_q: 0.434671\n",
      " 1547/2000: episode: 12, duration: 7.699s, episode steps: 156, steps per second: 20, episode reward: 0.750, mean reward: 0.005 [-0.000, 0.013], mean action: -0.164 [-1.127, 1.218], mean observation: 0.101 [-10.431, 19.962], loss: 0.000102, mean_squared_error: 0.000204, mean_q: 0.439232\n",
      " 1802/2000: episode: 13, duration: 12.273s, episode steps: 255, steps per second: 21, episode reward: -0.877, mean reward: -0.003 [-0.021, 0.009], mean action: -0.131 [-1.282, 1.328], mean observation: 0.049 [-39.823, 20.157], loss: 0.000142, mean_squared_error: 0.000285, mean_q: 0.428616\n",
      "done, took 98.796 seconds\n",
      "\n",
      "\n",
      "iteration: 209\n",
      "Training for 2000 steps ...\n",
      "  220/2000: episode: 1, duration: 8.592s, episode steps: 220, steps per second: 26, episode reward: -0.874, mean reward: -0.004 [-0.020, 0.009], mean action: -0.160 [-1.191, 1.250], mean observation: 0.042 [-35.180, 19.580], loss: --, mean_squared_error: --, mean_q: --\n",
      "  424/2000: episode: 2, duration: 7.911s, episode steps: 204, steps per second: 26, episode reward: -0.853, mean reward: -0.004 [-0.020, 0.009], mean action: -0.165 [-1.299, 1.247], mean observation: 0.038 [-36.933, 19.629], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 3, duration: 8.353s, episode steps: 213, steps per second: 26, episode reward: -0.862, mean reward: -0.004 [-0.020, 0.010], mean action: -0.149 [-1.275, 1.198], mean observation: 0.041 [-36.889, 19.941], loss: --, mean_squared_error: --, mean_q: --\n",
      "  858/2000: episode: 4, duration: 8.573s, episode steps: 221, steps per second: 26, episode reward: -0.874, mean reward: -0.004 [-0.020, 0.010], mean action: -0.150 [-1.223, 1.277], mean observation: 0.045 [-37.611, 19.894], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1079/2000: episode: 5, duration: 9.369s, episode steps: 221, steps per second: 24, episode reward: -0.857, mean reward: -0.004 [-0.020, 0.009], mean action: -0.155 [-1.252, 1.136], mean observation: 0.044 [-36.866, 19.757], loss: 0.000101, mean_squared_error: 0.000203, mean_q: 0.427786\n",
      " 1271/2000: episode: 6, duration: 9.055s, episode steps: 192, steps per second: 21, episode reward: -0.858, mean reward: -0.004 [-0.020, 0.009], mean action: -0.144 [-1.253, 1.258], mean observation: 0.037 [-37.623, 19.874], loss: 0.000359, mean_squared_error: 0.000717, mean_q: 0.434713\n",
      " 1436/2000: episode: 7, duration: 7.229s, episode steps: 165, steps per second: 23, episode reward: -0.822, mean reward: -0.005 [-0.021, 0.008], mean action: -0.146 [-1.270, 1.220], mean observation: 0.033 [-14.654, 19.588], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.433094\n",
      " 1596/2000: episode: 8, duration: 6.877s, episode steps: 160, steps per second: 23, episode reward: -0.753, mean reward: -0.005 [-0.021, 0.007], mean action: -0.174 [-1.180, 1.170], mean observation: 0.039 [-16.312, 18.519], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.423284\n",
      " 1760/2000: episode: 9, duration: 6.987s, episode steps: 164, steps per second: 23, episode reward: -0.737, mean reward: -0.004 [-0.021, 0.007], mean action: -0.176 [-1.146, 1.297], mean observation: 0.045 [-15.152, 20.138], loss: 0.000060, mean_squared_error: 0.000120, mean_q: 0.425264\n",
      " 1930/2000: episode: 10, duration: 7.244s, episode steps: 170, steps per second: 23, episode reward: -0.787, mean reward: -0.005 [-0.021, 0.007], mean action: -0.169 [-1.179, 1.203], mean observation: 0.041 [-12.297, 20.007], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.429657\n",
      "done, took 83.451 seconds\n",
      "\n",
      "\n",
      "iteration: 210\n",
      "Training for 2000 steps ...\n",
      "  176/2000: episode: 1, duration: 5.455s, episode steps: 176, steps per second: 32, episode reward: -0.762, mean reward: -0.004 [-0.021, 0.007], mean action: -0.164 [-1.193, 1.211], mean observation: 0.045 [-12.224, 19.727], loss: --, mean_squared_error: --, mean_q: --\n",
      "  355/2000: episode: 2, duration: 5.529s, episode steps: 179, steps per second: 32, episode reward: -0.763, mean reward: -0.004 [-0.021, 0.007], mean action: -0.136 [-1.246, 1.203], mean observation: 0.046 [-16.696, 19.240], loss: --, mean_squared_error: --, mean_q: --\n",
      "  532/2000: episode: 3, duration: 5.449s, episode steps: 177, steps per second: 32, episode reward: -0.755, mean reward: -0.004 [-0.021, 0.007], mean action: -0.184 [-1.258, 1.206], mean observation: 0.046 [-19.771, 19.887], loss: --, mean_squared_error: --, mean_q: --\n",
      "  714/2000: episode: 4, duration: 5.500s, episode steps: 182, steps per second: 33, episode reward: -0.777, mean reward: -0.004 [-0.021, 0.007], mean action: -0.128 [-1.192, 1.264], mean observation: 0.047 [-18.654, 19.638], loss: --, mean_squared_error: --, mean_q: --\n",
      "  891/2000: episode: 5, duration: 5.423s, episode steps: 177, steps per second: 33, episode reward: -0.754, mean reward: -0.004 [-0.021, 0.007], mean action: -0.137 [-1.221, 1.269], mean observation: 0.047 [-18.864, 19.762], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1069/2000: episode: 6, duration: 6.198s, episode steps: 178, steps per second: 29, episode reward: -0.783, mean reward: -0.004 [-0.021, 0.007], mean action: -0.159 [-1.398, 1.223], mean observation: 0.045 [-19.212, 19.500], loss: 0.000187, mean_squared_error: 0.000374, mean_q: 0.425880\n",
      " 1240/2000: episode: 7, duration: 7.254s, episode steps: 171, steps per second: 24, episode reward: -0.800, mean reward: -0.005 [-0.021, 0.007], mean action: -0.138 [-1.176, 1.364], mean observation: 0.040 [-18.467, 19.911], loss: 0.000412, mean_squared_error: 0.000824, mean_q: 0.423459\n",
      " 1406/2000: episode: 8, duration: 7.413s, episode steps: 166, steps per second: 22, episode reward: -0.740, mean reward: -0.004 [-0.021, 0.007], mean action: -0.145 [-1.287, 1.155], mean observation: 0.045 [-19.237, 19.666], loss: 0.000062, mean_squared_error: 0.000125, mean_q: 0.422565\n",
      " 1597/2000: episode: 9, duration: 8.651s, episode steps: 191, steps per second: 22, episode reward: -0.780, mean reward: -0.004 [-0.021, 0.007], mean action: -0.078 [-1.183, 1.222], mean observation: 0.049 [-19.949, 19.006], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.426561\n",
      " 1779/2000: episode: 10, duration: 7.700s, episode steps: 182, steps per second: 24, episode reward: -0.785, mean reward: -0.004 [-0.021, 0.007], mean action: -0.036 [-1.169, 1.224], mean observation: 0.045 [-11.781, 20.119], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.415647\n",
      " 1989/2000: episode: 11, duration: 9.188s, episode steps: 210, steps per second: 23, episode reward: -0.791, mean reward: -0.004 [-0.021, 0.007], mean action: -0.029 [-1.177, 1.236], mean observation: 0.053 [-20.913, 19.903], loss: 0.000045, mean_squared_error: 0.000089, mean_q: 0.416025\n",
      "done, took 74.440 seconds\n",
      "\n",
      "\n",
      "iteration: 211\n",
      "Training for 2000 steps ...\n",
      "  190/2000: episode: 1, duration: 6.441s, episode steps: 190, steps per second: 29, episode reward: -0.785, mean reward: -0.004 [-0.021, 0.007], mean action: -0.058 [-1.227, 1.248], mean observation: 0.047 [-20.556, 19.906], loss: --, mean_squared_error: --, mean_q: --\n",
      "  387/2000: episode: 2, duration: 6.812s, episode steps: 197, steps per second: 29, episode reward: -0.822, mean reward: -0.004 [-0.021, 0.007], mean action: -0.050 [-1.438, 1.214], mean observation: 0.046 [-21.939, 19.903], loss: --, mean_squared_error: --, mean_q: --\n",
      "  583/2000: episode: 3, duration: 6.654s, episode steps: 196, steps per second: 29, episode reward: -0.797, mean reward: -0.004 [-0.021, 0.007], mean action: -0.014 [-1.144, 1.221], mean observation: 0.048 [-21.984, 19.982], loss: --, mean_squared_error: --, mean_q: --\n",
      "  775/2000: episode: 4, duration: 6.564s, episode steps: 192, steps per second: 29, episode reward: -0.773, mean reward: -0.004 [-0.021, 0.008], mean action: -0.079 [-1.278, 1.148], mean observation: 0.049 [-20.636, 19.661], loss: --, mean_squared_error: --, mean_q: --\n",
      "  963/2000: episode: 5, duration: 6.415s, episode steps: 188, steps per second: 29, episode reward: -0.790, mean reward: -0.004 [-0.021, 0.007], mean action: -0.054 [-1.256, 1.215], mean observation: 0.047 [-20.161, 20.320], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1153/2000: episode: 6, duration: 8.060s, episode steps: 190, steps per second: 24, episode reward: -0.794, mean reward: -0.004 [-0.021, 0.007], mean action: -0.023 [-1.241, 1.297], mean observation: 0.048 [-19.051, 20.102], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.402398\n",
      " 1340/2000: episode: 7, duration: 9.062s, episode steps: 187, steps per second: 21, episode reward: -0.762, mean reward: -0.004 [-0.021, 0.007], mean action: 0.063 [-1.136, 1.379], mean observation: 0.049 [-23.253, 20.042], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.409903\n",
      " 1537/2000: episode: 8, duration: 9.229s, episode steps: 197, steps per second: 21, episode reward: -0.774, mean reward: -0.004 [-0.021, 0.007], mean action: 0.058 [-1.326, 1.197], mean observation: 0.050 [-18.483, 19.746], loss: 0.000123, mean_squared_error: 0.000247, mean_q: 0.414849\n",
      " 1713/2000: episode: 9, duration: 8.176s, episode steps: 176, steps per second: 22, episode reward: -0.786, mean reward: -0.004 [-0.021, 0.008], mean action: 0.036 [-1.192, 1.242], mean observation: 0.037 [-42.665, 19.723], loss: 0.000082, mean_squared_error: 0.000165, mean_q: 0.410158\n",
      " 1887/2000: episode: 10, duration: 7.961s, episode steps: 174, steps per second: 22, episode reward: -0.768, mean reward: -0.004 [-0.021, 0.007], mean action: 0.096 [-1.102, 1.216], mean observation: 0.044 [-20.345, 19.586], loss: 0.000121, mean_squared_error: 0.000241, mean_q: 0.411933\n",
      "done, took 81.312 seconds\n",
      "\n",
      "\n",
      "iteration: 212\n",
      "Training for 2000 steps ...\n",
      "  130/2000: episode: 1, duration: 4.411s, episode steps: 130, steps per second: 29, episode reward: -0.761, mean reward: -0.006 [-0.021, 0.008], mean action: -0.027 [-1.211, 1.210], mean observation: 0.017 [-21.990, 19.777], loss: --, mean_squared_error: --, mean_q: --\n",
      "  260/2000: episode: 2, duration: 4.352s, episode steps: 130, steps per second: 30, episode reward: -0.767, mean reward: -0.006 [-0.021, 0.008], mean action: -0.047 [-1.227, 1.246], mean observation: 0.015 [-23.225, 19.911], loss: --, mean_squared_error: --, mean_q: --\n",
      "  390/2000: episode: 3, duration: 4.373s, episode steps: 130, steps per second: 30, episode reward: -0.762, mean reward: -0.006 [-0.021, 0.008], mean action: -0.047 [-1.205, 1.143], mean observation: 0.018 [-19.771, 20.464], loss: --, mean_squared_error: --, mean_q: --\n",
      "  519/2000: episode: 4, duration: 4.390s, episode steps: 129, steps per second: 29, episode reward: -0.756, mean reward: -0.006 [-0.021, 0.008], mean action: -0.053 [-1.207, 1.181], mean observation: 0.015 [-27.794, 20.177], loss: --, mean_squared_error: --, mean_q: --\n",
      "  648/2000: episode: 5, duration: 4.445s, episode steps: 129, steps per second: 29, episode reward: -0.759, mean reward: -0.006 [-0.021, 0.008], mean action: -0.023 [-1.175, 1.203], mean observation: 0.016 [-27.027, 19.915], loss: --, mean_squared_error: --, mean_q: --\n",
      "  778/2000: episode: 6, duration: 4.376s, episode steps: 130, steps per second: 30, episode reward: -0.771, mean reward: -0.006 [-0.021, 0.008], mean action: -0.045 [-1.127, 1.153], mean observation: 0.015 [-24.645, 20.047], loss: --, mean_squared_error: --, mean_q: --\n",
      "  908/2000: episode: 7, duration: 4.359s, episode steps: 130, steps per second: 30, episode reward: -0.764, mean reward: -0.006 [-0.021, 0.008], mean action: -0.039 [-1.259, 1.136], mean observation: 0.017 [-21.096, 20.240], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1037/2000: episode: 8, duration: 4.699s, episode steps: 129, steps per second: 27, episode reward: -0.751, mean reward: -0.006 [-0.020, 0.008], mean action: -0.037 [-1.103, 1.155], mean observation: 0.017 [-22.700, 19.966], loss: 0.000063, mean_squared_error: 0.000126, mean_q: 0.405703\n",
      " 1169/2000: episode: 9, duration: 5.610s, episode steps: 132, steps per second: 24, episode reward: -0.778, mean reward: -0.006 [-0.021, 0.008], mean action: -0.019 [-1.226, 1.133], mean observation: 0.016 [-20.242, 19.959], loss: 0.000091, mean_squared_error: 0.000181, mean_q: 0.409701\n",
      " 1304/2000: episode: 10, duration: 6.020s, episode steps: 135, steps per second: 22, episode reward: -0.791, mean reward: -0.006 [-0.021, 0.007], mean action: -0.022 [-1.188, 1.213], mean observation: 0.013 [-30.115, 19.576], loss: 0.000164, mean_squared_error: 0.000328, mean_q: 0.391363\n",
      " 1440/2000: episode: 11, duration: 6.220s, episode steps: 136, steps per second: 22, episode reward: -0.825, mean reward: -0.006 [-0.021, 0.008], mean action: -0.040 [-1.245, 1.149], mean observation: 0.010 [-28.894, 19.451], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.407272\n",
      " 1563/2000: episode: 12, duration: 5.533s, episode steps: 123, steps per second: 22, episode reward: -0.747, mean reward: -0.006 [-0.021, 0.008], mean action: -0.078 [-1.185, 1.131], mean observation: 0.014 [-21.112, 19.934], loss: 0.000191, mean_squared_error: 0.000382, mean_q: 0.397044\n",
      " 1683/2000: episode: 13, duration: 5.677s, episode steps: 120, steps per second: 21, episode reward: -0.728, mean reward: -0.006 [-0.020, 0.008], mean action: -0.033 [-1.183, 1.208], mean observation: 0.015 [-26.172, 19.936], loss: 0.000162, mean_squared_error: 0.000325, mean_q: 0.398761\n",
      " 1805/2000: episode: 14, duration: 5.431s, episode steps: 122, steps per second: 22, episode reward: -0.751, mean reward: -0.006 [-0.021, 0.008], mean action: 0.047 [-1.229, 1.215], mean observation: 0.012 [-24.346, 18.761], loss: 0.000083, mean_squared_error: 0.000167, mean_q: 0.403538\n",
      " 1929/2000: episode: 15, duration: 5.503s, episode steps: 124, steps per second: 23, episode reward: -0.746, mean reward: -0.006 [-0.021, 0.009], mean action: 0.066 [-1.156, 1.197], mean observation: 0.013 [-23.549, 18.651], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.394519\n",
      "done, took 79.293 seconds\n",
      "\n",
      "\n",
      "iteration: 213\n",
      "Training for 2000 steps ...\n",
      "  121/2000: episode: 1, duration: 4.111s, episode steps: 121, steps per second: 29, episode reward: -0.744, mean reward: -0.006 [-0.021, 0.009], mean action: 0.106 [-1.181, 1.230], mean observation: 0.009 [-29.114, 19.162], loss: --, mean_squared_error: --, mean_q: --\n",
      "  242/2000: episode: 2, duration: 4.060s, episode steps: 121, steps per second: 30, episode reward: -0.750, mean reward: -0.006 [-0.021, 0.009], mean action: 0.081 [-1.245, 1.176], mean observation: 0.009 [-26.258, 18.452], loss: --, mean_squared_error: --, mean_q: --\n",
      "  363/2000: episode: 3, duration: 4.108s, episode steps: 121, steps per second: 29, episode reward: -0.739, mean reward: -0.006 [-0.021, 0.009], mean action: 0.074 [-1.177, 1.166], mean observation: 0.011 [-26.464, 18.711], loss: --, mean_squared_error: --, mean_q: --\n",
      "  484/2000: episode: 4, duration: 4.110s, episode steps: 121, steps per second: 29, episode reward: -0.734, mean reward: -0.006 [-0.021, 0.009], mean action: 0.078 [-1.203, 1.213], mean observation: 0.011 [-21.304, 19.148], loss: --, mean_squared_error: --, mean_q: --\n",
      "  605/2000: episode: 5, duration: 4.157s, episode steps: 121, steps per second: 29, episode reward: -0.744, mean reward: -0.006 [-0.021, 0.009], mean action: 0.071 [-1.188, 1.117], mean observation: 0.009 [-28.612, 18.836], loss: --, mean_squared_error: --, mean_q: --\n",
      "  726/2000: episode: 6, duration: 4.054s, episode steps: 121, steps per second: 30, episode reward: -0.736, mean reward: -0.006 [-0.021, 0.009], mean action: 0.091 [-1.177, 1.174], mean observation: 0.011 [-25.843, 18.964], loss: --, mean_squared_error: --, mean_q: --\n",
      "  847/2000: episode: 7, duration: 4.085s, episode steps: 121, steps per second: 30, episode reward: -0.741, mean reward: -0.006 [-0.021, 0.009], mean action: 0.111 [-1.230, 1.338], mean observation: 0.009 [-29.645, 18.968], loss: --, mean_squared_error: --, mean_q: --\n",
      "  968/2000: episode: 8, duration: 4.092s, episode steps: 121, steps per second: 30, episode reward: -0.747, mean reward: -0.006 [-0.021, 0.009], mean action: 0.139 [-1.139, 1.218], mean observation: 0.009 [-28.936, 18.919], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1089/2000: episode: 9, duration: 4.885s, episode steps: 121, steps per second: 25, episode reward: -0.747, mean reward: -0.006 [-0.021, 0.009], mean action: 0.097 [-1.226, 1.172], mean observation: 0.009 [-27.735, 18.322], loss: 0.000098, mean_squared_error: 0.000197, mean_q: 0.384339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1208/2000: episode: 10, duration: 5.002s, episode steps: 119, steps per second: 24, episode reward: -0.739, mean reward: -0.006 [-0.021, 0.009], mean action: 0.087 [-1.166, 1.137], mean observation: 0.011 [-20.896, 19.055], loss: 0.000088, mean_squared_error: 0.000176, mean_q: 0.388373\n",
      " 1324/2000: episode: 11, duration: 5.231s, episode steps: 116, steps per second: 22, episode reward: -0.706, mean reward: -0.006 [-0.021, 0.009], mean action: 0.130 [-1.126, 1.214], mean observation: 0.015 [-23.853, 19.110], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.385710\n",
      " 1440/2000: episode: 12, duration: 4.995s, episode steps: 116, steps per second: 23, episode reward: -0.697, mean reward: -0.006 [-0.020, 0.008], mean action: 0.005 [-1.240, 1.200], mean observation: 0.016 [-25.110, 18.995], loss: 0.000130, mean_squared_error: 0.000261, mean_q: 0.391988\n",
      " 1558/2000: episode: 13, duration: 5.417s, episode steps: 118, steps per second: 22, episode reward: -0.716, mean reward: -0.006 [-0.021, 0.008], mean action: 0.001 [-1.209, 1.174], mean observation: 0.012 [-29.943, 19.076], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.378744\n",
      " 1684/2000: episode: 14, duration: 5.755s, episode steps: 126, steps per second: 22, episode reward: -0.764, mean reward: -0.006 [-0.021, 0.009], mean action: 0.020 [-1.157, 1.131], mean observation: 0.011 [-27.811, 18.495], loss: 0.000101, mean_squared_error: 0.000201, mean_q: 0.387326\n",
      " 1826/2000: episode: 15, duration: 6.714s, episode steps: 142, steps per second: 21, episode reward: -0.801, mean reward: -0.006 [-0.021, 0.010], mean action: 0.012 [-1.241, 1.216], mean observation: 0.015 [-23.963, 19.046], loss: 0.000170, mean_squared_error: 0.000339, mean_q: 0.386049\n",
      " 1965/2000: episode: 16, duration: 6.362s, episode steps: 139, steps per second: 22, episode reward: -0.785, mean reward: -0.006 [-0.021, 0.010], mean action: -0.003 [-1.187, 1.181], mean observation: 0.018 [-17.501, 18.547], loss: 0.000095, mean_squared_error: 0.000189, mean_q: 0.391512\n",
      "done, took 79.416 seconds\n",
      "\n",
      "\n",
      "iteration: 214\n",
      "Training for 2000 steps ...\n",
      "  131/2000: episode: 1, duration: 4.699s, episode steps: 131, steps per second: 28, episode reward: -0.789, mean reward: -0.006 [-0.020, 0.010], mean action: -0.060 [-1.150, 1.126], mean observation: 0.013 [-24.180, 18.706], loss: --, mean_squared_error: --, mean_q: --\n",
      "  261/2000: episode: 2, duration: 4.941s, episode steps: 130, steps per second: 26, episode reward: -0.803, mean reward: -0.006 [-0.021, 0.010], mean action: -0.075 [-1.301, 1.101], mean observation: 0.009 [-40.792, 18.671], loss: --, mean_squared_error: --, mean_q: --\n",
      "  391/2000: episode: 3, duration: 4.577s, episode steps: 130, steps per second: 28, episode reward: -0.789, mean reward: -0.006 [-0.020, 0.010], mean action: -0.070 [-1.231, 1.268], mean observation: 0.012 [-24.784, 18.463], loss: --, mean_squared_error: --, mean_q: --\n",
      "  518/2000: episode: 4, duration: 4.723s, episode steps: 127, steps per second: 27, episode reward: -0.737, mean reward: -0.006 [-0.020, 0.010], mean action: -0.067 [-1.133, 1.179], mean observation: 0.015 [-20.001, 18.697], loss: --, mean_squared_error: --, mean_q: --\n",
      "  648/2000: episode: 5, duration: 4.834s, episode steps: 130, steps per second: 27, episode reward: -0.782, mean reward: -0.006 [-0.020, 0.010], mean action: -0.064 [-1.239, 1.152], mean observation: 0.009 [-34.361, 18.579], loss: --, mean_squared_error: --, mean_q: --\n",
      "  779/2000: episode: 6, duration: 4.864s, episode steps: 131, steps per second: 27, episode reward: -0.798, mean reward: -0.006 [-0.021, 0.010], mean action: -0.051 [-1.220, 1.236], mean observation: 0.017 [-25.356, 18.486], loss: --, mean_squared_error: --, mean_q: --\n",
      "  909/2000: episode: 7, duration: 4.860s, episode steps: 130, steps per second: 27, episode reward: -0.787, mean reward: -0.006 [-0.020, 0.010], mean action: -0.039 [-1.077, 1.195], mean observation: 0.008 [-33.008, 18.797], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1040/2000: episode: 8, duration: 5.366s, episode steps: 131, steps per second: 24, episode reward: -0.808, mean reward: -0.006 [-0.021, 0.010], mean action: -0.079 [-1.242, 1.092], mean observation: 0.013 [-41.742, 18.846], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.358629\n",
      " 1169/2000: episode: 9, duration: 6.008s, episode steps: 129, steps per second: 21, episode reward: -0.784, mean reward: -0.006 [-0.021, 0.010], mean action: -0.074 [-1.197, 1.086], mean observation: 0.010 [-43.228, 18.443], loss: 0.000171, mean_squared_error: 0.000343, mean_q: 0.381629\n",
      " 1304/2000: episode: 10, duration: 5.653s, episode steps: 135, steps per second: 24, episode reward: -0.775, mean reward: -0.006 [-0.020, 0.010], mean action: -0.042 [-1.213, 1.251], mean observation: 0.021 [-18.888, 18.869], loss: 0.000219, mean_squared_error: 0.000437, mean_q: 0.375751\n",
      " 1440/2000: episode: 11, duration: 6.279s, episode steps: 136, steps per second: 22, episode reward: -0.802, mean reward: -0.006 [-0.021, 0.010], mean action: -0.050 [-1.311, 1.156], mean observation: 0.016 [-16.793, 18.409], loss: 0.000130, mean_squared_error: 0.000261, mean_q: 0.373070\n",
      " 1577/2000: episode: 12, duration: 6.177s, episode steps: 137, steps per second: 22, episode reward: -0.793, mean reward: -0.006 [-0.020, 0.010], mean action: -0.024 [-1.159, 1.148], mean observation: 0.019 [-21.250, 19.691], loss: 0.000282, mean_squared_error: 0.000564, mean_q: 0.375427\n",
      " 1721/2000: episode: 13, duration: 6.112s, episode steps: 144, steps per second: 24, episode reward: -0.801, mean reward: -0.006 [-0.021, 0.010], mean action: 0.006 [-1.150, 1.267], mean observation: 0.036 [-20.044, 19.852], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.367270\n",
      "done, took 79.551 seconds\n",
      "\n",
      "\n",
      "iteration: 215\n",
      "Training for 2000 steps ...\n",
      "  360/2000: episode: 1, duration: 10.768s, episode steps: 360, steps per second: 33, episode reward: -0.833, mean reward: -0.002 [-0.018, 0.009], mean action: -0.014 [-1.350, 1.243], mean observation: 0.086 [-15.946, 20.141], loss: --, mean_squared_error: --, mean_q: --\n",
      "  716/2000: episode: 2, duration: 10.383s, episode steps: 356, steps per second: 34, episode reward: -0.823, mean reward: -0.002 [-0.017, 0.009], mean action: -0.033 [-1.243, 1.156], mean observation: 0.079 [-28.993, 19.719], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1068/2000: episode: 3, duration: 10.356s, episode steps: 352, steps per second: 34, episode reward: -0.829, mean reward: -0.002 [-0.018, 0.009], mean action: -0.028 [-1.278, 1.295], mean observation: 0.087 [-9.898, 19.879], loss: 0.000213, mean_squared_error: 0.000426, mean_q: 0.372679\n",
      " 1442/2000: episode: 4, duration: 18.216s, episode steps: 374, steps per second: 21, episode reward: 0.839, mean reward: 0.002 [-0.002, 0.013], mean action: 0.031 [-1.285, 1.302], mean observation: 0.120 [-10.032, 20.485], loss: 0.000195, mean_squared_error: 0.000389, mean_q: 0.368085\n",
      " 1691/2000: episode: 5, duration: 13.484s, episode steps: 249, steps per second: 18, episode reward: 0.894, mean reward: 0.004 [-0.002, 0.014], mean action: 0.070 [-1.314, 1.307], mean observation: 0.120 [-56.482, 20.159], loss: 0.000160, mean_squared_error: 0.000319, mean_q: 0.367001\n",
      " 1949/2000: episode: 6, duration: 13.982s, episode steps: 258, steps per second: 18, episode reward: 0.857, mean reward: 0.003 [-0.002, 0.015], mean action: -0.040 [-1.351, 1.281], mean observation: 0.111 [-38.413, 19.788], loss: 0.000285, mean_squared_error: 0.000571, mean_q: 0.369041\n",
      "done, took 80.191 seconds\n",
      "\n",
      "\n",
      "iteration: 216\n",
      "Training for 2000 steps ...\n",
      "  434/2000: episode: 1, duration: 19.632s, episode steps: 434, steps per second: 22, episode reward: 0.746, mean reward: 0.002 [-0.002, 0.012], mean action: -0.053 [-1.303, 1.479], mean observation: 0.108 [-22.257, 20.042], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1319/2000: episode: 2, duration: 36.726s, episode steps: 885, steps per second: 24, episode reward: 0.752, mean reward: 0.001 [-0.002, 0.013], mean action: -0.098 [-1.424, 1.617], mean observation: 0.109 [-21.590, 19.938], loss: 0.000150, mean_squared_error: 0.000301, mean_q: 0.369716\n",
      " 1550/2000: episode: 3, duration: 13.591s, episode steps: 231, steps per second: 17, episode reward: 0.789, mean reward: 0.003 [-0.002, 0.017], mean action: -0.007 [-1.216, 1.262], mean observation: 0.109 [-20.768, 20.088], loss: 0.000149, mean_squared_error: 0.000299, mean_q: 0.374083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1778/2000: episode: 4, duration: 13.051s, episode steps: 228, steps per second: 17, episode reward: 0.719, mean reward: 0.003 [-0.002, 0.013], mean action: 0.056 [-1.370, 1.339], mean observation: 0.112 [-10.408, 19.795], loss: 0.000164, mean_squared_error: 0.000328, mean_q: 0.370824\n",
      " 1985/2000: episode: 5, duration: 12.288s, episode steps: 207, steps per second: 17, episode reward: 0.642, mean reward: 0.003 [-0.002, 0.011], mean action: -0.020 [-1.277, 1.221], mean observation: 0.117 [-16.212, 20.067], loss: 0.000204, mean_squared_error: 0.000407, mean_q: 0.374353\n",
      "done, took 96.440 seconds\n",
      "\n",
      "\n",
      "iteration: 217\n",
      "Training for 2000 steps ...\n",
      "  194/2000: episode: 1, duration: 9.596s, episode steps: 194, steps per second: 20, episode reward: 0.588, mean reward: 0.003 [-0.001, 0.011], mean action: 0.015 [-1.349, 1.298], mean observation: 0.115 [-12.388, 19.729], loss: --, mean_squared_error: --, mean_q: --\n",
      "  378/2000: episode: 2, duration: 8.985s, episode steps: 184, steps per second: 20, episode reward: 0.591, mean reward: 0.003 [-0.001, 0.011], mean action: 0.026 [-1.171, 1.246], mean observation: 0.119 [-10.934, 19.709], loss: --, mean_squared_error: --, mean_q: --\n",
      "  570/2000: episode: 3, duration: 9.643s, episode steps: 192, steps per second: 20, episode reward: 0.582, mean reward: 0.003 [-0.001, 0.011], mean action: 0.007 [-1.278, 1.206], mean observation: 0.117 [-10.882, 19.760], loss: --, mean_squared_error: --, mean_q: --\n",
      "  767/2000: episode: 4, duration: 9.744s, episode steps: 197, steps per second: 20, episode reward: 0.594, mean reward: 0.003 [-0.001, 0.011], mean action: 0.050 [-1.202, 1.325], mean observation: 0.117 [-10.460, 19.955], loss: --, mean_squared_error: --, mean_q: --\n",
      "  960/2000: episode: 5, duration: 9.449s, episode steps: 193, steps per second: 20, episode reward: 0.578, mean reward: 0.003 [-0.001, 0.011], mean action: 0.067 [-1.198, 1.245], mean observation: 0.118 [-10.695, 19.991], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1153/2000: episode: 6, duration: 10.456s, episode steps: 193, steps per second: 18, episode reward: 0.532, mean reward: 0.003 [-0.001, 0.011], mean action: -0.002 [-1.208, 1.337], mean observation: 0.120 [-10.379, 19.990], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.373577\n",
      " 1374/2000: episode: 7, duration: 12.074s, episode steps: 221, steps per second: 18, episode reward: 0.497, mean reward: 0.002 [-0.002, 0.011], mean action: -0.023 [-1.188, 1.189], mean observation: 0.115 [-10.694, 19.809], loss: 0.000134, mean_squared_error: 0.000267, mean_q: 0.373268\n",
      " 1603/2000: episode: 8, duration: 12.073s, episode steps: 229, steps per second: 19, episode reward: 0.523, mean reward: 0.002 [-0.002, 0.011], mean action: -0.104 [-1.198, 1.170], mean observation: 0.116 [-18.403, 19.557], loss: 0.000266, mean_squared_error: 0.000532, mean_q: 0.374485\n",
      " 1826/2000: episode: 9, duration: 11.846s, episode steps: 223, steps per second: 19, episode reward: 0.551, mean reward: 0.002 [-0.002, 0.011], mean action: -0.141 [-1.233, 1.241], mean observation: 0.119 [-15.537, 19.146], loss: 0.000135, mean_squared_error: 0.000270, mean_q: 0.376123\n",
      "done, took 103.447 seconds\n",
      "\n",
      "\n",
      "iteration: 218\n",
      "Training for 2000 steps ...\n",
      "  181/2000: episode: 1, duration: 8.085s, episode steps: 181, steps per second: 22, episode reward: 0.505, mean reward: 0.003 [-0.002, 0.011], mean action: -0.189 [-1.229, 1.263], mean observation: 0.115 [-15.580, 18.639], loss: --, mean_squared_error: --, mean_q: --\n",
      "  357/2000: episode: 2, duration: 7.964s, episode steps: 176, steps per second: 22, episode reward: 0.505, mean reward: 0.003 [-0.002, 0.012], mean action: -0.165 [-1.228, 1.251], mean observation: 0.115 [-15.517, 18.491], loss: --, mean_squared_error: --, mean_q: --\n",
      "  533/2000: episode: 3, duration: 7.889s, episode steps: 176, steps per second: 22, episode reward: 0.513, mean reward: 0.003 [-0.002, 0.012], mean action: -0.163 [-1.230, 1.226], mean observation: 0.116 [-15.563, 18.538], loss: --, mean_squared_error: --, mean_q: --\n",
      "  707/2000: episode: 4, duration: 7.868s, episode steps: 174, steps per second: 22, episode reward: 0.483, mean reward: 0.003 [-0.002, 0.011], mean action: -0.197 [-1.334, 1.178], mean observation: 0.117 [-15.315, 18.460], loss: --, mean_squared_error: --, mean_q: --\n",
      "  883/2000: episode: 5, duration: 7.892s, episode steps: 176, steps per second: 22, episode reward: 0.525, mean reward: 0.003 [-0.002, 0.012], mean action: -0.167 [-1.413, 1.276], mean observation: 0.115 [-15.030, 18.341], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1057/2000: episode: 6, duration: 8.544s, episode steps: 174, steps per second: 20, episode reward: 0.504, mean reward: 0.003 [-0.002, 0.011], mean action: -0.130 [-1.189, 1.203], mean observation: 0.116 [-15.527, 18.472], loss: 0.000150, mean_squared_error: 0.000299, mean_q: 0.370122\n",
      " 1278/2000: episode: 7, duration: 11.348s, episode steps: 221, steps per second: 19, episode reward: 0.637, mean reward: 0.003 [-0.002, 0.010], mean action: -0.159 [-1.297, 1.265], mean observation: 0.112 [-15.291, 18.317], loss: 0.000104, mean_squared_error: 0.000209, mean_q: 0.370834\n",
      " 1501/2000: episode: 8, duration: 12.072s, episode steps: 223, steps per second: 18, episode reward: 0.726, mean reward: 0.003 [-0.002, 0.012], mean action: -0.043 [-1.173, 1.319], mean observation: 0.114 [-19.053, 14.718], loss: 0.000206, mean_squared_error: 0.000412, mean_q: 0.379225\n",
      " 1731/2000: episode: 9, duration: 12.004s, episode steps: 230, steps per second: 19, episode reward: 0.854, mean reward: 0.004 [-0.001, 0.014], mean action: 0.025 [-1.239, 1.274], mean observation: 0.135 [-45.762, 18.017], loss: 0.000280, mean_squared_error: 0.000560, mean_q: 0.379909\n",
      " 1933/2000: episode: 10, duration: 11.764s, episode steps: 202, steps per second: 17, episode reward: 0.785, mean reward: 0.004 [-0.003, 0.015], mean action: -0.103 [-1.251, 1.123], mean observation: 0.141 [-16.537, 17.224], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.375225\n",
      "done, took 99.332 seconds\n",
      "\n",
      "\n",
      "iteration: 219\n",
      "Training for 2000 steps ...\n",
      "  182/2000: episode: 1, duration: 9.185s, episode steps: 182, steps per second: 20, episode reward: 0.759, mean reward: 0.004 [-0.003, 0.015], mean action: -0.147 [-1.260, 1.149], mean observation: 0.144 [-24.995, 19.811], loss: --, mean_squared_error: --, mean_q: --\n",
      "  370/2000: episode: 2, duration: 9.462s, episode steps: 188, steps per second: 20, episode reward: 0.790, mean reward: 0.004 [-0.003, 0.015], mean action: -0.127 [-1.290, 1.184], mean observation: 0.147 [-36.421, 20.046], loss: --, mean_squared_error: --, mean_q: --\n",
      "  554/2000: episode: 3, duration: 9.230s, episode steps: 184, steps per second: 20, episode reward: 0.768, mean reward: 0.004 [-0.003, 0.015], mean action: -0.121 [-1.220, 1.288], mean observation: 0.144 [-22.341, 20.030], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 4, duration: 9.337s, episode steps: 193, steps per second: 21, episode reward: 0.775, mean reward: 0.004 [-0.003, 0.016], mean action: -0.096 [-1.202, 1.332], mean observation: 0.142 [-15.384, 19.918], loss: --, mean_squared_error: --, mean_q: --\n",
      "  928/2000: episode: 5, duration: 8.854s, episode steps: 181, steps per second: 20, episode reward: 0.771, mean reward: 0.004 [-0.003, 0.015], mean action: -0.114 [-1.139, 1.246], mean observation: 0.145 [-17.757, 20.024], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1111/2000: episode: 6, duration: 10.119s, episode steps: 183, steps per second: 18, episode reward: 0.752, mean reward: 0.004 [-0.003, 0.014], mean action: -0.134 [-1.331, 1.156], mean observation: 0.146 [-16.989, 20.143], loss: 0.000271, mean_squared_error: 0.000543, mean_q: 0.371589\n",
      " 1295/2000: episode: 7, duration: 11.121s, episode steps: 184, steps per second: 17, episode reward: 0.719, mean reward: 0.004 [-0.003, 0.014], mean action: -0.177 [-1.200, 1.261], mean observation: 0.141 [-31.528, 18.317], loss: 0.000203, mean_squared_error: 0.000405, mean_q: 0.379856\n",
      " 1495/2000: episode: 8, duration: 11.672s, episode steps: 200, steps per second: 17, episode reward: 0.727, mean reward: 0.004 [-0.003, 0.014], mean action: -0.186 [-1.211, 1.173], mean observation: 0.145 [-13.348, 15.955], loss: 0.000217, mean_squared_error: 0.000434, mean_q: 0.379081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1683/2000: episode: 9, duration: 11.215s, episode steps: 188, steps per second: 17, episode reward: 0.731, mean reward: 0.004 [-0.003, 0.013], mean action: -0.218 [-1.278, 1.142], mean observation: 0.146 [-20.568, 18.279], loss: 0.000230, mean_squared_error: 0.000459, mean_q: 0.386691\n",
      " 1964/2000: episode: 10, duration: 16.479s, episode steps: 281, steps per second: 17, episode reward: 0.735, mean reward: 0.003 [-0.001, 0.013], mean action: -0.164 [-1.234, 1.265], mean observation: 0.128 [-25.064, 14.832], loss: 0.000133, mean_squared_error: 0.000265, mean_q: 0.377213\n",
      "done, took 108.932 seconds\n",
      "\n",
      "\n",
      "iteration: 220\n",
      "Training for 2000 steps ...\n",
      "  234/2000: episode: 1, duration: 12.025s, episode steps: 234, steps per second: 19, episode reward: 0.728, mean reward: 0.003 [-0.002, 0.013], mean action: -0.163 [-1.159, 1.210], mean observation: 0.125 [-31.552, 14.978], loss: --, mean_squared_error: --, mean_q: --\n",
      "  469/2000: episode: 2, duration: 11.894s, episode steps: 235, steps per second: 20, episode reward: 0.726, mean reward: 0.003 [-0.002, 0.013], mean action: -0.231 [-1.248, 1.289], mean observation: 0.122 [-12.844, 14.907], loss: --, mean_squared_error: --, mean_q: --\n",
      "  697/2000: episode: 3, duration: 11.330s, episode steps: 228, steps per second: 20, episode reward: 0.729, mean reward: 0.003 [-0.002, 0.013], mean action: -0.248 [-1.189, 1.199], mean observation: 0.130 [-15.998, 14.965], loss: --, mean_squared_error: --, mean_q: --\n",
      "  931/2000: episode: 4, duration: 11.911s, episode steps: 234, steps per second: 20, episode reward: 0.743, mean reward: 0.003 [-0.002, 0.013], mean action: -0.200 [-1.291, 1.246], mean observation: 0.134 [-20.510, 15.108], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1159/2000: episode: 5, duration: 13.141s, episode steps: 228, steps per second: 17, episode reward: 0.728, mean reward: 0.003 [-0.002, 0.013], mean action: -0.207 [-1.175, 1.295], mean observation: 0.120 [-34.121, 15.055], loss: 0.000129, mean_squared_error: 0.000257, mean_q: 0.381104\n",
      " 1367/2000: episode: 6, duration: 12.362s, episode steps: 208, steps per second: 17, episode reward: 0.724, mean reward: 0.003 [-0.003, 0.013], mean action: -0.218 [-1.271, 1.190], mean observation: 0.126 [-14.404, 15.184], loss: 0.000166, mean_squared_error: 0.000333, mean_q: 0.380901\n",
      " 1595/2000: episode: 7, duration: 13.303s, episode steps: 228, steps per second: 17, episode reward: 0.702, mean reward: 0.003 [-0.002, 0.013], mean action: -0.059 [-1.246, 1.281], mean observation: 0.117 [-11.945, 15.106], loss: 0.000116, mean_squared_error: 0.000233, mean_q: 0.382583\n",
      " 1836/2000: episode: 8, duration: 14.568s, episode steps: 241, steps per second: 17, episode reward: 0.699, mean reward: 0.003 [-0.001, 0.013], mean action: 0.035 [-1.204, 1.124], mean observation: 0.126 [-10.397, 14.903], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.381475\n",
      "done, took 110.229 seconds\n",
      "\n",
      "\n",
      "iteration: 221\n",
      "Training for 2000 steps ...\n",
      "  239/2000: episode: 1, duration: 11.861s, episode steps: 239, steps per second: 20, episode reward: 0.685, mean reward: 0.003 [-0.001, 0.013], mean action: -0.090 [-1.302, 1.217], mean observation: 0.130 [-12.013, 14.614], loss: --, mean_squared_error: --, mean_q: --\n",
      "  463/2000: episode: 2, duration: 10.837s, episode steps: 224, steps per second: 21, episode reward: 0.675, mean reward: 0.003 [-0.001, 0.013], mean action: -0.063 [-1.195, 1.413], mean observation: 0.125 [-10.459, 15.030], loss: --, mean_squared_error: --, mean_q: --\n",
      "  700/2000: episode: 3, duration: 11.498s, episode steps: 237, steps per second: 21, episode reward: 0.702, mean reward: 0.003 [-0.002, 0.014], mean action: -0.085 [-1.212, 1.201], mean observation: 0.130 [-25.380, 14.902], loss: --, mean_squared_error: --, mean_q: --\n",
      "  931/2000: episode: 4, duration: 11.374s, episode steps: 231, steps per second: 20, episode reward: 0.694, mean reward: 0.003 [-0.001, 0.014], mean action: -0.036 [-1.154, 1.214], mean observation: 0.132 [-10.498, 15.156], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1154/2000: episode: 5, duration: 12.695s, episode steps: 223, steps per second: 18, episode reward: 0.695, mean reward: 0.003 [-0.001, 0.014], mean action: -0.054 [-1.190, 1.224], mean observation: 0.130 [-17.459, 14.532], loss: 0.000210, mean_squared_error: 0.000420, mean_q: 0.384219\n",
      " 1374/2000: episode: 6, duration: 13.310s, episode steps: 220, steps per second: 17, episode reward: 0.705, mean reward: 0.003 [-0.002, 0.014], mean action: -0.057 [-1.336, 1.204], mean observation: 0.132 [-10.316, 14.678], loss: 0.000152, mean_squared_error: 0.000303, mean_q: 0.390766\n",
      " 1604/2000: episode: 7, duration: 13.593s, episode steps: 230, steps per second: 17, episode reward: 0.732, mean reward: 0.003 [-0.001, 0.014], mean action: -0.052 [-1.215, 1.197], mean observation: 0.138 [-10.445, 15.055], loss: 0.000351, mean_squared_error: 0.000703, mean_q: 0.379176\n",
      " 1853/2000: episode: 8, duration: 14.776s, episode steps: 249, steps per second: 17, episode reward: 0.737, mean reward: 0.003 [-0.002, 0.014], mean action: -0.151 [-1.221, 1.271], mean observation: 0.135 [-18.752, 15.858], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.387974\n",
      "done, took 108.820 seconds\n",
      "\n",
      "\n",
      "iteration: 222\n",
      "Training for 2000 steps ...\n",
      "  252/2000: episode: 1, duration: 12.507s, episode steps: 252, steps per second: 20, episode reward: 0.716, mean reward: 0.003 [-0.001, 0.014], mean action: -0.082 [-1.354, 1.325], mean observation: 0.126 [-23.697, 17.013], loss: --, mean_squared_error: --, mean_q: --\n",
      "  475/2000: episode: 2, duration: 10.860s, episode steps: 223, steps per second: 21, episode reward: 0.700, mean reward: 0.003 [-0.002, 0.014], mean action: -0.078 [-1.178, 1.192], mean observation: 0.126 [-21.372, 16.373], loss: --, mean_squared_error: --, mean_q: --\n",
      "  698/2000: episode: 3, duration: 10.849s, episode steps: 223, steps per second: 21, episode reward: 0.703, mean reward: 0.003 [-0.001, 0.013], mean action: -0.085 [-1.276, 1.285], mean observation: 0.127 [-24.217, 17.129], loss: --, mean_squared_error: --, mean_q: --\n",
      "  955/2000: episode: 4, duration: 12.871s, episode steps: 257, steps per second: 20, episode reward: 0.721, mean reward: 0.003 [-0.001, 0.014], mean action: -0.081 [-1.192, 1.246], mean observation: 0.126 [-23.822, 17.009], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1204/2000: episode: 5, duration: 13.925s, episode steps: 249, steps per second: 18, episode reward: 0.708, mean reward: 0.003 [-0.002, 0.013], mean action: -0.137 [-1.287, 1.138], mean observation: 0.122 [-23.145, 18.409], loss: 0.000210, mean_squared_error: 0.000419, mean_q: 0.376708\n",
      " 1497/2000: episode: 6, duration: 16.716s, episode steps: 293, steps per second: 18, episode reward: 0.729, mean reward: 0.002 [-0.002, 0.013], mean action: -0.070 [-1.204, 1.224], mean observation: 0.114 [-20.696, 16.563], loss: 0.000147, mean_squared_error: 0.000293, mean_q: 0.386140\n",
      " 1772/2000: episode: 7, duration: 15.834s, episode steps: 275, steps per second: 17, episode reward: 0.718, mean reward: 0.003 [-0.002, 0.013], mean action: -0.115 [-1.300, 1.237], mean observation: 0.112 [-23.859, 17.052], loss: 0.000404, mean_squared_error: 0.000809, mean_q: 0.391052\n",
      "done, took 106.737 seconds\n",
      "\n",
      "\n",
      "iteration: 223\n",
      "Training for 2000 steps ...\n",
      "  245/2000: episode: 1, duration: 11.509s, episode steps: 245, steps per second: 21, episode reward: 0.736, mean reward: 0.003 [-0.001, 0.013], mean action: -0.096 [-1.237, 1.179], mean observation: 0.115 [-22.045, 16.958], loss: --, mean_squared_error: --, mean_q: --\n",
      "  494/2000: episode: 2, duration: 11.364s, episode steps: 249, steps per second: 22, episode reward: 0.759, mean reward: 0.003 [-0.001, 0.013], mean action: -0.103 [-1.315, 1.360], mean observation: 0.116 [-21.656, 16.767], loss: --, mean_squared_error: --, mean_q: --\n",
      "  752/2000: episode: 3, duration: 12.092s, episode steps: 258, steps per second: 21, episode reward: 0.761, mean reward: 0.003 [-0.002, 0.013], mean action: -0.154 [-1.320, 1.153], mean observation: 0.115 [-20.897, 16.519], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1013/2000: episode: 4, duration: 11.753s, episode steps: 261, steps per second: 22, episode reward: 0.776, mean reward: 0.003 [-0.002, 0.013], mean action: -0.120 [-1.234, 1.235], mean observation: 0.116 [-21.934, 16.910], loss: 0.000091, mean_squared_error: 0.000182, mean_q: 0.380311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1270/2000: episode: 5, duration: 14.389s, episode steps: 257, steps per second: 18, episode reward: 0.835, mean reward: 0.003 [-0.002, 0.014], mean action: -0.092 [-1.302, 1.285], mean observation: 0.127 [-21.495, 16.738], loss: 0.000333, mean_squared_error: 0.000667, mean_q: 0.389697\n",
      " 1523/2000: episode: 6, duration: 13.793s, episode steps: 253, steps per second: 18, episode reward: 0.757, mean reward: 0.003 [-0.002, 0.013], mean action: -0.040 [-1.257, 1.250], mean observation: 0.117 [-21.926, 16.846], loss: 0.000164, mean_squared_error: 0.000327, mean_q: 0.382835\n",
      " 1772/2000: episode: 7, duration: 13.949s, episode steps: 249, steps per second: 18, episode reward: 0.739, mean reward: 0.003 [-0.002, 0.013], mean action: -0.042 [-1.254, 1.154], mean observation: 0.113 [-20.247, 16.275], loss: 0.000153, mean_squared_error: 0.000306, mean_q: 0.382901\n",
      "done, took 101.245 seconds\n",
      "\n",
      "\n",
      "iteration: 224\n",
      "Training for 2000 steps ...\n",
      "  271/2000: episode: 1, duration: 12.360s, episode steps: 271, steps per second: 22, episode reward: 0.772, mean reward: 0.003 [-0.001, 0.013], mean action: -0.061 [-1.330, 1.177], mean observation: 0.122 [-21.377, 16.639], loss: --, mean_squared_error: --, mean_q: --\n",
      "  534/2000: episode: 2, duration: 12.225s, episode steps: 263, steps per second: 22, episode reward: 0.757, mean reward: 0.003 [-0.001, 0.013], mean action: -0.061 [-1.315, 1.331], mean observation: 0.118 [-20.679, 16.494], loss: --, mean_squared_error: --, mean_q: --\n",
      "  795/2000: episode: 3, duration: 12.141s, episode steps: 261, steps per second: 21, episode reward: 0.752, mean reward: 0.003 [-0.001, 0.013], mean action: -0.071 [-1.257, 1.280], mean observation: 0.118 [-22.627, 17.053], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1049/2000: episode: 4, duration: 12.322s, episode steps: 254, steps per second: 21, episode reward: 0.732, mean reward: 0.003 [-0.001, 0.013], mean action: -0.049 [-1.282, 1.447], mean observation: 0.115 [-21.121, 16.631], loss: 0.000368, mean_squared_error: 0.000736, mean_q: 0.384973\n",
      " 1298/2000: episode: 5, duration: 13.786s, episode steps: 249, steps per second: 18, episode reward: 0.738, mean reward: 0.003 [-0.002, 0.013], mean action: -0.057 [-1.377, 1.248], mean observation: 0.116 [-22.042, 16.811], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.383293\n",
      " 1556/2000: episode: 6, duration: 14.780s, episode steps: 258, steps per second: 17, episode reward: 0.734, mean reward: 0.003 [-0.002, 0.013], mean action: -0.055 [-1.200, 1.237], mean observation: 0.113 [-21.265, 16.649], loss: 0.000483, mean_squared_error: 0.000966, mean_q: 0.386919\n",
      "done, took 101.177 seconds\n",
      "\n",
      "\n",
      "iteration: 225\n",
      "Training for 2000 steps ...\n",
      "  311/2000: episode: 1, duration: 13.123s, episode steps: 311, steps per second: 24, episode reward: 0.926, mean reward: 0.003 [-0.001, 0.015], mean action: -0.077 [-1.313, 1.283], mean observation: 0.136 [-23.239, 17.700], loss: --, mean_squared_error: --, mean_q: --\n",
      "  628/2000: episode: 2, duration: 13.064s, episode steps: 317, steps per second: 24, episode reward: 0.921, mean reward: 0.003 [-0.001, 0.014], mean action: -0.145 [-1.317, 1.293], mean observation: 0.139 [-22.828, 17.481], loss: --, mean_squared_error: --, mean_q: --\n",
      "  918/2000: episode: 3, duration: 12.267s, episode steps: 290, steps per second: 24, episode reward: 0.874, mean reward: 0.003 [-0.001, 0.015], mean action: -0.126 [-1.258, 1.330], mean observation: 0.131 [-23.641, 17.799], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1186/2000: episode: 4, duration: 13.256s, episode steps: 268, steps per second: 20, episode reward: 0.833, mean reward: 0.003 [-0.001, 0.014], mean action: -0.185 [-1.247, 1.197], mean observation: 0.126 [-23.131, 17.685], loss: 0.000152, mean_squared_error: 0.000303, mean_q: 0.395357\n",
      " 1411/2000: episode: 5, duration: 11.718s, episode steps: 225, steps per second: 19, episode reward: 0.802, mean reward: 0.004 [-0.002, 0.013], mean action: -0.163 [-1.302, 1.266], mean observation: 0.123 [-23.074, 18.042], loss: 0.000184, mean_squared_error: 0.000368, mean_q: 0.393979\n",
      " 1696/2000: episode: 6, duration: 14.599s, episode steps: 285, steps per second: 20, episode reward: 0.948, mean reward: 0.003 [-0.002, 0.016], mean action: -0.163 [-1.256, 1.358], mean observation: 0.147 [-21.841, 17.544], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.391976\n",
      " 1934/2000: episode: 7, duration: 12.767s, episode steps: 238, steps per second: 19, episode reward: 0.860, mean reward: 0.004 [-0.002, 0.014], mean action: -0.165 [-1.201, 1.220], mean observation: 0.125 [-23.521, 18.225], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.387423\n",
      "done, took 94.594 seconds\n",
      "\n",
      "\n",
      "iteration: 226\n",
      "Training for 2000 steps ...\n",
      "  221/2000: episode: 1, duration: 9.777s, episode steps: 221, steps per second: 23, episode reward: 0.800, mean reward: 0.004 [-0.002, 0.013], mean action: -0.188 [-1.197, 1.359], mean observation: 0.116 [-24.188, 18.506], loss: --, mean_squared_error: --, mean_q: --\n",
      "  443/2000: episode: 2, duration: 9.329s, episode steps: 222, steps per second: 24, episode reward: 0.778, mean reward: 0.004 [-0.002, 0.014], mean action: -0.203 [-1.212, 1.167], mean observation: 0.115 [-23.695, 18.381], loss: --, mean_squared_error: --, mean_q: --\n",
      "  660/2000: episode: 3, duration: 9.249s, episode steps: 217, steps per second: 23, episode reward: 0.779, mean reward: 0.004 [-0.002, 0.013], mean action: -0.204 [-1.181, 1.242], mean observation: 0.113 [-24.080, 18.545], loss: --, mean_squared_error: --, mean_q: --\n",
      "  886/2000: episode: 4, duration: 9.795s, episode steps: 226, steps per second: 23, episode reward: 0.793, mean reward: 0.004 [-0.002, 0.013], mean action: -0.230 [-1.254, 1.160], mean observation: 0.117 [-24.001, 18.532], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1098/2000: episode: 5, duration: 9.984s, episode steps: 212, steps per second: 21, episode reward: 0.786, mean reward: 0.004 [-0.002, 0.013], mean action: -0.225 [-1.185, 1.153], mean observation: 0.113 [-23.780, 18.361], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.393322\n",
      " 1318/2000: episode: 6, duration: 11.544s, episode steps: 220, steps per second: 19, episode reward: 0.812, mean reward: 0.004 [-0.002, 0.013], mean action: -0.143 [-1.176, 1.367], mean observation: 0.120 [-24.184, 18.643], loss: 0.000175, mean_squared_error: 0.000350, mean_q: 0.389760\n",
      " 1539/2000: episode: 7, duration: 11.264s, episode steps: 221, steps per second: 20, episode reward: 0.771, mean reward: 0.003 [-0.002, 0.014], mean action: -0.206 [-1.335, 1.300], mean observation: 0.113 [-24.042, 18.505], loss: 0.000151, mean_squared_error: 0.000302, mean_q: 0.386163\n",
      " 1779/2000: episode: 8, duration: 12.306s, episode steps: 240, steps per second: 20, episode reward: 0.792, mean reward: 0.003 [-0.002, 0.014], mean action: -0.151 [-1.239, 1.154], mean observation: 0.116 [-24.534, 18.669], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.396564\n",
      " 1978/2000: episode: 9, duration: 7.334s, episode steps: 199, steps per second: 27, episode reward: -0.706, mean reward: -0.004 [-0.019, 0.009], mean action: -0.030 [-1.233, 1.196], mean observation: 0.094 [-26.323, 18.620], loss: 0.000171, mean_squared_error: 0.000341, mean_q: 0.388720\n",
      "done, took 92.147 seconds\n",
      "\n",
      "\n",
      "iteration: 227\n",
      "Training for 2000 steps ...\n",
      "  160/2000: episode: 1, duration: 4.212s, episode steps: 160, steps per second: 38, episode reward: -0.685, mean reward: -0.004 [-0.019, 0.009], mean action: -0.078 [-1.141, 1.205], mean observation: 0.089 [-26.553, 19.015], loss: --, mean_squared_error: --, mean_q: --\n",
      "  319/2000: episode: 2, duration: 4.238s, episode steps: 159, steps per second: 38, episode reward: -0.683, mean reward: -0.004 [-0.019, 0.009], mean action: -0.087 [-1.229, 1.183], mean observation: 0.088 [-26.905, 18.897], loss: --, mean_squared_error: --, mean_q: --\n",
      "  478/2000: episode: 3, duration: 4.371s, episode steps: 159, steps per second: 36, episode reward: -0.691, mean reward: -0.004 [-0.019, 0.009], mean action: -0.105 [-1.274, 1.149], mean observation: 0.087 [-26.343, 18.701], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 4, duration: 4.326s, episode steps: 159, steps per second: 37, episode reward: -0.681, mean reward: -0.004 [-0.019, 0.009], mean action: -0.097 [-1.212, 1.167], mean observation: 0.088 [-26.316, 18.770], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  799/2000: episode: 5, duration: 4.192s, episode steps: 162, steps per second: 39, episode reward: -0.689, mean reward: -0.004 [-0.019, 0.009], mean action: -0.078 [-1.178, 1.201], mean observation: 0.088 [-26.492, 18.694], loss: --, mean_squared_error: --, mean_q: --\n",
      "  965/2000: episode: 6, duration: 4.416s, episode steps: 166, steps per second: 38, episode reward: -0.701, mean reward: -0.004 [-0.019, 0.009], mean action: -0.092 [-1.263, 1.218], mean observation: 0.088 [-26.223, 18.715], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1134/2000: episode: 7, duration: 5.915s, episode steps: 169, steps per second: 29, episode reward: -0.704, mean reward: -0.004 [-0.019, 0.009], mean action: -0.118 [-1.318, 1.162], mean observation: 0.085 [-25.349, 18.530], loss: 0.000155, mean_squared_error: 0.000309, mean_q: 0.395636\n",
      " 1401/2000: episode: 8, duration: 12.422s, episode steps: 267, steps per second: 21, episode reward: 0.853, mean reward: 0.003 [-0.003, 0.013], mean action: -0.120 [-1.301, 1.190], mean observation: 0.128 [-20.318, 16.539], loss: 0.000391, mean_squared_error: 0.000782, mean_q: 0.390932\n",
      " 1661/2000: episode: 9, duration: 13.357s, episode steps: 260, steps per second: 19, episode reward: 0.752, mean reward: 0.003 [-0.004, 0.013], mean action: -0.083 [-1.368, 1.303], mean observation: 0.111 [-21.408, 16.975], loss: 0.000276, mean_squared_error: 0.000552, mean_q: 0.391147\n",
      " 1901/2000: episode: 10, duration: 12.483s, episode steps: 240, steps per second: 19, episode reward: 0.804, mean reward: 0.003 [-0.004, 0.013], mean action: -0.134 [-1.298, 1.215], mean observation: 0.116 [-21.078, 16.919], loss: 0.000238, mean_squared_error: 0.000476, mean_q: 0.381210\n",
      "done, took 75.683 seconds\n",
      "\n",
      "\n",
      "iteration: 228\n",
      "Training for 2000 steps ...\n",
      "  270/2000: episode: 1, duration: 12.378s, episode steps: 270, steps per second: 22, episode reward: 0.802, mean reward: 0.003 [-0.004, 0.013], mean action: -0.131 [-1.245, 1.351], mean observation: 0.117 [-21.133, 16.889], loss: --, mean_squared_error: --, mean_q: --\n",
      "  660/2000: episode: 2, duration: 14.142s, episode steps: 390, steps per second: 28, episode reward: 0.821, mean reward: 0.002 [-0.003, 0.013], mean action: -0.153 [-1.302, 1.282], mean observation: 0.124 [-21.110, 16.924], loss: --, mean_squared_error: --, mean_q: --\n",
      "  970/2000: episode: 3, duration: 13.654s, episode steps: 310, steps per second: 23, episode reward: 0.798, mean reward: 0.003 [-0.004, 0.013], mean action: -0.155 [-1.322, 1.371], mean observation: 0.117 [-22.302, 17.336], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1222/2000: episode: 4, duration: 13.409s, episode steps: 252, steps per second: 19, episode reward: 0.814, mean reward: 0.003 [-0.003, 0.013], mean action: -0.128 [-1.304, 1.192], mean observation: 0.118 [-21.651, 17.003], loss: 0.000248, mean_squared_error: 0.000496, mean_q: 0.385379\n",
      " 1588/2000: episode: 5, duration: 17.272s, episode steps: 366, steps per second: 21, episode reward: 0.877, mean reward: 0.002 [-0.003, 0.014], mean action: -0.135 [-1.344, 1.319], mean observation: 0.128 [-19.964, 16.565], loss: 0.000299, mean_squared_error: 0.000597, mean_q: 0.390546\n",
      " 1875/2000: episode: 6, duration: 15.726s, episode steps: 287, steps per second: 18, episode reward: 0.812, mean reward: 0.003 [-0.003, 0.013], mean action: -0.071 [-1.168, 1.211], mean observation: 0.118 [-19.734, 16.452], loss: 0.000321, mean_squared_error: 0.000642, mean_q: 0.390764\n",
      "done, took 92.910 seconds\n",
      "\n",
      "\n",
      "iteration: 229\n",
      "Training for 2000 steps ...\n",
      "  272/2000: episode: 1, duration: 11.897s, episode steps: 272, steps per second: 23, episode reward: 0.828, mean reward: 0.003 [-0.004, 0.014], mean action: -0.002 [-1.189, 1.254], mean observation: 0.120 [-26.391, 16.862], loss: --, mean_squared_error: --, mean_q: --\n",
      "  566/2000: episode: 2, duration: 12.834s, episode steps: 294, steps per second: 23, episode reward: 0.855, mean reward: 0.003 [-0.003, 0.014], mean action: -0.104 [-1.239, 1.188], mean observation: 0.126 [-22.175, 17.575], loss: --, mean_squared_error: --, mean_q: --\n",
      "  852/2000: episode: 3, duration: 12.830s, episode steps: 286, steps per second: 22, episode reward: 0.859, mean reward: 0.003 [-0.003, 0.014], mean action: -0.057 [-1.285, 1.331], mean observation: 0.123 [-26.206, 16.999], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1145/2000: episode: 4, duration: 14.611s, episode steps: 293, steps per second: 20, episode reward: 0.850, mean reward: 0.003 [-0.004, 0.014], mean action: -0.070 [-1.299, 1.363], mean observation: 0.122 [-21.430, 17.163], loss: 0.000143, mean_squared_error: 0.000285, mean_q: 0.387086\n",
      " 1496/2000: episode: 5, duration: 19.164s, episode steps: 351, steps per second: 18, episode reward: 0.858, mean reward: 0.002 [-0.003, 0.013], mean action: 0.010 [-1.168, 1.286], mean observation: 0.127 [-34.155, 16.796], loss: 0.000299, mean_squared_error: 0.000598, mean_q: 0.389343\n",
      " 1805/2000: episode: 6, duration: 17.172s, episode steps: 309, steps per second: 18, episode reward: 0.904, mean reward: 0.003 [-0.003, 0.014], mean action: -0.010 [-1.316, 1.152], mean observation: 0.130 [-20.364, 16.452], loss: 0.000167, mean_squared_error: 0.000334, mean_q: 0.389208\n",
      "done, took 99.371 seconds\n",
      "\n",
      "\n",
      "iteration: 230\n",
      "Training for 2000 steps ...\n",
      "  425/2000: episode: 1, duration: 16.131s, episode steps: 425, steps per second: 26, episode reward: -0.605, mean reward: -0.001 [-0.019, 0.010], mean action: 0.034 [-1.276, 1.354], mean observation: 0.110 [-27.172, 18.454], loss: --, mean_squared_error: --, mean_q: --\n",
      "  838/2000: episode: 2, duration: 15.493s, episode steps: 413, steps per second: 27, episode reward: -0.619, mean reward: -0.001 [-0.019, 0.010], mean action: 0.046 [-1.365, 1.337], mean observation: 0.111 [-27.444, 18.510], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1267/2000: episode: 3, duration: 17.752s, episode steps: 429, steps per second: 24, episode reward: -0.632, mean reward: -0.001 [-0.018, 0.011], mean action: 0.039 [-1.353, 1.344], mean observation: 0.112 [-27.157, 18.470], loss: 0.000173, mean_squared_error: 0.000347, mean_q: 0.384297\n",
      " 1429/2000: episode: 4, duration: 9.729s, episode steps: 162, steps per second: 17, episode reward: 0.752, mean reward: 0.005 [-0.001, 0.013], mean action: -0.050 [-1.277, 1.232], mean observation: 0.100 [-28.658, 18.633], loss: 0.000161, mean_squared_error: 0.000321, mean_q: 0.387483\n",
      " 1603/2000: episode: 5, duration: 10.063s, episode steps: 174, steps per second: 17, episode reward: 0.754, mean reward: 0.004 [-0.001, 0.014], mean action: -0.039 [-1.324, 1.228], mean observation: 0.104 [-26.002, 18.635], loss: 0.000227, mean_squared_error: 0.000454, mean_q: 0.387617\n",
      " 1810/2000: episode: 6, duration: 12.222s, episode steps: 207, steps per second: 17, episode reward: 0.807, mean reward: 0.004 [-0.002, 0.014], mean action: -0.022 [-1.237, 1.258], mean observation: 0.114 [-21.775, 17.508], loss: 0.000557, mean_squared_error: 0.001114, mean_q: 0.388658\n",
      "done, took 91.172 seconds\n",
      "\n",
      "\n",
      "iteration: 231\n",
      "Training for 2000 steps ...\n",
      "  224/2000: episode: 1, duration: 10.561s, episode steps: 224, steps per second: 21, episode reward: 0.867, mean reward: 0.004 [-0.001, 0.013], mean action: -0.011 [-1.311, 1.257], mean observation: 0.123 [-49.681, 17.380], loss: --, mean_squared_error: --, mean_q: --\n",
      "  392/2000: episode: 2, duration: 8.631s, episode steps: 168, steps per second: 19, episode reward: 0.767, mean reward: 0.005 [-0.002, 0.014], mean action: -0.035 [-1.145, 1.161], mean observation: 0.108 [-46.978, 17.293], loss: --, mean_squared_error: --, mean_q: --\n",
      "  609/2000: episode: 3, duration: 9.972s, episode steps: 217, steps per second: 22, episode reward: 0.823, mean reward: 0.004 [-0.001, 0.013], mean action: 0.006 [-1.206, 1.235], mean observation: 0.123 [-21.810, 17.517], loss: --, mean_squared_error: --, mean_q: --\n",
      "  783/2000: episode: 4, duration: 8.794s, episode steps: 174, steps per second: 20, episode reward: 0.796, mean reward: 0.005 [-0.001, 0.014], mean action: -0.007 [-1.187, 1.437], mean observation: 0.111 [-50.351, 17.636], loss: --, mean_squared_error: --, mean_q: --\n",
      "  959/2000: episode: 5, duration: 8.797s, episode steps: 176, steps per second: 20, episode reward: 0.775, mean reward: 0.004 [-0.001, 0.014], mean action: -0.038 [-1.177, 1.205], mean observation: 0.108 [-50.164, 17.776], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1140/2000: episode: 6, duration: 10.238s, episode steps: 181, steps per second: 18, episode reward: 0.786, mean reward: 0.004 [-0.001, 0.014], mean action: -0.007 [-1.218, 1.193], mean observation: 0.108 [-49.700, 17.431], loss: 0.000188, mean_squared_error: 0.000377, mean_q: 0.380932\n",
      " 1302/2000: episode: 7, duration: 8.865s, episode steps: 162, steps per second: 18, episode reward: 0.747, mean reward: 0.005 [-0.001, 0.013], mean action: 0.013 [-1.169, 1.152], mean observation: 0.099 [-31.603, 17.386], loss: 0.000361, mean_squared_error: 0.000722, mean_q: 0.382404\n",
      " 1458/2000: episode: 8, duration: 8.800s, episode steps: 156, steps per second: 18, episode reward: 0.729, mean reward: 0.005 [-0.001, 0.013], mean action: 0.021 [-1.116, 1.177], mean observation: 0.100 [-22.770, 17.107], loss: 0.000267, mean_squared_error: 0.000534, mean_q: 0.387546\n",
      " 1613/2000: episode: 9, duration: 8.425s, episode steps: 155, steps per second: 18, episode reward: 0.743, mean reward: 0.005 [-0.001, 0.013], mean action: 0.009 [-1.153, 1.133], mean observation: 0.097 [-42.994, 17.039], loss: 0.000198, mean_squared_error: 0.000397, mean_q: 0.394905\n",
      " 1772/2000: episode: 10, duration: 9.016s, episode steps: 159, steps per second: 18, episode reward: 0.744, mean reward: 0.005 [-0.001, 0.013], mean action: 0.023 [-1.159, 1.219], mean observation: 0.098 [-27.371, 17.458], loss: 0.000243, mean_squared_error: 0.000487, mean_q: 0.384431\n",
      " 1971/2000: episode: 11, duration: 11.419s, episode steps: 199, steps per second: 17, episode reward: 0.807, mean reward: 0.004 [-0.001, 0.013], mean action: 0.030 [-1.215, 1.310], mean observation: 0.114 [-25.797, 17.121], loss: 0.000178, mean_squared_error: 0.000356, mean_q: 0.384667\n",
      "done, took 105.355 seconds\n",
      "\n",
      "\n",
      "iteration: 232\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 19.627s, episode steps: 1000, steps per second: 51, episode reward: 0.006, mean reward: 0.000 [-0.003, 0.008], mean action: 0.094 [-1.595, 1.486], mean observation: 0.113 [-24.096, 17.935], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1186/2000: episode: 2, duration: 10.647s, episode steps: 186, steps per second: 17, episode reward: 0.759, mean reward: 0.004 [-0.001, 0.013], mean action: 0.018 [-1.233, 1.182], mean observation: 0.110 [-22.874, 17.844], loss: 0.000176, mean_squared_error: 0.000353, mean_q: 0.393653\n",
      " 1409/2000: episode: 3, duration: 12.725s, episode steps: 223, steps per second: 18, episode reward: 0.820, mean reward: 0.004 [-0.001, 0.013], mean action: 0.003 [-1.324, 1.248], mean observation: 0.114 [-41.695, 17.814], loss: 0.000366, mean_squared_error: 0.000731, mean_q: 0.391929\n",
      " 1599/2000: episode: 4, duration: 11.150s, episode steps: 190, steps per second: 17, episode reward: 0.801, mean reward: 0.004 [-0.002, 0.013], mean action: 0.043 [-1.186, 1.246], mean observation: 0.111 [-33.450, 17.497], loss: 0.000146, mean_squared_error: 0.000292, mean_q: 0.383456\n",
      " 1913/2000: episode: 5, duration: 15.648s, episode steps: 314, steps per second: 20, episode reward: 0.779, mean reward: 0.002 [-0.003, 0.013], mean action: -0.082 [-1.260, 1.285], mean observation: 0.114 [-22.129, 17.566], loss: 0.000349, mean_squared_error: 0.000698, mean_q: 0.392607\n",
      "done, took 74.204 seconds\n",
      "\n",
      "\n",
      "iteration: 233\n",
      "Training for 2000 steps ...\n",
      "  269/2000: episode: 1, duration: 10.807s, episode steps: 269, steps per second: 25, episode reward: 0.780, mean reward: 0.003 [-0.002, 0.013], mean action: -0.072 [-1.224, 1.232], mean observation: 0.112 [-41.500, 17.832], loss: --, mean_squared_error: --, mean_q: --\n",
      "  567/2000: episode: 2, duration: 12.754s, episode steps: 298, steps per second: 23, episode reward: 0.856, mean reward: 0.003 [-0.002, 0.013], mean action: 0.024 [-1.155, 1.370], mean observation: 0.125 [-40.795, 17.846], loss: --, mean_squared_error: --, mean_q: --\n",
      "  862/2000: episode: 3, duration: 12.846s, episode steps: 295, steps per second: 23, episode reward: 0.814, mean reward: 0.003 [-0.002, 0.013], mean action: -0.026 [-1.253, 1.253], mean observation: 0.117 [-39.652, 17.942], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1131/2000: episode: 4, duration: 12.549s, episode steps: 269, steps per second: 21, episode reward: 0.794, mean reward: 0.003 [-0.002, 0.014], mean action: -0.059 [-1.258, 1.197], mean observation: 0.115 [-27.408, 17.212], loss: 0.000223, mean_squared_error: 0.000447, mean_q: 0.389807\n",
      " 1424/2000: episode: 5, duration: 11.753s, episode steps: 293, steps per second: 25, episode reward: -0.773, mean reward: -0.003 [-0.019, 0.009], mean action: -0.079 [-1.437, 1.281], mean observation: 0.078 [-24.227, 17.647], loss: 0.000200, mean_squared_error: 0.000400, mean_q: 0.382527\n",
      " 1799/2000: episode: 6, duration: 19.836s, episode steps: 375, steps per second: 19, episode reward: 0.776, mean reward: 0.002 [-0.002, 0.012], mean action: 0.002 [-1.391, 1.421], mean observation: 0.119 [-27.283, 17.951], loss: 0.000181, mean_squared_error: 0.000362, mean_q: 0.390783\n",
      "done, took 90.181 seconds\n",
      "\n",
      "\n",
      "iteration: 234\n",
      "Training for 2000 steps ...\n",
      "  227/2000: episode: 1, duration: 10.257s, episode steps: 227, steps per second: 22, episode reward: 0.740, mean reward: 0.003 [-0.003, 0.014], mean action: -0.073 [-1.261, 1.192], mean observation: 0.121 [-26.372, 18.588], loss: --, mean_squared_error: --, mean_q: --\n",
      "  484/2000: episode: 2, duration: 11.263s, episode steps: 257, steps per second: 23, episode reward: 0.730, mean reward: 0.003 [-0.003, 0.013], mean action: -0.072 [-1.213, 1.351], mean observation: 0.123 [-27.608, 18.231], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1484/2000: episode: 3, duration: 31.381s, episode steps: 1000, steps per second: 32, episode reward: 0.049, mean reward: 0.000 [-0.003, 0.008], mean action: -0.019 [-1.278, 1.618], mean observation: 0.118 [-23.979, 18.279], loss: 0.000321, mean_squared_error: 0.000643, mean_q: 0.393553\n",
      "done, took 69.454 seconds\n",
      "\n",
      "\n",
      "iteration: 235\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 20.196s, episode steps: 1000, steps per second: 50, episode reward: 0.051, mean reward: 0.000 [-0.003, 0.008], mean action: -0.093 [-1.405, 1.501], mean observation: 0.116 [-25.987, 18.468], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 35.561s, episode steps: 1000, steps per second: 28, episode reward: 0.070, mean reward: 0.000 [-0.003, 0.008], mean action: -0.063 [-1.370, 1.317], mean observation: 0.116 [-26.832, 18.649], loss: 0.000270, mean_squared_error: 0.000540, mean_q: 0.396973\n",
      "done, took 55.773 seconds\n",
      "\n",
      "\n",
      "iteration: 236\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 20.091s, episode steps: 1000, steps per second: 50, episode reward: 0.051, mean reward: 0.000 [-0.004, 0.008], mean action: -0.048 [-1.405, 1.372], mean observation: 0.116 [-25.900, 18.373], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 37.833s, episode steps: 1000, steps per second: 26, episode reward: 0.064, mean reward: 0.000 [-0.004, 0.008], mean action: -0.046 [-1.527, 1.406], mean observation: 0.117 [-26.141, 18.575], loss: 0.000275, mean_squared_error: 0.000551, mean_q: 0.404973\n",
      "done, took 57.941 seconds\n",
      "\n",
      "\n",
      "iteration: 237\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 21.408s, episode steps: 1000, steps per second: 47, episode reward: 0.079, mean reward: 0.000 [-0.004, 0.009], mean action: -0.012 [-1.402, 1.357], mean observation: 0.120 [-24.963, 18.220], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1432/2000: episode: 2, duration: 20.121s, episode steps: 432, steps per second: 21, episode reward: 0.774, mean reward: 0.002 [-0.003, 0.016], mean action: -0.135 [-1.217, 1.354], mean observation: 0.119 [-25.313, 18.510], loss: 0.000341, mean_squared_error: 0.000683, mean_q: 0.411214\n",
      " 1583/2000: episode: 3, duration: 7.984s, episode steps: 151, steps per second: 19, episode reward: 0.804, mean reward: 0.005 [-0.001, 0.013], mean action: -0.111 [-1.139, 1.207], mean observation: 0.112 [-25.229, 18.361], loss: 0.000112, mean_squared_error: 0.000224, mean_q: 0.397963\n",
      " 1735/2000: episode: 4, duration: 7.909s, episode steps: 152, steps per second: 19, episode reward: 0.781, mean reward: 0.005 [-0.002, 0.012], mean action: -0.121 [-1.142, 1.135], mean observation: 0.112 [-25.179, 18.513], loss: 0.000209, mean_squared_error: 0.000419, mean_q: 0.405326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1890/2000: episode: 5, duration: 8.118s, episode steps: 155, steps per second: 19, episode reward: 0.779, mean reward: 0.005 [-0.002, 0.013], mean action: -0.123 [-1.227, 1.234], mean observation: 0.110 [-24.343, 17.988], loss: 0.000325, mean_squared_error: 0.000649, mean_q: 0.399483\n",
      "done, took 71.434 seconds\n",
      "\n",
      "\n",
      "iteration: 238\n",
      "Training for 2000 steps ...\n",
      "  214/2000: episode: 1, duration: 9.024s, episode steps: 214, steps per second: 24, episode reward: 0.777, mean reward: 0.004 [-0.002, 0.014], mean action: -0.114 [-1.230, 1.178], mean observation: 0.115 [-28.238, 18.569], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1214/2000: episode: 2, duration: 42.995s, episode steps: 1000, steps per second: 23, episode reward: 0.703, mean reward: 0.001 [-0.003, 0.012], mean action: -0.061 [-1.308, 1.540], mean observation: 0.119 [-26.068, 17.930], loss: 0.000220, mean_squared_error: 0.000440, mean_q: 0.415541\n",
      " 1370/2000: episode: 3, duration: 8.306s, episode steps: 156, steps per second: 19, episode reward: 0.766, mean reward: 0.005 [-0.001, 0.014], mean action: -0.071 [-1.201, 1.243], mean observation: 0.108 [-25.370, 18.386], loss: 0.000244, mean_squared_error: 0.000488, mean_q: 0.416983\n",
      " 1527/2000: episode: 4, duration: 8.345s, episode steps: 157, steps per second: 19, episode reward: 0.771, mean reward: 0.005 [-0.001, 0.014], mean action: -0.115 [-1.259, 1.182], mean observation: 0.102 [-25.396, 18.421], loss: 0.000584, mean_squared_error: 0.001168, mean_q: 0.410561\n",
      " 1688/2000: episode: 5, duration: 8.588s, episode steps: 161, steps per second: 19, episode reward: 0.779, mean reward: 0.005 [-0.002, 0.013], mean action: -0.080 [-1.295, 1.237], mean observation: 0.104 [-25.486, 18.467], loss: 0.000288, mean_squared_error: 0.000575, mean_q: 0.412140\n",
      " 1854/2000: episode: 6, duration: 8.736s, episode steps: 166, steps per second: 19, episode reward: 0.789, mean reward: 0.005 [-0.001, 0.014], mean action: -0.106 [-1.281, 1.123], mean observation: 0.108 [-25.693, 18.724], loss: 0.000403, mean_squared_error: 0.000807, mean_q: 0.404208\n",
      "done, took 93.698 seconds\n",
      "\n",
      "\n",
      "iteration: 239\n",
      "Training for 2000 steps ...\n",
      "  155/2000: episode: 1, duration: 6.499s, episode steps: 155, steps per second: 24, episode reward: 0.778, mean reward: 0.005 [-0.001, 0.014], mean action: -0.054 [-1.211, 1.247], mean observation: 0.105 [-24.009, 18.228], loss: --, mean_squared_error: --, mean_q: --\n",
      "  310/2000: episode: 2, duration: 6.592s, episode steps: 155, steps per second: 24, episode reward: 0.782, mean reward: 0.005 [-0.001, 0.014], mean action: -0.061 [-1.348, 1.239], mean observation: 0.107 [-25.383, 18.777], loss: --, mean_squared_error: --, mean_q: --\n",
      "  465/2000: episode: 3, duration: 6.381s, episode steps: 155, steps per second: 24, episode reward: 0.780, mean reward: 0.005 [-0.001, 0.014], mean action: -0.066 [-1.253, 1.142], mean observation: 0.105 [-23.835, 18.130], loss: --, mean_squared_error: --, mean_q: --\n",
      "  621/2000: episode: 4, duration: 6.669s, episode steps: 156, steps per second: 23, episode reward: 0.781, mean reward: 0.005 [-0.001, 0.014], mean action: -0.051 [-1.175, 1.176], mean observation: 0.107 [-24.540, 18.506], loss: --, mean_squared_error: --, mean_q: --\n",
      "  780/2000: episode: 5, duration: 6.665s, episode steps: 159, steps per second: 24, episode reward: 0.800, mean reward: 0.005 [-0.001, 0.014], mean action: -0.032 [-1.159, 1.250], mean observation: 0.105 [-24.328, 18.290], loss: --, mean_squared_error: --, mean_q: --\n",
      "  936/2000: episode: 6, duration: 6.730s, episode steps: 156, steps per second: 23, episode reward: 0.781, mean reward: 0.005 [-0.001, 0.014], mean action: -0.043 [-1.314, 1.158], mean observation: 0.106 [-23.989, 18.277], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1093/2000: episode: 7, duration: 7.523s, episode steps: 157, steps per second: 21, episode reward: 0.784, mean reward: 0.005 [-0.001, 0.014], mean action: -0.072 [-1.225, 1.223], mean observation: 0.107 [-25.770, 18.892], loss: 0.000271, mean_squared_error: 0.000542, mean_q: 0.422021\n",
      " 1250/2000: episode: 8, duration: 8.238s, episode steps: 157, steps per second: 19, episode reward: 0.771, mean reward: 0.005 [-0.001, 0.013], mean action: -0.078 [-1.267, 1.099], mean observation: 0.103 [-25.114, 18.576], loss: 0.000314, mean_squared_error: 0.000628, mean_q: 0.415067\n",
      " 1404/2000: episode: 9, duration: 7.681s, episode steps: 154, steps per second: 20, episode reward: 0.789, mean reward: 0.005 [-0.001, 0.013], mean action: -0.045 [-1.236, 1.193], mean observation: 0.108 [-25.162, 18.343], loss: 0.000481, mean_squared_error: 0.000963, mean_q: 0.400007\n",
      " 1559/2000: episode: 10, duration: 7.919s, episode steps: 155, steps per second: 20, episode reward: 0.768, mean reward: 0.005 [-0.001, 0.013], mean action: -0.032 [-1.149, 1.267], mean observation: 0.106 [-25.999, 18.655], loss: 0.000208, mean_squared_error: 0.000416, mean_q: 0.406416\n",
      " 1718/2000: episode: 11, duration: 8.130s, episode steps: 159, steps per second: 20, episode reward: 0.779, mean reward: 0.005 [-0.001, 0.013], mean action: -0.096 [-1.260, 1.138], mean observation: 0.106 [-25.126, 18.422], loss: 0.000144, mean_squared_error: 0.000289, mean_q: 0.413602\n",
      " 1879/2000: episode: 12, duration: 8.236s, episode steps: 161, steps per second: 20, episode reward: 0.772, mean reward: 0.005 [-0.001, 0.013], mean action: -0.090 [-1.231, 1.214], mean observation: 0.104 [-23.776, 18.025], loss: 0.000260, mean_squared_error: 0.000521, mean_q: 0.409303\n",
      "done, took 93.667 seconds\n",
      "\n",
      "\n",
      "iteration: 240\n",
      "Training for 2000 steps ...\n",
      "  160/2000: episode: 1, duration: 6.617s, episode steps: 160, steps per second: 24, episode reward: 0.755, mean reward: 0.005 [-0.001, 0.013], mean action: -0.089 [-1.232, 1.155], mean observation: 0.104 [-21.080, 15.029], loss: --, mean_squared_error: --, mean_q: --\n",
      "  322/2000: episode: 2, duration: 6.786s, episode steps: 162, steps per second: 24, episode reward: 0.763, mean reward: 0.005 [-0.001, 0.013], mean action: -0.045 [-1.208, 1.149], mean observation: 0.104 [-25.669, 15.073], loss: --, mean_squared_error: --, mean_q: --\n",
      "  484/2000: episode: 3, duration: 6.557s, episode steps: 162, steps per second: 25, episode reward: 0.759, mean reward: 0.005 [-0.001, 0.013], mean action: -0.077 [-1.200, 1.120], mean observation: 0.105 [-19.608, 15.343], loss: --, mean_squared_error: --, mean_q: --\n",
      "  647/2000: episode: 4, duration: 6.723s, episode steps: 163, steps per second: 24, episode reward: 0.765, mean reward: 0.005 [-0.001, 0.013], mean action: -0.046 [-1.348, 1.318], mean observation: 0.104 [-25.404, 15.253], loss: --, mean_squared_error: --, mean_q: --\n",
      "  808/2000: episode: 5, duration: 6.666s, episode steps: 161, steps per second: 24, episode reward: 0.775, mean reward: 0.005 [-0.001, 0.013], mean action: -0.057 [-1.167, 1.292], mean observation: 0.105 [-18.956, 15.117], loss: --, mean_squared_error: --, mean_q: --\n",
      "  968/2000: episode: 6, duration: 6.628s, episode steps: 160, steps per second: 24, episode reward: 0.753, mean reward: 0.005 [-0.001, 0.013], mean action: -0.056 [-1.192, 1.216], mean observation: 0.102 [-23.789, 14.657], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1128/2000: episode: 7, duration: 7.729s, episode steps: 160, steps per second: 21, episode reward: 0.757, mean reward: 0.005 [-0.001, 0.013], mean action: -0.060 [-1.239, 1.215], mean observation: 0.104 [-19.584, 15.096], loss: 0.000262, mean_squared_error: 0.000525, mean_q: 0.399617\n",
      " 1289/2000: episode: 8, duration: 8.463s, episode steps: 161, steps per second: 19, episode reward: 0.757, mean reward: 0.005 [-0.001, 0.013], mean action: -0.088 [-1.291, 1.171], mean observation: 0.103 [-21.600, 15.204], loss: 0.000182, mean_squared_error: 0.000364, mean_q: 0.414657\n",
      " 1448/2000: episode: 9, duration: 7.895s, episode steps: 159, steps per second: 20, episode reward: 0.763, mean reward: 0.005 [-0.001, 0.013], mean action: -0.053 [-1.207, 1.241], mean observation: 0.104 [-19.054, 15.443], loss: 0.000203, mean_squared_error: 0.000405, mean_q: 0.401433\n",
      " 1604/2000: episode: 10, duration: 7.988s, episode steps: 156, steps per second: 20, episode reward: 0.765, mean reward: 0.005 [-0.001, 0.013], mean action: -0.096 [-1.155, 1.212], mean observation: 0.104 [-19.207, 15.188], loss: 0.000221, mean_squared_error: 0.000442, mean_q: 0.400872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1762/2000: episode: 11, duration: 7.988s, episode steps: 158, steps per second: 20, episode reward: 0.775, mean reward: 0.005 [-0.001, 0.013], mean action: -0.062 [-1.144, 1.268], mean observation: 0.105 [-18.823, 15.091], loss: 0.000126, mean_squared_error: 0.000253, mean_q: 0.406952\n",
      " 1917/2000: episode: 12, duration: 7.706s, episode steps: 155, steps per second: 20, episode reward: 0.788, mean reward: 0.005 [-0.001, 0.013], mean action: -0.057 [-1.262, 1.189], mean observation: 0.107 [-19.185, 15.339], loss: 0.000428, mean_squared_error: 0.000856, mean_q: 0.406441\n",
      "done, took 91.784 seconds\n",
      "\n",
      "\n",
      "iteration: 241\n",
      "Training for 2000 steps ...\n",
      "  154/2000: episode: 1, duration: 6.012s, episode steps: 154, steps per second: 26, episode reward: 0.770, mean reward: 0.005 [-0.001, 0.013], mean action: -0.091 [-1.173, 1.183], mean observation: 0.104 [-19.172, 15.491], loss: --, mean_squared_error: --, mean_q: --\n",
      "  310/2000: episode: 2, duration: 5.915s, episode steps: 156, steps per second: 26, episode reward: 0.770, mean reward: 0.005 [-0.001, 0.013], mean action: -0.099 [-1.224, 1.180], mean observation: 0.105 [-18.980, 15.035], loss: --, mean_squared_error: --, mean_q: --\n",
      "  465/2000: episode: 3, duration: 6.045s, episode steps: 155, steps per second: 26, episode reward: 0.765, mean reward: 0.005 [-0.001, 0.013], mean action: -0.110 [-1.225, 1.166], mean observation: 0.104 [-18.815, 15.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  621/2000: episode: 4, duration: 6.009s, episode steps: 156, steps per second: 26, episode reward: 0.773, mean reward: 0.005 [-0.001, 0.013], mean action: -0.103 [-1.201, 1.139], mean observation: 0.105 [-18.786, 15.230], loss: --, mean_squared_error: --, mean_q: --\n",
      "  775/2000: episode: 5, duration: 5.984s, episode steps: 154, steps per second: 26, episode reward: 0.774, mean reward: 0.005 [-0.001, 0.013], mean action: -0.094 [-1.182, 1.193], mean observation: 0.105 [-19.366, 15.082], loss: --, mean_squared_error: --, mean_q: --\n",
      "  931/2000: episode: 6, duration: 6.039s, episode steps: 156, steps per second: 26, episode reward: 0.775, mean reward: 0.005 [-0.001, 0.013], mean action: -0.072 [-1.203, 1.140], mean observation: 0.105 [-18.821, 16.369], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1086/2000: episode: 7, duration: 6.675s, episode steps: 155, steps per second: 23, episode reward: 0.777, mean reward: 0.005 [-0.001, 0.013], mean action: -0.107 [-1.217, 1.254], mean observation: 0.105 [-19.008, 15.140], loss: 0.000117, mean_squared_error: 0.000235, mean_q: 0.416746\n",
      " 1244/2000: episode: 8, duration: 7.574s, episode steps: 158, steps per second: 21, episode reward: 0.775, mean reward: 0.005 [-0.001, 0.013], mean action: -0.077 [-1.281, 1.177], mean observation: 0.104 [-19.048, 14.874], loss: 0.000511, mean_squared_error: 0.001022, mean_q: 0.406825\n",
      " 1387/2000: episode: 9, duration: 7.328s, episode steps: 143, steps per second: 20, episode reward: 0.772, mean reward: 0.005 [-0.001, 0.013], mean action: -0.081 [-1.147, 1.252], mean observation: 0.103 [-26.019, 14.949], loss: 0.000188, mean_squared_error: 0.000377, mean_q: 0.407553\n",
      " 1527/2000: episode: 10, duration: 7.062s, episode steps: 140, steps per second: 20, episode reward: 0.770, mean reward: 0.006 [-0.001, 0.013], mean action: -0.062 [-1.220, 1.221], mean observation: 0.103 [-24.475, 15.311], loss: 0.000446, mean_squared_error: 0.000893, mean_q: 0.409755\n",
      " 1663/2000: episode: 11, duration: 6.929s, episode steps: 136, steps per second: 20, episode reward: 0.778, mean reward: 0.006 [-0.001, 0.013], mean action: -0.099 [-1.154, 1.112], mean observation: 0.101 [-26.475, 14.910], loss: 0.000466, mean_squared_error: 0.000932, mean_q: 0.400376\n",
      " 1794/2000: episode: 12, duration: 6.724s, episode steps: 131, steps per second: 19, episode reward: 0.766, mean reward: 0.006 [-0.001, 0.014], mean action: -0.111 [-1.186, 1.159], mean observation: 0.101 [-18.721, 15.536], loss: 0.000220, mean_squared_error: 0.000441, mean_q: 0.394998\n",
      " 1922/2000: episode: 13, duration: 6.303s, episode steps: 128, steps per second: 20, episode reward: 0.779, mean reward: 0.006 [-0.001, 0.015], mean action: -0.105 [-1.184, 1.173], mean observation: 0.102 [-23.706, 15.614], loss: 0.000416, mean_squared_error: 0.000832, mean_q: 0.406003\n",
      "done, took 88.458 seconds\n",
      "\n",
      "\n",
      "iteration: 242\n",
      "Training for 2000 steps ...\n",
      "  136/2000: episode: 1, duration: 5.360s, episode steps: 136, steps per second: 25, episode reward: 0.778, mean reward: 0.006 [-0.001, 0.013], mean action: -0.102 [-1.185, 1.136], mean observation: 0.102 [-22.342, 15.088], loss: --, mean_squared_error: --, mean_q: --\n",
      "  268/2000: episode: 2, duration: 5.062s, episode steps: 132, steps per second: 26, episode reward: 0.765, mean reward: 0.006 [-0.001, 0.015], mean action: -0.150 [-1.299, 1.109], mean observation: 0.102 [-19.332, 15.277], loss: --, mean_squared_error: --, mean_q: --\n",
      "  404/2000: episode: 3, duration: 5.326s, episode steps: 136, steps per second: 26, episode reward: 0.774, mean reward: 0.006 [-0.001, 0.013], mean action: -0.113 [-1.159, 1.158], mean observation: 0.100 [-26.099, 15.251], loss: --, mean_squared_error: --, mean_q: --\n",
      "  538/2000: episode: 4, duration: 5.339s, episode steps: 134, steps per second: 25, episode reward: 0.765, mean reward: 0.006 [-0.001, 0.013], mean action: -0.108 [-1.145, 1.161], mean observation: 0.101 [-22.532, 14.983], loss: --, mean_squared_error: --, mean_q: --\n",
      "  672/2000: episode: 5, duration: 5.318s, episode steps: 134, steps per second: 25, episode reward: 0.781, mean reward: 0.006 [-0.001, 0.014], mean action: -0.123 [-1.189, 1.102], mean observation: 0.101 [-23.397, 15.004], loss: --, mean_squared_error: --, mean_q: --\n",
      "  806/2000: episode: 6, duration: 5.322s, episode steps: 134, steps per second: 25, episode reward: 0.774, mean reward: 0.006 [-0.001, 0.014], mean action: -0.085 [-1.156, 1.361], mean observation: 0.102 [-25.388, 15.434], loss: --, mean_squared_error: --, mean_q: --\n",
      "  940/2000: episode: 7, duration: 5.327s, episode steps: 134, steps per second: 25, episode reward: 0.770, mean reward: 0.006 [-0.001, 0.013], mean action: -0.087 [-1.145, 1.194], mean observation: 0.101 [-22.971, 15.080], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1075/2000: episode: 8, duration: 6.139s, episode steps: 135, steps per second: 22, episode reward: 0.770, mean reward: 0.006 [-0.001, 0.013], mean action: -0.098 [-1.122, 1.153], mean observation: 0.100 [-21.423, 15.454], loss: 0.000131, mean_squared_error: 0.000263, mean_q: 0.402127\n",
      " 1204/2000: episode: 9, duration: 6.496s, episode steps: 129, steps per second: 20, episode reward: 0.749, mean reward: 0.006 [-0.001, 0.014], mean action: -0.119 [-1.236, 1.239], mean observation: 0.094 [-40.710, 15.072], loss: 0.000149, mean_squared_error: 0.000297, mean_q: 0.404251\n",
      " 1347/2000: episode: 10, duration: 7.130s, episode steps: 143, steps per second: 20, episode reward: 0.752, mean reward: 0.005 [-0.001, 0.013], mean action: -0.112 [-1.335, 1.154], mean observation: 0.101 [-18.830, 15.111], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.404743\n",
      " 1484/2000: episode: 11, duration: 6.647s, episode steps: 137, steps per second: 21, episode reward: 0.748, mean reward: 0.005 [-0.001, 0.014], mean action: -0.058 [-1.165, 1.156], mean observation: 0.102 [-19.230, 15.342], loss: 0.000162, mean_squared_error: 0.000323, mean_q: 0.408522\n",
      " 1623/2000: episode: 12, duration: 6.901s, episode steps: 139, steps per second: 20, episode reward: 0.748, mean reward: 0.005 [-0.001, 0.013], mean action: -0.114 [-1.213, 1.170], mean observation: 0.095 [-40.700, 15.453], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.398468\n",
      " 1759/2000: episode: 13, duration: 6.773s, episode steps: 136, steps per second: 20, episode reward: 0.744, mean reward: 0.005 [-0.001, 0.013], mean action: -0.102 [-1.189, 1.162], mean observation: 0.094 [-42.107, 15.158], loss: 0.000210, mean_squared_error: 0.000421, mean_q: 0.396350\n",
      " 1890/2000: episode: 14, duration: 6.151s, episode steps: 131, steps per second: 21, episode reward: 0.768, mean reward: 0.006 [-0.001, 0.015], mean action: -0.082 [-1.262, 1.241], mean observation: 0.095 [-39.087, 15.096], loss: 0.000387, mean_squared_error: 0.000775, mean_q: 0.403108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 88.718 seconds\n",
      "\n",
      "\n",
      "iteration: 243\n",
      "Training for 2000 steps ...\n",
      "  126/2000: episode: 1, duration: 5.038s, episode steps: 126, steps per second: 25, episode reward: 0.718, mean reward: 0.006 [-0.001, 0.014], mean action: -0.134 [-1.305, 1.311], mean observation: 0.095 [-43.421, 15.183], loss: --, mean_squared_error: --, mean_q: --\n",
      "  252/2000: episode: 2, duration: 5.092s, episode steps: 126, steps per second: 25, episode reward: 0.714, mean reward: 0.006 [-0.001, 0.014], mean action: -0.098 [-1.289, 1.222], mean observation: 0.090 [-43.505, 15.155], loss: --, mean_squared_error: --, mean_q: --\n",
      "  376/2000: episode: 3, duration: 4.811s, episode steps: 124, steps per second: 26, episode reward: 0.703, mean reward: 0.006 [-0.001, 0.014], mean action: -0.133 [-1.294, 1.170], mean observation: 0.089 [-43.855, 14.636], loss: --, mean_squared_error: --, mean_q: --\n",
      "  502/2000: episode: 4, duration: 5.002s, episode steps: 126, steps per second: 25, episode reward: 0.729, mean reward: 0.006 [-0.001, 0.014], mean action: -0.112 [-1.280, 1.213], mean observation: 0.097 [-36.682, 15.262], loss: --, mean_squared_error: --, mean_q: --\n",
      "  628/2000: episode: 5, duration: 4.891s, episode steps: 126, steps per second: 26, episode reward: 0.722, mean reward: 0.006 [-0.001, 0.014], mean action: -0.148 [-1.209, 1.082], mean observation: 0.096 [-41.561, 15.088], loss: --, mean_squared_error: --, mean_q: --\n",
      "  751/2000: episode: 6, duration: 4.878s, episode steps: 123, steps per second: 25, episode reward: 0.702, mean reward: 0.006 [-0.001, 0.014], mean action: -0.111 [-1.251, 1.237], mean observation: 0.088 [-42.161, 14.610], loss: --, mean_squared_error: --, mean_q: --\n",
      "  877/2000: episode: 7, duration: 5.029s, episode steps: 126, steps per second: 25, episode reward: 0.723, mean reward: 0.006 [-0.001, 0.014], mean action: -0.112 [-1.137, 1.215], mean observation: 0.090 [-42.809, 15.422], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1003/2000: episode: 8, duration: 5.125s, episode steps: 126, steps per second: 25, episode reward: 0.717, mean reward: 0.006 [-0.001, 0.014], mean action: -0.136 [-1.197, 1.241], mean observation: 0.087 [-42.961, 15.534], loss: 0.000112, mean_squared_error: 0.000223, mean_q: 0.359085\n",
      " 1129/2000: episode: 9, duration: 6.308s, episode steps: 126, steps per second: 20, episode reward: 0.715, mean reward: 0.006 [-0.001, 0.014], mean action: -0.130 [-1.238, 1.198], mean observation: 0.088 [-43.303, 14.741], loss: 0.000110, mean_squared_error: 0.000220, mean_q: 0.394854\n",
      " 1256/2000: episode: 10, duration: 6.182s, episode steps: 127, steps per second: 21, episode reward: 0.720, mean reward: 0.006 [-0.001, 0.014], mean action: -0.160 [-1.277, 1.184], mean observation: 0.089 [-42.228, 15.086], loss: 0.000130, mean_squared_error: 0.000260, mean_q: 0.406023\n",
      " 1387/2000: episode: 11, duration: 6.632s, episode steps: 131, steps per second: 20, episode reward: 0.750, mean reward: 0.006 [-0.001, 0.013], mean action: -0.073 [-1.102, 1.144], mean observation: 0.093 [-42.783, 15.203], loss: 0.000206, mean_squared_error: 0.000412, mean_q: 0.413350\n",
      " 1524/2000: episode: 12, duration: 6.918s, episode steps: 137, steps per second: 20, episode reward: 0.766, mean reward: 0.006 [-0.002, 0.013], mean action: -0.085 [-1.216, 1.180], mean observation: 0.099 [-25.686, 18.110], loss: 0.000371, mean_squared_error: 0.000743, mean_q: 0.390114\n",
      " 1672/2000: episode: 13, duration: 7.247s, episode steps: 148, steps per second: 20, episode reward: 0.766, mean reward: 0.005 [-0.002, 0.013], mean action: -0.120 [-1.164, 1.194], mean observation: 0.104 [-23.118, 17.576], loss: 0.000120, mean_squared_error: 0.000241, mean_q: 0.405500\n",
      " 1833/2000: episode: 14, duration: 8.052s, episode steps: 161, steps per second: 20, episode reward: 0.755, mean reward: 0.005 [-0.002, 0.013], mean action: -0.139 [-1.184, 1.213], mean observation: 0.103 [-23.501, 17.765], loss: 0.000111, mean_squared_error: 0.000221, mean_q: 0.410050\n",
      " 1980/2000: episode: 15, duration: 7.358s, episode steps: 147, steps per second: 20, episode reward: 0.769, mean reward: 0.005 [-0.002, 0.013], mean action: -0.159 [-1.209, 1.312], mean observation: 0.104 [-22.315, 17.302], loss: 0.000305, mean_squared_error: 0.000611, mean_q: 0.396943\n",
      "done, took 89.939 seconds\n",
      "\n",
      "\n",
      "iteration: 244\n",
      "Training for 2000 steps ...\n",
      "  156/2000: episode: 1, duration: 6.256s, episode steps: 156, steps per second: 25, episode reward: 0.766, mean reward: 0.005 [-0.004, 0.013], mean action: -0.156 [-1.245, 1.235], mean observation: 0.106 [-23.720, 17.532], loss: --, mean_squared_error: --, mean_q: --\n",
      "  314/2000: episode: 2, duration: 6.316s, episode steps: 158, steps per second: 25, episode reward: 0.769, mean reward: 0.005 [-0.004, 0.013], mean action: -0.151 [-1.231, 1.247], mean observation: 0.106 [-23.306, 17.668], loss: --, mean_squared_error: --, mean_q: --\n",
      "  470/2000: episode: 3, duration: 6.303s, episode steps: 156, steps per second: 25, episode reward: 0.768, mean reward: 0.005 [-0.004, 0.013], mean action: -0.163 [-1.159, 1.132], mean observation: 0.105 [-23.851, 17.776], loss: --, mean_squared_error: --, mean_q: --\n",
      "  625/2000: episode: 4, duration: 6.252s, episode steps: 155, steps per second: 25, episode reward: 0.777, mean reward: 0.005 [-0.003, 0.013], mean action: -0.166 [-1.196, 1.169], mean observation: 0.107 [-23.100, 17.423], loss: --, mean_squared_error: --, mean_q: --\n",
      "  780/2000: episode: 5, duration: 6.113s, episode steps: 155, steps per second: 25, episode reward: 0.769, mean reward: 0.005 [-0.004, 0.013], mean action: -0.164 [-1.184, 1.211], mean observation: 0.106 [-24.649, 18.135], loss: --, mean_squared_error: --, mean_q: --\n",
      "  932/2000: episode: 6, duration: 10.454s, episode steps: 152, steps per second: 15, episode reward: 0.780, mean reward: 0.005 [-0.004, 0.013], mean action: -0.151 [-1.158, 1.264], mean observation: 0.107 [-23.140, 17.613], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1085/2000: episode: 7, duration: 6.940s, episode steps: 153, steps per second: 22, episode reward: 0.773, mean reward: 0.005 [-0.004, 0.013], mean action: -0.161 [-1.182, 1.176], mean observation: 0.107 [-21.722, 16.966], loss: 0.000167, mean_squared_error: 0.000333, mean_q: 0.418018\n",
      " 1301/2000: episode: 8, duration: 10.625s, episode steps: 216, steps per second: 20, episode reward: 0.761, mean reward: 0.004 [-0.004, 0.013], mean action: -0.142 [-1.230, 1.138], mean observation: 0.109 [-23.420, 17.575], loss: 0.000224, mean_squared_error: 0.000449, mean_q: 0.396553\n",
      " 1467/2000: episode: 9, duration: 8.273s, episode steps: 166, steps per second: 20, episode reward: 0.779, mean reward: 0.005 [-0.002, 0.013], mean action: -0.190 [-1.160, 1.283], mean observation: 0.106 [-22.742, 17.497], loss: 0.000084, mean_squared_error: 0.000169, mean_q: 0.399121\n",
      "done, took 93.247 seconds\n",
      "\n",
      "\n",
      "iteration: 245\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 40.556s, episode steps: 1000, steps per second: 25, episode reward: 0.079, mean reward: 0.000 [-0.002, 0.009], mean action: -0.054 [-1.449, 1.324], mean observation: 0.118 [-22.498, 17.349], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 48.260s, episode steps: 1000, steps per second: 21, episode reward: 0.081, mean reward: 0.000 [-0.002, 0.009], mean action: -0.109 [-1.632, 1.440], mean observation: 0.120 [-22.999, 17.491], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.417692\n",
      "done, took 88.832 seconds\n",
      "\n",
      "\n",
      "iteration: 246\n",
      "Training for 2000 steps ...\n",
      "  618/2000: episode: 1, duration: 23.191s, episode steps: 618, steps per second: 27, episode reward: 0.807, mean reward: 0.001 [-0.004, 0.013], mean action: -0.246 [-1.401, 1.350], mean observation: 0.122 [-22.668, 17.381], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1107/2000: episode: 2, duration: 19.667s, episode steps: 489, steps per second: 25, episode reward: 0.821, mean reward: 0.002 [-0.004, 0.013], mean action: -0.199 [-1.306, 1.269], mean observation: 0.122 [-22.340, 17.325], loss: 0.000117, mean_squared_error: 0.000234, mean_q: 0.428383\n",
      " 1357/2000: episode: 3, duration: 12.918s, episode steps: 250, steps per second: 19, episode reward: 0.758, mean reward: 0.003 [-0.004, 0.013], mean action: -0.193 [-1.237, 1.169], mean observation: 0.118 [-23.045, 17.557], loss: 0.000308, mean_squared_error: 0.000616, mean_q: 0.420065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1580/2000: episode: 4, duration: 12.021s, episode steps: 223, steps per second: 19, episode reward: 0.730, mean reward: 0.003 [-0.004, 0.013], mean action: -0.168 [-1.208, 1.297], mean observation: 0.115 [-42.528, 17.145], loss: 0.000194, mean_squared_error: 0.000388, mean_q: 0.420347\n",
      " 1843/2000: episode: 5, duration: 13.503s, episode steps: 263, steps per second: 19, episode reward: 0.774, mean reward: 0.003 [-0.004, 0.013], mean action: -0.172 [-1.286, 1.315], mean observation: 0.119 [-23.745, 17.854], loss: 0.000133, mean_squared_error: 0.000267, mean_q: 0.416292\n",
      "done, took 89.575 seconds\n",
      "\n",
      "\n",
      "iteration: 247\n",
      "Training for 2000 steps ...\n",
      "  227/2000: episode: 1, duration: 9.320s, episode steps: 227, steps per second: 24, episode reward: 0.771, mean reward: 0.003 [-0.004, 0.013], mean action: -0.107 [-1.151, 1.231], mean observation: 0.116 [-22.644, 17.460], loss: --, mean_squared_error: --, mean_q: --\n",
      "  466/2000: episode: 2, duration: 10.044s, episode steps: 239, steps per second: 24, episode reward: 0.777, mean reward: 0.003 [-0.003, 0.013], mean action: -0.107 [-1.340, 1.206], mean observation: 0.117 [-20.774, 16.813], loss: --, mean_squared_error: --, mean_q: --\n",
      "  679/2000: episode: 3, duration: 8.319s, episode steps: 213, steps per second: 26, episode reward: 0.757, mean reward: 0.004 [-0.004, 0.014], mean action: -0.148 [-1.410, 1.136], mean observation: 0.116 [-22.556, 17.521], loss: --, mean_squared_error: --, mean_q: --\n",
      "  907/2000: episode: 4, duration: 9.474s, episode steps: 228, steps per second: 24, episode reward: 0.772, mean reward: 0.003 [-0.004, 0.014], mean action: -0.138 [-1.319, 1.424], mean observation: 0.117 [-23.190, 17.716], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1134/2000: episode: 5, duration: 9.994s, episode steps: 227, steps per second: 23, episode reward: 0.722, mean reward: 0.003 [-0.004, 0.013], mean action: -0.083 [-1.173, 1.208], mean observation: 0.118 [-21.158, 17.164], loss: 0.000135, mean_squared_error: 0.000271, mean_q: 0.424368\n",
      "done, took 87.519 seconds\n",
      "\n",
      "\n",
      "iteration: 248\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 22.575s, episode steps: 1000, steps per second: 44, episode reward: 0.088, mean reward: 0.000 [-0.003, 0.009], mean action: -0.039 [-1.330, 1.407], mean observation: 0.121 [-22.114, 17.342], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1619/2000: episode: 2, duration: 30.195s, episode steps: 619, steps per second: 21, episode reward: 0.727, mean reward: 0.001 [-0.003, 0.013], mean action: -0.136 [-1.319, 1.399], mean observation: 0.122 [-22.135, 17.231], loss: 0.000132, mean_squared_error: 0.000265, mean_q: 0.424643\n",
      "done, took 72.056 seconds\n",
      "\n",
      "\n",
      "iteration: 249\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 39.617s, episode steps: 1000, steps per second: 25, episode reward: 0.086, mean reward: 0.000 [-0.003, 0.009], mean action: -0.133 [-1.373, 1.513], mean observation: 0.123 [-22.343, 17.319], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1327/2000: episode: 2, duration: 15.716s, episode steps: 327, steps per second: 21, episode reward: 0.712, mean reward: 0.002 [-0.003, 0.013], mean action: -0.097 [-1.212, 1.234], mean observation: 0.121 [-22.526, 17.529], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.426161\n",
      " 1650/2000: episode: 3, duration: 15.430s, episode steps: 323, steps per second: 21, episode reward: 0.702, mean reward: 0.002 [-0.003, 0.013], mean action: -0.092 [-1.215, 1.283], mean observation: 0.120 [-22.430, 17.393], loss: 0.000220, mean_squared_error: 0.000440, mean_q: 0.434434\n",
      "done, took 88.677 seconds\n",
      "\n",
      "\n",
      "iteration: 250\n",
      "Training for 2000 steps ...\n",
      "  204/2000: episode: 1, duration: 8.483s, episode steps: 204, steps per second: 24, episode reward: 0.753, mean reward: 0.004 [-0.002, 0.013], mean action: -0.137 [-1.252, 1.352], mean observation: 0.116 [-18.711, 16.037], loss: --, mean_squared_error: --, mean_q: --\n",
      "  414/2000: episode: 2, duration: 8.964s, episode steps: 210, steps per second: 23, episode reward: 0.747, mean reward: 0.004 [-0.002, 0.013], mean action: -0.128 [-1.251, 1.189], mean observation: 0.115 [-18.300, 15.899], loss: --, mean_squared_error: --, mean_q: --\n",
      "  616/2000: episode: 3, duration: 8.588s, episode steps: 202, steps per second: 24, episode reward: 0.754, mean reward: 0.004 [-0.002, 0.013], mean action: -0.136 [-1.163, 1.260], mean observation: 0.116 [-18.858, 15.970], loss: --, mean_squared_error: --, mean_q: --\n",
      "  823/2000: episode: 4, duration: 8.743s, episode steps: 207, steps per second: 24, episode reward: 0.746, mean reward: 0.004 [-0.002, 0.013], mean action: -0.110 [-1.226, 1.189], mean observation: 0.115 [-18.663, 15.947], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1034/2000: episode: 5, duration: 9.387s, episode steps: 211, steps per second: 22, episode reward: 0.767, mean reward: 0.004 [-0.002, 0.013], mean action: -0.125 [-1.350, 1.308], mean observation: 0.118 [-19.176, 15.891], loss: 0.000098, mean_squared_error: 0.000195, mean_q: 0.426025\n",
      " 1244/2000: episode: 6, duration: 10.666s, episode steps: 210, steps per second: 20, episode reward: 0.736, mean reward: 0.004 [-0.003, 0.013], mean action: -0.141 [-1.343, 1.229], mean observation: 0.113 [-19.131, 15.944], loss: 0.000277, mean_squared_error: 0.000555, mean_q: 0.435440\n",
      " 1439/2000: episode: 7, duration: 10.054s, episode steps: 195, steps per second: 19, episode reward: 0.793, mean reward: 0.004 [-0.003, 0.013], mean action: -0.188 [-1.225, 1.302], mean observation: 0.121 [-19.789, 15.700], loss: 0.000309, mean_squared_error: 0.000619, mean_q: 0.438248\n",
      " 1846/2000: episode: 8, duration: 20.218s, episode steps: 407, steps per second: 20, episode reward: 0.719, mean reward: 0.002 [-0.004, 0.013], mean action: -0.110 [-1.467, 1.328], mean observation: 0.118 [-44.117, 16.110], loss: 0.000245, mean_squared_error: 0.000490, mean_q: 0.438271\n",
      "done, took 93.969 seconds\n",
      "\n",
      "\n",
      "iteration: 251\n",
      "Training for 2000 steps ...\n",
      "  414/2000: episode: 1, duration: 17.002s, episode steps: 414, steps per second: 24, episode reward: 0.749, mean reward: 0.002 [-0.003, 0.013], mean action: -0.135 [-1.418, 1.246], mean observation: 0.117 [-19.083, 15.744], loss: --, mean_squared_error: --, mean_q: --\n",
      "  849/2000: episode: 2, duration: 17.201s, episode steps: 435, steps per second: 25, episode reward: 0.717, mean reward: 0.002 [-0.003, 0.013], mean action: -0.116 [-1.251, 1.346], mean observation: 0.117 [-40.373, 16.012], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1849/2000: episode: 3, duration: 49.875s, episode steps: 1000, steps per second: 20, episode reward: 0.083, mean reward: 0.000 [-0.003, 0.011], mean action: -0.055 [-1.394, 1.565], mean observation: 0.119 [-18.706, 15.617], loss: 0.000163, mean_squared_error: 0.000326, mean_q: 0.438729\n",
      "done, took 90.901 seconds\n",
      "\n",
      "\n",
      "iteration: 252\n",
      "Training for 2000 steps ...\n",
      "  233/2000: episode: 1, duration: 9.194s, episode steps: 233, steps per second: 25, episode reward: 0.657, mean reward: 0.003 [-0.002, 0.012], mean action: -0.160 [-1.300, 1.274], mean observation: 0.128 [-18.874, 15.796], loss: --, mean_squared_error: --, mean_q: --\n",
      "  464/2000: episode: 2, duration: 9.345s, episode steps: 231, steps per second: 25, episode reward: 0.665, mean reward: 0.003 [-0.002, 0.012], mean action: -0.134 [-1.200, 1.220], mean observation: 0.128 [-18.989, 15.778], loss: --, mean_squared_error: --, mean_q: --\n",
      "  698/2000: episode: 3, duration: 9.216s, episode steps: 234, steps per second: 25, episode reward: 0.648, mean reward: 0.003 [-0.002, 0.012], mean action: -0.153 [-1.415, 1.186], mean observation: 0.127 [-18.635, 16.099], loss: --, mean_squared_error: --, mean_q: --\n",
      "  884/2000: episode: 4, duration: 6.843s, episode steps: 186, steps per second: 27, episode reward: 0.627, mean reward: 0.003 [-0.002, 0.012], mean action: -0.122 [-1.231, 1.240], mean observation: 0.130 [-19.576, 15.796], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1089/2000: episode: 5, duration: 8.937s, episode steps: 205, steps per second: 23, episode reward: 0.666, mean reward: 0.003 [-0.002, 0.012], mean action: -0.151 [-1.216, 1.159], mean observation: 0.126 [-19.215, 16.019], loss: 0.000186, mean_squared_error: 0.000372, mean_q: 0.441690\n",
      " 1353/2000: episode: 6, duration: 11.353s, episode steps: 264, steps per second: 23, episode reward: 0.653, mean reward: 0.002 [-0.002, 0.012], mean action: -0.097 [-1.290, 1.274], mean observation: 0.126 [-18.800, 15.956], loss: 0.000248, mean_squared_error: 0.000497, mean_q: 0.436842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1678/2000: episode: 7, duration: 14.275s, episode steps: 325, steps per second: 23, episode reward: 0.489, mean reward: 0.002 [-0.002, 0.010], mean action: -0.089 [-1.361, 1.324], mean observation: 0.131 [-18.862, 15.695], loss: 0.000187, mean_squared_error: 0.000375, mean_q: 0.436425\n",
      " 1990/2000: episode: 8, duration: 15.811s, episode steps: 312, steps per second: 20, episode reward: 0.644, mean reward: 0.002 [-0.003, 0.012], mean action: -0.106 [-1.227, 1.118], mean observation: 0.123 [-18.881, 16.076], loss: 0.000243, mean_squared_error: 0.000486, mean_q: 0.440494\n",
      "done, took 85.841 seconds\n",
      "\n",
      "\n",
      "iteration: 253\n",
      "Training for 2000 steps ...\n",
      "  284/2000: episode: 1, duration: 11.609s, episode steps: 284, steps per second: 24, episode reward: 0.654, mean reward: 0.002 [-0.003, 0.012], mean action: -0.154 [-1.390, 1.480], mean observation: 0.123 [-19.349, 15.968], loss: --, mean_squared_error: --, mean_q: --\n",
      "  542/2000: episode: 2, duration: 10.991s, episode steps: 258, steps per second: 23, episode reward: 0.633, mean reward: 0.002 [-0.003, 0.012], mean action: -0.146 [-1.247, 1.192], mean observation: 0.122 [-19.683, 15.618], loss: --, mean_squared_error: --, mean_q: --\n",
      "  807/2000: episode: 3, duration: 11.170s, episode steps: 265, steps per second: 24, episode reward: 0.636, mean reward: 0.002 [-0.003, 0.012], mean action: -0.151 [-1.353, 1.155], mean observation: 0.122 [-19.561, 15.899], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1096/2000: episode: 4, duration: 13.061s, episode steps: 289, steps per second: 22, episode reward: 0.638, mean reward: 0.002 [-0.003, 0.012], mean action: -0.146 [-1.254, 1.251], mean observation: 0.122 [-19.758, 15.848], loss: 0.000085, mean_squared_error: 0.000170, mean_q: 0.440759\n",
      " 1570/2000: episode: 5, duration: 24.612s, episode steps: 474, steps per second: 19, episode reward: 0.487, mean reward: 0.001 [-0.003, 0.010], mean action: -0.060 [-1.359, 1.234], mean observation: 0.124 [-21.613, 16.123], loss: 0.000234, mean_squared_error: 0.000469, mean_q: 0.441487\n",
      " 1808/2000: episode: 6, duration: 10.989s, episode steps: 238, steps per second: 22, episode reward: 0.481, mean reward: 0.002 [-0.002, 0.011], mean action: -0.126 [-1.274, 1.197], mean observation: 0.130 [-19.035, 16.177], loss: 0.000119, mean_squared_error: 0.000237, mean_q: 0.437773\n",
      "done, took 92.826 seconds\n",
      "\n",
      "\n",
      "iteration: 254\n",
      "Training for 2000 steps ...\n",
      "  510/2000: episode: 1, duration: 20.920s, episode steps: 510, steps per second: 24, episode reward: 0.482, mean reward: 0.001 [-0.002, 0.010], mean action: -0.165 [-1.366, 1.352], mean observation: 0.123 [-19.157, 15.972], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1172/2000: episode: 2, duration: 29.265s, episode steps: 662, steps per second: 23, episode reward: 0.482, mean reward: 0.001 [-0.003, 0.010], mean action: -0.155 [-1.378, 1.377], mean observation: 0.123 [-18.955, 16.031], loss: 0.000115, mean_squared_error: 0.000229, mean_q: 0.436338\n",
      "done, took 96.946 seconds\n",
      "\n",
      "\n",
      "iteration: 255\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 45.921s, episode steps: 1000, steps per second: 22, episode reward: 0.142, mean reward: 0.000 [-0.003, 0.009], mean action: -0.056 [-1.421, 1.531], mean observation: 0.119 [-22.641, 17.500], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 52.026s, episode steps: 1000, steps per second: 19, episode reward: 0.115, mean reward: 0.000 [-0.004, 0.008], mean action: -0.156 [-1.417, 1.325], mean observation: 0.123 [-21.887, 17.110], loss: 0.000193, mean_squared_error: 0.000385, mean_q: 0.448065\n",
      "done, took 97.964 seconds\n",
      "\n",
      "\n",
      "iteration: 256\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 30.231s, episode steps: 1000, steps per second: 33, episode reward: 0.114, mean reward: 0.000 [-0.004, 0.008], mean action: -0.037 [-1.318, 1.393], mean observation: 0.122 [-22.988, 17.564], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 52.915s, episode steps: 1000, steps per second: 19, episode reward: 0.081, mean reward: 0.000 [-0.003, 0.008], mean action: -0.159 [-1.519, 1.476], mean observation: 0.120 [-22.857, 17.424], loss: 0.000237, mean_squared_error: 0.000475, mean_q: 0.449950\n",
      "done, took 83.165 seconds\n",
      "\n",
      "\n",
      "iteration: 257\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 46.457s, episode steps: 1000, steps per second: 22, episode reward: 0.084, mean reward: 0.000 [-0.004, 0.008], mean action: -0.154 [-1.400, 1.329], mean observation: 0.116 [-22.437, 17.187], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 54.551s, episode steps: 1000, steps per second: 18, episode reward: 0.085, mean reward: 0.000 [-0.004, 0.008], mean action: -0.128 [-1.449, 1.459], mean observation: 0.114 [-22.711, 17.384], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.457034\n",
      "done, took 101.026 seconds\n",
      "\n",
      "\n",
      "iteration: 258\n",
      "Training for 2000 steps ...\n",
      "  286/2000: episode: 1, duration: 6.831s, episode steps: 286, steps per second: 42, episode reward: -0.820, mean reward: -0.003 [-0.019, 0.008], mean action: -0.195 [-1.300, 1.345], mean observation: 0.102 [-21.443, 17.074], loss: --, mean_squared_error: --, mean_q: --\n",
      "  609/2000: episode: 2, duration: 7.144s, episode steps: 323, steps per second: 45, episode reward: -0.820, mean reward: -0.003 [-0.019, 0.008], mean action: -0.220 [-1.208, 1.334], mean observation: 0.103 [-23.646, 17.899], loss: --, mean_squared_error: --, mean_q: --\n",
      "  957/2000: episode: 3, duration: 7.872s, episode steps: 348, steps per second: 44, episode reward: -0.821, mean reward: -0.002 [-0.019, 0.008], mean action: -0.233 [-1.248, 1.161], mean observation: 0.104 [-22.920, 17.581], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1252/2000: episode: 4, duration: 9.311s, episode steps: 295, steps per second: 32, episode reward: -0.815, mean reward: -0.003 [-0.018, 0.008], mean action: -0.132 [-1.155, 1.415], mean observation: 0.101 [-22.480, 17.368], loss: 0.000235, mean_squared_error: 0.000470, mean_q: 0.453196\n",
      " 1496/2000: episode: 5, duration: 8.389s, episode steps: 244, steps per second: 29, episode reward: -0.816, mean reward: -0.003 [-0.019, 0.010], mean action: -0.061 [-1.395, 1.194], mean observation: 0.100 [-23.664, 17.832], loss: 0.000426, mean_squared_error: 0.000853, mean_q: 0.457175\n",
      " 1757/2000: episode: 6, duration: 9.356s, episode steps: 261, steps per second: 28, episode reward: -0.814, mean reward: -0.003 [-0.019, 0.008], mean action: -0.092 [-1.301, 1.172], mean observation: 0.101 [-21.839, 17.282], loss: 0.000105, mean_squared_error: 0.000210, mean_q: 0.452851\n",
      "done, took 57.371 seconds\n",
      "\n",
      "\n",
      "iteration: 259\n",
      "Training for 2000 steps ...\n",
      "  208/2000: episode: 1, duration: 5.186s, episode steps: 208, steps per second: 40, episode reward: -0.828, mean reward: -0.004 [-0.019, 0.010], mean action: -0.073 [-1.286, 1.256], mean observation: 0.095 [-22.766, 17.390], loss: --, mean_squared_error: --, mean_q: --\n",
      "  411/2000: episode: 2, duration: 4.953s, episode steps: 203, steps per second: 41, episode reward: -0.816, mean reward: -0.004 [-0.019, 0.010], mean action: -0.126 [-1.329, 1.175], mean observation: 0.096 [-22.979, 17.498], loss: --, mean_squared_error: --, mean_q: --\n",
      "  617/2000: episode: 3, duration: 4.978s, episode steps: 206, steps per second: 41, episode reward: -0.813, mean reward: -0.004 [-0.019, 0.010], mean action: -0.110 [-1.275, 1.175], mean observation: 0.096 [-23.243, 17.745], loss: --, mean_squared_error: --, mean_q: --\n",
      "  818/2000: episode: 4, duration: 4.903s, episode steps: 201, steps per second: 41, episode reward: -0.824, mean reward: -0.004 [-0.019, 0.010], mean action: -0.097 [-1.226, 1.311], mean observation: 0.095 [-22.562, 17.353], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1018/2000: episode: 5, duration: 5.194s, episode steps: 200, steps per second: 39, episode reward: -0.815, mean reward: -0.004 [-0.019, 0.010], mean action: -0.090 [-1.181, 1.215], mean observation: 0.096 [-22.150, 17.424], loss: 0.000094, mean_squared_error: 0.000189, mean_q: 0.449410\n",
      " 1211/2000: episode: 6, duration: 6.591s, episode steps: 193, steps per second: 29, episode reward: -0.808, mean reward: -0.004 [-0.019, 0.010], mean action: -0.114 [-1.313, 1.161], mean observation: 0.095 [-23.074, 17.641], loss: 0.000160, mean_squared_error: 0.000320, mean_q: 0.454805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1456/2000: episode: 7, duration: 8.133s, episode steps: 245, steps per second: 30, episode reward: -0.824, mean reward: -0.003 [-0.019, 0.010], mean action: -0.149 [-1.190, 1.336], mean observation: 0.100 [-23.118, 17.660], loss: 0.000088, mean_squared_error: 0.000176, mean_q: 0.453506\n",
      " 1734/2000: episode: 8, duration: 8.972s, episode steps: 278, steps per second: 31, episode reward: -0.853, mean reward: -0.003 [-0.019, 0.011], mean action: -0.127 [-1.157, 1.212], mean observation: 0.098 [-21.870, 17.181], loss: 0.000159, mean_squared_error: 0.000318, mean_q: 0.454745\n",
      " 1925/2000: episode: 9, duration: 6.626s, episode steps: 191, steps per second: 29, episode reward: -0.809, mean reward: -0.004 [-0.019, 0.010], mean action: -0.161 [-1.295, 1.176], mean observation: 0.096 [-21.964, 17.259], loss: 0.000197, mean_squared_error: 0.000393, mean_q: 0.448830\n",
      "done, took 59.351 seconds\n",
      "\n",
      "\n",
      "iteration: 260\n",
      "Training for 2000 steps ...\n",
      "  226/2000: episode: 1, duration: 5.734s, episode steps: 226, steps per second: 39, episode reward: -0.821, mean reward: -0.004 [-0.019, 0.011], mean action: -0.125 [-1.159, 1.281], mean observation: 0.099 [-23.114, 17.636], loss: --, mean_squared_error: --, mean_q: --\n",
      "  446/2000: episode: 2, duration: 5.862s, episode steps: 220, steps per second: 38, episode reward: -0.820, mean reward: -0.004 [-0.019, 0.011], mean action: -0.111 [-1.171, 1.250], mean observation: 0.098 [-23.084, 17.674], loss: --, mean_squared_error: --, mean_q: --\n",
      "  677/2000: episode: 3, duration: 5.991s, episode steps: 231, steps per second: 39, episode reward: -0.834, mean reward: -0.004 [-0.019, 0.011], mean action: -0.136 [-1.244, 1.202], mean observation: 0.097 [-21.917, 17.208], loss: --, mean_squared_error: --, mean_q: --\n",
      "  905/2000: episode: 4, duration: 5.979s, episode steps: 228, steps per second: 38, episode reward: -0.823, mean reward: -0.004 [-0.019, 0.011], mean action: -0.137 [-1.206, 1.230], mean observation: 0.098 [-21.886, 17.337], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1127/2000: episode: 5, duration: 7.066s, episode steps: 222, steps per second: 31, episode reward: -0.822, mean reward: -0.004 [-0.019, 0.011], mean action: -0.134 [-1.234, 1.252], mean observation: 0.097 [-22.633, 17.407], loss: 0.000328, mean_squared_error: 0.000655, mean_q: 0.454492\n",
      " 1346/2000: episode: 6, duration: 8.049s, episode steps: 219, steps per second: 27, episode reward: -0.825, mean reward: -0.004 [-0.019, 0.011], mean action: -0.161 [-1.242, 1.219], mean observation: 0.096 [-22.245, 17.313], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.444229\n",
      " 1571/2000: episode: 7, duration: 8.351s, episode steps: 225, steps per second: 27, episode reward: -0.834, mean reward: -0.004 [-0.019, 0.010], mean action: -0.155 [-1.261, 1.228], mean observation: 0.095 [-22.183, 17.130], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.452280\n",
      " 1788/2000: episode: 8, duration: 7.971s, episode steps: 217, steps per second: 27, episode reward: -0.833, mean reward: -0.004 [-0.019, 0.011], mean action: -0.153 [-1.182, 1.276], mean observation: 0.095 [-22.951, 17.602], loss: 0.000110, mean_squared_error: 0.000220, mean_q: 0.457263\n",
      "done, took 63.201 seconds\n",
      "\n",
      "\n",
      "iteration: 261\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.884s, episode steps: 1000, steps per second: 42, episode reward: 0.011, mean reward: 0.000 [-0.005, 0.011], mean action: -0.310 [-1.647, 1.221], mean observation: 0.113 [-13.317, 15.951], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 42.023s, episode steps: 1000, steps per second: 24, episode reward: 0.166, mean reward: 0.000 [-0.005, 0.012], mean action: -0.410 [-1.541, 1.461], mean observation: 0.115 [-11.347, 16.160], loss: 0.000153, mean_squared_error: 0.000307, mean_q: 0.461702\n",
      "done, took 65.926 seconds\n",
      "\n",
      "\n",
      "iteration: 262\n",
      "Training for 2000 steps ...\n",
      "  224/2000: episode: 1, duration: 8.089s, episode steps: 224, steps per second: 28, episode reward: 0.475, mean reward: 0.002 [-0.005, 0.011], mean action: -0.127 [-1.475, 1.262], mean observation: 0.127 [-14.495, 17.354], loss: --, mean_squared_error: --, mean_q: --\n",
      "  443/2000: episode: 2, duration: 8.382s, episode steps: 219, steps per second: 26, episode reward: 0.469, mean reward: 0.002 [-0.005, 0.011], mean action: -0.148 [-1.256, 1.202], mean observation: 0.126 [-15.164, 18.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  650/2000: episode: 3, duration: 8.090s, episode steps: 207, steps per second: 26, episode reward: 0.474, mean reward: 0.002 [-0.005, 0.011], mean action: -0.124 [-1.197, 1.168], mean observation: 0.126 [-14.704, 16.623], loss: --, mean_squared_error: --, mean_q: --\n",
      "  870/2000: episode: 4, duration: 8.829s, episode steps: 220, steps per second: 25, episode reward: 0.478, mean reward: 0.002 [-0.005, 0.011], mean action: -0.151 [-1.181, 1.216], mean observation: 0.126 [-14.983, 16.578], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1067/2000: episode: 5, duration: 7.689s, episode steps: 197, steps per second: 26, episode reward: 0.472, mean reward: 0.002 [-0.005, 0.011], mean action: -0.092 [-1.193, 1.210], mean observation: 0.127 [-14.227, 16.614], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.467141\n",
      "done, took 78.163 seconds\n",
      "\n",
      "\n",
      "iteration: 263\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 33.320s, episode steps: 1000, steps per second: 30, episode reward: 0.043, mean reward: 0.000 [-0.005, 0.012], mean action: -0.245 [-1.360, 1.465], mean observation: 0.117 [-12.915, 17.182], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1395/2000: episode: 2, duration: 20.368s, episode steps: 395, steps per second: 19, episode reward: 0.485, mean reward: 0.001 [-0.005, 0.012], mean action: -0.079 [-1.156, 1.429], mean observation: 0.121 [-14.732, 17.895], loss: 0.000102, mean_squared_error: 0.000205, mean_q: 0.486615\n",
      " 1565/2000: episode: 3, duration: 8.726s, episode steps: 170, steps per second: 19, episode reward: 0.426, mean reward: 0.003 [-0.003, 0.011], mean action: -0.162 [-1.271, 1.274], mean observation: 0.130 [-14.488, 18.102], loss: 0.000146, mean_squared_error: 0.000292, mean_q: 0.487647\n",
      " 1874/2000: episode: 4, duration: 15.921s, episode steps: 309, steps per second: 19, episode reward: 0.462, mean reward: 0.001 [-0.005, 0.010], mean action: -0.233 [-1.358, 1.244], mean observation: 0.127 [-19.642, 20.089], loss: 0.000292, mean_squared_error: 0.000584, mean_q: 0.496127\n",
      "done, took 85.595 seconds\n",
      "\n",
      "\n",
      "iteration: 264\n",
      "Training for 2000 steps ...\n",
      "  255/2000: episode: 1, duration: 10.743s, episode steps: 255, steps per second: 24, episode reward: 0.452, mean reward: 0.002 [-0.005, 0.011], mean action: -0.227 [-1.150, 1.180], mean observation: 0.130 [-20.360, 19.735], loss: --, mean_squared_error: --, mean_q: --\n",
      "  512/2000: episode: 2, duration: 10.941s, episode steps: 257, steps per second: 23, episode reward: 0.447, mean reward: 0.002 [-0.006, 0.011], mean action: -0.216 [-1.218, 1.334], mean observation: 0.130 [-20.043, 19.772], loss: --, mean_squared_error: --, mean_q: --\n",
      "  816/2000: episode: 3, duration: 12.741s, episode steps: 304, steps per second: 24, episode reward: 0.450, mean reward: 0.001 [-0.006, 0.011], mean action: -0.236 [-1.284, 1.276], mean observation: 0.129 [-20.345, 19.833], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1090/2000: episode: 4, duration: 12.279s, episode steps: 274, steps per second: 22, episode reward: 0.445, mean reward: 0.002 [-0.005, 0.011], mean action: -0.220 [-1.299, 1.348], mean observation: 0.129 [-20.376, 19.748], loss: 0.000233, mean_squared_error: 0.000465, mean_q: 0.509110\n",
      " 1330/2000: episode: 5, duration: 12.057s, episode steps: 240, steps per second: 20, episode reward: 0.458, mean reward: 0.002 [-0.004, 0.010], mean action: -0.236 [-1.315, 1.298], mean observation: 0.131 [-20.134, 20.206], loss: 0.000164, mean_squared_error: 0.000327, mean_q: 0.501095\n",
      " 1521/2000: episode: 6, duration: 9.786s, episode steps: 191, steps per second: 20, episode reward: 0.449, mean reward: 0.002 [-0.003, 0.010], mean action: -0.156 [-1.200, 1.278], mean observation: 0.134 [-18.142, 20.036], loss: 0.000161, mean_squared_error: 0.000323, mean_q: 0.505774\n",
      " 1671/2000: episode: 7, duration: 7.623s, episode steps: 150, steps per second: 20, episode reward: 0.448, mean reward: 0.003 [-0.005, 0.010], mean action: -0.275 [-1.212, 1.240], mean observation: 0.135 [-18.952, 19.814], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.499014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1844/2000: episode: 8, duration: 8.838s, episode steps: 173, steps per second: 20, episode reward: 0.463, mean reward: 0.003 [-0.003, 0.010], mean action: -0.256 [-1.304, 1.160], mean observation: 0.136 [-18.624, 20.097], loss: 0.000153, mean_squared_error: 0.000306, mean_q: 0.501312\n",
      "done, took 93.433 seconds\n",
      "\n",
      "\n",
      "iteration: 265\n",
      "Training for 2000 steps ...\n",
      "  184/2000: episode: 1, duration: 7.652s, episode steps: 184, steps per second: 24, episode reward: 0.464, mean reward: 0.003 [-0.004, 0.010], mean action: -0.179 [-1.304, 1.214], mean observation: 0.132 [-18.049, 20.146], loss: --, mean_squared_error: --, mean_q: --\n",
      "  362/2000: episode: 2, duration: 7.485s, episode steps: 178, steps per second: 24, episode reward: 0.445, mean reward: 0.002 [-0.004, 0.010], mean action: -0.199 [-1.211, 1.189], mean observation: 0.131 [-18.366, 19.910], loss: --, mean_squared_error: --, mean_q: --\n",
      "  554/2000: episode: 3, duration: 7.881s, episode steps: 192, steps per second: 24, episode reward: 0.428, mean reward: 0.002 [-0.004, 0.010], mean action: -0.241 [-1.243, 1.220], mean observation: 0.129 [-18.337, 19.723], loss: --, mean_squared_error: --, mean_q: --\n",
      "  730/2000: episode: 4, duration: 7.449s, episode steps: 176, steps per second: 24, episode reward: 0.426, mean reward: 0.002 [-0.004, 0.010], mean action: -0.193 [-1.317, 1.121], mean observation: 0.130 [-17.972, 19.928], loss: --, mean_squared_error: --, mean_q: --\n",
      "  903/2000: episode: 5, duration: 7.039s, episode steps: 173, steps per second: 25, episode reward: 0.427, mean reward: 0.002 [-0.004, 0.010], mean action: -0.204 [-1.165, 1.232], mean observation: 0.129 [-18.344, 19.751], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1099/2000: episode: 6, duration: 9.123s, episode steps: 196, steps per second: 21, episode reward: 0.474, mean reward: 0.002 [-0.003, 0.010], mean action: -0.164 [-1.238, 1.441], mean observation: 0.133 [-17.965, 20.110], loss: 0.000130, mean_squared_error: 0.000260, mean_q: 0.501887\n",
      " 1314/2000: episode: 7, duration: 10.995s, episode steps: 215, steps per second: 20, episode reward: 0.476, mean reward: 0.002 [-0.003, 0.010], mean action: -0.131 [-1.255, 1.246], mean observation: 0.132 [-19.008, 19.880], loss: 0.000240, mean_squared_error: 0.000481, mean_q: 0.506819\n",
      " 1553/2000: episode: 8, duration: 12.152s, episode steps: 239, steps per second: 20, episode reward: 0.502, mean reward: 0.002 [-0.003, 0.010], mean action: -0.127 [-1.309, 1.205], mean observation: 0.132 [-18.334, 19.821], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.504855\n",
      " 1682/2000: episode: 9, duration: 7.215s, episode steps: 129, steps per second: 18, episode reward: 0.388, mean reward: 0.003 [-0.004, 0.010], mean action: -0.108 [-1.233, 1.194], mean observation: 0.126 [-35.091, 19.770], loss: 0.000137, mean_squared_error: 0.000274, mean_q: 0.504678\n",
      " 1870/2000: episode: 10, duration: 9.784s, episode steps: 188, steps per second: 19, episode reward: 0.684, mean reward: 0.004 [-0.004, 0.010], mean action: -0.105 [-1.176, 1.157], mean observation: 0.147 [-38.086, 20.418], loss: 0.000336, mean_squared_error: 0.000673, mean_q: 0.506764\n",
      "done, took 94.281 seconds\n",
      "\n",
      "\n",
      "iteration: 266\n",
      "Training for 2000 steps ...\n",
      "  249/2000: episode: 1, duration: 10.816s, episode steps: 249, steps per second: 23, episode reward: 0.728, mean reward: 0.003 [-0.004, 0.012], mean action: -0.089 [-1.268, 1.406], mean observation: 0.138 [-17.862, 19.864], loss: --, mean_squared_error: --, mean_q: --\n",
      "  508/2000: episode: 2, duration: 11.369s, episode steps: 259, steps per second: 23, episode reward: 0.730, mean reward: 0.003 [-0.003, 0.012], mean action: -0.079 [-1.226, 1.258], mean observation: 0.138 [-18.231, 20.220], loss: --, mean_squared_error: --, mean_q: --\n",
      "  755/2000: episode: 3, duration: 10.920s, episode steps: 247, steps per second: 23, episode reward: 0.717, mean reward: 0.003 [-0.004, 0.012], mean action: -0.079 [-1.204, 1.323], mean observation: 0.139 [-18.279, 20.099], loss: --, mean_squared_error: --, mean_q: --\n",
      "  987/2000: episode: 4, duration: 9.956s, episode steps: 232, steps per second: 23, episode reward: 0.692, mean reward: 0.003 [-0.004, 0.011], mean action: -0.109 [-1.265, 1.324], mean observation: 0.137 [-18.385, 19.778], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1311/2000: episode: 5, duration: 16.809s, episode steps: 324, steps per second: 19, episode reward: 0.711, mean reward: 0.002 [-0.004, 0.012], mean action: -0.050 [-1.279, 1.362], mean observation: 0.138 [-17.836, 19.964], loss: 0.000058, mean_squared_error: 0.000117, mean_q: 0.502633\n",
      " 1784/2000: episode: 6, duration: 24.777s, episode steps: 473, steps per second: 19, episode reward: 0.713, mean reward: 0.002 [-0.004, 0.012], mean action: -0.099 [-1.262, 1.325], mean observation: 0.130 [-18.301, 19.850], loss: 0.000090, mean_squared_error: 0.000180, mean_q: 0.502625\n",
      "done, took 97.146 seconds\n",
      "\n",
      "\n",
      "iteration: 267\n",
      "Training for 2000 steps ...\n",
      "  734/2000: episode: 1, duration: 26.972s, episode steps: 734, steps per second: 27, episode reward: 0.636, mean reward: 0.001 [-0.005, 0.011], mean action: -0.105 [-1.247, 1.492], mean observation: 0.118 [-14.659, 19.882], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1429/2000: episode: 2, duration: 32.678s, episode steps: 695, steps per second: 21, episode reward: 0.678, mean reward: 0.001 [-0.005, 0.012], mean action: -0.146 [-1.439, 1.338], mean observation: 0.120 [-17.366, 19.967], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.500858\n",
      " 1722/2000: episode: 3, duration: 17.210s, episode steps: 293, steps per second: 17, episode reward: 0.778, mean reward: 0.003 [-0.005, 0.013], mean action: -0.104 [-1.306, 1.212], mean observation: 0.133 [-11.402, 19.629], loss: 0.000270, mean_squared_error: 0.000540, mean_q: 0.497655\n",
      " 1934/2000: episode: 4, duration: 13.102s, episode steps: 212, steps per second: 16, episode reward: 0.720, mean reward: 0.003 [-0.005, 0.014], mean action: -0.175 [-1.268, 1.142], mean observation: 0.125 [-17.855, 19.700], loss: 0.000102, mean_squared_error: 0.000204, mean_q: 0.500036\n",
      "done, took 93.910 seconds\n",
      "\n",
      "\n",
      "iteration: 268\n",
      "Training for 2000 steps ...\n",
      "  321/2000: episode: 1, duration: 16.199s, episode steps: 321, steps per second: 20, episode reward: 0.786, mean reward: 0.002 [-0.003, 0.013], mean action: -0.165 [-1.332, 1.201], mean observation: 0.133 [-15.779, 19.687], loss: --, mean_squared_error: --, mean_q: --\n",
      "  667/2000: episode: 2, duration: 16.912s, episode steps: 346, steps per second: 20, episode reward: 0.782, mean reward: 0.002 [-0.004, 0.013], mean action: -0.145 [-1.341, 1.198], mean observation: 0.131 [-17.526, 22.063], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1065/2000: episode: 3, duration: 19.992s, episode steps: 398, steps per second: 20, episode reward: 0.772, mean reward: 0.002 [-0.004, 0.013], mean action: -0.098 [-1.402, 1.250], mean observation: 0.131 [-15.183, 19.469], loss: 0.000056, mean_squared_error: 0.000111, mean_q: 0.504241\n",
      " 1350/2000: episode: 4, duration: 16.543s, episode steps: 285, steps per second: 17, episode reward: 0.758, mean reward: 0.003 [-0.004, 0.013], mean action: -0.147 [-1.341, 1.262], mean observation: 0.131 [-11.344, 19.801], loss: 0.000189, mean_squared_error: 0.000377, mean_q: 0.501137\n",
      " 1639/2000: episode: 5, duration: 17.013s, episode steps: 289, steps per second: 17, episode reward: 0.804, mean reward: 0.003 [-0.004, 0.013], mean action: -0.178 [-1.346, 1.334], mean observation: 0.133 [-11.643, 19.728], loss: 0.000102, mean_squared_error: 0.000204, mean_q: 0.503898\n",
      " 1789/2000: episode: 6, duration: 8.977s, episode steps: 150, steps per second: 17, episode reward: 0.627, mean reward: 0.004 [-0.004, 0.013], mean action: -0.206 [-1.358, 1.323], mean observation: 0.111 [-15.926, 19.733], loss: 0.000123, mean_squared_error: 0.000246, mean_q: 0.502041\n",
      " 1934/2000: episode: 7, duration: 9.019s, episode steps: 145, steps per second: 16, episode reward: 0.617, mean reward: 0.004 [-0.003, 0.012], mean action: -0.227 [-1.149, 1.277], mean observation: 0.111 [-11.460, 19.717], loss: 0.000072, mean_squared_error: 0.000145, mean_q: 0.505555\n",
      "done, took 108.707 seconds\n",
      "\n",
      "\n",
      "iteration: 269\n",
      "Training for 2000 steps ...\n",
      "  146/2000: episode: 1, duration: 7.802s, episode steps: 146, steps per second: 19, episode reward: 0.628, mean reward: 0.004 [-0.004, 0.012], mean action: -0.218 [-1.217, 1.165], mean observation: 0.111 [-12.875, 19.667], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  404/2000: episode: 2, duration: 12.698s, episode steps: 258, steps per second: 20, episode reward: 0.896, mean reward: 0.003 [-0.004, 0.013], mean action: -0.254 [-1.356, 1.141], mean observation: 0.154 [-13.932, 19.794], loss: --, mean_squared_error: --, mean_q: --\n",
      "  619/2000: episode: 3, duration: 10.940s, episode steps: 215, steps per second: 20, episode reward: 0.834, mean reward: 0.004 [-0.003, 0.013], mean action: -0.258 [-1.205, 1.230], mean observation: 0.142 [-11.328, 19.610], loss: --, mean_squared_error: --, mean_q: --\n",
      "  863/2000: episode: 4, duration: 12.075s, episode steps: 244, steps per second: 20, episode reward: 0.866, mean reward: 0.004 [-0.003, 0.013], mean action: -0.270 [-1.218, 1.174], mean observation: 0.151 [-16.914, 19.731], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1103/2000: episode: 5, duration: 12.875s, episode steps: 240, steps per second: 19, episode reward: 0.873, mean reward: 0.004 [-0.003, 0.013], mean action: -0.267 [-1.276, 1.232], mean observation: 0.150 [-14.847, 19.671], loss: 0.000080, mean_squared_error: 0.000161, mean_q: 0.491664\n",
      " 1355/2000: episode: 6, duration: 15.087s, episode steps: 252, steps per second: 17, episode reward: 0.894, mean reward: 0.004 [-0.004, 0.014], mean action: -0.274 [-1.185, 1.275], mean observation: 0.153 [-12.317, 19.773], loss: 0.000229, mean_squared_error: 0.000459, mean_q: 0.494525\n",
      " 1572/2000: episode: 7, duration: 13.062s, episode steps: 217, steps per second: 17, episode reward: 0.846, mean reward: 0.004 [-0.003, 0.013], mean action: -0.310 [-1.243, 1.096], mean observation: 0.146 [-18.902, 19.540], loss: 0.000157, mean_squared_error: 0.000315, mean_q: 0.500157\n",
      " 1801/2000: episode: 8, duration: 13.785s, episode steps: 229, steps per second: 17, episode reward: 0.879, mean reward: 0.004 [-0.004, 0.013], mean action: -0.314 [-1.226, 1.134], mean observation: 0.151 [-19.951, 19.665], loss: 0.000225, mean_squared_error: 0.000450, mean_q: 0.500131\n",
      " 1936/2000: episode: 9, duration: 8.747s, episode steps: 135, steps per second: 15, episode reward: 0.601, mean reward: 0.004 [-0.004, 0.013], mean action: -0.218 [-1.220, 1.172], mean observation: 0.111 [-11.294, 19.733], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.503932\n",
      "done, took 111.176 seconds\n",
      "\n",
      "\n",
      "iteration: 270\n",
      "Training for 2000 steps ...\n",
      "  144/2000: episode: 1, duration: 8.017s, episode steps: 144, steps per second: 18, episode reward: 0.623, mean reward: 0.004 [-0.003, 0.013], mean action: -0.179 [-1.248, 1.127], mean observation: 0.116 [-12.767, 19.709], loss: --, mean_squared_error: --, mean_q: --\n",
      "  282/2000: episode: 2, duration: 7.827s, episode steps: 138, steps per second: 18, episode reward: 0.614, mean reward: 0.004 [-0.003, 0.013], mean action: -0.147 [-1.218, 1.207], mean observation: 0.113 [-12.307, 19.552], loss: --, mean_squared_error: --, mean_q: --\n",
      "  431/2000: episode: 3, duration: 8.322s, episode steps: 149, steps per second: 18, episode reward: 0.650, mean reward: 0.004 [-0.004, 0.013], mean action: -0.162 [-1.190, 1.185], mean observation: 0.118 [-11.402, 19.618], loss: --, mean_squared_error: --, mean_q: --\n",
      "  570/2000: episode: 4, duration: 7.842s, episode steps: 139, steps per second: 18, episode reward: 0.617, mean reward: 0.004 [-0.003, 0.013], mean action: -0.120 [-1.126, 1.249], mean observation: 0.113 [-11.272, 19.880], loss: --, mean_squared_error: --, mean_q: --\n",
      "  710/2000: episode: 5, duration: 7.850s, episode steps: 140, steps per second: 18, episode reward: 0.625, mean reward: 0.004 [-0.003, 0.013], mean action: -0.187 [-1.233, 1.192], mean observation: 0.114 [-12.459, 19.590], loss: --, mean_squared_error: --, mean_q: --\n",
      "  852/2000: episode: 6, duration: 8.017s, episode steps: 142, steps per second: 18, episode reward: 0.616, mean reward: 0.004 [-0.003, 0.013], mean action: -0.155 [-1.193, 1.111], mean observation: 0.115 [-12.021, 19.644], loss: --, mean_squared_error: --, mean_q: --\n",
      "  989/2000: episode: 7, duration: 7.776s, episode steps: 137, steps per second: 18, episode reward: 0.616, mean reward: 0.004 [-0.003, 0.013], mean action: -0.161 [-1.214, 1.293], mean observation: 0.113 [-14.010, 19.771], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1125/2000: episode: 8, duration: 8.671s, episode steps: 136, steps per second: 16, episode reward: 0.619, mean reward: 0.005 [-0.003, 0.013], mean action: -0.175 [-1.209, 1.148], mean observation: 0.112 [-12.308, 19.633], loss: 0.000358, mean_squared_error: 0.000716, mean_q: 0.500511\n",
      " 1306/2000: episode: 9, duration: 11.285s, episode steps: 181, steps per second: 16, episode reward: 0.710, mean reward: 0.004 [-0.003, 0.013], mean action: -0.183 [-1.315, 1.177], mean observation: 0.128 [-12.264, 19.725], loss: 0.000051, mean_squared_error: 0.000101, mean_q: 0.500952\n",
      " 1469/2000: episode: 10, duration: 10.389s, episode steps: 163, steps per second: 16, episode reward: 0.662, mean reward: 0.004 [-0.003, 0.013], mean action: -0.182 [-1.166, 1.202], mean observation: 0.120 [-13.229, 19.630], loss: 0.000183, mean_squared_error: 0.000366, mean_q: 0.497669\n",
      " 1603/2000: episode: 11, duration: 8.591s, episode steps: 134, steps per second: 16, episode reward: 0.614, mean reward: 0.005 [-0.004, 0.013], mean action: -0.230 [-1.212, 1.232], mean observation: 0.112 [-11.713, 19.784], loss: 0.000062, mean_squared_error: 0.000123, mean_q: 0.493471\n",
      " 1747/2000: episode: 12, duration: 9.260s, episode steps: 144, steps per second: 16, episode reward: 0.620, mean reward: 0.004 [-0.004, 0.013], mean action: -0.185 [-1.205, 1.319], mean observation: 0.115 [-11.651, 19.654], loss: 0.000192, mean_squared_error: 0.000384, mean_q: 0.495426\n",
      " 1881/2000: episode: 13, duration: 8.701s, episode steps: 134, steps per second: 15, episode reward: 0.598, mean reward: 0.004 [-0.004, 0.013], mean action: -0.171 [-1.155, 1.170], mean observation: 0.111 [-11.675, 19.833], loss: 0.000130, mean_squared_error: 0.000261, mean_q: 0.495827\n",
      "done, took 120.147 seconds\n",
      "\n",
      "\n",
      "iteration: 271\n",
      "Training for 2000 steps ...\n",
      "  134/2000: episode: 1, duration: 7.349s, episode steps: 134, steps per second: 18, episode reward: 0.589, mean reward: 0.004 [-0.004, 0.013], mean action: -0.192 [-1.245, 1.093], mean observation: 0.111 [-12.396, 19.704], loss: --, mean_squared_error: --, mean_q: --\n",
      "  264/2000: episode: 2, duration: 7.278s, episode steps: 130, steps per second: 18, episode reward: 0.599, mean reward: 0.005 [-0.004, 0.013], mean action: -0.168 [-1.137, 1.227], mean observation: 0.110 [-11.800, 19.748], loss: --, mean_squared_error: --, mean_q: --\n",
      "  399/2000: episode: 3, duration: 7.431s, episode steps: 135, steps per second: 18, episode reward: 0.599, mean reward: 0.004 [-0.004, 0.013], mean action: -0.177 [-1.244, 1.139], mean observation: 0.111 [-11.124, 19.717], loss: --, mean_squared_error: --, mean_q: --\n",
      "  534/2000: episode: 4, duration: 7.337s, episode steps: 135, steps per second: 18, episode reward: 0.591, mean reward: 0.004 [-0.004, 0.013], mean action: -0.180 [-1.187, 1.223], mean observation: 0.111 [-12.963, 19.705], loss: --, mean_squared_error: --, mean_q: --\n",
      "  669/2000: episode: 5, duration: 7.452s, episode steps: 135, steps per second: 18, episode reward: 0.595, mean reward: 0.004 [-0.004, 0.013], mean action: -0.135 [-1.109, 1.305], mean observation: 0.111 [-13.342, 19.730], loss: --, mean_squared_error: --, mean_q: --\n",
      "  803/2000: episode: 6, duration: 7.318s, episode steps: 134, steps per second: 18, episode reward: 0.601, mean reward: 0.004 [-0.004, 0.013], mean action: -0.192 [-1.176, 1.143], mean observation: 0.110 [-14.874, 19.541], loss: --, mean_squared_error: --, mean_q: --\n",
      "  937/2000: episode: 7, duration: 7.286s, episode steps: 134, steps per second: 18, episode reward: 0.595, mean reward: 0.004 [-0.004, 0.013], mean action: -0.175 [-1.193, 1.270], mean observation: 0.110 [-11.063, 19.683], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1071/2000: episode: 8, duration: 7.912s, episode steps: 134, steps per second: 17, episode reward: 0.595, mean reward: 0.004 [-0.004, 0.013], mean action: -0.202 [-1.261, 1.112], mean observation: 0.111 [-14.212, 19.634], loss: 0.000061, mean_squared_error: 0.000123, mean_q: 0.498899\n",
      " 1201/2000: episode: 9, duration: 8.583s, episode steps: 130, steps per second: 15, episode reward: 0.592, mean reward: 0.005 [-0.004, 0.013], mean action: -0.189 [-1.260, 1.123], mean observation: 0.109 [-11.650, 19.755], loss: 0.000372, mean_squared_error: 0.000744, mean_q: 0.493637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1332/2000: episode: 10, duration: 8.041s, episode steps: 131, steps per second: 16, episode reward: 0.602, mean reward: 0.005 [-0.004, 0.012], mean action: -0.167 [-1.224, 1.165], mean observation: 0.110 [-11.377, 19.740], loss: 0.000180, mean_squared_error: 0.000360, mean_q: 0.496208\n",
      " 1560/2000: episode: 11, duration: 13.613s, episode steps: 228, steps per second: 17, episode reward: 0.769, mean reward: 0.003 [-0.004, 0.013], mean action: -0.226 [-1.297, 1.276], mean observation: 0.134 [-37.283, 19.835], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.491205\n",
      " 1747/2000: episode: 12, duration: 11.067s, episode steps: 187, steps per second: 17, episode reward: 0.703, mean reward: 0.004 [-0.004, 0.014], mean action: -0.206 [-1.274, 1.190], mean observation: 0.127 [-24.515, 20.192], loss: 0.000120, mean_squared_error: 0.000241, mean_q: 0.495805\n",
      " 1886/2000: episode: 13, duration: 8.897s, episode steps: 139, steps per second: 16, episode reward: 0.593, mean reward: 0.004 [-0.004, 0.013], mean action: -0.194 [-1.320, 1.204], mean observation: 0.112 [-16.365, 19.876], loss: 0.000247, mean_squared_error: 0.000494, mean_q: 0.496308\n",
      "done, took 116.894 seconds\n",
      "\n",
      "\n",
      "iteration: 272\n",
      "Training for 2000 steps ...\n",
      "  125/2000: episode: 1, duration: 6.990s, episode steps: 125, steps per second: 18, episode reward: 0.579, mean reward: 0.005 [-0.004, 0.014], mean action: -0.177 [-1.130, 1.138], mean observation: 0.107 [-17.830, 19.676], loss: --, mean_squared_error: --, mean_q: --\n",
      "  250/2000: episode: 2, duration: 7.029s, episode steps: 125, steps per second: 18, episode reward: 0.570, mean reward: 0.005 [-0.004, 0.013], mean action: -0.189 [-1.202, 1.124], mean observation: 0.108 [-17.862, 19.728], loss: --, mean_squared_error: --, mean_q: --\n",
      "  379/2000: episode: 3, duration: 7.062s, episode steps: 129, steps per second: 18, episode reward: 0.579, mean reward: 0.004 [-0.004, 0.013], mean action: -0.188 [-1.158, 1.191], mean observation: 0.109 [-17.456, 20.095], loss: --, mean_squared_error: --, mean_q: --\n",
      "  509/2000: episode: 4, duration: 7.188s, episode steps: 130, steps per second: 18, episode reward: 0.588, mean reward: 0.005 [-0.004, 0.014], mean action: -0.195 [-1.246, 1.153], mean observation: 0.108 [-17.978, 19.835], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 5, duration: 6.954s, episode steps: 128, steps per second: 18, episode reward: 0.583, mean reward: 0.005 [-0.004, 0.013], mean action: -0.159 [-1.221, 1.270], mean observation: 0.109 [-17.779, 19.874], loss: --, mean_squared_error: --, mean_q: --\n",
      "  762/2000: episode: 6, duration: 6.964s, episode steps: 125, steps per second: 18, episode reward: 0.572, mean reward: 0.005 [-0.004, 0.014], mean action: -0.182 [-1.270, 1.202], mean observation: 0.108 [-17.202, 19.975], loss: --, mean_squared_error: --, mean_q: --\n",
      "  889/2000: episode: 7, duration: 7.128s, episode steps: 127, steps per second: 18, episode reward: 0.583, mean reward: 0.005 [-0.004, 0.013], mean action: -0.180 [-1.190, 1.154], mean observation: 0.110 [-18.130, 19.816], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1014/2000: episode: 8, duration: 6.988s, episode steps: 125, steps per second: 18, episode reward: 0.574, mean reward: 0.005 [-0.004, 0.013], mean action: -0.192 [-1.219, 1.264], mean observation: 0.108 [-18.116, 19.899], loss: 0.000044, mean_squared_error: 0.000088, mean_q: 0.502063\n",
      " 1148/2000: episode: 9, duration: 8.618s, episode steps: 134, steps per second: 16, episode reward: 0.583, mean reward: 0.004 [-0.004, 0.013], mean action: -0.169 [-1.173, 1.143], mean observation: 0.111 [-17.877, 19.953], loss: 0.000171, mean_squared_error: 0.000342, mean_q: 0.497324\n",
      " 1274/2000: episode: 10, duration: 8.123s, episode steps: 126, steps per second: 16, episode reward: 0.589, mean reward: 0.005 [-0.004, 0.013], mean action: -0.135 [-1.174, 1.135], mean observation: 0.111 [-16.718, 19.901], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.499410\n",
      " 1406/2000: episode: 11, duration: 8.586s, episode steps: 132, steps per second: 15, episode reward: 0.599, mean reward: 0.005 [-0.004, 0.013], mean action: -0.126 [-1.098, 1.181], mean observation: 0.111 [-17.196, 20.017], loss: 0.000176, mean_squared_error: 0.000352, mean_q: 0.496076\n",
      " 1572/2000: episode: 12, duration: 10.047s, episode steps: 166, steps per second: 17, episode reward: 0.663, mean reward: 0.004 [-0.003, 0.013], mean action: -0.188 [-1.290, 1.200], mean observation: 0.120 [-18.215, 19.934], loss: 0.000280, mean_squared_error: 0.000560, mean_q: 0.497784\n",
      " 1747/2000: episode: 13, duration: 10.801s, episode steps: 175, steps per second: 16, episode reward: 0.675, mean reward: 0.004 [-0.003, 0.013], mean action: -0.174 [-1.176, 1.212], mean observation: 0.120 [-38.439, 19.800], loss: 0.000230, mean_squared_error: 0.000460, mean_q: 0.492780\n",
      " 1935/2000: episode: 14, duration: 11.437s, episode steps: 188, steps per second: 16, episode reward: 0.767, mean reward: 0.004 [-0.004, 0.014], mean action: -0.185 [-1.193, 1.233], mean observation: 0.132 [-26.283, 19.896], loss: 0.000152, mean_squared_error: 0.000305, mean_q: 0.499763\n",
      "done, took 117.876 seconds\n",
      "\n",
      "\n",
      "iteration: 273\n",
      "Training for 2000 steps ...\n",
      "  198/2000: episode: 1, duration: 10.135s, episode steps: 198, steps per second: 20, episode reward: 0.705, mean reward: 0.004 [-0.004, 0.014], mean action: -0.180 [-1.170, 1.209], mean observation: 0.125 [-28.565, 19.919], loss: --, mean_squared_error: --, mean_q: --\n",
      "  402/2000: episode: 2, duration: 10.086s, episode steps: 204, steps per second: 20, episode reward: 0.686, mean reward: 0.003 [-0.004, 0.013], mean action: -0.198 [-1.210, 1.215], mean observation: 0.125 [-35.750, 20.097], loss: --, mean_squared_error: --, mean_q: --\n",
      "  592/2000: episode: 3, duration: 9.417s, episode steps: 190, steps per second: 20, episode reward: 0.664, mean reward: 0.003 [-0.003, 0.013], mean action: -0.188 [-1.218, 1.304], mean observation: 0.119 [-44.538, 20.044], loss: --, mean_squared_error: --, mean_q: --\n",
      "  784/2000: episode: 4, duration: 9.497s, episode steps: 192, steps per second: 20, episode reward: 0.682, mean reward: 0.004 [-0.004, 0.014], mean action: -0.181 [-1.280, 1.194], mean observation: 0.126 [-24.998, 20.049], loss: --, mean_squared_error: --, mean_q: --\n",
      "  987/2000: episode: 5, duration: 9.689s, episode steps: 203, steps per second: 21, episode reward: 0.693, mean reward: 0.003 [-0.004, 0.013], mean action: -0.183 [-1.184, 1.184], mean observation: 0.127 [-20.215, 19.968], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1177/2000: episode: 6, duration: 11.064s, episode steps: 190, steps per second: 17, episode reward: 0.683, mean reward: 0.004 [-0.003, 0.014], mean action: -0.162 [-1.216, 1.330], mean observation: 0.122 [-14.954, 20.086], loss: 0.000161, mean_squared_error: 0.000322, mean_q: 0.500045\n",
      " 1412/2000: episode: 7, duration: 13.981s, episode steps: 235, steps per second: 17, episode reward: 0.676, mean reward: 0.003 [-0.004, 0.013], mean action: -0.172 [-1.326, 1.238], mean observation: 0.125 [-44.075, 19.637], loss: 0.000247, mean_squared_error: 0.000494, mean_q: 0.491402\n",
      " 1663/2000: episode: 8, duration: 14.635s, episode steps: 251, steps per second: 17, episode reward: 0.718, mean reward: 0.003 [-0.004, 0.014], mean action: -0.193 [-1.207, 1.250], mean observation: 0.128 [-23.046, 19.985], loss: 0.000135, mean_squared_error: 0.000270, mean_q: 0.495006\n",
      " 1906/2000: episode: 9, duration: 13.745s, episode steps: 243, steps per second: 18, episode reward: 0.724, mean reward: 0.003 [-0.003, 0.013], mean action: -0.135 [-1.181, 1.269], mean observation: 0.129 [-16.686, 20.010], loss: 0.000141, mean_squared_error: 0.000282, mean_q: 0.492652\n",
      "done, took 107.969 seconds\n",
      "\n",
      "\n",
      "iteration: 274\n",
      "Training for 2000 steps ...\n",
      "  228/2000: episode: 1, duration: 11.247s, episode steps: 228, steps per second: 20, episode reward: 0.703, mean reward: 0.003 [-0.004, 0.014], mean action: -0.205 [-1.259, 1.169], mean observation: 0.126 [-20.927, 19.917], loss: --, mean_squared_error: --, mean_q: --\n",
      "  469/2000: episode: 2, duration: 11.312s, episode steps: 241, steps per second: 21, episode reward: 0.709, mean reward: 0.003 [-0.003, 0.013], mean action: -0.195 [-1.231, 1.211], mean observation: 0.127 [-14.763, 19.921], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  704/2000: episode: 3, duration: 11.446s, episode steps: 235, steps per second: 21, episode reward: 0.712, mean reward: 0.003 [-0.004, 0.014], mean action: -0.182 [-1.350, 1.293], mean observation: 0.125 [-24.151, 19.690], loss: --, mean_squared_error: --, mean_q: --\n",
      "  931/2000: episode: 4, duration: 10.926s, episode steps: 227, steps per second: 21, episode reward: 0.704, mean reward: 0.003 [-0.003, 0.014], mean action: -0.190 [-1.436, 1.351], mean observation: 0.127 [-11.772, 19.965], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1170/2000: episode: 5, duration: 13.199s, episode steps: 239, steps per second: 18, episode reward: 0.709, mean reward: 0.003 [-0.004, 0.013], mean action: -0.188 [-1.257, 1.243], mean observation: 0.124 [-14.844, 20.077], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.501901\n",
      " 1282/2000: episode: 6, duration: 7.721s, episode steps: 112, steps per second: 15, episode reward: 0.555, mean reward: 0.005 [-0.003, 0.013], mean action: -0.159 [-1.136, 1.084], mean observation: 0.100 [-50.108, 19.569], loss: 0.000140, mean_squared_error: 0.000281, mean_q: 0.498599\n",
      " 1405/2000: episode: 7, duration: 8.065s, episode steps: 123, steps per second: 15, episode reward: 0.620, mean reward: 0.005 [-0.003, 0.014], mean action: -0.156 [-1.145, 1.250], mean observation: 0.108 [-39.961, 19.913], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.495061\n",
      " 1513/2000: episode: 8, duration: 7.028s, episode steps: 108, steps per second: 15, episode reward: 0.564, mean reward: 0.005 [-0.002, 0.013], mean action: -0.158 [-1.157, 1.174], mean observation: 0.110 [-38.034, 19.907], loss: 0.000059, mean_squared_error: 0.000119, mean_q: 0.493307\n",
      " 1709/2000: episode: 9, duration: 11.644s, episode steps: 196, steps per second: 17, episode reward: 0.664, mean reward: 0.003 [-0.002, 0.013], mean action: -0.058 [-1.251, 1.243], mean observation: 0.119 [-10.420, 19.850], loss: 0.000104, mean_squared_error: 0.000208, mean_q: 0.489156\n",
      " 1885/2000: episode: 10, duration: 10.477s, episode steps: 176, steps per second: 17, episode reward: 0.647, mean reward: 0.004 [-0.002, 0.013], mean action: -0.106 [-1.178, 1.331], mean observation: 0.117 [-11.018, 20.085], loss: 0.000099, mean_squared_error: 0.000199, mean_q: 0.485615\n",
      "done, took 109.234 seconds\n",
      "\n",
      "\n",
      "iteration: 275\n",
      "Training for 2000 steps ...\n",
      "  255/2000: episode: 1, duration: 11.948s, episode steps: 255, steps per second: 21, episode reward: 0.776, mean reward: 0.003 [-0.002, 0.013], mean action: -0.126 [-1.340, 1.192], mean observation: 0.130 [-10.309, 20.565], loss: --, mean_squared_error: --, mean_q: --\n",
      "  393/2000: episode: 2, duration: 7.289s, episode steps: 138, steps per second: 19, episode reward: 0.610, mean reward: 0.004 [-0.003, 0.014], mean action: -0.194 [-1.233, 1.155], mean observation: 0.109 [-33.801, 20.256], loss: --, mean_squared_error: --, mean_q: --\n",
      "  528/2000: episode: 3, duration: 7.147s, episode steps: 135, steps per second: 19, episode reward: 0.582, mean reward: 0.004 [-0.003, 0.013], mean action: -0.183 [-1.181, 1.150], mean observation: 0.107 [-34.574, 20.122], loss: --, mean_squared_error: --, mean_q: --\n",
      "  670/2000: episode: 4, duration: 7.281s, episode steps: 142, steps per second: 20, episode reward: 0.618, mean reward: 0.004 [-0.002, 0.013], mean action: -0.176 [-1.272, 1.144], mean observation: 0.111 [-33.429, 20.859], loss: --, mean_squared_error: --, mean_q: --\n",
      "  806/2000: episode: 5, duration: 7.130s, episode steps: 136, steps per second: 19, episode reward: 0.620, mean reward: 0.005 [-0.003, 0.013], mean action: -0.190 [-1.233, 1.243], mean observation: 0.110 [-37.140, 20.685], loss: --, mean_squared_error: --, mean_q: --\n",
      "  942/2000: episode: 6, duration: 7.342s, episode steps: 136, steps per second: 19, episode reward: 0.606, mean reward: 0.004 [-0.003, 0.014], mean action: -0.166 [-1.138, 1.237], mean observation: 0.110 [-38.651, 20.388], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1078/2000: episode: 7, duration: 8.029s, episode steps: 136, steps per second: 17, episode reward: 0.607, mean reward: 0.004 [-0.003, 0.014], mean action: -0.159 [-1.237, 1.202], mean observation: 0.106 [-38.371, 20.154], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.486032\n",
      " 1215/2000: episode: 8, duration: 8.273s, episode steps: 137, steps per second: 17, episode reward: 0.585, mean reward: 0.004 [-0.003, 0.014], mean action: -0.192 [-1.199, 1.152], mean observation: 0.107 [-10.261, 19.104], loss: 0.000273, mean_squared_error: 0.000545, mean_q: 0.494287\n",
      " 1373/2000: episode: 9, duration: 10.087s, episode steps: 158, steps per second: 16, episode reward: 0.623, mean reward: 0.004 [-0.003, 0.013], mean action: -0.109 [-1.247, 1.160], mean observation: 0.114 [-10.404, 15.099], loss: 0.000170, mean_squared_error: 0.000341, mean_q: 0.489964\n",
      " 1580/2000: episode: 10, duration: 11.314s, episode steps: 207, steps per second: 18, episode reward: 0.644, mean reward: 0.003 [-0.002, 0.012], mean action: -0.130 [-1.342, 1.185], mean observation: 0.117 [-18.182, 15.818], loss: 0.000229, mean_squared_error: 0.000458, mean_q: 0.491469\n",
      "done, took 104.488 seconds\n",
      "\n",
      "\n",
      "iteration: 276\n",
      "Training for 2000 steps ...\n",
      "  287/2000: episode: 1, duration: 7.526s, episode steps: 287, steps per second: 38, episode reward: -0.839, mean reward: -0.003 [-0.019, 0.011], mean action: -0.062 [-1.416, 1.182], mean observation: 0.096 [-15.445, 15.326], loss: --, mean_squared_error: --, mean_q: --\n",
      "  590/2000: episode: 2, duration: 7.488s, episode steps: 303, steps per second: 40, episode reward: -0.843, mean reward: -0.003 [-0.019, 0.011], mean action: -0.042 [-1.174, 1.282], mean observation: 0.095 [-15.543, 14.983], loss: --, mean_squared_error: --, mean_q: --\n",
      "  868/2000: episode: 3, duration: 7.242s, episode steps: 278, steps per second: 38, episode reward: -0.846, mean reward: -0.003 [-0.019, 0.011], mean action: -0.058 [-1.303, 1.193], mean observation: 0.094 [-15.096, 14.775], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1210/2000: episode: 4, duration: 10.561s, episode steps: 342, steps per second: 32, episode reward: -0.802, mean reward: -0.002 [-0.019, 0.011], mean action: -0.006 [-1.304, 1.266], mean observation: 0.090 [-15.060, 14.813], loss: 0.000094, mean_squared_error: 0.000189, mean_q: 0.488446\n",
      " 1610/2000: episode: 5, duration: 13.456s, episode steps: 400, steps per second: 30, episode reward: -0.804, mean reward: -0.002 [-0.019, 0.011], mean action: -0.027 [-1.338, 1.360], mean observation: 0.094 [-15.058, 15.242], loss: 0.000054, mean_squared_error: 0.000109, mean_q: 0.485417\n",
      " 1771/2000: episode: 6, duration: 9.137s, episode steps: 161, steps per second: 18, episode reward: 0.634, mean reward: 0.004 [-0.004, 0.011], mean action: -0.183 [-1.218, 1.163], mean observation: 0.107 [-15.429, 15.100], loss: 0.000207, mean_squared_error: 0.000415, mean_q: 0.487832\n",
      " 1921/2000: episode: 7, duration: 8.802s, episode steps: 150, steps per second: 17, episode reward: 0.558, mean reward: 0.004 [-0.004, 0.014], mean action: -0.154 [-1.105, 1.234], mean observation: 0.097 [-50.331, 14.895], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.487649\n",
      "done, took 68.393 seconds\n",
      "\n",
      "\n",
      "iteration: 277\n",
      "Training for 2000 steps ...\n",
      "  231/2000: episode: 1, duration: 9.975s, episode steps: 231, steps per second: 23, episode reward: 0.728, mean reward: 0.003 [-0.003, 0.013], mean action: -0.062 [-1.335, 1.284], mean observation: 0.121 [-34.117, 17.498], loss: --, mean_squared_error: --, mean_q: --\n",
      "  437/2000: episode: 2, duration: 8.949s, episode steps: 206, steps per second: 23, episode reward: 0.679, mean reward: 0.003 [-0.003, 0.013], mean action: -0.037 [-1.323, 1.301], mean observation: 0.113 [-45.140, 15.549], loss: --, mean_squared_error: --, mean_q: --\n",
      "  633/2000: episode: 3, duration: 9.477s, episode steps: 196, steps per second: 21, episode reward: 0.648, mean reward: 0.003 [-0.003, 0.013], mean action: -0.032 [-1.211, 1.391], mean observation: 0.114 [-15.494, 17.482], loss: --, mean_squared_error: --, mean_q: --\n",
      "  842/2000: episode: 4, duration: 9.048s, episode steps: 209, steps per second: 23, episode reward: 0.689, mean reward: 0.003 [-0.003, 0.012], mean action: -0.034 [-1.136, 1.303], mean observation: 0.122 [-15.132, 17.419], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1050/2000: episode: 5, duration: 9.498s, episode steps: 208, steps per second: 22, episode reward: 0.676, mean reward: 0.003 [-0.003, 0.013], mean action: -0.065 [-1.290, 1.314], mean observation: 0.116 [-45.008, 17.650], loss: 0.000213, mean_squared_error: 0.000426, mean_q: 0.489307\n",
      " 1219/2000: episode: 6, duration: 9.074s, episode steps: 169, steps per second: 19, episode reward: 0.611, mean reward: 0.004 [-0.003, 0.016], mean action: -0.085 [-1.179, 1.201], mean observation: 0.102 [-45.575, 15.534], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.495232\n",
      " 1380/2000: episode: 7, duration: 7.916s, episode steps: 161, steps per second: 20, episode reward: 0.541, mean reward: 0.003 [-0.004, 0.015], mean action: -0.143 [-1.208, 1.150], mean observation: 0.106 [-15.105, 15.036], loss: 0.000133, mean_squared_error: 0.000265, mean_q: 0.492804\n",
      " 1506/2000: episode: 8, duration: 8.544s, episode steps: 126, steps per second: 15, episode reward: 0.578, mean reward: 0.005 [-0.004, 0.013], mean action: -0.160 [-1.239, 1.226], mean observation: 0.104 [-32.195, 15.260], loss: 0.000204, mean_squared_error: 0.000409, mean_q: 0.493792\n",
      " 1611/2000: episode: 9, duration: 5.944s, episode steps: 105, steps per second: 18, episode reward: 0.577, mean reward: 0.005 [-0.003, 0.016], mean action: -0.266 [-1.192, 1.085], mean observation: 0.102 [-42.007, 15.712], loss: 0.000132, mean_squared_error: 0.000264, mean_q: 0.487059\n",
      " 1730/2000: episode: 10, duration: 6.935s, episode steps: 119, steps per second: 17, episode reward: 0.634, mean reward: 0.005 [-0.004, 0.015], mean action: -0.261 [-1.237, 1.172], mean observation: 0.100 [-37.263, 14.844], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.491095\n",
      " 1864/2000: episode: 11, duration: 8.897s, episode steps: 134, steps per second: 15, episode reward: 0.620, mean reward: 0.005 [-0.004, 0.013], mean action: -0.181 [-1.250, 1.211], mean observation: 0.106 [-37.714, 14.616], loss: 0.000202, mean_squared_error: 0.000405, mean_q: 0.496739\n",
      "done, took 102.064 seconds\n",
      "\n",
      "\n",
      "iteration: 278\n",
      "Training for 2000 steps ...\n",
      "  173/2000: episode: 1, duration: 9.124s, episode steps: 173, steps per second: 19, episode reward: 0.664, mean reward: 0.004 [-0.002, 0.013], mean action: -0.103 [-1.173, 1.175], mean observation: 0.118 [-17.632, 14.912], loss: --, mean_squared_error: --, mean_q: --\n",
      "  344/2000: episode: 2, duration: 8.748s, episode steps: 171, steps per second: 20, episode reward: 0.655, mean reward: 0.004 [-0.002, 0.013], mean action: -0.117 [-1.157, 1.156], mean observation: 0.117 [-17.306, 14.886], loss: --, mean_squared_error: --, mean_q: --\n",
      "  522/2000: episode: 3, duration: 9.174s, episode steps: 178, steps per second: 19, episode reward: 0.654, mean reward: 0.004 [-0.002, 0.013], mean action: -0.106 [-1.322, 1.227], mean observation: 0.116 [-17.231, 15.429], loss: --, mean_squared_error: --, mean_q: --\n",
      "  699/2000: episode: 4, duration: 9.080s, episode steps: 177, steps per second: 19, episode reward: 0.650, mean reward: 0.004 [-0.003, 0.013], mean action: -0.093 [-1.135, 1.389], mean observation: 0.116 [-17.418, 15.166], loss: --, mean_squared_error: --, mean_q: --\n",
      "  839/2000: episode: 5, duration: 7.001s, episode steps: 140, steps per second: 20, episode reward: 0.646, mean reward: 0.005 [-0.002, 0.013], mean action: -0.191 [-1.210, 1.197], mean observation: 0.122 [-36.338, 14.917], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1008/2000: episode: 6, duration: 8.046s, episode steps: 169, steps per second: 21, episode reward: 0.724, mean reward: 0.004 [-0.002, 0.014], mean action: -0.145 [-1.175, 1.164], mean observation: 0.130 [-16.838, 15.102], loss: 0.000048, mean_squared_error: 0.000096, mean_q: 0.502910\n",
      " 1163/2000: episode: 7, duration: 9.767s, episode steps: 155, steps per second: 16, episode reward: 0.626, mean reward: 0.004 [-0.003, 0.013], mean action: -0.134 [-1.250, 1.119], mean observation: 0.117 [-39.345, 14.834], loss: 0.000239, mean_squared_error: 0.000478, mean_q: 0.483884\n",
      " 1370/2000: episode: 8, duration: 11.389s, episode steps: 207, steps per second: 18, episode reward: 0.570, mean reward: 0.003 [-0.002, 0.012], mean action: -0.094 [-1.250, 1.215], mean observation: 0.103 [-45.236, 14.835], loss: 0.000108, mean_squared_error: 0.000217, mean_q: 0.490218\n",
      " 1581/2000: episode: 9, duration: 11.965s, episode steps: 211, steps per second: 18, episode reward: 0.592, mean reward: 0.003 [-0.003, 0.013], mean action: -0.073 [-1.208, 1.241], mean observation: 0.110 [-15.569, 15.465], loss: 0.000116, mean_squared_error: 0.000233, mean_q: 0.488711\n",
      "done, took 104.204 seconds\n",
      "\n",
      "\n",
      "iteration: 279\n",
      "Training for 2000 steps ...\n",
      "  289/2000: episode: 1, duration: 13.100s, episode steps: 289, steps per second: 22, episode reward: 0.735, mean reward: 0.003 [-0.003, 0.013], mean action: -0.091 [-1.286, 1.213], mean observation: 0.126 [-19.634, 16.029], loss: --, mean_squared_error: --, mean_q: --\n",
      "  565/2000: episode: 2, duration: 12.181s, episode steps: 276, steps per second: 23, episode reward: 0.716, mean reward: 0.003 [-0.002, 0.013], mean action: -0.146 [-1.268, 1.175], mean observation: 0.126 [-15.021, 17.830], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1106/2000: episode: 3, duration: 24.202s, episode steps: 541, steps per second: 22, episode reward: 0.854, mean reward: 0.002 [-0.002, 0.013], mean action: -0.050 [-1.318, 1.504], mean observation: 0.132 [-15.261, 15.716], loss: 0.000156, mean_squared_error: 0.000311, mean_q: 0.492246\n",
      " 1494/2000: episode: 4, duration: 18.622s, episode steps: 388, steps per second: 21, episode reward: 0.763, mean reward: 0.002 [-0.002, 0.013], mean action: -0.046 [-1.366, 1.219], mean observation: 0.132 [-15.133, 16.528], loss: 0.000136, mean_squared_error: 0.000272, mean_q: 0.491435\n",
      "done, took 93.154 seconds\n",
      "\n",
      "\n",
      "iteration: 280\n",
      "Training for 2000 steps ...\n",
      "  115/2000: episode: 1, duration: 4.652s, episode steps: 115, steps per second: 25, episode reward: 0.698, mean reward: 0.006 [-0.001, 0.012], mean action: -0.185 [-1.183, 1.107], mean observation: 0.127 [-15.061, 14.854], loss: --, mean_squared_error: --, mean_q: --\n",
      "  230/2000: episode: 2, duration: 4.613s, episode steps: 115, steps per second: 25, episode reward: 0.704, mean reward: 0.006 [-0.001, 0.012], mean action: -0.171 [-1.122, 1.179], mean observation: 0.127 [-14.987, 14.850], loss: --, mean_squared_error: --, mean_q: --\n",
      "  344/2000: episode: 3, duration: 4.553s, episode steps: 114, steps per second: 25, episode reward: 0.708, mean reward: 0.006 [-0.001, 0.013], mean action: -0.203 [-1.260, 1.124], mean observation: 0.126 [-15.570, 14.749], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 4, duration: 4.643s, episode steps: 115, steps per second: 25, episode reward: 0.693, mean reward: 0.006 [-0.001, 0.012], mean action: -0.172 [-1.149, 1.214], mean observation: 0.127 [-15.251, 15.013], loss: --, mean_squared_error: --, mean_q: --\n",
      "  574/2000: episode: 5, duration: 4.688s, episode steps: 115, steps per second: 25, episode reward: 0.700, mean reward: 0.006 [-0.001, 0.012], mean action: -0.200 [-1.173, 1.168], mean observation: 0.127 [-15.520, 14.745], loss: --, mean_squared_error: --, mean_q: --\n",
      "  689/2000: episode: 6, duration: 4.647s, episode steps: 115, steps per second: 25, episode reward: 0.703, mean reward: 0.006 [-0.001, 0.012], mean action: -0.172 [-1.284, 1.149], mean observation: 0.127 [-15.265, 15.239], loss: --, mean_squared_error: --, mean_q: --\n",
      "  803/2000: episode: 7, duration: 4.369s, episode steps: 114, steps per second: 26, episode reward: 0.700, mean reward: 0.006 [-0.001, 0.013], mean action: -0.190 [-1.162, 1.193], mean observation: 0.126 [-14.838, 14.876], loss: --, mean_squared_error: --, mean_q: --\n",
      "  917/2000: episode: 8, duration: 4.558s, episode steps: 114, steps per second: 25, episode reward: 0.695, mean reward: 0.006 [-0.001, 0.012], mean action: -0.188 [-1.163, 1.110], mean observation: 0.127 [-15.323, 14.991], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1032/2000: episode: 9, duration: 4.976s, episode steps: 115, steps per second: 23, episode reward: 0.688, mean reward: 0.006 [-0.001, 0.012], mean action: -0.172 [-1.246, 1.231], mean observation: 0.127 [-15.345, 14.656], loss: 0.000037, mean_squared_error: 0.000073, mean_q: 0.489179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1146/2000: episode: 10, duration: 5.609s, episode steps: 114, steps per second: 20, episode reward: 0.710, mean reward: 0.006 [-0.001, 0.012], mean action: -0.162 [-1.263, 1.241], mean observation: 0.127 [-15.147, 14.710], loss: 0.000031, mean_squared_error: 0.000062, mean_q: 0.493156\n",
      " 1262/2000: episode: 11, duration: 5.523s, episode steps: 116, steps per second: 21, episode reward: 0.723, mean reward: 0.006 [-0.001, 0.013], mean action: -0.175 [-1.123, 1.302], mean observation: 0.127 [-15.297, 15.118], loss: 0.000134, mean_squared_error: 0.000268, mean_q: 0.492243\n",
      " 1541/2000: episode: 12, duration: 12.028s, episode steps: 279, steps per second: 23, episode reward: -0.784, mean reward: -0.003 [-0.020, 0.011], mean action: -0.100 [-1.185, 1.270], mean observation: 0.086 [-30.909, 19.096], loss: 0.000172, mean_squared_error: 0.000344, mean_q: 0.487038\n",
      " 1913/2000: episode: 13, duration: 16.338s, episode steps: 372, steps per second: 23, episode reward: -0.751, mean reward: -0.002 [-0.019, 0.011], mean action: -0.030 [-1.299, 1.329], mean observation: 0.096 [-17.401, 17.763], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.488762\n",
      "done, took 85.615 seconds\n",
      "\n",
      "\n",
      "iteration: 281\n",
      "Training for 2000 steps ...\n",
      "  179/2000: episode: 1, duration: 7.056s, episode steps: 179, steps per second: 25, episode reward: 0.712, mean reward: 0.004 [-0.003, 0.012], mean action: -0.122 [-1.233, 1.177], mean observation: 0.125 [-17.526, 14.998], loss: --, mean_squared_error: --, mean_q: --\n",
      "  401/2000: episode: 2, duration: 8.540s, episode steps: 222, steps per second: 26, episode reward: 0.706, mean reward: 0.003 [-0.003, 0.012], mean action: -0.055 [-1.291, 1.287], mean observation: 0.126 [-17.839, 16.329], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1168/2000: episode: 3, duration: 27.113s, episode steps: 767, steps per second: 28, episode reward: 0.726, mean reward: 0.001 [-0.003, 0.013], mean action: 0.013 [-1.403, 1.345], mean observation: 0.120 [-16.958, 19.574], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.482409\n",
      " 1350/2000: episode: 4, duration: 9.296s, episode steps: 182, steps per second: 20, episode reward: 0.700, mean reward: 0.004 [-0.003, 0.012], mean action: -0.178 [-1.226, 1.251], mean observation: 0.130 [-17.127, 16.025], loss: 0.000194, mean_squared_error: 0.000388, mean_q: 0.486802\n",
      " 1673/2000: episode: 5, duration: 15.850s, episode steps: 323, steps per second: 20, episode reward: 0.748, mean reward: 0.002 [-0.002, 0.013], mean action: -0.123 [-1.280, 1.299], mean observation: 0.133 [-38.718, 18.643], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.486248\n",
      " 1949/2000: episode: 6, duration: 13.135s, episode steps: 276, steps per second: 21, episode reward: 0.785, mean reward: 0.003 [-0.003, 0.014], mean action: -0.195 [-1.262, 1.212], mean observation: 0.128 [-17.612, 14.988], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.484617\n",
      "done, took 83.962 seconds\n",
      "\n",
      "\n",
      "iteration: 282\n",
      "Training for 2000 steps ...\n",
      "  189/2000: episode: 1, duration: 7.540s, episode steps: 189, steps per second: 25, episode reward: 0.741, mean reward: 0.004 [-0.003, 0.013], mean action: -0.254 [-1.247, 1.359], mean observation: 0.133 [-17.122, 15.485], loss: --, mean_squared_error: --, mean_q: --\n",
      "  377/2000: episode: 2, duration: 7.410s, episode steps: 188, steps per second: 25, episode reward: 0.714, mean reward: 0.004 [-0.003, 0.013], mean action: -0.232 [-1.181, 1.228], mean observation: 0.130 [-17.541, 14.927], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 3, duration: 12.424s, episode steps: 265, steps per second: 21, episode reward: 0.742, mean reward: 0.003 [-0.003, 0.013], mean action: -0.235 [-1.302, 1.273], mean observation: 0.128 [-40.565, 15.313], loss: --, mean_squared_error: --, mean_q: --\n",
      "  852/2000: episode: 4, duration: 8.138s, episode steps: 210, steps per second: 26, episode reward: 0.742, mean reward: 0.004 [-0.003, 0.013], mean action: -0.236 [-1.370, 1.359], mean observation: 0.131 [-17.838, 15.213], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1410/2000: episode: 5, duration: 24.291s, episode steps: 558, steps per second: 23, episode reward: 0.766, mean reward: 0.001 [-0.005, 0.013], mean action: -0.225 [-1.299, 1.354], mean observation: 0.130 [-25.984, 19.275], loss: 0.000109, mean_squared_error: 0.000217, mean_q: 0.483380\n",
      " 1520/2000: episode: 6, duration: 4.980s, episode steps: 110, steps per second: 22, episode reward: 0.729, mean reward: 0.007 [-0.002, 0.015], mean action: -0.298 [-1.193, 1.210], mean observation: 0.127 [-17.212, 14.962], loss: 0.000232, mean_squared_error: 0.000464, mean_q: 0.478806\n",
      " 1690/2000: episode: 7, duration: 9.940s, episode steps: 170, steps per second: 17, episode reward: 0.749, mean reward: 0.004 [-0.003, 0.013], mean action: -0.207 [-1.185, 1.138], mean observation: 0.137 [-35.140, 15.286], loss: 0.000130, mean_squared_error: 0.000260, mean_q: 0.477570\n",
      " 1782/2000: episode: 8, duration: 3.438s, episode steps: 92, steps per second: 27, episode reward: 0.437, mean reward: 0.005 [-0.003, 0.010], mean action: -0.320 [-1.135, 1.142], mean observation: 0.140 [-17.129, 14.987], loss: 0.000155, mean_squared_error: 0.000309, mean_q: 0.488012\n",
      " 1875/2000: episode: 9, duration: 3.408s, episode steps: 93, steps per second: 27, episode reward: 0.446, mean reward: 0.005 [-0.003, 0.010], mean action: -0.347 [-1.164, 1.144], mean observation: 0.141 [-17.407, 15.053], loss: 0.000065, mean_squared_error: 0.000131, mean_q: 0.481313\n",
      " 1969/2000: episode: 10, duration: 3.510s, episode steps: 94, steps per second: 27, episode reward: 0.449, mean reward: 0.005 [-0.003, 0.010], mean action: -0.334 [-1.229, 1.173], mean observation: 0.140 [-17.096, 15.378], loss: 0.000048, mean_squared_error: 0.000096, mean_q: 0.485344\n",
      "done, took 87.016 seconds\n",
      "\n",
      "\n",
      "iteration: 283\n",
      "Training for 2000 steps ...\n",
      "   93/2000: episode: 1, duration: 2.609s, episode steps: 93, steps per second: 36, episode reward: 0.448, mean reward: 0.005 [-0.003, 0.010], mean action: -0.330 [-1.147, 1.149], mean observation: 0.141 [-17.384, 15.055], loss: --, mean_squared_error: --, mean_q: --\n",
      "  184/2000: episode: 2, duration: 2.628s, episode steps: 91, steps per second: 35, episode reward: 0.437, mean reward: 0.005 [-0.003, 0.010], mean action: -0.313 [-1.120, 1.217], mean observation: 0.140 [-17.141, 14.748], loss: --, mean_squared_error: --, mean_q: --\n",
      "  275/2000: episode: 3, duration: 2.631s, episode steps: 91, steps per second: 35, episode reward: 0.437, mean reward: 0.005 [-0.003, 0.010], mean action: -0.345 [-1.223, 1.117], mean observation: 0.140 [-17.194, 15.218], loss: --, mean_squared_error: --, mean_q: --\n",
      "  366/2000: episode: 4, duration: 2.543s, episode steps: 91, steps per second: 36, episode reward: 0.436, mean reward: 0.005 [-0.003, 0.010], mean action: -0.331 [-1.198, 1.106], mean observation: 0.140 [-16.892, 15.186], loss: --, mean_squared_error: --, mean_q: --\n",
      "  458/2000: episode: 5, duration: 2.599s, episode steps: 92, steps per second: 35, episode reward: 0.440, mean reward: 0.005 [-0.003, 0.010], mean action: -0.324 [-1.160, 1.257], mean observation: 0.141 [-17.565, 14.722], loss: --, mean_squared_error: --, mean_q: --\n",
      "  551/2000: episode: 6, duration: 2.650s, episode steps: 93, steps per second: 35, episode reward: 0.450, mean reward: 0.005 [-0.003, 0.010], mean action: -0.330 [-1.217, 1.183], mean observation: 0.141 [-17.290, 15.336], loss: --, mean_squared_error: --, mean_q: --\n",
      "  643/2000: episode: 7, duration: 2.580s, episode steps: 92, steps per second: 36, episode reward: 0.443, mean reward: 0.005 [-0.003, 0.010], mean action: -0.325 [-1.162, 1.161], mean observation: 0.140 [-17.032, 15.297], loss: --, mean_squared_error: --, mean_q: --\n",
      "  735/2000: episode: 8, duration: 2.585s, episode steps: 92, steps per second: 36, episode reward: 0.441, mean reward: 0.005 [-0.003, 0.010], mean action: -0.332 [-1.165, 1.095], mean observation: 0.141 [-17.274, 15.284], loss: --, mean_squared_error: --, mean_q: --\n",
      "  827/2000: episode: 9, duration: 2.579s, episode steps: 92, steps per second: 36, episode reward: 0.443, mean reward: 0.005 [-0.003, 0.010], mean action: -0.347 [-1.243, 1.139], mean observation: 0.140 [-17.340, 14.585], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  920/2000: episode: 10, duration: 2.593s, episode steps: 93, steps per second: 36, episode reward: 0.449, mean reward: 0.005 [-0.003, 0.010], mean action: -0.344 [-1.174, 1.069], mean observation: 0.140 [-17.641, 15.261], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1012/2000: episode: 11, duration: 2.725s, episode steps: 92, steps per second: 34, episode reward: 0.440, mean reward: 0.005 [-0.003, 0.010], mean action: -0.313 [-1.137, 1.128], mean observation: 0.141 [-17.510, 15.532], loss: 0.000038, mean_squared_error: 0.000077, mean_q: 0.495192\n",
      " 1100/2000: episode: 12, duration: 3.512s, episode steps: 88, steps per second: 25, episode reward: 0.419, mean reward: 0.005 [-0.003, 0.010], mean action: -0.326 [-1.131, 1.108], mean observation: 0.137 [-17.108, 15.231], loss: 0.000032, mean_squared_error: 0.000063, mean_q: 0.486448\n",
      " 1196/2000: episode: 13, duration: 3.594s, episode steps: 96, steps per second: 27, episode reward: 0.452, mean reward: 0.005 [-0.002, 0.010], mean action: -0.342 [-1.209, 1.061], mean observation: 0.141 [-17.044, 15.242], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.477491\n",
      " 1289/2000: episode: 14, duration: 3.366s, episode steps: 93, steps per second: 28, episode reward: 0.442, mean reward: 0.005 [-0.002, 0.010], mean action: -0.344 [-1.246, 1.133], mean observation: 0.141 [-17.453, 14.968], loss: 0.000155, mean_squared_error: 0.000310, mean_q: 0.480374\n",
      " 1384/2000: episode: 15, duration: 3.468s, episode steps: 95, steps per second: 27, episode reward: 0.443, mean reward: 0.005 [-0.001, 0.010], mean action: -0.329 [-1.144, 1.126], mean observation: 0.141 [-16.677, 14.853], loss: 0.000175, mean_squared_error: 0.000351, mean_q: 0.473109\n",
      " 1478/2000: episode: 16, duration: 3.567s, episode steps: 94, steps per second: 26, episode reward: 0.448, mean reward: 0.005 [-0.001, 0.011], mean action: -0.324 [-1.182, 1.102], mean observation: 0.140 [-15.695, 15.090], loss: 0.000034, mean_squared_error: 0.000067, mean_q: 0.478051\n",
      " 1570/2000: episode: 17, duration: 3.409s, episode steps: 92, steps per second: 27, episode reward: 0.441, mean reward: 0.005 [-0.001, 0.010], mean action: -0.308 [-1.143, 1.259], mean observation: 0.141 [-15.374, 15.075], loss: 0.000048, mean_squared_error: 0.000095, mean_q: 0.476052\n",
      " 1665/2000: episode: 18, duration: 3.564s, episode steps: 95, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.010], mean action: -0.332 [-1.132, 1.092], mean observation: 0.141 [-15.425, 14.330], loss: 0.000169, mean_squared_error: 0.000338, mean_q: 0.467393\n",
      " 1763/2000: episode: 19, duration: 3.639s, episode steps: 98, steps per second: 27, episode reward: 0.457, mean reward: 0.005 [-0.002, 0.010], mean action: -0.315 [-1.163, 1.193], mean observation: 0.142 [-15.625, 14.718], loss: 0.000043, mean_squared_error: 0.000085, mean_q: 0.481189\n",
      " 1860/2000: episode: 20, duration: 3.645s, episode steps: 97, steps per second: 27, episode reward: 0.459, mean reward: 0.005 [-0.001, 0.010], mean action: -0.305 [-1.128, 1.155], mean observation: 0.142 [-15.400, 15.121], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.474956\n",
      " 1956/2000: episode: 21, duration: 3.646s, episode steps: 96, steps per second: 26, episode reward: 0.447, mean reward: 0.005 [-0.001, 0.010], mean action: -0.293 [-1.132, 1.112], mean observation: 0.141 [-15.177, 14.915], loss: 0.000861, mean_squared_error: 0.001723, mean_q: 0.472047\n",
      "done, took 66.232 seconds\n",
      "\n",
      "\n",
      "iteration: 284\n",
      "Training for 2000 steps ...\n",
      "  100/2000: episode: 1, duration: 2.597s, episode steps: 100, steps per second: 39, episode reward: 0.458, mean reward: 0.005 [-0.001, 0.010], mean action: -0.326 [-1.150, 1.176], mean observation: 0.141 [-15.472, 14.942], loss: --, mean_squared_error: --, mean_q: --\n",
      "  200/2000: episode: 2, duration: 2.616s, episode steps: 100, steps per second: 38, episode reward: 0.457, mean reward: 0.005 [-0.001, 0.010], mean action: -0.312 [-1.194, 1.127], mean observation: 0.142 [-15.200, 14.933], loss: --, mean_squared_error: --, mean_q: --\n",
      "  300/2000: episode: 3, duration: 2.531s, episode steps: 100, steps per second: 40, episode reward: 0.460, mean reward: 0.005 [-0.001, 0.010], mean action: -0.297 [-1.145, 1.154], mean observation: 0.142 [-15.249, 15.065], loss: --, mean_squared_error: --, mean_q: --\n",
      "  400/2000: episode: 4, duration: 2.607s, episode steps: 100, steps per second: 38, episode reward: 0.457, mean reward: 0.005 [-0.001, 0.010], mean action: -0.319 [-1.141, 1.261], mean observation: 0.142 [-15.675, 15.120], loss: --, mean_squared_error: --, mean_q: --\n",
      "  500/2000: episode: 5, duration: 2.575s, episode steps: 100, steps per second: 39, episode reward: 0.458, mean reward: 0.005 [-0.001, 0.010], mean action: -0.345 [-1.237, 1.060], mean observation: 0.142 [-15.040, 14.941], loss: --, mean_squared_error: --, mean_q: --\n",
      "  600/2000: episode: 6, duration: 2.573s, episode steps: 100, steps per second: 39, episode reward: 0.463, mean reward: 0.005 [-0.001, 0.010], mean action: -0.309 [-1.241, 1.265], mean observation: 0.142 [-15.407, 14.676], loss: --, mean_squared_error: --, mean_q: --\n",
      "  700/2000: episode: 7, duration: 2.543s, episode steps: 100, steps per second: 39, episode reward: 0.463, mean reward: 0.005 [-0.001, 0.010], mean action: -0.303 [-1.172, 1.172], mean observation: 0.142 [-15.451, 14.911], loss: --, mean_squared_error: --, mean_q: --\n",
      "  800/2000: episode: 8, duration: 2.631s, episode steps: 100, steps per second: 38, episode reward: 0.464, mean reward: 0.005 [-0.001, 0.010], mean action: -0.338 [-1.191, 1.175], mean observation: 0.141 [-15.102, 14.988], loss: --, mean_squared_error: --, mean_q: --\n",
      "  901/2000: episode: 9, duration: 2.628s, episode steps: 101, steps per second: 38, episode reward: 0.459, mean reward: 0.005 [-0.001, 0.010], mean action: -0.328 [-1.181, 1.194], mean observation: 0.143 [-15.303, 15.454], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1077/2000: episode: 10, duration: 5.937s, episode steps: 176, steps per second: 30, episode reward: 0.476, mean reward: 0.003 [-0.002, 0.010], mean action: -0.257 [-1.186, 1.155], mean observation: 0.136 [-15.488, 15.214], loss: 0.000129, mean_squared_error: 0.000259, mean_q: 0.472665\n",
      " 1176/2000: episode: 11, duration: 3.532s, episode steps: 99, steps per second: 28, episode reward: 0.456, mean reward: 0.005 [-0.001, 0.010], mean action: -0.321 [-1.195, 1.190], mean observation: 0.141 [-15.509, 14.543], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.471878\n",
      " 1274/2000: episode: 12, duration: 3.716s, episode steps: 98, steps per second: 26, episode reward: 0.451, mean reward: 0.005 [-0.001, 0.010], mean action: -0.322 [-1.241, 1.150], mean observation: 0.140 [-15.265, 14.791], loss: 0.000034, mean_squared_error: 0.000068, mean_q: 0.476924\n",
      "done, took 70.284 seconds\n",
      "\n",
      "\n",
      "iteration: 285\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.758s, episode steps: 1000, steps per second: 42, episode reward: 0.069, mean reward: 0.000 [-0.002, 0.011], mean action: -0.218 [-1.576, 1.390], mean observation: 0.106 [-15.064, 14.170], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1521/2000: episode: 2, duration: 23.522s, episode steps: 521, steps per second: 22, episode reward: 0.466, mean reward: 0.001 [-0.002, 0.011], mean action: -0.121 [-1.349, 1.370], mean observation: 0.118 [-27.704, 14.367], loss: 0.000099, mean_squared_error: 0.000197, mean_q: 0.469170\n",
      "done, took 71.049 seconds\n",
      "\n",
      "\n",
      "iteration: 286\n",
      "Training for 2000 steps ...\n",
      "  402/2000: episode: 1, duration: 14.967s, episode steps: 402, steps per second: 27, episode reward: 0.546, mean reward: 0.001 [-0.002, 0.011], mean action: -0.003 [-1.355, 1.275], mean observation: 0.113 [-56.975, 17.709], loss: --, mean_squared_error: --, mean_q: --\n",
      "  832/2000: episode: 2, duration: 15.418s, episode steps: 430, steps per second: 28, episode reward: 0.550, mean reward: 0.001 [-0.002, 0.011], mean action: 0.001 [-1.309, 1.457], mean observation: 0.117 [-51.221, 20.208], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1353/2000: episode: 3, duration: 23.595s, episode steps: 521, steps per second: 22, episode reward: 0.533, mean reward: 0.001 [-0.002, 0.011], mean action: -0.040 [-1.359, 1.225], mean observation: 0.119 [-43.874, 17.715], loss: 0.000157, mean_squared_error: 0.000313, mean_q: 0.468602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1524/2000: episode: 4, duration: 7.497s, episode steps: 171, steps per second: 23, episode reward: 0.559, mean reward: 0.003 [-0.002, 0.011], mean action: -0.068 [-1.194, 1.178], mean observation: 0.120 [-13.537, 18.511], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.460464\n",
      " 1703/2000: episode: 5, duration: 7.854s, episode steps: 179, steps per second: 23, episode reward: 0.582, mean reward: 0.003 [-0.001, 0.011], mean action: -0.082 [-1.118, 1.211], mean observation: 0.121 [-13.660, 17.858], loss: 0.000031, mean_squared_error: 0.000062, mean_q: 0.468737\n",
      " 1895/2000: episode: 6, duration: 9.755s, episode steps: 192, steps per second: 20, episode reward: 0.549, mean reward: 0.003 [-0.002, 0.011], mean action: -0.086 [-1.198, 1.181], mean observation: 0.121 [-24.215, 17.882], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.465609\n",
      "done, took 83.670 seconds\n",
      "\n",
      "\n",
      "iteration: 287\n",
      "Training for 2000 steps ...\n",
      "  209/2000: episode: 1, duration: 8.400s, episode steps: 209, steps per second: 25, episode reward: 0.568, mean reward: 0.003 [-0.002, 0.011], mean action: -0.063 [-1.278, 1.249], mean observation: 0.118 [-40.437, 18.745], loss: --, mean_squared_error: --, mean_q: --\n",
      "  399/2000: episode: 2, duration: 7.186s, episode steps: 190, steps per second: 26, episode reward: 0.576, mean reward: 0.003 [-0.002, 0.011], mean action: -0.096 [-1.253, 1.283], mean observation: 0.123 [-14.362, 17.418], loss: --, mean_squared_error: --, mean_q: --\n",
      "  572/2000: episode: 3, duration: 6.905s, episode steps: 173, steps per second: 25, episode reward: 0.538, mean reward: 0.003 [-0.002, 0.011], mean action: -0.076 [-1.234, 1.140], mean observation: 0.123 [-37.158, 17.800], loss: --, mean_squared_error: --, mean_q: --\n",
      "  744/2000: episode: 4, duration: 6.491s, episode steps: 172, steps per second: 26, episode reward: 0.559, mean reward: 0.003 [-0.002, 0.011], mean action: -0.070 [-1.201, 1.251], mean observation: 0.124 [-23.746, 17.566], loss: --, mean_squared_error: --, mean_q: --\n",
      "  963/2000: episode: 5, duration: 8.358s, episode steps: 219, steps per second: 26, episode reward: 0.554, mean reward: 0.003 [-0.002, 0.011], mean action: -0.058 [-1.176, 1.246], mean observation: 0.125 [-21.992, 18.138], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1161/2000: episode: 6, duration: 10.047s, episode steps: 198, steps per second: 20, episode reward: 0.535, mean reward: 0.003 [-0.002, 0.011], mean action: -0.120 [-1.239, 1.136], mean observation: 0.124 [-28.423, 18.105], loss: 0.000078, mean_squared_error: 0.000157, mean_q: 0.465259\n",
      " 1303/2000: episode: 7, duration: 6.224s, episode steps: 142, steps per second: 23, episode reward: 0.566, mean reward: 0.004 [-0.002, 0.011], mean action: -0.188 [-1.393, 1.184], mean observation: 0.120 [-11.913, 17.976], loss: 0.000043, mean_squared_error: 0.000086, mean_q: 0.462576\n",
      " 1443/2000: episode: 8, duration: 5.994s, episode steps: 140, steps per second: 23, episode reward: 0.544, mean reward: 0.004 [-0.002, 0.011], mean action: -0.154 [-1.182, 1.271], mean observation: 0.122 [-11.612, 18.197], loss: 0.000139, mean_squared_error: 0.000278, mean_q: 0.461746\n",
      " 1611/2000: episode: 9, duration: 7.347s, episode steps: 168, steps per second: 23, episode reward: 0.517, mean reward: 0.003 [-0.002, 0.011], mean action: -0.229 [-1.231, 1.250], mean observation: 0.116 [-11.547, 17.850], loss: 0.000499, mean_squared_error: 0.000997, mean_q: 0.453670\n",
      " 1765/2000: episode: 10, duration: 6.344s, episode steps: 154, steps per second: 24, episode reward: 0.541, mean reward: 0.004 [-0.002, 0.011], mean action: -0.249 [-1.375, 1.126], mean observation: 0.115 [-12.727, 17.925], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.460900\n",
      " 1905/2000: episode: 11, duration: 5.656s, episode steps: 140, steps per second: 25, episode reward: 0.543, mean reward: 0.004 [-0.001, 0.011], mean action: -0.179 [-1.277, 1.225], mean observation: 0.117 [-13.633, 18.420], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.461181\n",
      "done, took 83.213 seconds\n",
      "\n",
      "\n",
      "iteration: 288\n",
      "Training for 2000 steps ...\n",
      "  152/2000: episode: 1, duration: 5.085s, episode steps: 152, steps per second: 30, episode reward: 0.530, mean reward: 0.003 [-0.002, 0.011], mean action: -0.164 [-1.244, 1.187], mean observation: 0.115 [-10.430, 18.420], loss: --, mean_squared_error: --, mean_q: --\n",
      "  300/2000: episode: 2, duration: 4.824s, episode steps: 148, steps per second: 31, episode reward: 0.526, mean reward: 0.004 [-0.002, 0.011], mean action: -0.151 [-1.116, 1.145], mean observation: 0.115 [-10.762, 18.102], loss: --, mean_squared_error: --, mean_q: --\n",
      "  459/2000: episode: 3, duration: 5.210s, episode steps: 159, steps per second: 31, episode reward: 0.543, mean reward: 0.003 [-0.002, 0.011], mean action: -0.189 [-1.272, 1.163], mean observation: 0.114 [-10.655, 17.890], loss: --, mean_squared_error: --, mean_q: --\n",
      "  609/2000: episode: 4, duration: 4.955s, episode steps: 150, steps per second: 30, episode reward: 0.525, mean reward: 0.003 [-0.002, 0.011], mean action: -0.135 [-1.189, 1.293], mean observation: 0.115 [-10.560, 18.444], loss: --, mean_squared_error: --, mean_q: --\n",
      "  759/2000: episode: 5, duration: 4.789s, episode steps: 150, steps per second: 31, episode reward: 0.544, mean reward: 0.004 [-0.002, 0.011], mean action: -0.189 [-1.375, 1.276], mean observation: 0.117 [-12.592, 18.508], loss: --, mean_squared_error: --, mean_q: --\n",
      "  899/2000: episode: 6, duration: 4.325s, episode steps: 140, steps per second: 32, episode reward: 0.517, mean reward: 0.004 [-0.002, 0.011], mean action: -0.183 [-1.152, 1.230], mean observation: 0.118 [-10.473, 18.113], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1041/2000: episode: 7, duration: 5.097s, episode steps: 142, steps per second: 28, episode reward: 0.524, mean reward: 0.004 [-0.002, 0.011], mean action: -0.161 [-1.161, 1.159], mean observation: 0.115 [-10.802, 17.849], loss: 0.000040, mean_squared_error: 0.000081, mean_q: 0.457138\n",
      " 1251/2000: episode: 8, duration: 9.617s, episode steps: 210, steps per second: 22, episode reward: 0.510, mean reward: 0.002 [-0.002, 0.011], mean action: -0.166 [-1.219, 1.265], mean observation: 0.116 [-11.684, 18.195], loss: 0.000129, mean_squared_error: 0.000257, mean_q: 0.458988\n",
      " 1387/2000: episode: 9, duration: 5.456s, episode steps: 136, steps per second: 25, episode reward: 0.540, mean reward: 0.004 [-0.002, 0.011], mean action: -0.113 [-1.139, 1.094], mean observation: 0.116 [-11.581, 18.207], loss: 0.000387, mean_squared_error: 0.000774, mean_q: 0.456648\n",
      " 1780/2000: episode: 10, duration: 18.346s, episode steps: 393, steps per second: 21, episode reward: 0.572, mean reward: 0.001 [-0.001, 0.011], mean action: -0.100 [-1.227, 1.225], mean observation: 0.118 [-20.568, 18.144], loss: 0.000086, mean_squared_error: 0.000172, mean_q: 0.458007\n",
      "done, took 78.711 seconds\n",
      "\n",
      "\n",
      "iteration: 289\n",
      "Training for 2000 steps ...\n",
      "  438/2000: episode: 1, duration: 17.235s, episode steps: 438, steps per second: 25, episode reward: 0.556, mean reward: 0.001 [-0.002, 0.011], mean action: -0.247 [-1.435, 1.139], mean observation: 0.115 [-21.106, 17.494], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1282/2000: episode: 2, duration: 34.397s, episode steps: 844, steps per second: 25, episode reward: 0.550, mean reward: 0.001 [-0.002, 0.011], mean action: -0.184 [-1.238, 1.338], mean observation: 0.112 [-26.791, 18.253], loss: 0.000199, mean_squared_error: 0.000398, mean_q: 0.458206\n",
      " 1652/2000: episode: 3, duration: 17.371s, episode steps: 370, steps per second: 21, episode reward: 0.578, mean reward: 0.002 [-0.002, 0.011], mean action: -0.149 [-1.245, 1.355], mean observation: 0.112 [-11.478, 18.217], loss: 0.000139, mean_squared_error: 0.000279, mean_q: 0.455713\n",
      " 1935/2000: episode: 4, duration: 13.265s, episode steps: 283, steps per second: 21, episode reward: 0.563, mean reward: 0.002 [-0.002, 0.011], mean action: -0.138 [-1.335, 1.281], mean observation: 0.115 [-11.542, 18.012], loss: 0.000049, mean_squared_error: 0.000097, mean_q: 0.454478\n",
      "done, took 85.756 seconds\n",
      "\n",
      "\n",
      "iteration: 290\n",
      "Training for 2000 steps ...\n",
      "  921/2000: episode: 1, duration: 28.096s, episode steps: 921, steps per second: 33, episode reward: 0.505, mean reward: 0.001 [-0.002, 0.010], mean action: -0.069 [-1.247, 1.321], mean observation: 0.113 [-18.702, 18.236], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1656/2000: episode: 2, duration: 31.666s, episode steps: 735, steps per second: 23, episode reward: 0.543, mean reward: 0.001 [-0.002, 0.010], mean action: 0.001 [-1.325, 1.412], mean observation: 0.117 [-13.876, 18.312], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.456786\n",
      "done, took 76.842 seconds\n",
      "\n",
      "\n",
      "iteration: 291\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 35.095s, episode steps: 1000, steps per second: 28, episode reward: 0.135, mean reward: 0.000 [-0.003, 0.010], mean action: 0.093 [-1.481, 1.553], mean observation: 0.106 [-16.891, 18.183], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1692/2000: episode: 2, duration: 33.521s, episode steps: 692, steps per second: 21, episode reward: 0.540, mean reward: 0.001 [-0.003, 0.011], mean action: -0.007 [-1.337, 1.432], mean observation: 0.116 [-18.858, 18.423], loss: 0.000070, mean_squared_error: 0.000139, mean_q: 0.452885\n",
      " 1933/2000: episode: 3, duration: 11.668s, episode steps: 241, steps per second: 21, episode reward: 0.549, mean reward: 0.002 [-0.002, 0.010], mean action: 0.108 [-1.311, 1.154], mean observation: 0.123 [-20.096, 17.966], loss: 0.000070, mean_squared_error: 0.000140, mean_q: 0.455968\n",
      "done, took 83.896 seconds\n",
      "\n",
      "\n",
      "iteration: 292\n",
      "Training for 2000 steps ...\n",
      "  221/2000: episode: 1, duration: 8.080s, episode steps: 221, steps per second: 27, episode reward: 0.531, mean reward: 0.002 [-0.001, 0.010], mean action: 0.164 [-1.210, 1.232], mean observation: 0.125 [-22.870, 18.231], loss: --, mean_squared_error: --, mean_q: --\n",
      "  419/2000: episode: 2, duration: 7.457s, episode steps: 198, steps per second: 27, episode reward: 0.538, mean reward: 0.003 [-0.001, 0.010], mean action: 0.177 [-1.217, 1.288], mean observation: 0.126 [-23.779, 18.042], loss: --, mean_squared_error: --, mean_q: --\n",
      "  599/2000: episode: 3, duration: 6.447s, episode steps: 180, steps per second: 28, episode reward: 0.543, mean reward: 0.003 [-0.001, 0.010], mean action: 0.133 [-1.219, 1.156], mean observation: 0.129 [-19.631, 18.395], loss: --, mean_squared_error: --, mean_q: --\n",
      "  783/2000: episode: 4, duration: 6.709s, episode steps: 184, steps per second: 27, episode reward: 0.526, mean reward: 0.003 [-0.001, 0.011], mean action: 0.160 [-1.215, 1.231], mean observation: 0.127 [-20.624, 18.145], loss: --, mean_squared_error: --, mean_q: --\n",
      "  966/2000: episode: 5, duration: 6.762s, episode steps: 183, steps per second: 27, episode reward: 0.561, mean reward: 0.003 [-0.001, 0.011], mean action: 0.164 [-1.136, 1.203], mean observation: 0.128 [-20.896, 18.077], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1157/2000: episode: 6, duration: 8.638s, episode steps: 191, steps per second: 22, episode reward: 0.557, mean reward: 0.003 [-0.001, 0.011], mean action: 0.147 [-1.280, 1.191], mean observation: 0.127 [-25.987, 18.257], loss: 0.000084, mean_squared_error: 0.000168, mean_q: 0.454578\n",
      " 1328/2000: episode: 7, duration: 7.981s, episode steps: 171, steps per second: 21, episode reward: 0.539, mean reward: 0.003 [-0.001, 0.010], mean action: 0.156 [-1.154, 1.266], mean observation: 0.130 [-13.346, 18.395], loss: 0.000098, mean_squared_error: 0.000197, mean_q: 0.454922\n",
      " 1538/2000: episode: 8, duration: 10.376s, episode steps: 210, steps per second: 20, episode reward: 0.570, mean reward: 0.003 [-0.001, 0.011], mean action: 0.130 [-1.336, 1.186], mean observation: 0.127 [-21.437, 18.437], loss: 0.000104, mean_squared_error: 0.000207, mean_q: 0.448193\n",
      " 1668/2000: episode: 9, duration: 5.402s, episode steps: 130, steps per second: 24, episode reward: 0.538, mean reward: 0.004 [0.000, 0.011], mean action: 0.130 [-1.166, 1.159], mean observation: 0.132 [-10.199, 18.091], loss: 0.000042, mean_squared_error: 0.000083, mean_q: 0.452697\n",
      " 1850/2000: episode: 10, duration: 9.786s, episode steps: 182, steps per second: 19, episode reward: 0.516, mean reward: 0.003 [-0.001, 0.011], mean action: 0.133 [-1.211, 1.237], mean observation: 0.120 [-14.648, 17.675], loss: 0.000042, mean_squared_error: 0.000083, mean_q: 0.455290\n",
      "done, took 85.229 seconds\n",
      "\n",
      "\n",
      "iteration: 293\n",
      "Training for 2000 steps ...\n",
      "  338/2000: episode: 1, duration: 12.803s, episode steps: 338, steps per second: 26, episode reward: 0.528, mean reward: 0.002 [-0.001, 0.010], mean action: 0.146 [-1.296, 1.263], mean observation: 0.122 [-10.192, 18.104], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1323/2000: episode: 2, duration: 37.420s, episode steps: 985, steps per second: 26, episode reward: 0.562, mean reward: 0.001 [-0.001, 0.010], mean action: 0.096 [-1.433, 1.329], mean observation: 0.123 [-10.690, 17.623], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.452145\n",
      " 1472/2000: episode: 3, duration: 6.402s, episode steps: 149, steps per second: 23, episode reward: 0.559, mean reward: 0.004 [0.000, 0.010], mean action: 0.135 [-1.259, 1.307], mean observation: 0.129 [-10.286, 18.964], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.449990\n",
      " 1628/2000: episode: 4, duration: 6.656s, episode steps: 156, steps per second: 23, episode reward: 0.549, mean reward: 0.004 [0.000, 0.010], mean action: 0.189 [-1.209, 1.192], mean observation: 0.126 [-10.206, 18.999], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.451756\n",
      " 1787/2000: episode: 5, duration: 7.593s, episode steps: 159, steps per second: 21, episode reward: 0.504, mean reward: 0.003 [-0.001, 0.010], mean action: 0.247 [-1.304, 1.186], mean observation: 0.117 [-10.179, 18.079], loss: 0.000038, mean_squared_error: 0.000077, mean_q: 0.449168\n",
      " 1931/2000: episode: 6, duration: 6.911s, episode steps: 144, steps per second: 21, episode reward: 0.504, mean reward: 0.004 [-0.002, 0.010], mean action: 0.224 [-1.182, 1.241], mean observation: 0.117 [-10.251, 18.530], loss: 0.000124, mean_squared_error: 0.000247, mean_q: 0.453768\n",
      "done, took 81.337 seconds\n",
      "\n",
      "\n",
      "iteration: 294\n",
      "Training for 2000 steps ...\n",
      "  158/2000: episode: 1, duration: 5.619s, episode steps: 158, steps per second: 28, episode reward: 0.525, mean reward: 0.003 [-0.001, 0.010], mean action: 0.226 [-1.159, 1.156], mean observation: 0.125 [-17.033, 18.310], loss: --, mean_squared_error: --, mean_q: --\n",
      "  311/2000: episode: 2, duration: 5.345s, episode steps: 153, steps per second: 29, episode reward: 0.520, mean reward: 0.003 [-0.001, 0.010], mean action: 0.243 [-1.096, 1.143], mean observation: 0.126 [-17.578, 18.318], loss: --, mean_squared_error: --, mean_q: --\n",
      "  473/2000: episode: 3, duration: 6.100s, episode steps: 162, steps per second: 27, episode reward: 0.537, mean reward: 0.003 [-0.002, 0.010], mean action: 0.244 [-1.181, 1.232], mean observation: 0.125 [-17.735, 18.253], loss: --, mean_squared_error: --, mean_q: --\n",
      "  722/2000: episode: 4, duration: 9.264s, episode steps: 249, steps per second: 27, episode reward: 0.544, mean reward: 0.002 [-0.001, 0.010], mean action: 0.203 [-1.275, 1.199], mean observation: 0.120 [-17.333, 18.648], loss: --, mean_squared_error: --, mean_q: --\n",
      "  947/2000: episode: 5, duration: 7.715s, episode steps: 225, steps per second: 29, episode reward: 0.549, mean reward: 0.002 [-0.002, 0.010], mean action: 0.231 [-1.212, 1.305], mean observation: 0.126 [-17.235, 17.793], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1227/2000: episode: 6, duration: 13.010s, episode steps: 280, steps per second: 22, episode reward: 0.516, mean reward: 0.002 [-0.002, 0.010], mean action: 0.266 [-1.170, 1.290], mean observation: 0.121 [-18.621, 18.575], loss: 0.000096, mean_squared_error: 0.000192, mean_q: 0.451322\n",
      " 1355/2000: episode: 7, duration: 5.419s, episode steps: 128, steps per second: 24, episode reward: 0.535, mean reward: 0.004 [0.000, 0.010], mean action: 0.209 [-1.125, 1.201], mean observation: 0.128 [-10.220, 18.311], loss: 0.000038, mean_squared_error: 0.000076, mean_q: 0.446513\n",
      " 1530/2000: episode: 8, duration: 6.925s, episode steps: 175, steps per second: 25, episode reward: 0.534, mean reward: 0.003 [-0.000, 0.010], mean action: 0.201 [-1.276, 1.217], mean observation: 0.124 [-10.556, 17.897], loss: 0.000259, mean_squared_error: 0.000519, mean_q: 0.445255\n",
      " 1708/2000: episode: 9, duration: 7.373s, episode steps: 178, steps per second: 24, episode reward: 0.548, mean reward: 0.003 [-0.001, 0.010], mean action: 0.207 [-1.147, 1.225], mean observation: 0.124 [-10.179, 17.850], loss: 0.000086, mean_squared_error: 0.000171, mean_q: 0.443314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1865/2000: episode: 10, duration: 6.248s, episode steps: 157, steps per second: 25, episode reward: 0.520, mean reward: 0.003 [-0.001, 0.010], mean action: 0.175 [-1.360, 1.263], mean observation: 0.127 [-10.229, 18.516], loss: 0.000151, mean_squared_error: 0.000302, mean_q: 0.449471\n",
      "done, took 78.606 seconds\n",
      "\n",
      "\n",
      "iteration: 295\n",
      "Training for 2000 steps ...\n",
      "  168/2000: episode: 1, duration: 5.697s, episode steps: 168, steps per second: 29, episode reward: 0.555, mean reward: 0.003 [-0.001, 0.010], mean action: 0.225 [-1.131, 1.154], mean observation: 0.130 [-15.739, 18.023], loss: --, mean_squared_error: --, mean_q: --\n",
      "  342/2000: episode: 2, duration: 5.696s, episode steps: 174, steps per second: 31, episode reward: 0.540, mean reward: 0.003 [-0.001, 0.010], mean action: 0.216 [-1.172, 1.189], mean observation: 0.130 [-15.497, 18.331], loss: --, mean_squared_error: --, mean_q: --\n",
      "  505/2000: episode: 3, duration: 5.436s, episode steps: 163, steps per second: 30, episode reward: 0.548, mean reward: 0.003 [-0.001, 0.010], mean action: 0.208 [-1.171, 1.208], mean observation: 0.128 [-16.609, 18.161], loss: --, mean_squared_error: --, mean_q: --\n",
      "  670/2000: episode: 4, duration: 5.428s, episode steps: 165, steps per second: 30, episode reward: 0.530, mean reward: 0.003 [-0.001, 0.010], mean action: 0.211 [-1.206, 1.095], mean observation: 0.130 [-19.091, 18.077], loss: --, mean_squared_error: --, mean_q: --\n",
      "  845/2000: episode: 5, duration: 5.823s, episode steps: 175, steps per second: 30, episode reward: 0.543, mean reward: 0.003 [-0.002, 0.010], mean action: 0.211 [-1.232, 1.190], mean observation: 0.130 [-15.048, 17.833], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1004/2000: episode: 6, duration: 5.203s, episode steps: 159, steps per second: 31, episode reward: 0.539, mean reward: 0.003 [-0.001, 0.010], mean action: 0.236 [-1.295, 1.287], mean observation: 0.131 [-14.200, 18.521], loss: 0.000025, mean_squared_error: 0.000050, mean_q: 0.459561\n",
      " 1178/2000: episode: 7, duration: 7.647s, episode steps: 174, steps per second: 23, episode reward: 0.558, mean reward: 0.003 [-0.001, 0.010], mean action: 0.202 [-1.233, 1.175], mean observation: 0.130 [-14.394, 18.239], loss: 0.000096, mean_squared_error: 0.000193, mean_q: 0.438823\n",
      " 1457/2000: episode: 8, duration: 12.633s, episode steps: 279, steps per second: 22, episode reward: 0.551, mean reward: 0.002 [-0.002, 0.010], mean action: 0.189 [-1.173, 1.249], mean observation: 0.124 [-19.528, 18.151], loss: 0.000180, mean_squared_error: 0.000360, mean_q: 0.437941\n",
      " 1779/2000: episode: 9, duration: 13.699s, episode steps: 322, steps per second: 24, episode reward: 0.555, mean reward: 0.002 [-0.002, 0.010], mean action: 0.144 [-1.364, 1.260], mean observation: 0.121 [-21.424, 18.519], loss: 0.000103, mean_squared_error: 0.000206, mean_q: 0.438259\n",
      "done, took 77.079 seconds\n",
      "\n",
      "\n",
      "iteration: 296\n",
      "Training for 2000 steps ...\n",
      "  591/2000: episode: 1, duration: 20.312s, episode steps: 591, steps per second: 29, episode reward: 0.545, mean reward: 0.001 [-0.002, 0.010], mean action: 0.111 [-1.418, 1.280], mean observation: 0.118 [-16.442, 18.483], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1591/2000: episode: 2, duration: 36.620s, episode steps: 1000, steps per second: 27, episode reward: 0.147, mean reward: 0.000 [-0.002, 0.010], mean action: 0.140 [-1.423, 1.358], mean observation: 0.114 [-13.921, 18.609], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.438489\n",
      "done, took 73.869 seconds\n",
      "\n",
      "\n",
      "iteration: 297\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.620s, episode steps: 1000, steps per second: 42, episode reward: 0.141, mean reward: 0.000 [-0.003, 0.010], mean action: 0.037 [-1.383, 1.294], mean observation: 0.115 [-17.493, 18.112], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 39.215s, episode steps: 1000, steps per second: 26, episode reward: 0.133, mean reward: 0.000 [-0.003, 0.010], mean action: -0.067 [-1.314, 1.309], mean observation: 0.117 [-16.909, 18.120], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.433447\n",
      "done, took 62.855 seconds\n",
      "\n",
      "\n",
      "iteration: 298\n",
      "Training for 2000 steps ...\n",
      "  334/2000: episode: 1, duration: 12.614s, episode steps: 334, steps per second: 26, episode reward: 0.701, mean reward: 0.002 [-0.001, 0.012], mean action: 0.015 [-1.307, 1.349], mean observation: 0.114 [-19.256, 17.702], loss: --, mean_squared_error: --, mean_q: --\n",
      "  782/2000: episode: 2, duration: 17.152s, episode steps: 448, steps per second: 26, episode reward: 0.701, mean reward: 0.002 [-0.001, 0.012], mean action: 0.011 [-1.299, 1.497], mean observation: 0.116 [-15.573, 18.020], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1782/2000: episode: 3, duration: 39.579s, episode steps: 1000, steps per second: 25, episode reward: 0.135, mean reward: 0.000 [-0.001, 0.010], mean action: -0.004 [-1.532, 1.355], mean observation: 0.116 [-19.042, 18.389], loss: 0.000109, mean_squared_error: 0.000219, mean_q: 0.429205\n",
      "done, took 78.662 seconds\n",
      "\n",
      "\n",
      "iteration: 299\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 35.396s, episode steps: 1000, steps per second: 28, episode reward: 0.140, mean reward: 0.000 [-0.002, 0.011], mean action: -0.102 [-1.385, 1.424], mean observation: 0.113 [-23.770, 18.347], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1440/2000: episode: 2, duration: 19.259s, episode steps: 440, steps per second: 23, episode reward: 0.704, mean reward: 0.002 [-0.002, 0.012], mean action: -0.018 [-1.311, 1.328], mean observation: 0.114 [-21.969, 18.718], loss: 0.000058, mean_squared_error: 0.000115, mean_q: 0.424069\n",
      " 1738/2000: episode: 3, duration: 12.721s, episode steps: 298, steps per second: 23, episode reward: 0.673, mean reward: 0.002 [-0.002, 0.012], mean action: 0.001 [-1.303, 1.272], mean observation: 0.115 [-18.232, 18.063], loss: 0.000073, mean_squared_error: 0.000146, mean_q: 0.424616\n",
      "done, took 81.295 seconds\n",
      "\n",
      "\n",
      "iteration: 300\n",
      "Training for 2000 steps ...\n",
      "  668/2000: episode: 1, duration: 24.740s, episode steps: 668, steps per second: 27, episode reward: 0.677, mean reward: 0.001 [-0.003, 0.013], mean action: -0.026 [-1.325, 1.449], mean observation: 0.112 [-17.330, 17.392], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1668/2000: episode: 2, duration: 37.096s, episode steps: 1000, steps per second: 27, episode reward: 0.105, mean reward: 0.000 [-0.002, 0.011], mean action: 0.017 [-1.569, 1.501], mean observation: 0.113 [-19.133, 18.651], loss: 0.000119, mean_squared_error: 0.000239, mean_q: 0.423261\n",
      "done, took 72.810 seconds\n",
      "\n",
      "\n",
      "iteration: 301\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 26.317s, episode steps: 1000, steps per second: 38, episode reward: 0.087, mean reward: 0.000 [-0.002, 0.010], mean action: 0.118 [-1.384, 1.362], mean observation: 0.117 [-10.919, 15.516], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 35.115s, episode steps: 1000, steps per second: 28, episode reward: 0.115, mean reward: 0.000 [-0.003, 0.010], mean action: 0.192 [-1.370, 1.292], mean observation: 0.114 [-10.967, 15.609], loss: 0.000086, mean_squared_error: 0.000173, mean_q: 0.421674\n",
      "done, took 61.450 seconds\n",
      "\n",
      "\n",
      "iteration: 302\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 20.849s, episode steps: 1000, steps per second: 48, episode reward: 0.111, mean reward: 0.000 [-0.002, 0.010], mean action: 0.271 [-1.525, 1.500], mean observation: 0.112 [-10.843, 20.410], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 34.509s, episode steps: 1000, steps per second: 29, episode reward: 0.025, mean reward: 0.000 [-0.002, 0.010], mean action: 0.007 [-1.348, 1.406], mean observation: 0.112 [-10.797, 20.465], loss: 0.000064, mean_squared_error: 0.000129, mean_q: 0.422172\n",
      "done, took 55.376 seconds\n",
      "\n",
      "\n",
      "iteration: 303\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 19.506s, episode steps: 1000, steps per second: 51, episode reward: 0.022, mean reward: 0.000 [-0.003, 0.010], mean action: 0.110 [-1.617, 1.484], mean observation: 0.112 [-10.786, 20.192], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1319/2000: episode: 2, duration: 11.395s, episode steps: 319, steps per second: 28, episode reward: -0.877, mean reward: -0.003 [-0.019, 0.010], mean action: -0.010 [-1.298, 1.227], mean observation: 0.093 [-10.820, 20.341], loss: 0.000079, mean_squared_error: 0.000157, mean_q: 0.426795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1514/2000: episode: 3, duration: 6.510s, episode steps: 195, steps per second: 30, episode reward: -0.867, mean reward: -0.004 [-0.019, 0.010], mean action: -0.003 [-1.179, 1.166], mean observation: 0.082 [-10.845, 20.253], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.418207\n",
      " 1678/2000: episode: 4, duration: 5.686s, episode steps: 164, steps per second: 29, episode reward: -0.873, mean reward: -0.005 [-0.020, 0.010], mean action: -0.003 [-1.219, 1.159], mean observation: 0.064 [-14.154, 20.589], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.417564\n",
      " 1830/2000: episode: 5, duration: 5.276s, episode steps: 152, steps per second: 29, episode reward: -0.858, mean reward: -0.006 [-0.020, 0.010], mean action: 0.011 [-1.118, 1.142], mean observation: 0.065 [-14.277, 20.423], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.414162\n",
      " 1992/2000: episode: 6, duration: 5.353s, episode steps: 162, steps per second: 30, episode reward: -0.861, mean reward: -0.005 [-0.020, 0.010], mean action: 0.021 [-1.184, 1.145], mean observation: 0.064 [-13.656, 20.297], loss: 0.000062, mean_squared_error: 0.000125, mean_q: 0.419153\n",
      "done, took 54.323 seconds\n",
      "\n",
      "\n",
      "iteration: 304\n",
      "Training for 2000 steps ...\n",
      "  154/2000: episode: 1, duration: 3.825s, episode steps: 154, steps per second: 40, episode reward: -0.864, mean reward: -0.006 [-0.020, 0.010], mean action: 0.029 [-1.237, 1.227], mean observation: 0.059 [-11.441, 20.426], loss: --, mean_squared_error: --, mean_q: --\n",
      "  306/2000: episode: 2, duration: 3.621s, episode steps: 152, steps per second: 42, episode reward: -0.858, mean reward: -0.006 [-0.020, 0.010], mean action: 0.055 [-1.190, 1.303], mean observation: 0.059 [-11.268, 20.235], loss: --, mean_squared_error: --, mean_q: --\n",
      "  460/2000: episode: 3, duration: 3.705s, episode steps: 154, steps per second: 42, episode reward: -0.862, mean reward: -0.006 [-0.020, 0.010], mean action: 0.027 [-1.214, 1.103], mean observation: 0.060 [-12.263, 20.355], loss: --, mean_squared_error: --, mean_q: --\n",
      "  608/2000: episode: 4, duration: 3.735s, episode steps: 148, steps per second: 40, episode reward: -0.852, mean reward: -0.006 [-0.020, 0.010], mean action: 0.028 [-1.211, 1.144], mean observation: 0.056 [-12.024, 20.346], loss: --, mean_squared_error: --, mean_q: --\n",
      "  762/2000: episode: 5, duration: 3.748s, episode steps: 154, steps per second: 41, episode reward: -0.877, mean reward: -0.006 [-0.020, 0.010], mean action: 0.010 [-1.226, 1.249], mean observation: 0.057 [-11.236, 20.443], loss: --, mean_squared_error: --, mean_q: --\n",
      "  914/2000: episode: 6, duration: 3.755s, episode steps: 152, steps per second: 40, episode reward: -0.867, mean reward: -0.006 [-0.020, 0.010], mean action: 0.054 [-1.102, 1.221], mean observation: 0.060 [-11.451, 20.610], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1069/2000: episode: 7, duration: 4.411s, episode steps: 155, steps per second: 35, episode reward: -0.875, mean reward: -0.006 [-0.020, 0.010], mean action: 0.043 [-1.165, 1.160], mean observation: 0.062 [-11.581, 20.360], loss: 0.000041, mean_squared_error: 0.000082, mean_q: 0.410476\n",
      " 1228/2000: episode: 8, duration: 5.271s, episode steps: 159, steps per second: 30, episode reward: -0.871, mean reward: -0.005 [-0.020, 0.010], mean action: 0.021 [-1.247, 1.161], mean observation: 0.056 [-11.858, 20.478], loss: 0.000077, mean_squared_error: 0.000153, mean_q: 0.406438\n",
      " 1409/2000: episode: 9, duration: 5.872s, episode steps: 181, steps per second: 31, episode reward: -0.871, mean reward: -0.005 [-0.020, 0.010], mean action: 0.016 [-1.245, 1.134], mean observation: 0.063 [-10.854, 20.307], loss: 0.000109, mean_squared_error: 0.000218, mean_q: 0.403909\n",
      " 1563/2000: episode: 10, duration: 5.546s, episode steps: 154, steps per second: 28, episode reward: -0.863, mean reward: -0.006 [-0.022, 0.010], mean action: 0.079 [-1.222, 1.139], mean observation: 0.050 [-27.498, 20.608], loss: 0.000158, mean_squared_error: 0.000316, mean_q: 0.407062\n",
      " 1728/2000: episode: 11, duration: 6.520s, episode steps: 165, steps per second: 25, episode reward: -0.835, mean reward: -0.005 [-0.022, 0.011], mean action: 0.108 [-1.205, 1.222], mean observation: 0.055 [-28.067, 20.324], loss: 0.000033, mean_squared_error: 0.000065, mean_q: 0.402514\n",
      " 1945/2000: episode: 12, duration: 7.713s, episode steps: 217, steps per second: 28, episode reward: -0.809, mean reward: -0.004 [-0.020, 0.011], mean action: -0.033 [-1.183, 1.264], mean observation: 0.058 [-24.524, 20.200], loss: 0.000067, mean_squared_error: 0.000134, mean_q: 0.405779\n",
      "done, took 60.206 seconds\n",
      "\n",
      "\n",
      "iteration: 305\n",
      "Training for 2000 steps ...\n",
      "  231/2000: episode: 1, duration: 6.040s, episode steps: 231, steps per second: 38, episode reward: -0.802, mean reward: -0.003 [-0.020, 0.011], mean action: -0.033 [-1.186, 1.283], mean observation: 0.061 [-18.054, 20.299], loss: --, mean_squared_error: --, mean_q: --\n",
      "  559/2000: episode: 2, duration: 11.324s, episode steps: 328, steps per second: 29, episode reward: 0.730, mean reward: 0.002 [-0.002, 0.013], mean action: -0.155 [-1.257, 1.209], mean observation: 0.114 [-21.887, 20.189], loss: --, mean_squared_error: --, mean_q: --\n",
      "  777/2000: episode: 3, duration: 5.574s, episode steps: 218, steps per second: 39, episode reward: -0.803, mean reward: -0.004 [-0.020, 0.011], mean action: -0.053 [-1.204, 1.198], mean observation: 0.057 [-18.631, 20.250], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1012/2000: episode: 4, duration: 6.248s, episode steps: 235, steps per second: 38, episode reward: -0.805, mean reward: -0.003 [-0.020, 0.011], mean action: -0.061 [-1.324, 1.245], mean observation: 0.062 [-23.582, 20.245], loss: 0.000139, mean_squared_error: 0.000279, mean_q: 0.370589\n",
      " 1206/2000: episode: 5, duration: 7.184s, episode steps: 194, steps per second: 27, episode reward: -0.811, mean reward: -0.004 [-0.020, 0.011], mean action: 0.063 [-1.231, 1.180], mean observation: 0.052 [-22.825, 20.233], loss: 0.000237, mean_squared_error: 0.000474, mean_q: 0.395363\n",
      " 1438/2000: episode: 6, duration: 8.164s, episode steps: 232, steps per second: 28, episode reward: -0.792, mean reward: -0.003 [-0.020, 0.011], mean action: -0.018 [-1.273, 1.276], mean observation: 0.063 [-25.992, 20.355], loss: 0.000069, mean_squared_error: 0.000137, mean_q: 0.396869\n",
      " 1603/2000: episode: 7, duration: 6.149s, episode steps: 165, steps per second: 27, episode reward: -0.795, mean reward: -0.005 [-0.020, 0.011], mean action: 0.109 [-1.211, 1.232], mean observation: 0.045 [-26.969, 20.137], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.394365\n",
      "done, took 65.323 seconds\n",
      "\n",
      "\n",
      "iteration: 306\n",
      "Training for 2000 steps ...\n",
      "  191/2000: episode: 1, duration: 5.041s, episode steps: 191, steps per second: 38, episode reward: -0.813, mean reward: -0.004 [-0.020, 0.011], mean action: 0.077 [-1.296, 1.216], mean observation: 0.048 [-23.646, 20.532], loss: --, mean_squared_error: --, mean_q: --\n",
      "  394/2000: episode: 2, duration: 5.655s, episode steps: 203, steps per second: 36, episode reward: -0.806, mean reward: -0.004 [-0.020, 0.011], mean action: 0.088 [-1.265, 1.262], mean observation: 0.053 [-23.362, 20.237], loss: --, mean_squared_error: --, mean_q: --\n",
      "  584/2000: episode: 3, duration: 5.268s, episode steps: 190, steps per second: 36, episode reward: -0.804, mean reward: -0.004 [-0.020, 0.011], mean action: 0.088 [-1.201, 1.168], mean observation: 0.049 [-23.515, 20.345], loss: --, mean_squared_error: --, mean_q: --\n",
      "  775/2000: episode: 4, duration: 5.007s, episode steps: 191, steps per second: 38, episode reward: -0.816, mean reward: -0.004 [-0.020, 0.011], mean action: 0.112 [-1.202, 1.337], mean observation: 0.046 [-24.098, 20.354], loss: --, mean_squared_error: --, mean_q: --\n",
      "  970/2000: episode: 5, duration: 5.240s, episode steps: 195, steps per second: 37, episode reward: -0.804, mean reward: -0.004 [-0.020, 0.010], mean action: 0.078 [-1.265, 1.400], mean observation: 0.051 [-23.001, 20.488], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1160/2000: episode: 6, duration: 6.629s, episode steps: 190, steps per second: 29, episode reward: -0.806, mean reward: -0.004 [-0.020, 0.011], mean action: 0.096 [-1.156, 1.208], mean observation: 0.048 [-24.683, 20.547], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.386829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1396/2000: episode: 7, duration: 8.798s, episode steps: 236, steps per second: 27, episode reward: -0.802, mean reward: -0.003 [-0.020, 0.011], mean action: 0.006 [-1.378, 1.333], mean observation: 0.062 [-23.119, 20.198], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.384874\n",
      " 1549/2000: episode: 8, duration: 5.610s, episode steps: 153, steps per second: 27, episode reward: -0.815, mean reward: -0.005 [-0.020, 0.011], mean action: -0.009 [-1.253, 1.168], mean observation: 0.033 [-26.117, 20.396], loss: 0.000035, mean_squared_error: 0.000071, mean_q: 0.386748\n",
      " 1693/2000: episode: 9, duration: 5.420s, episode steps: 144, steps per second: 27, episode reward: -0.813, mean reward: -0.006 [-0.021, 0.011], mean action: 0.051 [-1.189, 1.176], mean observation: 0.028 [-27.144, 20.541], loss: 0.000032, mean_squared_error: 0.000064, mean_q: 0.384321\n",
      " 1862/2000: episode: 10, duration: 6.050s, episode steps: 169, steps per second: 28, episode reward: -0.811, mean reward: -0.005 [-0.020, 0.011], mean action: 0.010 [-1.226, 1.176], mean observation: 0.041 [-26.850, 20.150], loss: 0.000055, mean_squared_error: 0.000110, mean_q: 0.382129\n",
      "done, took 63.747 seconds\n",
      "\n",
      "\n",
      "iteration: 307\n",
      "Training for 2000 steps ...\n",
      "  164/2000: episode: 1, duration: 4.329s, episode steps: 164, steps per second: 38, episode reward: -0.791, mean reward: -0.005 [-0.020, 0.011], mean action: 0.048 [-1.146, 1.251], mean observation: 0.044 [-16.792, 20.355], loss: --, mean_squared_error: --, mean_q: --\n",
      "  333/2000: episode: 2, duration: 4.493s, episode steps: 169, steps per second: 38, episode reward: -0.810, mean reward: -0.005 [-0.020, 0.011], mean action: 0.037 [-1.144, 1.215], mean observation: 0.044 [-17.645, 20.458], loss: --, mean_squared_error: --, mean_q: --\n",
      "  493/2000: episode: 3, duration: 4.277s, episode steps: 160, steps per second: 37, episode reward: -0.791, mean reward: -0.005 [-0.020, 0.011], mean action: 0.043 [-1.179, 1.161], mean observation: 0.040 [-19.085, 20.368], loss: --, mean_squared_error: --, mean_q: --\n",
      "  652/2000: episode: 4, duration: 4.259s, episode steps: 159, steps per second: 37, episode reward: -0.800, mean reward: -0.005 [-0.021, 0.011], mean action: 0.042 [-1.127, 1.173], mean observation: 0.038 [-21.217, 20.243], loss: --, mean_squared_error: --, mean_q: --\n",
      "  814/2000: episode: 5, duration: 4.339s, episode steps: 162, steps per second: 37, episode reward: -0.790, mean reward: -0.005 [-0.020, 0.011], mean action: 0.029 [-1.207, 1.192], mean observation: 0.041 [-19.560, 20.360], loss: --, mean_squared_error: --, mean_q: --\n",
      "  979/2000: episode: 6, duration: 4.297s, episode steps: 165, steps per second: 38, episode reward: -0.804, mean reward: -0.005 [-0.020, 0.011], mean action: 0.039 [-1.245, 1.133], mean observation: 0.042 [-16.919, 20.265], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1144/2000: episode: 7, duration: 5.876s, episode steps: 165, steps per second: 28, episode reward: -0.802, mean reward: -0.005 [-0.020, 0.011], mean action: 0.074 [-1.148, 1.306], mean observation: 0.042 [-14.398, 20.346], loss: 0.000050, mean_squared_error: 0.000099, mean_q: 0.376173\n",
      " 1351/2000: episode: 8, duration: 7.923s, episode steps: 207, steps per second: 26, episode reward: -0.815, mean reward: -0.004 [-0.019, 0.011], mean action: -0.045 [-1.241, 1.216], mean observation: 0.069 [-22.547, 20.194], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.374082\n",
      "done, took 71.556 seconds\n",
      "\n",
      "\n",
      "iteration: 308\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 44.887s, episode steps: 1000, steps per second: 22, episode reward: 0.128, mean reward: 0.000 [-0.001, 0.010], mean action: -0.042 [-1.334, 1.405], mean observation: 0.119 [-20.161, 20.349], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 43.299s, episode steps: 1000, steps per second: 23, episode reward: 0.058, mean reward: 0.000 [-0.001, 0.011], mean action: -0.103 [-1.395, 1.326], mean observation: 0.113 [-21.926, 20.303], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.376272\n",
      "done, took 88.205 seconds\n",
      "\n",
      "\n",
      "iteration: 309\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 35.416s, episode steps: 1000, steps per second: 28, episode reward: 0.086, mean reward: 0.000 [-0.003, 0.010], mean action: 0.033 [-1.459, 1.358], mean observation: 0.118 [-10.770, 20.222], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 41.431s, episode steps: 1000, steps per second: 24, episode reward: 0.123, mean reward: 0.000 [-0.003, 0.010], mean action: 0.119 [-1.227, 1.510], mean observation: 0.116 [-13.261, 20.233], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.386866\n",
      "done, took 76.864 seconds\n",
      "\n",
      "\n",
      "iteration: 310\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 30.895s, episode steps: 1000, steps per second: 32, episode reward: 0.119, mean reward: 0.000 [-0.002, 0.010], mean action: 0.141 [-1.204, 1.466], mean observation: 0.119 [-10.819, 20.390], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 38.896s, episode steps: 1000, steps per second: 26, episode reward: 0.088, mean reward: 0.000 [-0.002, 0.010], mean action: -0.068 [-1.401, 1.287], mean observation: 0.117 [-10.707, 20.339], loss: 0.000122, mean_squared_error: 0.000245, mean_q: 0.397210\n",
      "done, took 69.809 seconds\n",
      "\n",
      "\n",
      "iteration: 311\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.186s, episode steps: 1000, steps per second: 43, episode reward: 0.093, mean reward: 0.000 [-0.002, 0.010], mean action: -0.013 [-1.418, 1.462], mean observation: 0.118 [-10.625, 19.981], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 46.580s, episode steps: 1000, steps per second: 21, episode reward: 0.106, mean reward: 0.000 [-0.002, 0.010], mean action: 0.104 [-1.315, 1.492], mean observation: 0.118 [-10.568, 20.181], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.392697\n",
      "done, took 69.784 seconds\n",
      "\n",
      "\n",
      "iteration: 312\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 36.184s, episode steps: 1000, steps per second: 28, episode reward: 0.112, mean reward: 0.000 [-0.002, 0.010], mean action: 0.045 [-1.463, 1.310], mean observation: 0.121 [-10.891, 20.317], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 35.136s, episode steps: 1000, steps per second: 28, episode reward: 0.126, mean reward: 0.000 [-0.002, 0.010], mean action: 0.215 [-1.358, 1.406], mean observation: 0.114 [-10.763, 20.352], loss: 0.000120, mean_squared_error: 0.000241, mean_q: 0.395394\n",
      "done, took 71.337 seconds\n",
      "\n",
      "\n",
      "iteration: 313\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 26.737s, episode steps: 1000, steps per second: 37, episode reward: 0.109, mean reward: 0.000 [-0.002, 0.011], mean action: 0.223 [-1.392, 1.514], mean observation: 0.113 [-10.931, 20.416], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 43.707s, episode steps: 1000, steps per second: 23, episode reward: 0.104, mean reward: 0.000 [-0.002, 0.011], mean action: 0.247 [-1.395, 1.356], mean observation: 0.113 [-10.940, 20.311], loss: 0.000067, mean_squared_error: 0.000133, mean_q: 0.396862\n",
      "done, took 70.462 seconds\n",
      "\n",
      "\n",
      "iteration: 314\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 20.454s, episode steps: 1000, steps per second: 49, episode reward: 0.052, mean reward: 0.000 [-0.002, 0.010], mean action: 0.150 [-1.463, 1.356], mean observation: 0.110 [-10.727, 20.230], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 45.297s, episode steps: 1000, steps per second: 22, episode reward: 0.097, mean reward: 0.000 [-0.002, 0.010], mean action: 0.194 [-1.310, 1.362], mean observation: 0.114 [-10.759, 20.337], loss: 0.000075, mean_squared_error: 0.000149, mean_q: 0.396585\n",
      "done, took 65.770 seconds\n",
      "\n",
      "\n",
      "iteration: 315\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 41.688s, episode steps: 1000, steps per second: 24, episode reward: 0.109, mean reward: 0.000 [-0.002, 0.010], mean action: 0.189 [-1.744, 1.610], mean observation: 0.115 [-10.649, 19.569], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 45.799s, episode steps: 1000, steps per second: 22, episode reward: 0.075, mean reward: 0.000 [-0.002, 0.010], mean action: 0.129 [-1.486, 1.359], mean observation: 0.114 [-10.523, 19.915], loss: 0.000159, mean_squared_error: 0.000319, mean_q: 0.395309\n",
      "done, took 87.503 seconds\n",
      "\n",
      "\n",
      "iteration: 316\n",
      "Training for 2000 steps ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000/2000: episode: 1, duration: 23.785s, episode steps: 1000, steps per second: 42, episode reward: 0.101, mean reward: 0.000 [-0.002, 0.010], mean action: 0.067 [-1.475, 1.273], mean observation: 0.113 [-10.715, 19.838], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 45.313s, episode steps: 1000, steps per second: 22, episode reward: 0.082, mean reward: 0.000 [-0.002, 0.010], mean action: 0.147 [-1.373, 1.520], mean observation: 0.116 [-10.720, 19.902], loss: 0.000064, mean_squared_error: 0.000127, mean_q: 0.396501\n",
      "done, took 69.116 seconds\n",
      "\n",
      "\n",
      "iteration: 317\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 24.634s, episode steps: 1000, steps per second: 41, episode reward: 0.096, mean reward: 0.000 [-0.004, 0.007], mean action: 0.271 [-1.343, 1.424], mean observation: 0.119 [-8.179, 21.110], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 35.887s, episode steps: 1000, steps per second: 28, episode reward: 0.082, mean reward: 0.000 [-0.004, 0.007], mean action: 0.359 [-1.271, 1.405], mean observation: 0.110 [-8.095, 21.080], loss: 0.000092, mean_squared_error: 0.000184, mean_q: 0.397791\n",
      "done, took 60.538 seconds\n",
      "\n",
      "\n",
      "iteration: 318\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 25.991s, episode steps: 1000, steps per second: 38, episode reward: 0.090, mean reward: 0.000 [-0.002, 0.009], mean action: 0.312 [-1.370, 1.361], mean observation: 0.116 [-8.909, 13.712], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1774/2000: episode: 2, duration: 34.773s, episode steps: 774, steps per second: 22, episode reward: -0.811, mean reward: -0.001 [-0.019, 0.009], mean action: 0.245 [-1.361, 1.375], mean observation: 0.103 [-8.861, 13.870], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.395946\n",
      "done, took 71.062 seconds\n",
      "\n",
      "\n",
      "iteration: 319\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 33.442s, episode steps: 1000, steps per second: 30, episode reward: 0.091, mean reward: 0.000 [-0.002, 0.010], mean action: 0.103 [-1.575, 1.323], mean observation: 0.121 [-10.561, 14.928], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1418/2000: episode: 2, duration: 16.707s, episode steps: 418, steps per second: 25, episode reward: -0.755, mean reward: -0.002 [-0.019, 0.010], mean action: 0.189 [-1.336, 1.419], mean observation: 0.099 [-10.735, 15.299], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.398936\n",
      " 1909/2000: episode: 3, duration: 19.280s, episode steps: 491, steps per second: 25, episode reward: -0.766, mean reward: -0.002 [-0.019, 0.010], mean action: 0.173 [-1.349, 1.262], mean observation: 0.105 [-10.766, 15.316], loss: 0.000085, mean_squared_error: 0.000170, mean_q: 0.394459\n",
      "done, took 73.890 seconds\n",
      "\n",
      "\n",
      "iteration: 320\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 37.663s, episode steps: 1000, steps per second: 27, episode reward: 0.125, mean reward: 0.000 [-0.001, 0.011], mean action: -0.027 [-1.364, 1.353], mean observation: 0.127 [-19.048, 15.338], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1337/2000: episode: 2, duration: 13.605s, episode steps: 337, steps per second: 25, episode reward: -0.736, mean reward: -0.002 [-0.019, 0.011], mean action: 0.096 [-1.236, 1.254], mean observation: 0.099 [-18.729, 15.360], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.394567\n",
      "done, took 79.061 seconds\n",
      "\n",
      "\n",
      "iteration: 321\n",
      "Training for 2000 steps ...\n",
      "  684/2000: episode: 1, duration: 25.652s, episode steps: 684, steps per second: 27, episode reward: -0.731, mean reward: -0.001 [-0.019, 0.010], mean action: 0.071 [-1.473, 1.289], mean observation: 0.109 [-18.672, 15.709], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1068/2000: episode: 2, duration: 14.010s, episode steps: 384, steps per second: 27, episode reward: -0.740, mean reward: -0.002 [-0.019, 0.010], mean action: 0.026 [-1.296, 1.203], mean observation: 0.101 [-18.984, 15.278], loss: 0.000034, mean_squared_error: 0.000068, mean_q: 0.394009\n",
      "done, took 74.618 seconds\n",
      "\n",
      "\n",
      "iteration: 322\n",
      "Training for 2000 steps ...\n",
      "  708/2000: episode: 1, duration: 17.277s, episode steps: 708, steps per second: 41, episode reward: -1.058, mean reward: -0.001 [-0.019, 0.007], mean action: 0.124 [-1.393, 1.322], mean observation: 0.096 [-10.371, 18.358], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1708/2000: episode: 2, duration: 34.048s, episode steps: 1000, steps per second: 29, episode reward: 0.095, mean reward: 0.000 [-0.004, 0.007], mean action: 0.224 [-1.447, 1.359], mean observation: 0.111 [-10.505, 17.445], loss: 0.000115, mean_squared_error: 0.000231, mean_q: 0.392781\n",
      " 1899/2000: episode: 3, duration: 6.874s, episode steps: 191, steps per second: 28, episode reward: -0.973, mean reward: -0.005 [-0.019, 0.006], mean action: 0.141 [-1.304, 1.167], mean observation: 0.053 [-8.239, 17.560], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.388341\n",
      "done, took 62.586 seconds\n",
      "\n",
      "\n",
      "iteration: 323\n",
      "Training for 2000 steps ...\n",
      "  271/2000: episode: 1, duration: 6.262s, episode steps: 271, steps per second: 43, episode reward: -0.920, mean reward: -0.003 [-0.019, 0.006], mean action: 0.120 [-1.242, 1.268], mean observation: 0.074 [-8.188, 21.160], loss: --, mean_squared_error: --, mean_q: --\n",
      "  490/2000: episode: 2, duration: 5.357s, episode steps: 219, steps per second: 41, episode reward: -0.936, mean reward: -0.004 [-0.019, 0.006], mean action: 0.130 [-1.196, 1.191], mean observation: 0.064 [-8.133, 21.187], loss: --, mean_squared_error: --, mean_q: --\n",
      "  696/2000: episode: 3, duration: 5.001s, episode steps: 206, steps per second: 41, episode reward: -0.981, mean reward: -0.005 [-0.019, 0.006], mean action: 0.122 [-1.199, 1.231], mean observation: 0.056 [-8.113, 21.131], loss: --, mean_squared_error: --, mean_q: --\n",
      "  916/2000: episode: 4, duration: 5.392s, episode steps: 220, steps per second: 41, episode reward: -0.947, mean reward: -0.004 [-0.019, 0.006], mean action: 0.105 [-1.215, 1.299], mean observation: 0.063 [-8.133, 21.195], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1128/2000: episode: 5, duration: 6.359s, episode steps: 212, steps per second: 33, episode reward: -0.934, mean reward: -0.004 [-0.020, 0.006], mean action: 0.134 [-1.119, 1.294], mean observation: 0.062 [-8.102, 21.022], loss: 0.000136, mean_squared_error: 0.000272, mean_q: 0.389008\n",
      " 1315/2000: episode: 6, duration: 6.442s, episode steps: 187, steps per second: 29, episode reward: -1.000, mean reward: -0.005 [-0.020, 0.006], mean action: 0.129 [-1.198, 1.240], mean observation: 0.053 [-8.127, 21.205], loss: 0.000177, mean_squared_error: 0.000353, mean_q: 0.388199\n",
      "done, took 62.729 seconds\n",
      "\n",
      "\n",
      "iteration: 324\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.635s, episode steps: 1000, steps per second: 42, episode reward: 0.124, mean reward: 0.000 [-0.003, 0.007], mean action: 0.210 [-1.443, 1.413], mean observation: 0.113 [-8.120, 21.041], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 42.122s, episode steps: 1000, steps per second: 24, episode reward: 0.111, mean reward: 0.000 [-0.003, 0.007], mean action: 0.118 [-1.656, 1.413], mean observation: 0.118 [-8.098, 20.947], loss: 0.000160, mean_squared_error: 0.000319, mean_q: 0.388873\n",
      "done, took 65.774 seconds\n",
      "\n",
      "\n",
      "iteration: 325\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 44.848s, episode steps: 1000, steps per second: 22, episode reward: 0.117, mean reward: 0.000 [-0.002, 0.007], mean action: 0.225 [-1.448, 1.639], mean observation: 0.120 [-9.226, 20.901], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 43.553s, episode steps: 1000, steps per second: 23, episode reward: 0.122, mean reward: 0.000 [-0.001, 0.007], mean action: -0.004 [-1.662, 1.477], mean observation: 0.119 [-9.390, 20.965], loss: 0.000122, mean_squared_error: 0.000244, mean_q: 0.388748\n",
      "done, took 88.419 seconds\n",
      "\n",
      "\n",
      "iteration: 326\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 35.833s, episode steps: 1000, steps per second: 28, episode reward: 0.123, mean reward: 0.000 [-0.002, 0.008], mean action: -0.067 [-1.372, 1.402], mean observation: 0.124 [-14.539, 20.951], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/2000: episode: 2, duration: 40.160s, episode steps: 1000, steps per second: 25, episode reward: 0.138, mean reward: 0.000 [-0.002, 0.007], mean action: 0.134 [-1.368, 1.422], mean observation: 0.119 [-15.285, 20.689], loss: 0.000141, mean_squared_error: 0.000281, mean_q: 0.392358\n",
      "done, took 76.011 seconds\n",
      "\n",
      "\n",
      "iteration: 327\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 41.578s, episode steps: 1000, steps per second: 24, episode reward: 0.142, mean reward: 0.000 [-0.001, 0.007], mean action: 0.073 [-1.470, 1.474], mean observation: 0.122 [-14.978, 20.981], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 43.841s, episode steps: 1000, steps per second: 23, episode reward: 0.107, mean reward: 0.000 [-0.001, 0.008], mean action: 0.112 [-1.311, 1.397], mean observation: 0.123 [-13.077, 20.827], loss: 0.000115, mean_squared_error: 0.000230, mean_q: 0.388702\n",
      "done, took 85.436 seconds\n",
      "\n",
      "\n",
      "iteration: 328\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 24.010s, episode steps: 1000, steps per second: 42, episode reward: 0.095, mean reward: 0.000 [-0.002, 0.008], mean action: 0.175 [-1.431, 1.457], mean observation: 0.122 [-8.109, 20.879], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 38.557s, episode steps: 1000, steps per second: 26, episode reward: 0.070, mean reward: 0.000 [-0.003, 0.008], mean action: 0.160 [-1.396, 1.447], mean observation: 0.124 [-8.130, 20.964], loss: 0.000137, mean_squared_error: 0.000275, mean_q: 0.391947\n",
      "done, took 62.585 seconds\n",
      "\n",
      "\n",
      "iteration: 329\n",
      "Training for 2000 steps ...\n",
      "  182/2000: episode: 1, duration: 5.094s, episode steps: 182, steps per second: 36, episode reward: -0.898, mean reward: -0.005 [-0.020, 0.004], mean action: 0.210 [-1.270, 1.260], mean observation: 0.056 [-41.161, 18.525], loss: --, mean_squared_error: --, mean_q: --\n",
      "  355/2000: episode: 2, duration: 5.126s, episode steps: 173, steps per second: 34, episode reward: -0.909, mean reward: -0.005 [-0.020, 0.004], mean action: 0.215 [-1.171, 1.134], mean observation: 0.051 [-43.960, 18.501], loss: --, mean_squared_error: --, mean_q: --\n",
      "  549/2000: episode: 3, duration: 5.335s, episode steps: 194, steps per second: 36, episode reward: -0.892, mean reward: -0.005 [-0.019, 0.004], mean action: 0.217 [-1.157, 1.212], mean observation: 0.060 [-42.969, 18.466], loss: --, mean_squared_error: --, mean_q: --\n",
      "  749/2000: episode: 4, duration: 5.393s, episode steps: 200, steps per second: 37, episode reward: -0.867, mean reward: -0.004 [-0.019, 0.004], mean action: 0.188 [-1.186, 1.244], mean observation: 0.063 [-40.225, 18.340], loss: --, mean_squared_error: --, mean_q: --\n",
      "  919/2000: episode: 5, duration: 4.939s, episode steps: 170, steps per second: 34, episode reward: -0.896, mean reward: -0.005 [-0.020, 0.004], mean action: 0.236 [-1.181, 1.337], mean observation: 0.052 [-43.952, 18.428], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1108/2000: episode: 6, duration: 6.334s, episode steps: 189, steps per second: 30, episode reward: -0.876, mean reward: -0.005 [-0.019, 0.004], mean action: 0.205 [-1.240, 1.294], mean observation: 0.059 [-42.642, 18.466], loss: 0.000033, mean_squared_error: 0.000066, mean_q: 0.384080\n",
      " 1380/2000: episode: 7, duration: 9.363s, episode steps: 272, steps per second: 29, episode reward: -0.864, mean reward: -0.003 [-0.019, 0.004], mean action: 0.172 [-1.286, 1.315], mean observation: 0.074 [-44.738, 19.126], loss: 0.000215, mean_squared_error: 0.000431, mean_q: 0.390291\n",
      "done, took 68.400 seconds\n",
      "\n",
      "\n",
      "iteration: 330\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 36.505s, episode steps: 1000, steps per second: 27, episode reward: 0.046, mean reward: 0.000 [-0.001, 0.006], mean action: 0.250 [-1.379, 1.346], mean observation: 0.107 [-15.651, 12.501], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 47.647s, episode steps: 1000, steps per second: 21, episode reward: 0.093, mean reward: 0.000 [-0.001, 0.006], mean action: 0.346 [-1.330, 1.543], mean observation: 0.115 [-24.627, 12.595], loss: 0.000141, mean_squared_error: 0.000283, mean_q: 0.386865\n",
      "done, took 84.170 seconds\n",
      "\n",
      "\n",
      "iteration: 331\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 41.612s, episode steps: 1000, steps per second: 24, episode reward: 0.115, mean reward: 0.000 [-0.004, 0.007], mean action: 0.414 [-1.346, 1.411], mean observation: 0.121 [-13.375, 12.577], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 49.196s, episode steps: 1000, steps per second: 20, episode reward: 0.097, mean reward: 0.000 [-0.003, 0.006], mean action: 0.297 [-1.329, 1.463], mean observation: 0.120 [-14.084, 12.752], loss: 0.000124, mean_squared_error: 0.000249, mean_q: 0.389575\n",
      "done, took 90.827 seconds\n",
      "\n",
      "\n",
      "iteration: 332\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 30.809s, episode steps: 1000, steps per second: 32, episode reward: 0.117, mean reward: 0.000 [-0.003, 0.010], mean action: 0.156 [-1.567, 1.435], mean observation: 0.124 [-11.663, 15.020], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 45.130s, episode steps: 1000, steps per second: 22, episode reward: 0.084, mean reward: 0.000 [-0.005, 0.010], mean action: 0.249 [-1.341, 1.429], mean observation: 0.122 [-10.881, 15.348], loss: 0.000114, mean_squared_error: 0.000228, mean_q: 0.388466\n",
      "done, took 75.956 seconds\n",
      "\n",
      "\n",
      "iteration: 333\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 30.876s, episode steps: 1000, steps per second: 32, episode reward: 0.090, mean reward: 0.000 [-0.003, 0.012], mean action: 0.191 [-1.603, 1.423], mean observation: 0.115 [-14.596, 18.003], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1855/2000: episode: 2, duration: 34.088s, episode steps: 855, steps per second: 25, episode reward: 0.531, mean reward: 0.001 [-0.003, 0.012], mean action: 0.323 [-1.453, 1.368], mean observation: 0.121 [-18.535, 17.678], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.389361\n",
      "done, took 71.393 seconds\n",
      "\n",
      "\n",
      "iteration: 334\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 31.305s, episode steps: 1000, steps per second: 32, episode reward: 0.107, mean reward: 0.000 [-0.002, 0.007], mean action: 0.081 [-1.408, 1.394], mean observation: 0.113 [-26.279, 15.466], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 39.293s, episode steps: 1000, steps per second: 25, episode reward: 0.132, mean reward: 0.000 [-0.002, 0.008], mean action: 0.151 [-1.448, 1.369], mean observation: 0.112 [-29.086, 15.946], loss: 0.000107, mean_squared_error: 0.000215, mean_q: 0.391176\n",
      "done, took 70.616 seconds\n",
      "\n",
      "\n",
      "iteration: 335\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 38.485s, episode steps: 1000, steps per second: 26, episode reward: 0.133, mean reward: 0.000 [-0.003, 0.006], mean action: 0.097 [-1.536, 1.219], mean observation: 0.123 [-18.246, 13.703], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 42.104s, episode steps: 1000, steps per second: 24, episode reward: 0.133, mean reward: 0.000 [-0.003, 0.006], mean action: 0.119 [-1.378, 1.480], mean observation: 0.123 [-29.291, 18.123], loss: 0.000130, mean_squared_error: 0.000260, mean_q: 0.388816\n",
      "done, took 80.608 seconds\n",
      "\n",
      "\n",
      "iteration: 336\n",
      "Training for 2000 steps ...\n",
      "  159/2000: episode: 1, duration: 5.755s, episode steps: 159, steps per second: 28, episode reward: 0.548, mean reward: 0.003 [-0.002, 0.011], mean action: 0.262 [-1.182, 1.184], mean observation: 0.115 [-8.779, 13.771], loss: --, mean_squared_error: --, mean_q: --\n",
      "  319/2000: episode: 2, duration: 5.589s, episode steps: 160, steps per second: 29, episode reward: 0.584, mean reward: 0.004 [-0.002, 0.011], mean action: 0.247 [-1.246, 1.325], mean observation: 0.115 [-8.559, 13.429], loss: --, mean_squared_error: --, mean_q: --\n",
      "  479/2000: episode: 3, duration: 5.628s, episode steps: 160, steps per second: 28, episode reward: 0.571, mean reward: 0.004 [-0.002, 0.011], mean action: 0.252 [-1.177, 1.169], mean observation: 0.114 [-9.077, 13.708], loss: --, mean_squared_error: --, mean_q: --\n",
      "  639/2000: episode: 4, duration: 5.863s, episode steps: 160, steps per second: 27, episode reward: 0.553, mean reward: 0.003 [-0.002, 0.011], mean action: 0.249 [-1.247, 1.140], mean observation: 0.112 [-8.816, 13.667], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  807/2000: episode: 5, duration: 6.039s, episode steps: 168, steps per second: 28, episode reward: 0.565, mean reward: 0.003 [-0.002, 0.011], mean action: 0.239 [-1.201, 1.153], mean observation: 0.113 [-8.595, 13.885], loss: --, mean_squared_error: --, mean_q: --\n",
      "  967/2000: episode: 6, duration: 5.620s, episode steps: 160, steps per second: 28, episode reward: 0.564, mean reward: 0.004 [-0.003, 0.011], mean action: 0.246 [-1.202, 1.234], mean observation: 0.115 [-8.393, 13.203], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1125/2000: episode: 7, duration: 6.884s, episode steps: 158, steps per second: 23, episode reward: 0.537, mean reward: 0.003 [-0.002, 0.011], mean action: 0.269 [-1.181, 1.237], mean observation: 0.115 [-9.161, 13.512], loss: 0.000027, mean_squared_error: 0.000053, mean_q: 0.387164\n",
      " 1275/2000: episode: 8, duration: 6.371s, episode steps: 150, steps per second: 24, episode reward: 0.612, mean reward: 0.004 [-0.003, 0.011], mean action: 0.243 [-1.196, 1.247], mean observation: 0.117 [-18.028, 13.870], loss: 0.000099, mean_squared_error: 0.000199, mean_q: 0.388118\n",
      " 1431/2000: episode: 9, duration: 6.302s, episode steps: 156, steps per second: 25, episode reward: 0.724, mean reward: 0.005 [-0.002, 0.012], mean action: 0.272 [-1.132, 1.189], mean observation: 0.129 [-8.472, 13.754], loss: 0.000177, mean_squared_error: 0.000355, mean_q: 0.391203\n",
      "done, took 76.150 seconds\n",
      "\n",
      "\n",
      "iteration: 337\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 33.611s, episode steps: 1000, steps per second: 30, episode reward: 0.126, mean reward: 0.000 [-0.004, 0.011], mean action: 0.113 [-1.450, 1.415], mean observation: 0.122 [-39.039, 17.444], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 48.622s, episode steps: 1000, steps per second: 21, episode reward: 0.124, mean reward: 0.000 [-0.004, 0.010], mean action: 0.117 [-1.489, 1.385], mean observation: 0.117 [-18.937, 17.586], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.390763\n",
      "done, took 82.250 seconds\n",
      "\n",
      "\n",
      "iteration: 338\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 39.162s, episode steps: 1000, steps per second: 26, episode reward: 0.130, mean reward: 0.000 [-0.003, 0.010], mean action: 0.071 [-1.504, 1.360], mean observation: 0.119 [-22.660, 18.047], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 47.282s, episode steps: 1000, steps per second: 21, episode reward: 0.124, mean reward: 0.000 [-0.003, 0.010], mean action: 0.297 [-1.455, 1.357], mean observation: 0.120 [-20.048, 17.635], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.388914\n",
      "done, took 86.463 seconds\n",
      "\n",
      "\n",
      "iteration: 339\n",
      "Training for 2000 steps ...\n",
      "  198/2000: episode: 1, duration: 9.002s, episode steps: 198, steps per second: 22, episode reward: 0.626, mean reward: 0.003 [-0.002, 0.012], mean action: 0.494 [-1.155, 1.260], mean observation: 0.101 [-21.358, 19.374], loss: --, mean_squared_error: --, mean_q: --\n",
      "  405/2000: episode: 2, duration: 9.038s, episode steps: 207, steps per second: 23, episode reward: 0.628, mean reward: 0.003 [-0.002, 0.012], mean action: 0.487 [-1.197, 1.262], mean observation: 0.101 [-20.326, 19.472], loss: --, mean_squared_error: --, mean_q: --\n",
      "  602/2000: episode: 3, duration: 7.965s, episode steps: 197, steps per second: 25, episode reward: 0.626, mean reward: 0.003 [-0.002, 0.012], mean action: 0.484 [-1.173, 1.192], mean observation: 0.103 [-22.486, 19.364], loss: --, mean_squared_error: --, mean_q: --\n",
      "  806/2000: episode: 4, duration: 9.372s, episode steps: 204, steps per second: 22, episode reward: 0.640, mean reward: 0.003 [-0.002, 0.012], mean action: 0.457 [-1.157, 1.265], mean observation: 0.102 [-21.396, 19.339], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1030/2000: episode: 5, duration: 10.454s, episode steps: 224, steps per second: 21, episode reward: 0.669, mean reward: 0.003 [-0.002, 0.012], mean action: 0.451 [-1.206, 1.289], mean observation: 0.103 [-21.297, 19.417], loss: 0.000162, mean_squared_error: 0.000324, mean_q: 0.373609\n",
      " 1226/2000: episode: 6, duration: 10.619s, episode steps: 196, steps per second: 18, episode reward: 0.606, mean reward: 0.003 [-0.002, 0.012], mean action: 0.476 [-1.155, 1.450], mean observation: 0.102 [-19.567, 19.200], loss: 0.000193, mean_squared_error: 0.000386, mean_q: 0.392814\n",
      " 1354/2000: episode: 7, duration: 5.813s, episode steps: 128, steps per second: 22, episode reward: 0.615, mean reward: 0.005 [-0.001, 0.012], mean action: 0.456 [-1.370, 1.189], mean observation: 0.095 [-12.918, 18.701], loss: 0.000328, mean_squared_error: 0.000656, mean_q: 0.387672\n",
      " 1484/2000: episode: 8, duration: 5.894s, episode steps: 130, steps per second: 22, episode reward: 0.636, mean reward: 0.005 [-0.001, 0.012], mean action: 0.429 [-1.160, 1.171], mean observation: 0.103 [-10.455, 18.883], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.386497\n",
      " 1609/2000: episode: 9, duration: 5.512s, episode steps: 125, steps per second: 23, episode reward: 0.635, mean reward: 0.005 [-0.000, 0.012], mean action: 0.472 [-1.115, 1.147], mean observation: 0.102 [-17.919, 19.164], loss: 0.000137, mean_squared_error: 0.000273, mean_q: 0.386420\n",
      " 1743/2000: episode: 10, duration: 6.009s, episode steps: 134, steps per second: 22, episode reward: 0.644, mean reward: 0.005 [-0.001, 0.012], mean action: 0.472 [-1.153, 1.222], mean observation: 0.106 [-14.457, 19.077], loss: 0.000170, mean_squared_error: 0.000340, mean_q: 0.391511\n",
      " 1882/2000: episode: 11, duration: 6.153s, episode steps: 139, steps per second: 23, episode reward: 0.644, mean reward: 0.005 [-0.001, 0.012], mean action: 0.420 [-1.107, 1.154], mean observation: 0.105 [-11.613, 18.869], loss: 0.000125, mean_squared_error: 0.000251, mean_q: 0.384761\n",
      "done, took 91.248 seconds\n",
      "\n",
      "\n",
      "iteration: 340\n",
      "Training for 2000 steps ...\n",
      "  153/2000: episode: 1, duration: 5.496s, episode steps: 153, steps per second: 28, episode reward: 0.639, mean reward: 0.004 [-0.002, 0.012], mean action: 0.484 [-1.104, 1.196], mean observation: 0.106 [-24.671, 18.967], loss: --, mean_squared_error: --, mean_q: --\n",
      "  298/2000: episode: 2, duration: 5.048s, episode steps: 145, steps per second: 29, episode reward: 0.644, mean reward: 0.004 [-0.002, 0.012], mean action: 0.475 [-1.143, 1.199], mean observation: 0.102 [-23.492, 18.997], loss: --, mean_squared_error: --, mean_q: --\n",
      "  452/2000: episode: 3, duration: 5.688s, episode steps: 154, steps per second: 27, episode reward: 0.645, mean reward: 0.004 [-0.002, 0.012], mean action: 0.469 [-1.156, 1.204], mean observation: 0.107 [-23.992, 18.916], loss: --, mean_squared_error: --, mean_q: --\n",
      "  597/2000: episode: 4, duration: 5.450s, episode steps: 145, steps per second: 27, episode reward: 0.604, mean reward: 0.004 [-0.002, 0.012], mean action: 0.455 [-1.204, 1.170], mean observation: 0.102 [-20.410, 18.878], loss: --, mean_squared_error: --, mean_q: --\n",
      "  750/2000: episode: 5, duration: 5.929s, episode steps: 153, steps per second: 26, episode reward: 0.634, mean reward: 0.004 [-0.003, 0.012], mean action: 0.473 [-1.190, 1.237], mean observation: 0.101 [-23.624, 18.920], loss: --, mean_squared_error: --, mean_q: --\n",
      "  896/2000: episode: 6, duration: 5.233s, episode steps: 146, steps per second: 28, episode reward: 0.642, mean reward: 0.004 [-0.002, 0.012], mean action: 0.468 [-1.260, 1.190], mean observation: 0.104 [-22.685, 19.052], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1048/2000: episode: 7, duration: 6.060s, episode steps: 152, steps per second: 25, episode reward: 0.660, mean reward: 0.004 [-0.002, 0.012], mean action: 0.466 [-1.214, 1.202], mean observation: 0.107 [-23.756, 18.891], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.385345\n",
      " 1194/2000: episode: 8, duration: 6.958s, episode steps: 146, steps per second: 21, episode reward: 0.599, mean reward: 0.004 [-0.003, 0.012], mean action: 0.472 [-1.201, 1.188], mean observation: 0.102 [-24.091, 19.028], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.387638\n",
      " 1349/2000: episode: 9, duration: 7.056s, episode steps: 155, steps per second: 22, episode reward: 0.594, mean reward: 0.004 [-0.003, 0.012], mean action: 0.486 [-1.197, 1.258], mean observation: 0.104 [-20.665, 19.205], loss: 0.000111, mean_squared_error: 0.000222, mean_q: 0.385446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1537/2000: episode: 10, duration: 8.485s, episode steps: 188, steps per second: 22, episode reward: 0.634, mean reward: 0.003 [-0.002, 0.012], mean action: 0.489 [-1.212, 1.269], mean observation: 0.107 [-22.874, 19.189], loss: 0.000095, mean_squared_error: 0.000190, mean_q: 0.388586\n",
      " 1702/2000: episode: 11, duration: 7.409s, episode steps: 165, steps per second: 22, episode reward: 0.625, mean reward: 0.004 [-0.002, 0.012], mean action: 0.460 [-1.224, 1.240], mean observation: 0.099 [-28.687, 19.418], loss: 0.000070, mean_squared_error: 0.000141, mean_q: 0.380015\n",
      " 1854/2000: episode: 12, duration: 7.250s, episode steps: 152, steps per second: 21, episode reward: 0.640, mean reward: 0.004 [-0.001, 0.012], mean action: 0.496 [-1.225, 1.300], mean observation: 0.105 [-21.663, 19.617], loss: 0.000167, mean_squared_error: 0.000334, mean_q: 0.380623\n",
      "done, took 82.778 seconds\n",
      "\n",
      "\n",
      "iteration: 341\n",
      "Training for 2000 steps ...\n",
      "  143/2000: episode: 1, duration: 5.400s, episode steps: 143, steps per second: 26, episode reward: 0.643, mean reward: 0.004 [-0.001, 0.012], mean action: 0.456 [-1.165, 1.167], mean observation: 0.099 [-20.586, 19.253], loss: --, mean_squared_error: --, mean_q: --\n",
      "  291/2000: episode: 2, duration: 5.452s, episode steps: 148, steps per second: 27, episode reward: 0.648, mean reward: 0.004 [-0.001, 0.012], mean action: 0.479 [-1.131, 1.187], mean observation: 0.101 [-20.880, 19.002], loss: --, mean_squared_error: --, mean_q: --\n",
      "  432/2000: episode: 3, duration: 5.373s, episode steps: 141, steps per second: 26, episode reward: 0.630, mean reward: 0.004 [-0.001, 0.012], mean action: 0.499 [-1.170, 1.218], mean observation: 0.101 [-19.389, 19.245], loss: --, mean_squared_error: --, mean_q: --\n",
      "  576/2000: episode: 4, duration: 5.645s, episode steps: 144, steps per second: 26, episode reward: 0.634, mean reward: 0.004 [-0.001, 0.012], mean action: 0.480 [-1.188, 1.188], mean observation: 0.099 [-20.927, 18.964], loss: --, mean_squared_error: --, mean_q: --\n",
      "  719/2000: episode: 5, duration: 5.670s, episode steps: 143, steps per second: 25, episode reward: 0.634, mean reward: 0.004 [-0.001, 0.012], mean action: 0.473 [-1.125, 1.243], mean observation: 0.099 [-21.689, 18.897], loss: --, mean_squared_error: --, mean_q: --\n",
      "  861/2000: episode: 6, duration: 5.410s, episode steps: 142, steps per second: 26, episode reward: 0.633, mean reward: 0.004 [-0.001, 0.012], mean action: 0.482 [-1.219, 1.247], mean observation: 0.099 [-20.977, 18.910], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1012/2000: episode: 7, duration: 5.723s, episode steps: 151, steps per second: 26, episode reward: 0.639, mean reward: 0.004 [-0.001, 0.012], mean action: 0.491 [-1.191, 1.224], mean observation: 0.101 [-20.303, 19.113], loss: 0.000022, mean_squared_error: 0.000045, mean_q: 0.392059\n",
      " 1161/2000: episode: 8, duration: 7.215s, episode steps: 149, steps per second: 21, episode reward: 0.640, mean reward: 0.004 [-0.001, 0.013], mean action: 0.485 [-1.129, 1.264], mean observation: 0.100 [-21.534, 18.768], loss: 0.000119, mean_squared_error: 0.000239, mean_q: 0.382489\n",
      " 1348/2000: episode: 9, duration: 8.052s, episode steps: 187, steps per second: 23, episode reward: 0.650, mean reward: 0.003 [-0.002, 0.012], mean action: 0.517 [-1.219, 1.207], mean observation: 0.103 [-21.536, 19.203], loss: 0.000116, mean_squared_error: 0.000232, mean_q: 0.379873\n",
      "done, took 82.088 seconds\n",
      "\n",
      "\n",
      "iteration: 342\n",
      "Training for 2000 steps ...\n",
      "  178/2000: episode: 1, duration: 5.820s, episode steps: 178, steps per second: 31, episode reward: 0.658, mean reward: 0.004 [-0.002, 0.012], mean action: 0.532 [-1.154, 1.362], mean observation: 0.101 [-20.493, 18.890], loss: --, mean_squared_error: --, mean_q: --\n",
      "  366/2000: episode: 2, duration: 6.058s, episode steps: 188, steps per second: 31, episode reward: 0.653, mean reward: 0.003 [-0.002, 0.012], mean action: 0.515 [-1.259, 1.218], mean observation: 0.104 [-20.489, 18.863], loss: --, mean_squared_error: --, mean_q: --\n",
      "  559/2000: episode: 3, duration: 6.144s, episode steps: 193, steps per second: 31, episode reward: 0.652, mean reward: 0.003 [-0.002, 0.012], mean action: 0.470 [-1.141, 1.152], mean observation: 0.103 [-20.111, 19.009], loss: --, mean_squared_error: --, mean_q: --\n",
      "  752/2000: episode: 4, duration: 6.183s, episode steps: 193, steps per second: 31, episode reward: 0.655, mean reward: 0.003 [-0.002, 0.012], mean action: 0.507 [-1.121, 1.205], mean observation: 0.104 [-20.790, 18.998], loss: --, mean_squared_error: --, mean_q: --\n",
      "  936/2000: episode: 5, duration: 6.084s, episode steps: 184, steps per second: 30, episode reward: 0.637, mean reward: 0.003 [-0.002, 0.012], mean action: 0.495 [-1.217, 1.164], mean observation: 0.103 [-20.719, 18.658], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1114/2000: episode: 6, duration: 6.936s, episode steps: 178, steps per second: 26, episode reward: 0.646, mean reward: 0.004 [-0.002, 0.012], mean action: 0.528 [-1.111, 1.238], mean observation: 0.102 [-21.327, 18.988], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.381122\n",
      " 1276/2000: episode: 7, duration: 7.217s, episode steps: 162, steps per second: 22, episode reward: 0.639, mean reward: 0.004 [-0.002, 0.012], mean action: 0.521 [-1.128, 1.277], mean observation: 0.097 [-16.189, 19.045], loss: 0.000128, mean_squared_error: 0.000256, mean_q: 0.381268\n",
      " 1849/2000: episode: 8, duration: 24.391s, episode steps: 573, steps per second: 23, episode reward: 0.690, mean reward: 0.001 [-0.002, 0.012], mean action: 0.502 [-1.251, 1.423], mean observation: 0.106 [-27.322, 18.902], loss: 0.000081, mean_squared_error: 0.000163, mean_q: 0.381471\n",
      "done, took 74.633 seconds\n",
      "\n",
      "\n",
      "iteration: 343\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 36.426s, episode steps: 1000, steps per second: 27, episode reward: 0.039, mean reward: 0.000 [-0.002, 0.011], mean action: 0.320 [-1.403, 1.354], mean observation: 0.103 [-23.936, 19.124], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1302/2000: episode: 2, duration: 13.976s, episode steps: 302, steps per second: 22, episode reward: 0.639, mean reward: 0.002 [-0.002, 0.012], mean action: 0.409 [-1.247, 1.367], mean observation: 0.105 [-20.640, 19.428], loss: 0.000161, mean_squared_error: 0.000322, mean_q: 0.375787\n",
      " 1843/2000: episode: 3, duration: 23.089s, episode steps: 541, steps per second: 23, episode reward: -0.792, mean reward: -0.001 [-0.020, 0.012], mean action: 0.310 [-1.349, 1.253], mean observation: 0.102 [-24.794, 19.214], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.379510\n",
      "done, took 79.123 seconds\n",
      "\n",
      "\n",
      "iteration: 344\n",
      "Training for 2000 steps ...\n",
      "  182/2000: episode: 1, duration: 4.995s, episode steps: 182, steps per second: 36, episode reward: -0.792, mean reward: -0.004 [-0.019, 0.009], mean action: 0.312 [-1.239, 1.151], mean observation: 0.069 [-40.473, 12.881], loss: --, mean_squared_error: --, mean_q: --\n",
      "  329/2000: episode: 2, duration: 4.649s, episode steps: 147, steps per second: 32, episode reward: -0.806, mean reward: -0.005 [-0.020, 0.009], mean action: 0.369 [-1.110, 1.199], mean observation: 0.059 [-39.953, 12.496], loss: --, mean_squared_error: --, mean_q: --\n",
      "  473/2000: episode: 3, duration: 4.574s, episode steps: 144, steps per second: 31, episode reward: -0.796, mean reward: -0.006 [-0.020, 0.009], mean action: 0.372 [-1.209, 1.117], mean observation: 0.060 [-40.749, 12.368], loss: --, mean_squared_error: --, mean_q: --\n",
      "  620/2000: episode: 4, duration: 4.520s, episode steps: 147, steps per second: 33, episode reward: -0.792, mean reward: -0.005 [-0.020, 0.009], mean action: 0.390 [-1.198, 1.206], mean observation: 0.067 [-35.733, 12.557], loss: --, mean_squared_error: --, mean_q: --\n",
      "  770/2000: episode: 5, duration: 4.091s, episode steps: 150, steps per second: 37, episode reward: -0.765, mean reward: -0.005 [-0.020, 0.009], mean action: 0.405 [-1.269, 1.300], mean observation: 0.072 [-36.145, 12.605], loss: --, mean_squared_error: --, mean_q: --\n",
      "  918/2000: episode: 6, duration: 4.474s, episode steps: 148, steps per second: 33, episode reward: -0.801, mean reward: -0.005 [-0.020, 0.009], mean action: 0.380 [-1.258, 1.155], mean observation: 0.063 [-39.732, 12.560], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1062/2000: episode: 7, duration: 5.146s, episode steps: 144, steps per second: 28, episode reward: -0.804, mean reward: -0.006 [-0.020, 0.009], mean action: 0.395 [-1.127, 1.205], mean observation: 0.058 [-40.803, 12.538], loss: 0.000212, mean_squared_error: 0.000424, mean_q: 0.370460\n",
      " 1242/2000: episode: 8, duration: 6.090s, episode steps: 180, steps per second: 30, episode reward: -0.748, mean reward: -0.004 [-0.020, 0.011], mean action: 0.391 [-1.142, 1.193], mean observation: 0.078 [-15.060, 15.844], loss: 0.000131, mean_squared_error: 0.000263, mean_q: 0.373299\n",
      "done, took 69.977 seconds\n",
      "\n",
      "\n",
      "iteration: 345\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 32.557s, episode steps: 1000, steps per second: 31, episode reward: 0.144, mean reward: 0.000 [-0.002, 0.008], mean action: 0.369 [-1.344, 1.387], mean observation: 0.127 [-23.418, 17.353], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 41.988s, episode steps: 1000, steps per second: 24, episode reward: 0.145, mean reward: 0.000 [-0.002, 0.008], mean action: 0.309 [-1.456, 1.471], mean observation: 0.120 [-23.817, 17.649], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.372353\n",
      "done, took 74.563 seconds\n",
      "\n",
      "\n",
      "iteration: 346\n",
      "Training for 2000 steps ...\n",
      "  606/2000: episode: 1, duration: 20.957s, episode steps: 606, steps per second: 29, episode reward: 0.717, mean reward: 0.001 [-0.003, 0.012], mean action: 0.357 [-1.363, 1.376], mean observation: 0.123 [-23.830, 17.604], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1175/2000: episode: 2, duration: 21.776s, episode steps: 569, steps per second: 26, episode reward: 0.726, mean reward: 0.001 [-0.003, 0.012], mean action: 0.355 [-1.350, 1.301], mean observation: 0.123 [-24.253, 17.850], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.368490\n",
      " 1446/2000: episode: 3, duration: 12.470s, episode steps: 271, steps per second: 22, episode reward: 0.747, mean reward: 0.003 [-0.002, 0.012], mean action: 0.376 [-1.245, 1.228], mean observation: 0.120 [-24.153, 17.603], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.369286\n",
      "done, took 77.118 seconds\n",
      "\n",
      "\n",
      "iteration: 347\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 24.850s, episode steps: 1000, steps per second: 40, episode reward: 0.089, mean reward: 0.000 [-0.002, 0.005], mean action: 0.274 [-1.658, 1.531], mean observation: 0.122 [-24.584, 17.764], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 39.758s, episode steps: 1000, steps per second: 25, episode reward: 0.041, mean reward: 0.000 [-0.002, 0.005], mean action: 0.209 [-1.436, 1.675], mean observation: 0.120 [-24.687, 17.848], loss: 0.000095, mean_squared_error: 0.000190, mean_q: 0.369425\n",
      "done, took 64.624 seconds\n",
      "\n",
      "\n",
      "iteration: 348\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.664s, episode steps: 1000, steps per second: 42, episode reward: 0.055, mean reward: 0.000 [-0.002, 0.009], mean action: 0.290 [-1.426, 1.337], mean observation: 0.122 [-17.589, 16.372], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 34.420s, episode steps: 1000, steps per second: 29, episode reward: 0.194, mean reward: 0.000 [-0.002, 0.008], mean action: 0.409 [-1.361, 1.392], mean observation: 0.126 [-17.124, 16.144], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.365960\n",
      "done, took 58.102 seconds\n",
      "\n",
      "\n",
      "iteration: 349\n",
      "Training for 2000 steps ...\n",
      "  548/2000: episode: 1, duration: 20.361s, episode steps: 548, steps per second: 27, episode reward: 0.773, mean reward: 0.001 [-0.002, 0.013], mean action: 0.398 [-1.303, 1.274], mean observation: 0.116 [-17.006, 17.079], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1548/2000: episode: 2, duration: 37.014s, episode steps: 1000, steps per second: 27, episode reward: 0.081, mean reward: 0.000 [-0.002, 0.008], mean action: 0.366 [-1.374, 1.462], mean observation: 0.115 [-17.852, 17.437], loss: 0.000091, mean_squared_error: 0.000181, mean_q: 0.363100\n",
      "done, took 75.384 seconds\n",
      "\n",
      "\n",
      "iteration: 350\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 32.245s, episode steps: 1000, steps per second: 31, episode reward: 0.060, mean reward: 0.000 [-0.002, 0.008], mean action: 0.293 [-1.345, 1.420], mean observation: 0.118 [-21.739, 16.636], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 34.485s, episode steps: 1000, steps per second: 29, episode reward: 0.101, mean reward: 0.000 [-0.001, 0.007], mean action: 0.418 [-1.190, 1.449], mean observation: 0.131 [-21.736, 16.700], loss: 0.000101, mean_squared_error: 0.000203, mean_q: 0.363135\n",
      "done, took 66.748 seconds\n",
      "\n",
      "\n",
      "iteration: 351\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 31.093s, episode steps: 1000, steps per second: 32, episode reward: 0.086, mean reward: 0.000 [-0.002, 0.004], mean action: 0.459 [-1.245, 1.509], mean observation: 0.123 [-13.074, 15.320], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1913/2000: episode: 2, duration: 35.134s, episode steps: 913, steps per second: 26, episode reward: 0.569, mean reward: 0.001 [-0.002, 0.011], mean action: 0.320 [-1.449, 1.498], mean observation: 0.122 [-13.630, 15.686], loss: 0.000108, mean_squared_error: 0.000216, mean_q: 0.358468\n",
      "done, took 70.156 seconds\n",
      "\n",
      "\n",
      "iteration: 352\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 27.111s, episode steps: 1000, steps per second: 37, episode reward: 0.022, mean reward: 0.000 [-0.001, 0.003], mean action: 0.265 [-1.168, 1.412], mean observation: 0.126 [-13.270, 15.653], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1307/2000: episode: 2, duration: 11.588s, episode steps: 307, steps per second: 26, episode reward: 0.589, mean reward: 0.002 [-0.001, 0.013], mean action: 0.369 [-1.364, 1.335], mean observation: 0.135 [-13.233, 15.587], loss: 0.000130, mean_squared_error: 0.000261, mean_q: 0.358453\n",
      " 1575/2000: episode: 3, duration: 8.647s, episode steps: 268, steps per second: 31, episode reward: -0.801, mean reward: -0.003 [-0.019, 0.003], mean action: 0.310 [-1.372, 1.308], mean observation: 0.104 [-12.931, 15.557], loss: 0.000115, mean_squared_error: 0.000229, mean_q: 0.356004\n",
      "done, took 64.541 seconds\n",
      "\n",
      "\n",
      "iteration: 353\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 34.508s, episode steps: 1000, steps per second: 29, episode reward: 0.054, mean reward: 0.000 [-0.002, 0.003], mean action: 0.336 [-1.417, 1.427], mean observation: 0.123 [-9.154, 14.832], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1265/2000: episode: 2, duration: 8.057s, episode steps: 265, steps per second: 33, episode reward: -0.809, mean reward: -0.003 [-0.019, 0.003], mean action: 0.272 [-1.191, 1.268], mean observation: 0.104 [-8.947, 14.962], loss: 0.000090, mean_squared_error: 0.000179, mean_q: 0.352137\n",
      "done, took 71.821 seconds\n",
      "\n",
      "\n",
      "iteration: 354\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 27.654s, episode steps: 1000, steps per second: 36, episode reward: 0.093, mean reward: 0.000 [-0.002, 0.007], mean action: 0.460 [-1.386, 1.582], mean observation: 0.124 [-10.115, 16.141], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1363/2000: episode: 2, duration: 14.244s, episode steps: 363, steps per second: 25, episode reward: 0.517, mean reward: 0.001 [-0.002, 0.010], mean action: 0.398 [-1.230, 1.385], mean observation: 0.135 [-10.768, 16.549], loss: 0.000059, mean_squared_error: 0.000118, mean_q: 0.355601\n",
      " 1501/2000: episode: 3, duration: 5.120s, episode steps: 138, steps per second: 27, episode reward: 0.501, mean reward: 0.004 [-0.002, 0.010], mean action: 0.458 [-1.069, 1.232], mean observation: 0.146 [-7.960, 12.422], loss: 0.000119, mean_squared_error: 0.000238, mean_q: 0.360580\n",
      " 1649/2000: episode: 4, duration: 5.640s, episode steps: 148, steps per second: 26, episode reward: 0.494, mean reward: 0.003 [-0.001, 0.010], mean action: 0.442 [-1.160, 1.139], mean observation: 0.144 [-8.357, 13.466], loss: 0.000099, mean_squared_error: 0.000197, mean_q: 0.365035\n",
      "done, took 66.763 seconds\n",
      "\n",
      "\n",
      "iteration: 355\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.523s, episode steps: 1000, steps per second: 43, episode reward: 0.099, mean reward: 0.000 [-0.002, 0.007], mean action: 0.364 [-1.432, 1.503], mean observation: 0.125 [-9.440, 15.166], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1386/2000: episode: 2, duration: 15.210s, episode steps: 386, steps per second: 25, episode reward: 0.502, mean reward: 0.001 [-0.002, 0.010], mean action: 0.415 [-1.426, 1.236], mean observation: 0.133 [-9.661, 15.112], loss: 0.000053, mean_squared_error: 0.000106, mean_q: 0.362189\n",
      " 1594/2000: episode: 3, duration: 8.156s, episode steps: 208, steps per second: 26, episode reward: 0.485, mean reward: 0.002 [-0.001, 0.010], mean action: 0.493 [-1.196, 1.283], mean observation: 0.138 [-9.780, 16.115], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.361905\n",
      " 1842/2000: episode: 4, duration: 9.541s, episode steps: 248, steps per second: 26, episode reward: 0.497, mean reward: 0.002 [-0.001, 0.010], mean action: 0.446 [-1.339, 1.264], mean observation: 0.139 [-9.595, 14.968], loss: 0.000143, mean_squared_error: 0.000286, mean_q: 0.358033\n",
      "done, took 61.847 seconds\n",
      "\n",
      "\n",
      "iteration: 356\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 21.552s, episode steps: 1000, steps per second: 46, episode reward: 0.159, mean reward: 0.000 [-0.002, 0.006], mean action: 0.554 [-1.308, 1.439], mean observation: 0.126 [-15.332, 15.114], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1433/2000: episode: 2, duration: 16.300s, episode steps: 433, steps per second: 27, episode reward: 0.519, mean reward: 0.001 [-0.002, 0.010], mean action: 0.467 [-1.196, 1.330], mean observation: 0.135 [-13.452, 14.576], loss: 0.000101, mean_squared_error: 0.000201, mean_q: 0.362894\n",
      "done, took 59.004 seconds\n",
      "\n",
      "\n",
      "iteration: 357\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.178s, episode steps: 1000, steps per second: 43, episode reward: 0.067, mean reward: 0.000 [-0.001, 0.005], mean action: 0.424 [-1.435, 1.406], mean observation: 0.123 [-13.889, 15.273], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 40.573s, episode steps: 1000, steps per second: 25, episode reward: 0.073, mean reward: 0.000 [-0.002, 0.004], mean action: 0.481 [-1.393, 1.536], mean observation: 0.126 [-13.058, 15.029], loss: 0.000112, mean_squared_error: 0.000224, mean_q: 0.365135\n",
      "done, took 63.770 seconds\n",
      "\n",
      "\n",
      "iteration: 358\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 30.666s, episode steps: 1000, steps per second: 33, episode reward: 0.089, mean reward: 0.000 [-0.002, 0.003], mean action: 0.362 [-1.450, 1.463], mean observation: 0.128 [-20.584, 15.377], loss: --, mean_squared_error: --, mean_q: --\n",
      " 2000/2000: episode: 2, duration: 41.969s, episode steps: 1000, steps per second: 24, episode reward: 0.130, mean reward: 0.000 [-0.001, 0.003], mean action: 0.607 [-1.352, 1.539], mean observation: 0.125 [-19.447, 14.863], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.365704\n",
      "done, took 72.655 seconds\n",
      "\n",
      "\n",
      "iteration: 359\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 36.450s, episode steps: 1000, steps per second: 27, episode reward: 0.117, mean reward: 0.000 [-0.001, 0.003], mean action: 0.583 [-1.339, 1.459], mean observation: 0.123 [-10.371, 14.479], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1717/2000: episode: 2, duration: 29.181s, episode steps: 717, steps per second: 25, episode reward: 0.472, mean reward: 0.001 [-0.001, 0.010], mean action: 0.523 [-1.382, 1.346], mean observation: 0.129 [-9.474, 13.600], loss: 0.000080, mean_squared_error: 0.000160, mean_q: 0.360545\n",
      "done, took 78.063 seconds\n",
      "\n",
      "\n",
      "iteration: 360\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 36.140s, episode steps: 1000, steps per second: 28, episode reward: 0.122, mean reward: 0.000 [-0.001, 0.004], mean action: 0.294 [-1.459, 1.483], mean observation: 0.127 [-22.917, 19.315], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1407/2000: episode: 2, duration: 15.820s, episode steps: 407, steps per second: 26, episode reward: 0.486, mean reward: 0.001 [-0.001, 0.010], mean action: 0.345 [-1.266, 1.263], mean observation: 0.132 [-20.477, 19.161], loss: 0.000089, mean_squared_error: 0.000179, mean_q: 0.358161\n",
      " 1738/2000: episode: 3, duration: 14.165s, episode steps: 331, steps per second: 23, episode reward: 0.606, mean reward: 0.002 [-0.001, 0.012], mean action: 0.480 [-1.213, 1.310], mean observation: 0.135 [-26.037, 18.861], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.359908\n",
      "done, took 78.197 seconds\n",
      "\n",
      "\n",
      "iteration: 361\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 34.145s, episode steps: 1000, steps per second: 29, episode reward: 0.100, mean reward: 0.000 [-0.001, 0.004], mean action: 0.288 [-1.265, 1.421], mean observation: 0.125 [-28.570, 18.743], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1539/2000: episode: 2, duration: 20.326s, episode steps: 539, steps per second: 27, episode reward: 0.606, mean reward: 0.001 [-0.002, 0.014], mean action: 0.345 [-1.399, 1.287], mean observation: 0.132 [-25.874, 18.941], loss: 0.000125, mean_squared_error: 0.000250, mean_q: 0.353936\n",
      " 1804/2000: episode: 3, duration: 9.689s, episode steps: 265, steps per second: 27, episode reward: 0.600, mean reward: 0.002 [-0.001, 0.013], mean action: 0.383 [-1.259, 1.214], mean observation: 0.137 [-16.295, 19.076], loss: 0.000077, mean_squared_error: 0.000155, mean_q: 0.353132\n",
      "done, took 72.038 seconds\n",
      "\n",
      "\n",
      "iteration: 362\n",
      "Training for 2000 steps ...\n",
      "  142/2000: episode: 1, duration: 5.005s, episode steps: 142, steps per second: 28, episode reward: 0.671, mean reward: 0.005 [-0.001, 0.012], mean action: 0.344 [-1.132, 1.244], mean observation: 0.142 [-17.086, 18.381], loss: --, mean_squared_error: --, mean_q: --\n",
      "  285/2000: episode: 2, duration: 4.949s, episode steps: 143, steps per second: 29, episode reward: 0.661, mean reward: 0.005 [-0.001, 0.012], mean action: 0.325 [-1.146, 1.225], mean observation: 0.142 [-17.396, 20.034], loss: --, mean_squared_error: --, mean_q: --\n",
      "  430/2000: episode: 3, duration: 5.007s, episode steps: 145, steps per second: 29, episode reward: 0.673, mean reward: 0.005 [-0.001, 0.012], mean action: 0.326 [-1.153, 1.184], mean observation: 0.142 [-17.360, 18.502], loss: --, mean_squared_error: --, mean_q: --\n",
      "  574/2000: episode: 4, duration: 5.006s, episode steps: 144, steps per second: 29, episode reward: 0.666, mean reward: 0.005 [-0.001, 0.012], mean action: 0.307 [-1.299, 1.204], mean observation: 0.142 [-17.381, 19.272], loss: --, mean_squared_error: --, mean_q: --\n",
      "  718/2000: episode: 5, duration: 4.994s, episode steps: 144, steps per second: 29, episode reward: 0.672, mean reward: 0.005 [-0.001, 0.012], mean action: 0.314 [-1.138, 1.110], mean observation: 0.143 [-17.860, 18.733], loss: --, mean_squared_error: --, mean_q: --\n",
      "  865/2000: episode: 6, duration: 5.307s, episode steps: 147, steps per second: 28, episode reward: 0.696, mean reward: 0.005 [-0.001, 0.012], mean action: 0.330 [-1.199, 1.192], mean observation: 0.143 [-17.218, 17.896], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1009/2000: episode: 7, duration: 5.275s, episode steps: 144, steps per second: 27, episode reward: 0.687, mean reward: 0.005 [-0.001, 0.012], mean action: 0.357 [-1.200, 1.265], mean observation: 0.142 [-17.000, 18.911], loss: 0.000027, mean_squared_error: 0.000055, mean_q: 0.359180\n",
      " 1142/2000: episode: 8, duration: 5.758s, episode steps: 133, steps per second: 23, episode reward: 0.674, mean reward: 0.005 [-0.001, 0.013], mean action: 0.350 [-1.161, 1.143], mean observation: 0.143 [-16.464, 18.610], loss: 0.000048, mean_squared_error: 0.000097, mean_q: 0.352725\n",
      " 1270/2000: episode: 9, duration: 5.348s, episode steps: 128, steps per second: 24, episode reward: 0.659, mean reward: 0.005 [-0.001, 0.013], mean action: 0.351 [-1.122, 1.265], mean observation: 0.143 [-19.006, 19.094], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.348389\n",
      " 1397/2000: episode: 10, duration: 5.329s, episode steps: 127, steps per second: 24, episode reward: 0.656, mean reward: 0.005 [-0.001, 0.013], mean action: 0.332 [-1.103, 1.245], mean observation: 0.145 [-19.643, 18.812], loss: 0.000069, mean_squared_error: 0.000137, mean_q: 0.352487\n",
      " 1514/2000: episode: 11, duration: 4.613s, episode steps: 117, steps per second: 25, episode reward: 0.617, mean reward: 0.005 [-0.000, 0.014], mean action: 0.292 [-1.179, 1.156], mean observation: 0.143 [-23.121, 19.288], loss: 0.000080, mean_squared_error: 0.000160, mean_q: 0.351768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1630/2000: episode: 12, duration: 4.656s, episode steps: 116, steps per second: 25, episode reward: 0.609, mean reward: 0.005 [-0.002, 0.013], mean action: 0.345 [-1.143, 1.223], mean observation: 0.144 [-24.140, 17.546], loss: 0.000016, mean_squared_error: 0.000033, mean_q: 0.347356\n",
      " 1748/2000: episode: 13, duration: 4.721s, episode steps: 118, steps per second: 25, episode reward: 0.635, mean reward: 0.005 [-0.002, 0.014], mean action: 0.338 [-1.088, 1.251], mean observation: 0.142 [-23.544, 18.047], loss: 0.000056, mean_squared_error: 0.000111, mean_q: 0.347460\n",
      " 1868/2000: episode: 14, duration: 4.732s, episode steps: 120, steps per second: 25, episode reward: 0.629, mean reward: 0.005 [-0.003, 0.014], mean action: 0.308 [-1.200, 1.186], mean observation: 0.143 [-18.617, 17.457], loss: 0.000072, mean_squared_error: 0.000143, mean_q: 0.344193\n",
      " 1988/2000: episode: 15, duration: 4.725s, episode steps: 120, steps per second: 25, episode reward: 0.625, mean reward: 0.005 [-0.002, 0.014], mean action: 0.315 [-1.169, 1.245], mean observation: 0.143 [-23.828, 17.998], loss: 0.000099, mean_squared_error: 0.000199, mean_q: 0.344467\n",
      "done, took 76.087 seconds\n",
      "\n",
      "\n",
      "iteration: 363\n",
      "Training for 2000 steps ...\n",
      "  127/2000: episode: 1, duration: 3.834s, episode steps: 127, steps per second: 33, episode reward: 0.643, mean reward: 0.005 [-0.001, 0.014], mean action: 0.354 [-1.132, 1.234], mean observation: 0.145 [-22.582, 16.459], loss: --, mean_squared_error: --, mean_q: --\n",
      "  256/2000: episode: 2, duration: 3.746s, episode steps: 129, steps per second: 34, episode reward: 0.644, mean reward: 0.005 [-0.002, 0.014], mean action: 0.312 [-1.123, 1.178], mean observation: 0.144 [-22.179, 15.763], loss: --, mean_squared_error: --, mean_q: --\n",
      "  382/2000: episode: 3, duration: 3.696s, episode steps: 126, steps per second: 34, episode reward: 0.588, mean reward: 0.005 [-0.002, 0.013], mean action: 0.293 [-1.218, 1.127], mean observation: 0.143 [-16.524, 15.639], loss: --, mean_squared_error: --, mean_q: --\n",
      "  508/2000: episode: 4, duration: 3.661s, episode steps: 126, steps per second: 34, episode reward: 0.612, mean reward: 0.005 [-0.001, 0.013], mean action: 0.315 [-1.287, 1.152], mean observation: 0.146 [-22.418, 16.095], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 5, duration: 3.961s, episode steps: 129, steps per second: 33, episode reward: 0.639, mean reward: 0.005 [-0.002, 0.014], mean action: 0.362 [-1.075, 1.251], mean observation: 0.143 [-20.398, 15.933], loss: --, mean_squared_error: --, mean_q: --\n",
      "  764/2000: episode: 6, duration: 3.782s, episode steps: 127, steps per second: 34, episode reward: 0.636, mean reward: 0.005 [-0.002, 0.014], mean action: 0.301 [-1.192, 1.192], mean observation: 0.143 [-17.483, 15.779], loss: --, mean_squared_error: --, mean_q: --\n",
      "  891/2000: episode: 7, duration: 3.747s, episode steps: 127, steps per second: 34, episode reward: 0.631, mean reward: 0.005 [-0.002, 0.014], mean action: 0.332 [-1.082, 1.161], mean observation: 0.145 [-22.151, 17.138], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1017/2000: episode: 8, duration: 3.953s, episode steps: 126, steps per second: 32, episode reward: 0.609, mean reward: 0.005 [-0.002, 0.013], mean action: 0.332 [-1.144, 1.247], mean observation: 0.143 [-20.332, 15.899], loss: 0.000015, mean_squared_error: 0.000031, mean_q: 0.349561\n",
      " 1142/2000: episode: 9, duration: 5.079s, episode steps: 125, steps per second: 25, episode reward: 0.656, mean reward: 0.005 [-0.003, 0.014], mean action: 0.349 [-1.077, 1.223], mean observation: 0.145 [-24.695, 18.180], loss: 0.000116, mean_squared_error: 0.000232, mean_q: 0.345891\n",
      " 1271/2000: episode: 10, duration: 5.212s, episode steps: 129, steps per second: 25, episode reward: 0.658, mean reward: 0.005 [-0.002, 0.013], mean action: 0.340 [-1.216, 1.239], mean observation: 0.145 [-21.601, 16.219], loss: 0.000242, mean_squared_error: 0.000484, mean_q: 0.345587\n",
      " 1402/2000: episode: 11, duration: 5.499s, episode steps: 131, steps per second: 24, episode reward: 0.679, mean reward: 0.005 [-0.002, 0.013], mean action: 0.241 [-1.206, 1.242], mean observation: 0.144 [-14.244, 15.978], loss: 0.000072, mean_squared_error: 0.000145, mean_q: 0.346740\n",
      " 1526/2000: episode: 12, duration: 5.233s, episode steps: 124, steps per second: 24, episode reward: 0.668, mean reward: 0.005 [-0.002, 0.013], mean action: 0.305 [-1.084, 1.227], mean observation: 0.144 [-18.874, 16.240], loss: 0.000084, mean_squared_error: 0.000167, mean_q: 0.343137\n",
      " 1651/2000: episode: 13, duration: 5.283s, episode steps: 125, steps per second: 24, episode reward: 0.661, mean reward: 0.005 [-0.002, 0.013], mean action: 0.343 [-1.109, 1.214], mean observation: 0.143 [-16.699, 15.568], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.346507\n",
      " 1773/2000: episode: 14, duration: 4.945s, episode steps: 122, steps per second: 25, episode reward: 0.645, mean reward: 0.005 [-0.001, 0.013], mean action: 0.328 [-1.146, 1.140], mean observation: 0.145 [-22.984, 16.098], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.344625\n",
      " 1892/2000: episode: 15, duration: 4.815s, episode steps: 119, steps per second: 25, episode reward: 0.666, mean reward: 0.006 [-0.001, 0.014], mean action: 0.344 [-1.094, 1.199], mean observation: 0.143 [-25.476, 15.729], loss: 0.000013, mean_squared_error: 0.000026, mean_q: 0.341651\n",
      "done, took 70.418 seconds\n",
      "\n",
      "\n",
      "iteration: 364\n",
      "Training for 2000 steps ...\n",
      "  112/2000: episode: 1, duration: 3.019s, episode steps: 112, steps per second: 37, episode reward: 0.496, mean reward: 0.004 [-0.001, 0.011], mean action: 0.279 [-1.195, 1.198], mean observation: 0.153 [-23.103, 15.944], loss: --, mean_squared_error: --, mean_q: --\n",
      "  226/2000: episode: 2, duration: 3.056s, episode steps: 114, steps per second: 37, episode reward: 0.501, mean reward: 0.004 [-0.001, 0.011], mean action: 0.270 [-1.170, 1.197], mean observation: 0.152 [-21.910, 16.366], loss: --, mean_squared_error: --, mean_q: --\n",
      "  337/2000: episode: 3, duration: 2.947s, episode steps: 111, steps per second: 38, episode reward: 0.506, mean reward: 0.005 [-0.001, 0.011], mean action: 0.266 [-1.131, 1.137], mean observation: 0.153 [-21.318, 15.946], loss: --, mean_squared_error: --, mean_q: --\n",
      "  448/2000: episode: 4, duration: 2.916s, episode steps: 111, steps per second: 38, episode reward: 0.503, mean reward: 0.005 [-0.001, 0.011], mean action: 0.293 [-1.121, 1.266], mean observation: 0.152 [-18.700, 16.034], loss: --, mean_squared_error: --, mean_q: --\n",
      "  557/2000: episode: 5, duration: 2.867s, episode steps: 109, steps per second: 38, episode reward: 0.506, mean reward: 0.005 [-0.001, 0.011], mean action: 0.271 [-1.366, 1.157], mean observation: 0.153 [-19.697, 15.912], loss: --, mean_squared_error: --, mean_q: --\n",
      "  665/2000: episode: 6, duration: 2.907s, episode steps: 108, steps per second: 37, episode reward: 0.498, mean reward: 0.005 [-0.001, 0.011], mean action: 0.264 [-1.187, 1.208], mean observation: 0.153 [-19.192, 16.369], loss: --, mean_squared_error: --, mean_q: --\n",
      "  777/2000: episode: 7, duration: 2.969s, episode steps: 112, steps per second: 38, episode reward: 0.506, mean reward: 0.005 [-0.001, 0.011], mean action: 0.272 [-1.080, 1.096], mean observation: 0.153 [-20.724, 16.056], loss: --, mean_squared_error: --, mean_q: --\n",
      "  887/2000: episode: 8, duration: 2.937s, episode steps: 110, steps per second: 37, episode reward: 0.500, mean reward: 0.005 [-0.001, 0.011], mean action: 0.297 [-1.169, 1.164], mean observation: 0.153 [-19.933, 16.035], loss: --, mean_squared_error: --, mean_q: --\n",
      "  997/2000: episode: 9, duration: 2.955s, episode steps: 110, steps per second: 37, episode reward: 0.497, mean reward: 0.005 [-0.001, 0.011], mean action: 0.293 [-1.111, 1.143], mean observation: 0.153 [-19.937, 16.136], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1107/2000: episode: 10, duration: 3.977s, episode steps: 110, steps per second: 28, episode reward: 0.503, mean reward: 0.005 [-0.001, 0.011], mean action: 0.275 [-1.234, 1.164], mean observation: 0.153 [-18.657, 16.345], loss: 0.000058, mean_squared_error: 0.000117, mean_q: 0.341610\n",
      " 1216/2000: episode: 11, duration: 3.846s, episode steps: 109, steps per second: 28, episode reward: 0.498, mean reward: 0.005 [-0.002, 0.011], mean action: 0.211 [-1.095, 1.172], mean observation: 0.151 [-18.400, 15.701], loss: 0.000044, mean_squared_error: 0.000087, mean_q: 0.343849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1321/2000: episode: 12, duration: 3.597s, episode steps: 105, steps per second: 29, episode reward: 0.493, mean reward: 0.005 [-0.001, 0.011], mean action: 0.255 [-1.119, 1.161], mean observation: 0.152 [-25.521, 15.346], loss: 0.000120, mean_squared_error: 0.000239, mean_q: 0.338595\n",
      " 1424/2000: episode: 13, duration: 3.518s, episode steps: 103, steps per second: 29, episode reward: 0.502, mean reward: 0.005 [-0.001, 0.011], mean action: 0.209 [-1.119, 1.190], mean observation: 0.155 [-21.307, 17.709], loss: 0.000136, mean_squared_error: 0.000272, mean_q: 0.337879\n",
      " 1525/2000: episode: 14, duration: 3.411s, episode steps: 101, steps per second: 30, episode reward: 0.504, mean reward: 0.005 [-0.000, 0.011], mean action: 0.238 [-1.187, 1.163], mean observation: 0.155 [-17.332, 17.606], loss: 0.000064, mean_squared_error: 0.000127, mean_q: 0.342891\n",
      " 1625/2000: episode: 15, duration: 3.549s, episode steps: 100, steps per second: 28, episode reward: 0.489, mean reward: 0.005 [-0.001, 0.010], mean action: 0.251 [-1.170, 1.125], mean observation: 0.154 [-16.794, 16.613], loss: 0.000073, mean_squared_error: 0.000145, mean_q: 0.342401\n",
      " 1718/2000: episode: 16, duration: 3.389s, episode steps: 93, steps per second: 27, episode reward: 0.481, mean reward: 0.005 [-0.001, 0.013], mean action: 0.325 [-1.261, 1.182], mean observation: 0.156 [-9.593, 17.314], loss: 0.000064, mean_squared_error: 0.000128, mean_q: 0.340925\n",
      " 1818/2000: episode: 17, duration: 3.556s, episode steps: 100, steps per second: 28, episode reward: 0.492, mean reward: 0.005 [-0.002, 0.011], mean action: 0.283 [-1.220, 1.165], mean observation: 0.157 [-20.802, 14.453], loss: 0.000071, mean_squared_error: 0.000142, mean_q: 0.339041\n",
      " 1919/2000: episode: 18, duration: 3.560s, episode steps: 101, steps per second: 28, episode reward: 0.486, mean reward: 0.005 [-0.001, 0.011], mean action: 0.296 [-1.180, 1.184], mean observation: 0.154 [-21.626, 16.723], loss: 0.000015, mean_squared_error: 0.000031, mean_q: 0.343596\n",
      "done, took 61.961 seconds\n",
      "\n",
      "\n",
      "iteration: 365\n",
      "Training for 2000 steps ...\n",
      "  102/2000: episode: 1, duration: 2.616s, episode steps: 102, steps per second: 39, episode reward: 0.492, mean reward: 0.005 [-0.002, 0.011], mean action: 0.288 [-1.183, 1.161], mean observation: 0.153 [-21.932, 14.841], loss: --, mean_squared_error: --, mean_q: --\n",
      "  205/2000: episode: 2, duration: 2.666s, episode steps: 103, steps per second: 39, episode reward: 0.486, mean reward: 0.005 [-0.002, 0.011], mean action: 0.272 [-1.109, 1.214], mean observation: 0.155 [-20.252, 14.174], loss: --, mean_squared_error: --, mean_q: --\n",
      "  308/2000: episode: 3, duration: 2.661s, episode steps: 103, steps per second: 39, episode reward: 0.491, mean reward: 0.005 [-0.002, 0.011], mean action: 0.294 [-1.117, 1.227], mean observation: 0.154 [-24.567, 14.193], loss: --, mean_squared_error: --, mean_q: --\n",
      "  409/2000: episode: 4, duration: 2.614s, episode steps: 101, steps per second: 39, episode reward: 0.484, mean reward: 0.005 [-0.002, 0.011], mean action: 0.324 [-1.114, 1.241], mean observation: 0.155 [-23.885, 14.779], loss: --, mean_squared_error: --, mean_q: --\n",
      "  511/2000: episode: 5, duration: 2.729s, episode steps: 102, steps per second: 37, episode reward: 0.484, mean reward: 0.005 [-0.001, 0.011], mean action: 0.277 [-1.207, 1.233], mean observation: 0.152 [-23.458, 14.384], loss: --, mean_squared_error: --, mean_q: --\n",
      "  613/2000: episode: 6, duration: 2.681s, episode steps: 102, steps per second: 38, episode reward: 0.492, mean reward: 0.005 [-0.002, 0.011], mean action: 0.284 [-1.189, 1.185], mean observation: 0.155 [-20.014, 14.489], loss: --, mean_squared_error: --, mean_q: --\n",
      "  716/2000: episode: 7, duration: 2.636s, episode steps: 103, steps per second: 39, episode reward: 0.490, mean reward: 0.005 [-0.001, 0.011], mean action: 0.280 [-1.188, 1.210], mean observation: 0.153 [-19.251, 14.499], loss: --, mean_squared_error: --, mean_q: --\n",
      "  818/2000: episode: 8, duration: 2.694s, episode steps: 102, steps per second: 38, episode reward: 0.488, mean reward: 0.005 [-0.002, 0.011], mean action: 0.274 [-1.147, 1.146], mean observation: 0.153 [-23.725, 14.748], loss: --, mean_squared_error: --, mean_q: --\n",
      "  919/2000: episode: 9, duration: 2.690s, episode steps: 101, steps per second: 38, episode reward: 0.484, mean reward: 0.005 [-0.002, 0.011], mean action: 0.312 [-1.105, 1.203], mean observation: 0.152 [-24.605, 14.552], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1021/2000: episode: 10, duration: 2.871s, episode steps: 102, steps per second: 36, episode reward: 0.491, mean reward: 0.005 [-0.001, 0.011], mean action: 0.288 [-1.103, 1.131], mean observation: 0.155 [-19.866, 14.332], loss: 0.000169, mean_squared_error: 0.000338, mean_q: 0.335654\n",
      " 1121/2000: episode: 11, duration: 3.541s, episode steps: 100, steps per second: 28, episode reward: 0.501, mean reward: 0.005 [-0.002, 0.011], mean action: 0.287 [-1.124, 1.177], mean observation: 0.156 [-17.469, 14.691], loss: 0.000114, mean_squared_error: 0.000229, mean_q: 0.342344\n",
      " 1221/2000: episode: 12, duration: 3.570s, episode steps: 100, steps per second: 28, episode reward: 0.488, mean reward: 0.005 [-0.002, 0.011], mean action: 0.273 [-1.255, 1.194], mean observation: 0.156 [-18.394, 14.996], loss: 0.000055, mean_squared_error: 0.000111, mean_q: 0.340443\n",
      " 1314/2000: episode: 13, duration: 3.342s, episode steps: 93, steps per second: 28, episode reward: 0.491, mean reward: 0.005 [-0.002, 0.011], mean action: 0.344 [-1.147, 1.209], mean observation: 0.157 [-6.728, 15.541], loss: 0.000193, mean_squared_error: 0.000386, mean_q: 0.340298\n",
      " 1414/2000: episode: 14, duration: 3.445s, episode steps: 100, steps per second: 29, episode reward: 0.496, mean reward: 0.005 [0.000, 0.011], mean action: 0.303 [-1.105, 1.141], mean observation: 0.155 [-14.762, 15.633], loss: 0.000155, mean_squared_error: 0.000311, mean_q: 0.337221\n",
      " 1517/2000: episode: 15, duration: 3.476s, episode steps: 103, steps per second: 30, episode reward: 0.491, mean reward: 0.005 [0.001, 0.011], mean action: 0.312 [-1.181, 1.197], mean observation: 0.152 [-16.601, 16.028], loss: 0.000102, mean_squared_error: 0.000204, mean_q: 0.338450\n",
      " 1612/2000: episode: 16, duration: 3.403s, episode steps: 95, steps per second: 28, episode reward: 0.493, mean reward: 0.005 [-0.001, 0.011], mean action: 0.335 [-1.149, 1.116], mean observation: 0.156 [-8.895, 15.915], loss: 0.000051, mean_squared_error: 0.000101, mean_q: 0.341465\n",
      " 1723/2000: episode: 17, duration: 3.900s, episode steps: 111, steps per second: 28, episode reward: 0.502, mean reward: 0.005 [-0.002, 0.011], mean action: 0.293 [-1.100, 1.246], mean observation: 0.153 [-16.383, 15.790], loss: 0.000057, mean_squared_error: 0.000114, mean_q: 0.336974\n",
      " 1824/2000: episode: 18, duration: 3.348s, episode steps: 101, steps per second: 30, episode reward: 0.498, mean reward: 0.005 [-0.001, 0.011], mean action: 0.302 [-1.139, 1.139], mean observation: 0.154 [-18.380, 16.217], loss: 0.000019, mean_squared_error: 0.000039, mean_q: 0.333494\n",
      " 1922/2000: episode: 19, duration: 3.221s, episode steps: 98, steps per second: 30, episode reward: 0.499, mean reward: 0.005 [-0.000, 0.011], mean action: 0.301 [-1.155, 1.250], mean observation: 0.156 [-12.066, 17.475], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.331940\n",
      "done, took 60.843 seconds\n",
      "\n",
      "\n",
      "iteration: 366\n",
      "Training for 2000 steps ...\n",
      "  117/2000: episode: 1, duration: 3.037s, episode steps: 117, steps per second: 39, episode reward: 0.505, mean reward: 0.004 [-0.001, 0.011], mean action: 0.240 [-1.182, 1.167], mean observation: 0.153 [-23.566, 17.300], loss: --, mean_squared_error: --, mean_q: --\n",
      "  234/2000: episode: 2, duration: 3.195s, episode steps: 117, steps per second: 37, episode reward: 0.514, mean reward: 0.004 [-0.001, 0.011], mean action: 0.273 [-1.115, 1.156], mean observation: 0.153 [-17.411, 17.114], loss: --, mean_squared_error: --, mean_q: --\n",
      "  350/2000: episode: 3, duration: 3.156s, episode steps: 116, steps per second: 37, episode reward: 0.511, mean reward: 0.004 [-0.001, 0.011], mean action: 0.261 [-1.159, 1.198], mean observation: 0.154 [-18.403, 17.110], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  464/2000: episode: 4, duration: 3.091s, episode steps: 114, steps per second: 37, episode reward: 0.496, mean reward: 0.004 [-0.001, 0.011], mean action: 0.249 [-1.174, 1.138], mean observation: 0.152 [-16.948, 18.097], loss: --, mean_squared_error: --, mean_q: --\n",
      "  579/2000: episode: 5, duration: 3.162s, episode steps: 115, steps per second: 36, episode reward: 0.499, mean reward: 0.004 [-0.001, 0.011], mean action: 0.252 [-1.159, 1.120], mean observation: 0.153 [-19.393, 17.457], loss: --, mean_squared_error: --, mean_q: --\n",
      "  694/2000: episode: 6, duration: 3.127s, episode steps: 115, steps per second: 37, episode reward: 0.508, mean reward: 0.004 [-0.001, 0.011], mean action: 0.251 [-1.180, 1.135], mean observation: 0.153 [-18.312, 17.471], loss: --, mean_squared_error: --, mean_q: --\n",
      "  809/2000: episode: 7, duration: 3.125s, episode steps: 115, steps per second: 37, episode reward: 0.499, mean reward: 0.004 [-0.001, 0.011], mean action: 0.229 [-1.211, 1.156], mean observation: 0.152 [-18.109, 17.815], loss: --, mean_squared_error: --, mean_q: --\n",
      "  925/2000: episode: 8, duration: 3.137s, episode steps: 116, steps per second: 37, episode reward: 0.500, mean reward: 0.004 [-0.001, 0.011], mean action: 0.242 [-1.220, 1.125], mean observation: 0.152 [-24.809, 17.204], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1039/2000: episode: 9, duration: 3.537s, episode steps: 114, steps per second: 32, episode reward: 0.503, mean reward: 0.004 [-0.001, 0.011], mean action: 0.272 [-1.147, 1.126], mean observation: 0.152 [-16.654, 17.252], loss: 0.000017, mean_squared_error: 0.000034, mean_q: 0.337170\n",
      " 1152/2000: episode: 10, duration: 4.133s, episode steps: 113, steps per second: 27, episode reward: 0.498, mean reward: 0.004 [-0.001, 0.011], mean action: 0.240 [-1.242, 1.236], mean observation: 0.151 [-21.415, 18.193], loss: 0.000156, mean_squared_error: 0.000312, mean_q: 0.334811\n",
      " 1266/2000: episode: 11, duration: 4.160s, episode steps: 114, steps per second: 27, episode reward: 0.505, mean reward: 0.004 [-0.001, 0.011], mean action: 0.260 [-1.158, 1.167], mean observation: 0.151 [-24.905, 16.489], loss: 0.000035, mean_squared_error: 0.000070, mean_q: 0.335396\n",
      " 1373/2000: episode: 12, duration: 3.787s, episode steps: 107, steps per second: 28, episode reward: 0.497, mean reward: 0.005 [-0.001, 0.011], mean action: 0.283 [-1.158, 1.152], mean observation: 0.153 [-17.130, 20.830], loss: 0.000091, mean_squared_error: 0.000183, mean_q: 0.333943\n",
      " 1521/2000: episode: 13, duration: 5.517s, episode steps: 148, steps per second: 27, episode reward: 0.465, mean reward: 0.003 [-0.001, 0.010], mean action: 0.253 [-1.192, 1.123], mean observation: 0.143 [-23.481, 20.941], loss: 0.000019, mean_squared_error: 0.000039, mean_q: 0.333382\n",
      "done, took 63.579 seconds\n",
      "\n",
      "\n",
      "iteration: 367\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 25.306s, episode steps: 1000, steps per second: 40, episode reward: 0.035, mean reward: 0.000 [-0.003, 0.003], mean action: 0.324 [-1.376, 1.401], mean observation: 0.125 [-20.210, 17.424], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1202/2000: episode: 2, duration: 6.868s, episode steps: 202, steps per second: 29, episode reward: 0.483, mean reward: 0.002 [-0.001, 0.011], mean action: 0.294 [-1.140, 1.208], mean observation: 0.142 [-20.088, 17.301], loss: 0.000051, mean_squared_error: 0.000103, mean_q: 0.331660\n",
      " 1316/2000: episode: 3, duration: 4.003s, episode steps: 114, steps per second: 28, episode reward: 0.482, mean reward: 0.004 [-0.001, 0.010], mean action: 0.242 [-1.131, 1.141], mean observation: 0.148 [-25.158, 17.988], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.332002\n",
      " 1489/2000: episode: 4, duration: 6.736s, episode steps: 173, steps per second: 26, episode reward: 0.481, mean reward: 0.003 [-0.001, 0.011], mean action: 0.314 [-1.166, 1.320], mean observation: 0.140 [-22.698, 16.818], loss: 0.000032, mean_squared_error: 0.000065, mean_q: 0.329609\n",
      "done, took 62.412 seconds\n",
      "\n",
      "\n",
      "iteration: 368\n",
      "Training for 2000 steps ...\n",
      "  235/2000: episode: 1, duration: 7.167s, episode steps: 235, steps per second: 33, episode reward: 0.471, mean reward: 0.002 [-0.001, 0.010], mean action: 0.276 [-1.136, 1.184], mean observation: 0.133 [-25.227, 17.880], loss: --, mean_squared_error: --, mean_q: --\n",
      "  480/2000: episode: 2, duration: 7.341s, episode steps: 245, steps per second: 33, episode reward: 0.468, mean reward: 0.002 [-0.001, 0.010], mean action: 0.284 [-1.265, 1.187], mean observation: 0.133 [-24.116, 17.500], loss: --, mean_squared_error: --, mean_q: --\n",
      "  626/2000: episode: 3, duration: 4.053s, episode steps: 146, steps per second: 36, episode reward: 0.493, mean reward: 0.003 [-0.001, 0.010], mean action: 0.213 [-1.247, 1.189], mean observation: 0.142 [-24.350, 17.710], loss: --, mean_squared_error: --, mean_q: --\n",
      "  878/2000: episode: 4, duration: 7.450s, episode steps: 252, steps per second: 34, episode reward: 0.475, mean reward: 0.002 [-0.001, 0.010], mean action: 0.287 [-1.285, 1.276], mean observation: 0.133 [-24.093, 17.533], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1033/2000: episode: 5, duration: 4.848s, episode steps: 155, steps per second: 32, episode reward: 0.491, mean reward: 0.003 [-0.001, 0.010], mean action: 0.260 [-1.139, 1.174], mean observation: 0.142 [-23.098, 17.182], loss: 0.000028, mean_squared_error: 0.000057, mean_q: 0.330081\n",
      " 1401/2000: episode: 6, duration: 13.973s, episode steps: 368, steps per second: 26, episode reward: 0.473, mean reward: 0.001 [-0.001, 0.011], mean action: 0.356 [-1.333, 1.290], mean observation: 0.127 [-24.855, 17.817], loss: 0.000067, mean_squared_error: 0.000133, mean_q: 0.328013\n",
      "done, took 64.432 seconds\n",
      "\n",
      "\n",
      "iteration: 369\n",
      "Training for 2000 steps ...\n",
      "  861/2000: episode: 1, duration: 26.541s, episode steps: 861, steps per second: 32, episode reward: 0.476, mean reward: 0.001 [-0.001, 0.010], mean action: 0.292 [-1.391, 1.485], mean observation: 0.126 [-25.678, 18.023], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1221/2000: episode: 2, duration: 11.520s, episode steps: 360, steps per second: 31, episode reward: 0.475, mean reward: 0.001 [-0.001, 0.011], mean action: 0.364 [-1.150, 1.376], mean observation: 0.128 [-24.569, 17.670], loss: 0.000080, mean_squared_error: 0.000161, mean_q: 0.330869\n",
      " 1537/2000: episode: 3, duration: 12.102s, episode steps: 316, steps per second: 26, episode reward: 0.463, mean reward: 0.001 [-0.001, 0.010], mean action: 0.273 [-1.289, 1.318], mean observation: 0.129 [-23.652, 17.487], loss: 0.000029, mean_squared_error: 0.000059, mean_q: 0.327750\n",
      " 1645/2000: episode: 4, duration: 3.906s, episode steps: 108, steps per second: 28, episode reward: 0.491, mean reward: 0.005 [-0.001, 0.011], mean action: 0.252 [-1.124, 1.190], mean observation: 0.147 [-24.695, 18.025], loss: 0.000038, mean_squared_error: 0.000076, mean_q: 0.332586\n",
      " 1750/2000: episode: 5, duration: 3.820s, episode steps: 105, steps per second: 27, episode reward: 0.486, mean reward: 0.005 [-0.001, 0.010], mean action: 0.269 [-1.111, 1.204], mean observation: 0.149 [-25.614, 18.462], loss: 0.000106, mean_squared_error: 0.000211, mean_q: 0.330390\n",
      " 1924/2000: episode: 6, duration: 7.119s, episode steps: 174, steps per second: 24, episode reward: 0.468, mean reward: 0.003 [-0.001, 0.010], mean action: 0.277 [-1.248, 1.222], mean observation: 0.138 [-25.094, 18.217], loss: 0.000039, mean_squared_error: 0.000078, mean_q: 0.327840\n",
      "done, took 68.988 seconds\n",
      "\n",
      "\n",
      "iteration: 370\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 23.863s, episode steps: 1000, steps per second: 42, episode reward: 0.054, mean reward: 0.000 [-0.001, 0.007], mean action: 0.310 [-1.391, 1.497], mean observation: 0.117 [-24.954, 18.085], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1569/2000: episode: 2, duration: 20.407s, episode steps: 569, steps per second: 28, episode reward: 0.461, mean reward: 0.001 [-0.002, 0.010], mean action: 0.296 [-1.278, 1.239], mean observation: 0.124 [-25.486, 18.322], loss: 0.000045, mean_squared_error: 0.000090, mean_q: 0.328899\n",
      " 1682/2000: episode: 3, duration: 4.131s, episode steps: 113, steps per second: 27, episode reward: 0.488, mean reward: 0.004 [-0.000, 0.010], mean action: 0.260 [-1.165, 1.153], mean observation: 0.149 [-13.393, 16.946], loss: 0.000183, mean_squared_error: 0.000365, mean_q: 0.330333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1833/2000: episode: 4, duration: 5.971s, episode steps: 151, steps per second: 25, episode reward: 0.455, mean reward: 0.003 [-0.001, 0.010], mean action: 0.267 [-1.153, 1.131], mean observation: 0.137 [-13.427, 16.192], loss: 0.000069, mean_squared_error: 0.000139, mean_q: 0.331197\n",
      "done, took 62.381 seconds\n",
      "\n",
      "\n",
      "iteration: 371\n",
      "Training for 2000 steps ...\n",
      "  167/2000: episode: 1, duration: 4.893s, episode steps: 167, steps per second: 34, episode reward: 0.466, mean reward: 0.003 [-0.001, 0.010], mean action: 0.211 [-1.117, 1.232], mean observation: 0.137 [-12.298, 16.037], loss: --, mean_squared_error: --, mean_q: --\n",
      "  375/2000: episode: 2, duration: 6.922s, episode steps: 208, steps per second: 30, episode reward: 0.456, mean reward: 0.002 [-0.001, 0.010], mean action: 0.222 [-1.211, 1.200], mean observation: 0.133 [-12.596, 16.182], loss: --, mean_squared_error: --, mean_q: --\n",
      "  700/2000: episode: 3, duration: 11.370s, episode steps: 325, steps per second: 29, episode reward: 0.479, mean reward: 0.001 [-0.001, 0.010], mean action: 0.202 [-1.309, 1.349], mean observation: 0.133 [-12.217, 15.937], loss: --, mean_squared_error: --, mean_q: --\n",
      "  915/2000: episode: 4, duration: 6.855s, episode steps: 215, steps per second: 31, episode reward: 0.470, mean reward: 0.002 [-0.001, 0.010], mean action: 0.214 [-1.135, 1.257], mean observation: 0.135 [-13.438, 16.785], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1143/2000: episode: 5, duration: 8.772s, episode steps: 228, steps per second: 26, episode reward: 0.459, mean reward: 0.002 [-0.001, 0.010], mean action: 0.221 [-1.257, 1.311], mean observation: 0.133 [-12.415, 16.091], loss: 0.000072, mean_squared_error: 0.000145, mean_q: 0.330483\n",
      "done, took 69.993 seconds\n",
      "\n",
      "\n",
      "iteration: 372\n",
      "Training for 2000 steps ...\n",
      " 1000/2000: episode: 1, duration: 21.556s, episode steps: 1000, steps per second: 46, episode reward: 0.108, mean reward: 0.000 [-0.001, 0.010], mean action: 0.115 [-1.239, 1.345], mean observation: 0.116 [-11.594, 15.703], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1421/2000: episode: 2, duration: 14.985s, episode steps: 421, steps per second: 28, episode reward: 0.493, mean reward: 0.001 [-0.001, 0.010], mean action: 0.205 [-1.284, 1.398], mean observation: 0.127 [-10.664, 15.207], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.329159\n",
      " 1591/2000: episode: 3, duration: 6.647s, episode steps: 170, steps per second: 26, episode reward: 0.485, mean reward: 0.003 [-0.001, 0.013], mean action: 0.213 [-1.181, 1.225], mean observation: 0.137 [-12.766, 16.270], loss: 0.000112, mean_squared_error: 0.000223, mean_q: 0.328932\n",
      "done, took 59.763 seconds\n",
      "\n",
      "\n",
      "iteration: 373\n",
      "Training for 2000 steps ...\n",
      "  352/2000: episode: 1, duration: 9.768s, episode steps: 352, steps per second: 36, episode reward: -0.782, mean reward: -0.002 [-0.020, 0.003], mean action: 0.186 [-1.301, 1.275], mean observation: 0.093 [-15.760, 15.245], loss: --, mean_squared_error: --, mean_q: --\n",
      "  676/2000: episode: 2, duration: 9.794s, episode steps: 324, steps per second: 33, episode reward: -0.738, mean reward: -0.002 [-0.019, 0.003], mean action: 0.197 [-1.202, 1.159], mean observation: 0.094 [-16.159, 15.396], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1215/2000: episode: 3, duration: 18.474s, episode steps: 539, steps per second: 29, episode reward: -0.796, mean reward: -0.001 [-0.020, 0.004], mean action: 0.118 [-1.280, 1.449], mean observation: 0.102 [-17.147, 15.347], loss: 0.000025, mean_squared_error: 0.000049, mean_q: 0.332730\n",
      " 1355/2000: episode: 4, duration: 6.445s, episode steps: 140, steps per second: 22, episode reward: -0.772, mean reward: -0.006 [-0.019, 0.003], mean action: 0.132 [-1.432, 1.123], mean observation: 0.047 [-38.367, 16.367], loss: 0.000016, mean_squared_error: 0.000033, mean_q: 0.329409\n",
      " 1496/2000: episode: 5, duration: 6.380s, episode steps: 141, steps per second: 22, episode reward: -0.764, mean reward: -0.005 [-0.020, 0.003], mean action: 0.145 [-1.180, 1.231], mean observation: 0.058 [-48.041, 16.088], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.330151\n",
      " 1633/2000: episode: 6, duration: 6.312s, episode steps: 137, steps per second: 22, episode reward: -0.781, mean reward: -0.006 [-0.020, 0.007], mean action: 0.175 [-1.153, 1.233], mean observation: 0.061 [-21.378, 16.092], loss: 0.000055, mean_squared_error: 0.000111, mean_q: 0.333558\n",
      " 1765/2000: episode: 7, duration: 6.055s, episode steps: 132, steps per second: 22, episode reward: -0.784, mean reward: -0.006 [-0.020, 0.007], mean action: 0.183 [-1.187, 1.334], mean observation: 0.055 [-20.766, 16.699], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.335343\n",
      " 1897/2000: episode: 8, duration: 5.931s, episode steps: 132, steps per second: 22, episode reward: -0.779, mean reward: -0.006 [-0.020, 0.006], mean action: 0.199 [-1.098, 1.245], mean observation: 0.057 [-21.666, 16.206], loss: 0.000053, mean_squared_error: 0.000105, mean_q: 0.335156\n",
      "done, took 74.300 seconds\n",
      "\n",
      "\n",
      "iteration: 374\n",
      "Training for 2000 steps ...\n",
      "  134/2000: episode: 1, duration: 4.597s, episode steps: 134, steps per second: 29, episode reward: -0.768, mean reward: -0.006 [-0.020, 0.007], mean action: 0.200 [-1.192, 1.292], mean observation: 0.060 [-20.090, 16.760], loss: --, mean_squared_error: --, mean_q: --\n",
      "  269/2000: episode: 2, duration: 4.641s, episode steps: 135, steps per second: 29, episode reward: -0.786, mean reward: -0.006 [-0.020, 0.007], mean action: 0.197 [-1.339, 1.235], mean observation: 0.054 [-20.082, 16.340], loss: --, mean_squared_error: --, mean_q: --\n",
      "  402/2000: episode: 3, duration: 4.585s, episode steps: 133, steps per second: 29, episode reward: -0.773, mean reward: -0.006 [-0.020, 0.007], mean action: 0.201 [-1.111, 1.203], mean observation: 0.058 [-20.075, 16.389], loss: --, mean_squared_error: --, mean_q: --\n",
      "  533/2000: episode: 4, duration: 4.485s, episode steps: 131, steps per second: 29, episode reward: -0.765, mean reward: -0.006 [-0.020, 0.007], mean action: 0.162 [-1.208, 1.088], mean observation: 0.059 [-19.610, 16.287], loss: --, mean_squared_error: --, mean_q: --\n",
      "  664/2000: episode: 5, duration: 4.496s, episode steps: 131, steps per second: 29, episode reward: -0.771, mean reward: -0.006 [-0.020, 0.007], mean action: 0.191 [-1.142, 1.182], mean observation: 0.059 [-21.582, 16.565], loss: --, mean_squared_error: --, mean_q: --\n",
      "  797/2000: episode: 6, duration: 4.454s, episode steps: 133, steps per second: 30, episode reward: -0.766, mean reward: -0.006 [-0.020, 0.007], mean action: 0.189 [-1.243, 1.229], mean observation: 0.060 [-20.700, 16.406], loss: --, mean_squared_error: --, mean_q: --\n",
      "  931/2000: episode: 7, duration: 4.620s, episode steps: 134, steps per second: 29, episode reward: -0.789, mean reward: -0.006 [-0.020, 0.007], mean action: 0.189 [-1.172, 1.206], mean observation: 0.057 [-20.151, 16.595], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1064/2000: episode: 8, duration: 5.230s, episode steps: 133, steps per second: 25, episode reward: -0.778, mean reward: -0.006 [-0.020, 0.007], mean action: 0.207 [-1.179, 1.183], mean observation: 0.055 [-19.868, 16.359], loss: 0.000070, mean_squared_error: 0.000139, mean_q: 0.328041\n",
      " 1195/2000: episode: 9, duration: 5.922s, episode steps: 131, steps per second: 22, episode reward: -0.787, mean reward: -0.006 [-0.020, 0.006], mean action: 0.196 [-1.161, 1.164], mean observation: 0.053 [-24.815, 16.663], loss: 0.000085, mean_squared_error: 0.000170, mean_q: 0.333288\n",
      " 1318/2000: episode: 10, duration: 5.403s, episode steps: 123, steps per second: 23, episode reward: -0.758, mean reward: -0.006 [-0.020, 0.008], mean action: 0.200 [-1.284, 1.264], mean observation: 0.054 [-23.974, 16.418], loss: 0.000146, mean_squared_error: 0.000293, mean_q: 0.332503\n",
      " 1451/2000: episode: 11, duration: 5.254s, episode steps: 133, steps per second: 25, episode reward: -0.713, mean reward: -0.005 [-0.020, 0.010], mean action: 0.224 [-1.129, 1.195], mean observation: 0.066 [-14.626, 15.137], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.331152\n",
      " 1697/2000: episode: 12, duration: 7.683s, episode steps: 246, steps per second: 32, episode reward: -0.710, mean reward: -0.003 [-0.019, 0.010], mean action: 0.228 [-1.191, 1.164], mean observation: 0.095 [-15.978, 14.963], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.332926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1836/2000: episode: 13, duration: 6.313s, episode steps: 139, steps per second: 22, episode reward: -0.754, mean reward: -0.005 [-0.019, 0.008], mean action: 0.276 [-1.180, 1.344], mean observation: 0.069 [-33.287, 16.466], loss: 0.000135, mean_squared_error: 0.000269, mean_q: 0.331334\n",
      " 1984/2000: episode: 14, duration: 5.912s, episode steps: 148, steps per second: 25, episode reward: -0.762, mean reward: -0.005 [-0.019, 0.009], mean action: 0.269 [-1.142, 1.175], mean observation: 0.078 [-16.696, 15.149], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.330323\n",
      "done, took 74.856 seconds\n",
      "\n",
      "\n",
      "iteration: 375\n",
      "Training for 2000 steps ...\n",
      "  150/2000: episode: 1, duration: 4.447s, episode steps: 150, steps per second: 34, episode reward: -0.750, mean reward: -0.005 [-0.019, 0.009], mean action: 0.262 [-1.215, 1.206], mean observation: 0.077 [-13.854, 15.986], loss: --, mean_squared_error: --, mean_q: --\n",
      "  295/2000: episode: 2, duration: 4.603s, episode steps: 145, steps per second: 32, episode reward: -0.753, mean reward: -0.005 [-0.019, 0.009], mean action: 0.250 [-1.337, 1.168], mean observation: 0.074 [-14.661, 15.338], loss: --, mean_squared_error: --, mean_q: --\n",
      "  440/2000: episode: 3, duration: 4.515s, episode steps: 145, steps per second: 32, episode reward: -0.738, mean reward: -0.005 [-0.019, 0.009], mean action: 0.262 [-1.140, 1.157], mean observation: 0.076 [-14.437, 15.166], loss: --, mean_squared_error: --, mean_q: --\n",
      "  586/2000: episode: 4, duration: 4.679s, episode steps: 146, steps per second: 31, episode reward: -0.742, mean reward: -0.005 [-0.019, 0.009], mean action: 0.274 [-1.184, 1.225], mean observation: 0.077 [-15.332, 15.644], loss: --, mean_squared_error: --, mean_q: --\n",
      "  731/2000: episode: 5, duration: 4.755s, episode steps: 145, steps per second: 30, episode reward: -0.755, mean reward: -0.005 [-0.019, 0.010], mean action: 0.258 [-1.193, 1.262], mean observation: 0.079 [-13.890, 15.298], loss: --, mean_squared_error: --, mean_q: --\n",
      "  890/2000: episode: 6, duration: 4.557s, episode steps: 159, steps per second: 35, episode reward: -0.758, mean reward: -0.005 [-0.019, 0.009], mean action: 0.232 [-1.226, 1.171], mean observation: 0.077 [-15.287, 15.623], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1034/2000: episode: 7, duration: 4.944s, episode steps: 144, steps per second: 29, episode reward: -0.742, mean reward: -0.005 [-0.019, 0.009], mean action: 0.246 [-1.212, 1.117], mean observation: 0.080 [-14.476, 15.985], loss: 0.000031, mean_squared_error: 0.000062, mean_q: 0.324180\n",
      " 1192/2000: episode: 8, duration: 6.001s, episode steps: 158, steps per second: 26, episode reward: -0.750, mean reward: -0.005 [-0.019, 0.009], mean action: 0.254 [-1.188, 1.151], mean observation: 0.079 [-18.665, 15.879], loss: 0.000162, mean_squared_error: 0.000323, mean_q: 0.328438\n",
      " 1376/2000: episode: 9, duration: 6.712s, episode steps: 184, steps per second: 27, episode reward: -0.792, mean reward: -0.004 [-0.018, 0.010], mean action: 0.264 [-1.211, 1.319], mean observation: 0.065 [-9.735, 15.299], loss: 0.000200, mean_squared_error: 0.000399, mean_q: 0.326995\n",
      " 1585/2000: episode: 10, duration: 7.195s, episode steps: 209, steps per second: 29, episode reward: -0.798, mean reward: -0.004 [-0.018, 0.010], mean action: 0.262 [-1.214, 1.242], mean observation: 0.070 [-11.244, 15.908], loss: 0.000043, mean_squared_error: 0.000087, mean_q: 0.328077\n",
      " 1885/2000: episode: 11, duration: 10.281s, episode steps: 300, steps per second: 29, episode reward: -0.776, mean reward: -0.003 [-0.018, 0.010], mean action: 0.210 [-1.355, 1.193], mean observation: 0.086 [-13.589, 13.349], loss: 0.000090, mean_squared_error: 0.000180, mean_q: 0.327033\n",
      "done, took 68.396 seconds\n",
      "\n",
      "\n",
      "iteration: 376\n",
      "Training for 2000 steps ...\n",
      "  301/2000: episode: 1, duration: 9.990s, episode steps: 301, steps per second: 30, episode reward: -0.862, mean reward: -0.003 [-0.018, 0.010], mean action: 0.163 [-1.255, 1.243], mean observation: 0.069 [-47.921, 21.986], loss: --, mean_squared_error: --, mean_q: --\n",
      "  538/2000: episode: 2, duration: 6.717s, episode steps: 237, steps per second: 35, episode reward: -0.841, mean reward: -0.004 [-0.018, 0.011], mean action: 0.179 [-1.178, 1.307], mean observation: 0.064 [-40.087, 14.989], loss: --, mean_squared_error: --, mean_q: --\n",
      "  776/2000: episode: 3, duration: 6.696s, episode steps: 238, steps per second: 36, episode reward: -0.848, mean reward: -0.004 [-0.018, 0.011], mean action: 0.180 [-1.236, 1.273], mean observation: 0.067 [-36.549, 14.799], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1094/2000: episode: 4, duration: 10.092s, episode steps: 318, steps per second: 32, episode reward: -0.812, mean reward: -0.003 [-0.018, 0.011], mean action: 0.212 [-1.310, 1.246], mean observation: 0.083 [-45.250, 15.468], loss: 0.000068, mean_squared_error: 0.000136, mean_q: 0.328204\n",
      " 1206/2000: episode: 5, duration: 5.100s, episode steps: 112, steps per second: 22, episode reward: 0.398, mean reward: 0.004 [-0.002, 0.011], mean action: 0.161 [-1.166, 1.213], mean observation: 0.127 [-9.240, 13.765], loss: 0.000041, mean_squared_error: 0.000082, mean_q: 0.322592\n",
      " 1295/2000: episode: 6, duration: 3.722s, episode steps: 89, steps per second: 24, episode reward: 0.385, mean reward: 0.004 [-0.001, 0.011], mean action: 0.204 [-1.212, 1.187], mean observation: 0.131 [-9.296, 13.812], loss: 0.000095, mean_squared_error: 0.000191, mean_q: 0.318168\n",
      " 1387/2000: episode: 7, duration: 3.769s, episode steps: 92, steps per second: 24, episode reward: 0.430, mean reward: 0.005 [-0.001, 0.010], mean action: 0.162 [-1.127, 1.144], mean observation: 0.137 [-9.290, 14.114], loss: 0.000195, mean_squared_error: 0.000391, mean_q: 0.322594\n",
      " 1477/2000: episode: 8, duration: 3.953s, episode steps: 90, steps per second: 23, episode reward: 0.399, mean reward: 0.004 [-0.001, 0.011], mean action: 0.194 [-1.129, 1.156], mean observation: 0.132 [-18.363, 19.166], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.321618\n",
      " 1567/2000: episode: 9, duration: 3.649s, episode steps: 90, steps per second: 25, episode reward: 0.413, mean reward: 0.005 [-0.000, 0.011], mean action: 0.161 [-1.270, 1.136], mean observation: 0.134 [-17.774, 19.124], loss: 0.000093, mean_squared_error: 0.000185, mean_q: 0.315029\n",
      " 1653/2000: episode: 10, duration: 3.352s, episode steps: 86, steps per second: 26, episode reward: 0.403, mean reward: 0.005 [0.000, 0.011], mean action: 0.128 [-1.088, 1.116], mean observation: 0.134 [-17.482, 19.276], loss: 0.000178, mean_squared_error: 0.000356, mean_q: 0.313795\n",
      " 1740/2000: episode: 11, duration: 3.290s, episode steps: 87, steps per second: 26, episode reward: 0.447, mean reward: 0.005 [0.000, 0.011], mean action: 0.151 [-1.090, 1.161], mean observation: 0.141 [-21.447, 18.965], loss: 0.000112, mean_squared_error: 0.000223, mean_q: 0.315677\n",
      " 1825/2000: episode: 12, duration: 3.198s, episode steps: 85, steps per second: 27, episode reward: 0.449, mean reward: 0.005 [0.001, 0.011], mean action: 0.138 [-1.104, 1.129], mean observation: 0.142 [-19.767, 19.078], loss: 0.000074, mean_squared_error: 0.000149, mean_q: 0.314687\n",
      " 1909/2000: episode: 13, duration: 3.091s, episode steps: 84, steps per second: 27, episode reward: 0.457, mean reward: 0.005 [0.001, 0.011], mean action: 0.181 [-1.108, 1.192], mean observation: 0.145 [-9.163, 19.195], loss: 0.000106, mean_squared_error: 0.000213, mean_q: 0.319866\n",
      " 1990/2000: episode: 14, duration: 2.929s, episode steps: 81, steps per second: 28, episode reward: 0.455, mean reward: 0.006 [0.001, 0.011], mean action: 0.191 [-1.089, 1.148], mean observation: 0.146 [-9.240, 19.135], loss: 0.000058, mean_squared_error: 0.000116, mean_q: 0.314443\n",
      "done, took 70.297 seconds\n",
      "\n",
      "\n",
      "iteration: 377\n",
      "Training for 2000 steps ...\n",
      "   81/2000: episode: 1, duration: 2.194s, episode steps: 81, steps per second: 37, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.188 [-1.067, 1.153], mean observation: 0.147 [-9.190, 19.074], loss: --, mean_squared_error: --, mean_q: --\n",
      "  163/2000: episode: 2, duration: 2.191s, episode steps: 82, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.169 [-1.132, 1.140], mean observation: 0.147 [-9.252, 19.066], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  244/2000: episode: 3, duration: 2.180s, episode steps: 81, steps per second: 37, episode reward: 0.453, mean reward: 0.006 [0.001, 0.011], mean action: 0.195 [-1.073, 1.153], mean observation: 0.147 [-9.194, 19.026], loss: --, mean_squared_error: --, mean_q: --\n",
      "  325/2000: episode: 4, duration: 2.168s, episode steps: 81, steps per second: 37, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.196 [-1.128, 1.124], mean observation: 0.147 [-9.243, 19.030], loss: --, mean_squared_error: --, mean_q: --\n",
      "  406/2000: episode: 5, duration: 2.176s, episode steps: 81, steps per second: 37, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.188 [-1.156, 1.194], mean observation: 0.147 [-9.298, 19.088], loss: --, mean_squared_error: --, mean_q: --\n",
      "  487/2000: episode: 6, duration: 2.221s, episode steps: 81, steps per second: 36, episode reward: 0.454, mean reward: 0.006 [0.001, 0.011], mean action: 0.194 [-1.062, 1.204], mean observation: 0.147 [-9.330, 19.374], loss: --, mean_squared_error: --, mean_q: --\n",
      "  568/2000: episode: 7, duration: 2.160s, episode steps: 81, steps per second: 38, episode reward: 0.454, mean reward: 0.006 [0.001, 0.011], mean action: 0.188 [-1.222, 1.225], mean observation: 0.147 [-9.151, 19.180], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 8, duration: 2.185s, episode steps: 81, steps per second: 37, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.173 [-1.206, 1.178], mean observation: 0.147 [-9.283, 19.154], loss: --, mean_squared_error: --, mean_q: --\n",
      "  731/2000: episode: 9, duration: 2.212s, episode steps: 82, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.167 [-1.108, 1.117], mean observation: 0.147 [-9.267, 19.055], loss: --, mean_squared_error: --, mean_q: --\n",
      "  812/2000: episode: 10, duration: 2.178s, episode steps: 81, steps per second: 37, episode reward: 0.451, mean reward: 0.006 [0.001, 0.011], mean action: 0.171 [-1.140, 1.195], mean observation: 0.147 [-9.301, 19.243], loss: --, mean_squared_error: --, mean_q: --\n",
      "  894/2000: episode: 11, duration: 2.213s, episode steps: 82, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.175 [-1.209, 1.182], mean observation: 0.148 [-9.291, 19.184], loss: --, mean_squared_error: --, mean_q: --\n",
      "  975/2000: episode: 12, duration: 2.206s, episode steps: 81, steps per second: 37, episode reward: 0.453, mean reward: 0.006 [0.001, 0.011], mean action: 0.191 [-1.147, 1.173], mean observation: 0.147 [-9.330, 19.245], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1056/2000: episode: 13, duration: 2.689s, episode steps: 81, steps per second: 30, episode reward: 0.454, mean reward: 0.006 [0.001, 0.011], mean action: 0.169 [-1.151, 1.105], mean observation: 0.147 [-9.260, 18.773], loss: 0.000233, mean_squared_error: 0.000466, mean_q: 0.321805\n",
      " 1137/2000: episode: 14, duration: 2.957s, episode steps: 81, steps per second: 27, episode reward: 0.454, mean reward: 0.006 [0.001, 0.011], mean action: 0.174 [-1.153, 1.108], mean observation: 0.147 [-9.193, 19.135], loss: 0.000099, mean_squared_error: 0.000198, mean_q: 0.312914\n",
      " 1218/2000: episode: 15, duration: 2.968s, episode steps: 81, steps per second: 27, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.194 [-1.046, 1.135], mean observation: 0.147 [-9.381, 19.219], loss: 0.000366, mean_squared_error: 0.000732, mean_q: 0.315485\n",
      " 1300/2000: episode: 16, duration: 2.996s, episode steps: 82, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.193 [-1.082, 1.173], mean observation: 0.147 [-9.288, 19.019], loss: 0.000100, mean_squared_error: 0.000199, mean_q: 0.315743\n",
      " 1381/2000: episode: 17, duration: 2.866s, episode steps: 81, steps per second: 28, episode reward: 0.453, mean reward: 0.006 [0.001, 0.011], mean action: 0.188 [-1.132, 1.165], mean observation: 0.147 [-9.163, 19.114], loss: 0.000129, mean_squared_error: 0.000258, mean_q: 0.318706\n",
      " 1463/2000: episode: 18, duration: 2.949s, episode steps: 82, steps per second: 28, episode reward: 0.455, mean reward: 0.006 [0.001, 0.011], mean action: 0.180 [-1.126, 1.134], mean observation: 0.147 [-9.057, 19.361], loss: 0.000056, mean_squared_error: 0.000112, mean_q: 0.317857\n",
      " 1547/2000: episode: 19, duration: 3.039s, episode steps: 84, steps per second: 28, episode reward: 0.468, mean reward: 0.006 [0.001, 0.011], mean action: 0.193 [-1.155, 1.173], mean observation: 0.149 [-9.381, 19.075], loss: 0.000077, mean_squared_error: 0.000154, mean_q: 0.317771\n",
      " 1630/2000: episode: 20, duration: 2.978s, episode steps: 83, steps per second: 28, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.175 [-1.113, 1.197], mean observation: 0.149 [-9.244, 19.315], loss: 0.000035, mean_squared_error: 0.000069, mean_q: 0.313967\n",
      " 1714/2000: episode: 21, duration: 3.073s, episode steps: 84, steps per second: 27, episode reward: 0.467, mean reward: 0.006 [0.001, 0.011], mean action: 0.180 [-1.157, 1.211], mean observation: 0.148 [-9.304, 19.043], loss: 0.000082, mean_squared_error: 0.000165, mean_q: 0.316627\n",
      " 1797/2000: episode: 22, duration: 3.046s, episode steps: 83, steps per second: 27, episode reward: 0.465, mean reward: 0.006 [0.001, 0.011], mean action: 0.198 [-1.134, 1.258], mean observation: 0.148 [-9.356, 19.080], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.314435\n",
      " 1878/2000: episode: 23, duration: 2.991s, episode steps: 81, steps per second: 27, episode reward: 0.450, mean reward: 0.006 [0.001, 0.011], mean action: 0.186 [-1.246, 1.152], mean observation: 0.148 [-9.310, 20.768], loss: 0.000063, mean_squared_error: 0.000127, mean_q: 0.312927\n",
      " 1960/2000: episode: 24, duration: 3.002s, episode steps: 82, steps per second: 27, episode reward: 0.456, mean reward: 0.006 [0.001, 0.011], mean action: 0.174 [-1.168, 1.143], mean observation: 0.146 [-9.659, 19.413], loss: 0.000069, mean_squared_error: 0.000138, mean_q: 0.315383\n",
      "done, took 63.713 seconds\n",
      "\n",
      "\n",
      "iteration: 378\n",
      "Training for 2000 steps ...\n",
      "   82/2000: episode: 1, duration: 2.274s, episode steps: 82, steps per second: 36, episode reward: 0.456, mean reward: 0.006 [0.001, 0.011], mean action: 0.201 [-1.105, 1.154], mean observation: 0.146 [-9.700, 19.554], loss: --, mean_squared_error: --, mean_q: --\n",
      "  163/2000: episode: 2, duration: 2.169s, episode steps: 81, steps per second: 37, episode reward: 0.449, mean reward: 0.006 [0.001, 0.011], mean action: 0.221 [-1.112, 1.210], mean observation: 0.146 [-9.786, 19.863], loss: --, mean_squared_error: --, mean_q: --\n",
      "  244/2000: episode: 3, duration: 2.154s, episode steps: 81, steps per second: 38, episode reward: 0.449, mean reward: 0.006 [0.001, 0.011], mean action: 0.191 [-1.172, 1.254], mean observation: 0.147 [-9.761, 19.862], loss: --, mean_squared_error: --, mean_q: --\n",
      "  326/2000: episode: 4, duration: 2.224s, episode steps: 82, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.207 [-1.135, 1.146], mean observation: 0.146 [-9.919, 20.308], loss: --, mean_squared_error: --, mean_q: --\n",
      "  406/2000: episode: 5, duration: 2.188s, episode steps: 80, steps per second: 37, episode reward: 0.444, mean reward: 0.006 [0.001, 0.011], mean action: 0.205 [-1.137, 1.129], mean observation: 0.146 [-9.934, 20.263], loss: --, mean_squared_error: --, mean_q: --\n",
      "  487/2000: episode: 6, duration: 2.163s, episode steps: 81, steps per second: 37, episode reward: 0.451, mean reward: 0.006 [0.001, 0.011], mean action: 0.187 [-1.155, 1.137], mean observation: 0.146 [-9.798, 19.903], loss: --, mean_squared_error: --, mean_q: --\n",
      "  569/2000: episode: 7, duration: 2.193s, episode steps: 82, steps per second: 37, episode reward: 0.454, mean reward: 0.006 [0.001, 0.011], mean action: 0.206 [-1.077, 1.143], mean observation: 0.146 [-9.917, 20.259], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 8, duration: 2.142s, episode steps: 80, steps per second: 37, episode reward: 0.443, mean reward: 0.006 [0.001, 0.011], mean action: 0.212 [-1.128, 1.144], mean observation: 0.146 [-9.749, 19.878], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  731/2000: episode: 9, duration: 2.192s, episode steps: 82, steps per second: 37, episode reward: 0.456, mean reward: 0.006 [0.001, 0.011], mean action: 0.211 [-1.119, 1.172], mean observation: 0.147 [-9.858, 20.160], loss: --, mean_squared_error: --, mean_q: --\n",
      "  813/2000: episode: 10, duration: 2.224s, episode steps: 82, steps per second: 37, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.196 [-1.154, 1.118], mean observation: 0.146 [-9.878, 20.480], loss: --, mean_squared_error: --, mean_q: --\n",
      "  894/2000: episode: 11, duration: 2.161s, episode steps: 81, steps per second: 37, episode reward: 0.453, mean reward: 0.006 [0.001, 0.011], mean action: 0.209 [-1.178, 1.186], mean observation: 0.145 [-9.828, 20.034], loss: --, mean_squared_error: --, mean_q: --\n",
      "  975/2000: episode: 12, duration: 2.165s, episode steps: 81, steps per second: 37, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.196 [-1.142, 1.168], mean observation: 0.146 [-9.858, 19.986], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1056/2000: episode: 13, duration: 2.678s, episode steps: 81, steps per second: 30, episode reward: 0.450, mean reward: 0.006 [0.001, 0.011], mean action: 0.155 [-1.168, 1.103], mean observation: 0.146 [-9.823, 19.813], loss: 0.000161, mean_squared_error: 0.000322, mean_q: 0.314429\n",
      " 1139/2000: episode: 14, duration: 2.973s, episode steps: 83, steps per second: 28, episode reward: 0.466, mean reward: 0.006 [0.001, 0.011], mean action: 0.227 [-1.100, 1.228], mean observation: 0.148 [-9.151, 19.490], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.307870\n",
      " 1221/2000: episode: 15, duration: 2.969s, episode steps: 82, steps per second: 28, episode reward: 0.456, mean reward: 0.006 [0.001, 0.011], mean action: 0.191 [-1.187, 1.179], mean observation: 0.149 [-9.231, 18.938], loss: 0.000048, mean_squared_error: 0.000096, mean_q: 0.314939\n",
      " 1304/2000: episode: 16, duration: 3.003s, episode steps: 83, steps per second: 28, episode reward: 0.468, mean reward: 0.006 [0.001, 0.011], mean action: 0.184 [-1.197, 1.109], mean observation: 0.148 [-9.215, 19.220], loss: 0.000023, mean_squared_error: 0.000045, mean_q: 0.315233\n",
      " 1387/2000: episode: 17, duration: 3.015s, episode steps: 83, steps per second: 28, episode reward: 0.464, mean reward: 0.006 [0.001, 0.011], mean action: 0.204 [-1.167, 1.185], mean observation: 0.149 [-9.304, 19.004], loss: 0.000079, mean_squared_error: 0.000158, mean_q: 0.310766\n",
      " 1469/2000: episode: 18, duration: 2.977s, episode steps: 82, steps per second: 28, episode reward: 0.461, mean reward: 0.006 [0.001, 0.011], mean action: 0.184 [-1.179, 1.085], mean observation: 0.149 [-9.559, 17.842], loss: 0.000087, mean_squared_error: 0.000173, mean_q: 0.315521\n",
      " 1542/2000: episode: 19, duration: 2.655s, episode steps: 73, steps per second: 27, episode reward: 0.424, mean reward: 0.006 [0.001, 0.013], mean action: 0.208 [-1.147, 1.095], mean observation: 0.142 [-10.097, 17.515], loss: 0.000060, mean_squared_error: 0.000120, mean_q: 0.312569\n",
      " 1615/2000: episode: 20, duration: 2.676s, episode steps: 73, steps per second: 27, episode reward: 0.421, mean reward: 0.006 [0.001, 0.013], mean action: 0.188 [-1.147, 1.117], mean observation: 0.142 [-10.000, 17.515], loss: 0.000055, mean_squared_error: 0.000110, mean_q: 0.314965\n",
      " 1688/2000: episode: 21, duration: 2.703s, episode steps: 73, steps per second: 27, episode reward: 0.422, mean reward: 0.006 [0.001, 0.013], mean action: 0.174 [-1.160, 1.158], mean observation: 0.142 [-9.958, 17.725], loss: 0.000045, mean_squared_error: 0.000090, mean_q: 0.308875\n",
      " 1761/2000: episode: 22, duration: 2.665s, episode steps: 73, steps per second: 27, episode reward: 0.419, mean reward: 0.006 [0.001, 0.013], mean action: 0.178 [-1.167, 1.183], mean observation: 0.142 [-10.176, 17.473], loss: 0.000037, mean_squared_error: 0.000074, mean_q: 0.313938\n",
      " 1830/2000: episode: 23, duration: 2.559s, episode steps: 69, steps per second: 27, episode reward: 0.394, mean reward: 0.006 [0.000, 0.013], mean action: 0.176 [-1.173, 1.165], mean observation: 0.136 [-10.190, 19.883], loss: 0.000073, mean_squared_error: 0.000147, mean_q: 0.310018\n",
      " 1913/2000: episode: 24, duration: 3.096s, episode steps: 83, steps per second: 27, episode reward: 0.461, mean reward: 0.006 [0.000, 0.012], mean action: 0.218 [-1.194, 1.194], mean observation: 0.148 [-10.147, 19.905], loss: 0.000023, mean_squared_error: 0.000045, mean_q: 0.311756\n",
      " 1996/2000: episode: 25, duration: 3.094s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.197 [-1.061, 1.112], mean observation: 0.147 [-10.395, 20.343], loss: 0.000057, mean_squared_error: 0.000115, mean_q: 0.314112\n",
      "done, took 63.634 seconds\n",
      "\n",
      "\n",
      "iteration: 379\n",
      "Training for 2000 steps ...\n",
      "   83/2000: episode: 1, duration: 2.254s, episode steps: 83, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.189 [-1.171, 1.219], mean observation: 0.146 [-10.325, 20.409], loss: --, mean_squared_error: --, mean_q: --\n",
      "  166/2000: episode: 2, duration: 2.284s, episode steps: 83, steps per second: 36, episode reward: 0.461, mean reward: 0.006 [0.001, 0.011], mean action: 0.200 [-1.157, 1.141], mean observation: 0.146 [-10.304, 20.438], loss: --, mean_squared_error: --, mean_q: --\n",
      "  250/2000: episode: 3, duration: 2.301s, episode steps: 84, steps per second: 37, episode reward: 0.464, mean reward: 0.006 [0.001, 0.011], mean action: 0.191 [-1.130, 1.204], mean observation: 0.146 [-10.453, 20.077], loss: --, mean_squared_error: --, mean_q: --\n",
      "  333/2000: episode: 4, duration: 2.271s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.191 [-1.179, 1.156], mean observation: 0.146 [-10.309, 20.418], loss: --, mean_squared_error: --, mean_q: --\n",
      "  417/2000: episode: 5, duration: 2.327s, episode steps: 84, steps per second: 36, episode reward: 0.464, mean reward: 0.006 [0.001, 0.011], mean action: 0.190 [-1.087, 1.187], mean observation: 0.147 [-10.385, 20.351], loss: --, mean_squared_error: --, mean_q: --\n",
      "  500/2000: episode: 6, duration: 2.253s, episode steps: 83, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.198 [-1.143, 1.146], mean observation: 0.146 [-10.336, 20.385], loss: --, mean_squared_error: --, mean_q: --\n",
      "  584/2000: episode: 7, duration: 2.283s, episode steps: 84, steps per second: 37, episode reward: 0.464, mean reward: 0.006 [0.001, 0.011], mean action: 0.175 [-1.157, 1.120], mean observation: 0.147 [-10.407, 20.168], loss: --, mean_squared_error: --, mean_q: --\n",
      "  667/2000: episode: 8, duration: 2.228s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.203 [-1.147, 1.173], mean observation: 0.146 [-10.305, 20.058], loss: --, mean_squared_error: --, mean_q: --\n",
      "  750/2000: episode: 9, duration: 2.308s, episode steps: 83, steps per second: 36, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.189 [-1.080, 1.135], mean observation: 0.146 [-10.299, 20.456], loss: --, mean_squared_error: --, mean_q: --\n",
      "  834/2000: episode: 10, duration: 2.329s, episode steps: 84, steps per second: 36, episode reward: 0.467, mean reward: 0.006 [0.001, 0.011], mean action: 0.181 [-1.123, 1.160], mean observation: 0.146 [-10.371, 20.479], loss: --, mean_squared_error: --, mean_q: --\n",
      "  917/2000: episode: 11, duration: 2.288s, episode steps: 83, steps per second: 36, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.184 [-1.140, 1.115], mean observation: 0.146 [-10.402, 20.345], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1000/2000: episode: 12, duration: 2.246s, episode steps: 83, steps per second: 37, episode reward: 0.455, mean reward: 0.005 [0.001, 0.011], mean action: 0.190 [-1.085, 1.137], mean observation: 0.147 [-10.342, 20.277], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1084/2000: episode: 13, duration: 3.139s, episode steps: 84, steps per second: 27, episode reward: 0.463, mean reward: 0.006 [0.001, 0.011], mean action: 0.211 [-1.083, 1.142], mean observation: 0.146 [-10.397, 20.337], loss: 0.000048, mean_squared_error: 0.000097, mean_q: 0.306010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1167/2000: episode: 14, duration: 3.094s, episode steps: 83, steps per second: 27, episode reward: 0.456, mean reward: 0.005 [-0.000, 0.011], mean action: 0.194 [-1.180, 1.199], mean observation: 0.145 [-10.327, 20.148], loss: 0.000023, mean_squared_error: 0.000046, mean_q: 0.309873\n",
      " 1250/2000: episode: 15, duration: 3.039s, episode steps: 83, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.000, 0.011], mean action: 0.160 [-1.189, 1.114], mean observation: 0.146 [-10.335, 20.101], loss: 0.000126, mean_squared_error: 0.000252, mean_q: 0.315866\n",
      " 1334/2000: episode: 16, duration: 3.094s, episode steps: 84, steps per second: 27, episode reward: 0.463, mean reward: 0.006 [0.000, 0.011], mean action: 0.196 [-1.156, 1.195], mean observation: 0.146 [-10.425, 20.072], loss: 0.000142, mean_squared_error: 0.000284, mean_q: 0.317596\n",
      " 1417/2000: episode: 17, duration: 3.056s, episode steps: 83, steps per second: 27, episode reward: 0.456, mean reward: 0.005 [-0.000, 0.011], mean action: 0.185 [-1.133, 1.165], mean observation: 0.146 [-10.270, 20.016], loss: 0.000101, mean_squared_error: 0.000203, mean_q: 0.310337\n",
      " 1500/2000: episode: 18, duration: 3.065s, episode steps: 83, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.000, 0.011], mean action: 0.181 [-1.107, 1.199], mean observation: 0.146 [-10.392, 20.248], loss: 0.000024, mean_squared_error: 0.000048, mean_q: 0.310310\n",
      " 1583/2000: episode: 19, duration: 3.069s, episode steps: 83, steps per second: 27, episode reward: 0.452, mean reward: 0.005 [-0.000, 0.011], mean action: 0.181 [-1.184, 1.132], mean observation: 0.146 [-10.351, 20.145], loss: 0.000101, mean_squared_error: 0.000202, mean_q: 0.309495\n",
      " 1666/2000: episode: 20, duration: 3.063s, episode steps: 83, steps per second: 27, episode reward: 0.454, mean reward: 0.005 [-0.000, 0.011], mean action: 0.181 [-1.164, 1.199], mean observation: 0.146 [-10.384, 19.912], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.317985\n",
      " 1749/2000: episode: 21, duration: 3.077s, episode steps: 83, steps per second: 27, episode reward: 0.455, mean reward: 0.005 [-0.000, 0.011], mean action: 0.183 [-1.186, 1.099], mean observation: 0.146 [-10.311, 20.188], loss: 0.000070, mean_squared_error: 0.000140, mean_q: 0.313898\n",
      " 1833/2000: episode: 22, duration: 3.091s, episode steps: 84, steps per second: 27, episode reward: 0.462, mean reward: 0.005 [-0.001, 0.011], mean action: 0.198 [-1.143, 1.189], mean observation: 0.146 [-10.366, 19.913], loss: 0.000043, mean_squared_error: 0.000086, mean_q: 0.314860\n",
      " 1916/2000: episode: 23, duration: 3.146s, episode steps: 83, steps per second: 26, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.011], mean action: 0.196 [-1.204, 1.210], mean observation: 0.146 [-10.369, 20.037], loss: 0.000051, mean_squared_error: 0.000102, mean_q: 0.311946\n",
      " 1999/2000: episode: 24, duration: 3.071s, episode steps: 83, steps per second: 27, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.011], mean action: 0.177 [-1.167, 1.179], mean observation: 0.145 [-10.359, 19.901], loss: 0.000157, mean_squared_error: 0.000313, mean_q: 0.311184\n",
      "done, took 64.503 seconds\n",
      "\n",
      "\n",
      "iteration: 380\n",
      "Training for 2000 steps ...\n",
      "   84/2000: episode: 1, duration: 2.306s, episode steps: 84, steps per second: 36, episode reward: 0.462, mean reward: 0.006 [-0.001, 0.011], mean action: 0.179 [-1.145, 1.134], mean observation: 0.145 [-10.289, 20.372], loss: --, mean_squared_error: --, mean_q: --\n",
      "  167/2000: episode: 2, duration: 2.298s, episode steps: 83, steps per second: 36, episode reward: 0.457, mean reward: 0.006 [-0.001, 0.011], mean action: 0.177 [-1.179, 1.165], mean observation: 0.145 [-10.349, 20.058], loss: --, mean_squared_error: --, mean_q: --\n",
      "  250/2000: episode: 3, duration: 2.254s, episode steps: 83, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [-0.000, 0.011], mean action: 0.188 [-1.161, 1.166], mean observation: 0.145 [-10.314, 20.123], loss: --, mean_squared_error: --, mean_q: --\n",
      "  333/2000: episode: 4, duration: 2.271s, episode steps: 83, steps per second: 37, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.011], mean action: 0.178 [-1.140, 1.127], mean observation: 0.146 [-10.363, 20.113], loss: --, mean_squared_error: --, mean_q: --\n",
      "  416/2000: episode: 5, duration: 2.265s, episode steps: 83, steps per second: 37, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.011], mean action: 0.180 [-1.124, 1.167], mean observation: 0.146 [-10.390, 20.091], loss: --, mean_squared_error: --, mean_q: --\n",
      "  500/2000: episode: 6, duration: 2.274s, episode steps: 84, steps per second: 37, episode reward: 0.464, mean reward: 0.006 [-0.000, 0.011], mean action: 0.186 [-1.204, 1.161], mean observation: 0.146 [-10.427, 19.856], loss: --, mean_squared_error: --, mean_q: --\n",
      "  583/2000: episode: 7, duration: 2.278s, episode steps: 83, steps per second: 36, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.011], mean action: 0.182 [-1.176, 1.192], mean observation: 0.146 [-10.362, 19.937], loss: --, mean_squared_error: --, mean_q: --\n",
      "  666/2000: episode: 8, duration: 2.253s, episode steps: 83, steps per second: 37, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.011], mean action: 0.174 [-1.119, 1.121], mean observation: 0.145 [-10.331, 20.114], loss: --, mean_squared_error: --, mean_q: --\n",
      "  749/2000: episode: 9, duration: 2.272s, episode steps: 83, steps per second: 37, episode reward: 0.455, mean reward: 0.005 [-0.001, 0.011], mean action: 0.187 [-1.162, 1.139], mean observation: 0.145 [-10.326, 20.070], loss: --, mean_squared_error: --, mean_q: --\n",
      "  832/2000: episode: 10, duration: 2.217s, episode steps: 83, steps per second: 37, episode reward: 0.454, mean reward: 0.005 [-0.001, 0.011], mean action: 0.194 [-1.072, 1.171], mean observation: 0.146 [-10.370, 19.950], loss: --, mean_squared_error: --, mean_q: --\n",
      "  915/2000: episode: 11, duration: 2.255s, episode steps: 83, steps per second: 37, episode reward: 0.456, mean reward: 0.005 [-0.000, 0.011], mean action: 0.188 [-1.088, 1.157], mean observation: 0.146 [-10.320, 20.164], loss: --, mean_squared_error: --, mean_q: --\n",
      "  999/2000: episode: 12, duration: 2.287s, episode steps: 84, steps per second: 37, episode reward: 0.464, mean reward: 0.006 [-0.001, 0.011], mean action: 0.209 [-1.174, 1.265], mean observation: 0.146 [-10.382, 20.097], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1082/2000: episode: 13, duration: 3.117s, episode steps: 83, steps per second: 27, episode reward: 0.452, mean reward: 0.005 [-0.001, 0.011], mean action: 0.183 [-1.122, 1.136], mean observation: 0.146 [-10.409, 20.245], loss: 0.000097, mean_squared_error: 0.000194, mean_q: 0.308981\n",
      " 1165/2000: episode: 14, duration: 3.072s, episode steps: 83, steps per second: 27, episode reward: 0.454, mean reward: 0.005 [-0.000, 0.011], mean action: 0.194 [-1.174, 1.196], mean observation: 0.145 [-10.348, 20.040], loss: 0.000096, mean_squared_error: 0.000193, mean_q: 0.312150\n",
      " 1248/2000: episode: 15, duration: 3.125s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [-0.001, 0.011], mean action: 0.208 [-1.168, 1.155], mean observation: 0.146 [-10.250, 20.112], loss: 0.000134, mean_squared_error: 0.000269, mean_q: 0.314704\n",
      " 1331/2000: episode: 16, duration: 3.033s, episode steps: 83, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.011], mean action: 0.206 [-1.143, 1.204], mean observation: 0.145 [-10.392, 20.006], loss: 0.000023, mean_squared_error: 0.000046, mean_q: 0.313608\n",
      " 1414/2000: episode: 17, duration: 3.103s, episode steps: 83, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.011], mean action: 0.191 [-1.089, 1.202], mean observation: 0.146 [-10.356, 20.038], loss: 0.000046, mean_squared_error: 0.000093, mean_q: 0.314829\n",
      " 1497/2000: episode: 18, duration: 3.016s, episode steps: 83, steps per second: 28, episode reward: 0.451, mean reward: 0.005 [-0.001, 0.011], mean action: 0.195 [-1.105, 1.167], mean observation: 0.146 [-10.336, 20.028], loss: 0.000138, mean_squared_error: 0.000276, mean_q: 0.312113\n",
      " 1580/2000: episode: 19, duration: 3.095s, episode steps: 83, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.001, 0.011], mean action: 0.176 [-1.210, 1.129], mean observation: 0.146 [-10.390, 20.027], loss: 0.000082, mean_squared_error: 0.000164, mean_q: 0.309872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1663/2000: episode: 20, duration: 3.019s, episode steps: 83, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [-0.000, 0.011], mean action: 0.187 [-1.141, 1.177], mean observation: 0.145 [-10.321, 19.935], loss: 0.000078, mean_squared_error: 0.000156, mean_q: 0.306052\n",
      " 1746/2000: episode: 21, duration: 3.041s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.208 [-1.089, 1.183], mean observation: 0.146 [-10.318, 20.073], loss: 0.000126, mean_squared_error: 0.000251, mean_q: 0.310411\n",
      " 1828/2000: episode: 22, duration: 2.975s, episode steps: 82, steps per second: 28, episode reward: 0.451, mean reward: 0.005 [0.001, 0.011], mean action: 0.192 [-1.134, 1.107], mean observation: 0.146 [-10.322, 20.135], loss: 0.000093, mean_squared_error: 0.000187, mean_q: 0.308300\n",
      " 1909/2000: episode: 23, duration: 3.102s, episode steps: 81, steps per second: 26, episode reward: 0.449, mean reward: 0.006 [0.001, 0.012], mean action: 0.195 [-1.212, 1.231], mean observation: 0.146 [-10.360, 20.336], loss: 0.000031, mean_squared_error: 0.000061, mean_q: 0.304897\n",
      " 1992/2000: episode: 24, duration: 3.059s, episode steps: 83, steps per second: 27, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.198 [-1.145, 1.156], mean observation: 0.146 [-10.406, 20.091], loss: 0.000016, mean_squared_error: 0.000033, mean_q: 0.305186\n",
      "done, took 64.562 seconds\n",
      "\n",
      "\n",
      "iteration: 381\n",
      "Training for 2000 steps ...\n",
      "   83/2000: episode: 1, duration: 2.310s, episode steps: 83, steps per second: 36, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.192 [-1.115, 1.144], mean observation: 0.147 [-10.341, 19.819], loss: --, mean_squared_error: --, mean_q: --\n",
      "  165/2000: episode: 2, duration: 2.236s, episode steps: 82, steps per second: 37, episode reward: 0.451, mean reward: 0.006 [0.001, 0.011], mean action: 0.173 [-1.109, 1.103], mean observation: 0.146 [-10.378, 20.158], loss: --, mean_squared_error: --, mean_q: --\n",
      "  248/2000: episode: 3, duration: 2.338s, episode steps: 83, steps per second: 35, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.181 [-1.215, 1.127], mean observation: 0.146 [-10.353, 19.936], loss: --, mean_squared_error: --, mean_q: --\n",
      "  331/2000: episode: 4, duration: 2.268s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.182 [-1.100, 1.115], mean observation: 0.147 [-10.365, 20.144], loss: --, mean_squared_error: --, mean_q: --\n",
      "  414/2000: episode: 5, duration: 2.304s, episode steps: 83, steps per second: 36, episode reward: 0.461, mean reward: 0.006 [0.001, 0.011], mean action: 0.197 [-1.091, 1.224], mean observation: 0.146 [-10.399, 20.004], loss: --, mean_squared_error: --, mean_q: --\n",
      "  496/2000: episode: 6, duration: 2.277s, episode steps: 82, steps per second: 36, episode reward: 0.452, mean reward: 0.006 [0.001, 0.011], mean action: 0.186 [-1.088, 1.098], mean observation: 0.146 [-10.293, 20.071], loss: --, mean_squared_error: --, mean_q: --\n",
      "  579/2000: episode: 7, duration: 2.292s, episode steps: 83, steps per second: 36, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.190 [-1.118, 1.175], mean observation: 0.147 [-10.373, 20.020], loss: --, mean_squared_error: --, mean_q: --\n",
      "  662/2000: episode: 8, duration: 2.259s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.173 [-1.111, 1.107], mean observation: 0.146 [-10.377, 19.932], loss: --, mean_squared_error: --, mean_q: --\n",
      "  745/2000: episode: 9, duration: 2.212s, episode steps: 83, steps per second: 38, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.170 [-1.276, 1.177], mean observation: 0.146 [-10.385, 20.187], loss: --, mean_squared_error: --, mean_q: --\n",
      "  828/2000: episode: 10, duration: 2.233s, episode steps: 83, steps per second: 37, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.205 [-1.105, 1.153], mean observation: 0.146 [-10.305, 20.140], loss: --, mean_squared_error: --, mean_q: --\n",
      "  911/2000: episode: 11, duration: 2.196s, episode steps: 83, steps per second: 38, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.182 [-1.176, 1.118], mean observation: 0.146 [-10.455, 20.033], loss: --, mean_squared_error: --, mean_q: --\n",
      "  994/2000: episode: 12, duration: 2.256s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.221 [-1.083, 1.185], mean observation: 0.146 [-10.396, 20.097], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1077/2000: episode: 13, duration: 3.053s, episode steps: 83, steps per second: 27, episode reward: 0.462, mean reward: 0.006 [0.001, 0.011], mean action: 0.182 [-1.131, 1.180], mean observation: 0.146 [-10.285, 19.938], loss: 0.000121, mean_squared_error: 0.000242, mean_q: 0.308113\n",
      " 1160/2000: episode: 14, duration: 2.993s, episode steps: 83, steps per second: 28, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.198 [-1.184, 1.168], mean observation: 0.146 [-10.460, 20.010], loss: 0.000037, mean_squared_error: 0.000074, mean_q: 0.303702\n",
      " 1243/2000: episode: 15, duration: 3.049s, episode steps: 83, steps per second: 27, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.207 [-1.056, 1.119], mean observation: 0.146 [-10.320, 20.036], loss: 0.000090, mean_squared_error: 0.000180, mean_q: 0.307037\n",
      " 1326/2000: episode: 16, duration: 3.083s, episode steps: 83, steps per second: 27, episode reward: 0.455, mean reward: 0.005 [0.000, 0.011], mean action: 0.198 [-1.153, 1.208], mean observation: 0.146 [-10.404, 19.941], loss: 0.000118, mean_squared_error: 0.000237, mean_q: 0.304977\n",
      " 1409/2000: episode: 17, duration: 3.045s, episode steps: 83, steps per second: 27, episode reward: 0.455, mean reward: 0.005 [0.001, 0.011], mean action: 0.204 [-1.194, 1.189], mean observation: 0.146 [-10.351, 19.973], loss: 0.000049, mean_squared_error: 0.000098, mean_q: 0.303282\n",
      " 1492/2000: episode: 18, duration: 3.034s, episode steps: 83, steps per second: 27, episode reward: 0.455, mean reward: 0.005 [0.000, 0.011], mean action: 0.204 [-1.201, 1.183], mean observation: 0.146 [-10.423, 20.275], loss: 0.000021, mean_squared_error: 0.000043, mean_q: 0.298998\n",
      " 1575/2000: episode: 19, duration: 3.022s, episode steps: 83, steps per second: 27, episode reward: 0.454, mean reward: 0.005 [0.000, 0.011], mean action: 0.180 [-1.178, 1.181], mean observation: 0.146 [-10.371, 19.990], loss: 0.000081, mean_squared_error: 0.000161, mean_q: 0.301864\n",
      " 1658/2000: episode: 20, duration: 3.110s, episode steps: 83, steps per second: 27, episode reward: 0.455, mean reward: 0.005 [0.000, 0.011], mean action: 0.194 [-1.097, 1.185], mean observation: 0.145 [-10.376, 20.011], loss: 0.000028, mean_squared_error: 0.000056, mean_q: 0.304334\n",
      " 1741/2000: episode: 21, duration: 3.086s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.168 [-1.201, 1.146], mean observation: 0.146 [-10.287, 20.151], loss: 0.000107, mean_squared_error: 0.000214, mean_q: 0.303239\n",
      " 1824/2000: episode: 22, duration: 3.059s, episode steps: 83, steps per second: 27, episode reward: 0.455, mean reward: 0.005 [0.001, 0.011], mean action: 0.200 [-1.111, 1.216], mean observation: 0.146 [-10.437, 19.968], loss: 0.000065, mean_squared_error: 0.000130, mean_q: 0.302462\n",
      " 1906/2000: episode: 23, duration: 3.008s, episode steps: 82, steps per second: 27, episode reward: 0.450, mean reward: 0.005 [0.001, 0.011], mean action: 0.192 [-1.067, 1.125], mean observation: 0.146 [-10.311, 20.219], loss: 0.000087, mean_squared_error: 0.000174, mean_q: 0.302429\n",
      " 1988/2000: episode: 24, duration: 2.984s, episode steps: 82, steps per second: 27, episode reward: 0.451, mean reward: 0.005 [0.001, 0.011], mean action: 0.179 [-1.147, 1.228], mean observation: 0.146 [-10.232, 20.138], loss: 0.000039, mean_squared_error: 0.000078, mean_q: 0.302370\n",
      "done, took 64.565 seconds\n",
      "\n",
      "\n",
      "iteration: 382\n",
      "Training for 2000 steps ...\n",
      "   83/2000: episode: 1, duration: 2.265s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.184 [-1.141, 1.201], mean observation: 0.146 [-10.317, 20.116], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  166/2000: episode: 2, duration: 2.254s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.000, 0.011], mean action: 0.217 [-1.116, 1.303], mean observation: 0.145 [-10.313, 19.974], loss: --, mean_squared_error: --, mean_q: --\n",
      "  249/2000: episode: 3, duration: 2.241s, episode steps: 83, steps per second: 37, episode reward: 0.453, mean reward: 0.005 [0.000, 0.011], mean action: 0.183 [-1.124, 1.122], mean observation: 0.146 [-10.370, 20.171], loss: --, mean_squared_error: --, mean_q: --\n",
      "  332/2000: episode: 4, duration: 2.254s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.182 [-1.198, 1.176], mean observation: 0.146 [-10.349, 20.043], loss: --, mean_squared_error: --, mean_q: --\n",
      "  415/2000: episode: 5, duration: 2.325s, episode steps: 83, steps per second: 36, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.206 [-1.099, 1.198], mean observation: 0.146 [-10.359, 19.877], loss: --, mean_squared_error: --, mean_q: --\n",
      "  498/2000: episode: 6, duration: 2.253s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.187 [-1.300, 1.161], mean observation: 0.146 [-10.305, 20.151], loss: --, mean_squared_error: --, mean_q: --\n",
      "  581/2000: episode: 7, duration: 2.217s, episode steps: 83, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [0.000, 0.011], mean action: 0.194 [-1.105, 1.136], mean observation: 0.146 [-10.371, 19.971], loss: --, mean_squared_error: --, mean_q: --\n",
      "  664/2000: episode: 8, duration: 2.257s, episode steps: 83, steps per second: 37, episode reward: 0.456, mean reward: 0.005 [0.001, 0.011], mean action: 0.182 [-1.117, 1.129], mean observation: 0.146 [-10.333, 20.050], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 9, duration: 2.255s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.000, 0.011], mean action: 0.209 [-1.113, 1.098], mean observation: 0.146 [-10.295, 20.068], loss: --, mean_squared_error: --, mean_q: --\n",
      "  830/2000: episode: 10, duration: 2.280s, episode steps: 83, steps per second: 36, episode reward: 0.454, mean reward: 0.005 [0.001, 0.011], mean action: 0.178 [-1.122, 1.072], mean observation: 0.146 [-10.374, 19.991], loss: --, mean_squared_error: --, mean_q: --\n",
      "  913/2000: episode: 11, duration: 2.266s, episode steps: 83, steps per second: 37, episode reward: 0.458, mean reward: 0.006 [0.001, 0.011], mean action: 0.180 [-1.202, 1.159], mean observation: 0.146 [-10.301, 20.012], loss: --, mean_squared_error: --, mean_q: --\n",
      "  996/2000: episode: 12, duration: 2.270s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.000, 0.011], mean action: 0.176 [-1.209, 1.130], mean observation: 0.145 [-10.351, 20.381], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1079/2000: episode: 13, duration: 2.998s, episode steps: 83, steps per second: 28, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.196 [-1.070, 1.128], mean observation: 0.145 [-10.302, 20.006], loss: 0.000068, mean_squared_error: 0.000135, mean_q: 0.301761\n",
      " 1161/2000: episode: 14, duration: 3.031s, episode steps: 82, steps per second: 27, episode reward: 0.449, mean reward: 0.005 [0.000, 0.011], mean action: 0.180 [-1.226, 1.215], mean observation: 0.146 [-10.299, 20.250], loss: 0.000127, mean_squared_error: 0.000255, mean_q: 0.292808\n",
      " 1244/2000: episode: 15, duration: 3.066s, episode steps: 83, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.192 [-1.084, 1.156], mean observation: 0.146 [-10.284, 20.148], loss: 0.000233, mean_squared_error: 0.000466, mean_q: 0.302561\n",
      " 1327/2000: episode: 16, duration: 3.035s, episode steps: 83, steps per second: 27, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.193 [-1.241, 1.241], mean observation: 0.146 [-10.386, 20.154], loss: 0.000060, mean_squared_error: 0.000121, mean_q: 0.301027\n",
      " 1409/2000: episode: 17, duration: 2.961s, episode steps: 82, steps per second: 28, episode reward: 0.450, mean reward: 0.005 [0.000, 0.011], mean action: 0.218 [-1.200, 1.220], mean observation: 0.146 [-10.278, 20.296], loss: 0.000115, mean_squared_error: 0.000229, mean_q: 0.298001\n",
      " 1492/2000: episode: 18, duration: 3.066s, episode steps: 83, steps per second: 27, episode reward: 0.454, mean reward: 0.005 [0.000, 0.011], mean action: 0.182 [-1.106, 1.101], mean observation: 0.146 [-10.423, 20.108], loss: 0.000066, mean_squared_error: 0.000133, mean_q: 0.300270\n",
      " 1575/2000: episode: 19, duration: 3.020s, episode steps: 83, steps per second: 27, episode reward: 0.458, mean reward: 0.006 [0.000, 0.011], mean action: 0.174 [-1.263, 1.130], mean observation: 0.146 [-10.352, 20.063], loss: 0.000093, mean_squared_error: 0.000185, mean_q: 0.304799\n",
      " 1658/2000: episode: 20, duration: 3.037s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.190 [-1.126, 1.166], mean observation: 0.146 [-10.329, 20.172], loss: 0.000020, mean_squared_error: 0.000040, mean_q: 0.298855\n",
      " 1741/2000: episode: 21, duration: 3.050s, episode steps: 83, steps per second: 27, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.202 [-1.189, 1.153], mean observation: 0.146 [-10.342, 20.114], loss: 0.000083, mean_squared_error: 0.000165, mean_q: 0.301002\n",
      " 1825/2000: episode: 22, duration: 3.099s, episode steps: 84, steps per second: 27, episode reward: 0.465, mean reward: 0.006 [0.001, 0.011], mean action: 0.207 [-1.116, 1.148], mean observation: 0.146 [-10.348, 19.942], loss: 0.000018, mean_squared_error: 0.000036, mean_q: 0.299924\n",
      " 1908/2000: episode: 23, duration: 3.026s, episode steps: 83, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.183 [-1.264, 1.128], mean observation: 0.146 [-10.337, 20.038], loss: 0.000019, mean_squared_error: 0.000038, mean_q: 0.299592\n",
      " 1991/2000: episode: 24, duration: 3.185s, episode steps: 83, steps per second: 26, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.184 [-1.132, 1.162], mean observation: 0.146 [-10.306, 19.963], loss: 0.000085, mean_squared_error: 0.000169, mean_q: 0.296472\n",
      "done, took 64.320 seconds\n",
      "\n",
      "\n",
      "iteration: 383\n",
      "Training for 2000 steps ...\n",
      "   83/2000: episode: 1, duration: 2.278s, episode steps: 83, steps per second: 36, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.201 [-1.198, 1.173], mean observation: 0.146 [-10.321, 19.998], loss: --, mean_squared_error: --, mean_q: --\n",
      "  166/2000: episode: 2, duration: 2.287s, episode steps: 83, steps per second: 36, episode reward: 0.459, mean reward: 0.006 [0.000, 0.011], mean action: 0.188 [-1.107, 1.154], mean observation: 0.146 [-10.325, 19.919], loss: --, mean_squared_error: --, mean_q: --\n",
      "  249/2000: episode: 3, duration: 2.279s, episode steps: 83, steps per second: 36, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.200 [-1.080, 1.152], mean observation: 0.146 [-10.318, 20.223], loss: --, mean_squared_error: --, mean_q: --\n",
      "  332/2000: episode: 4, duration: 2.247s, episode steps: 83, steps per second: 37, episode reward: 0.455, mean reward: 0.005 [0.001, 0.011], mean action: 0.168 [-1.184, 1.135], mean observation: 0.147 [-10.399, 19.816], loss: --, mean_squared_error: --, mean_q: --\n",
      "  415/2000: episode: 5, duration: 2.265s, episode steps: 83, steps per second: 37, episode reward: 0.456, mean reward: 0.005 [0.000, 0.011], mean action: 0.192 [-1.184, 1.144], mean observation: 0.146 [-10.397, 19.908], loss: --, mean_squared_error: --, mean_q: --\n",
      "  498/2000: episode: 6, duration: 2.235s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.000, 0.011], mean action: 0.201 [-1.084, 1.196], mean observation: 0.146 [-10.367, 20.053], loss: --, mean_squared_error: --, mean_q: --\n",
      "  581/2000: episode: 7, duration: 2.274s, episode steps: 83, steps per second: 37, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.210 [-1.143, 1.201], mean observation: 0.146 [-10.357, 20.294], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  664/2000: episode: 8, duration: 2.300s, episode steps: 83, steps per second: 36, episode reward: 0.458, mean reward: 0.006 [0.000, 0.011], mean action: 0.209 [-1.071, 1.260], mean observation: 0.146 [-10.344, 20.018], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 9, duration: 2.276s, episode steps: 83, steps per second: 36, episode reward: 0.460, mean reward: 0.006 [0.000, 0.011], mean action: 0.201 [-1.240, 1.200], mean observation: 0.146 [-10.340, 20.197], loss: --, mean_squared_error: --, mean_q: --\n",
      "  830/2000: episode: 10, duration: 2.230s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.202 [-1.274, 1.192], mean observation: 0.146 [-10.383, 20.182], loss: --, mean_squared_error: --, mean_q: --\n",
      "  913/2000: episode: 11, duration: 2.244s, episode steps: 83, steps per second: 37, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.201 [-1.096, 1.131], mean observation: 0.146 [-10.268, 20.243], loss: --, mean_squared_error: --, mean_q: --\n",
      "  996/2000: episode: 12, duration: 2.248s, episode steps: 83, steps per second: 37, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.200 [-1.112, 1.102], mean observation: 0.147 [-10.341, 20.025], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1080/2000: episode: 13, duration: 2.998s, episode steps: 84, steps per second: 28, episode reward: 0.465, mean reward: 0.006 [0.001, 0.011], mean action: 0.186 [-1.132, 1.130], mean observation: 0.147 [-10.496, 19.966], loss: 0.000094, mean_squared_error: 0.000188, mean_q: 0.296702\n",
      " 1163/2000: episode: 14, duration: 3.088s, episode steps: 83, steps per second: 27, episode reward: 0.458, mean reward: 0.006 [0.000, 0.011], mean action: 0.181 [-1.278, 1.089], mean observation: 0.146 [-10.361, 20.221], loss: 0.000022, mean_squared_error: 0.000044, mean_q: 0.295141\n",
      " 1246/2000: episode: 15, duration: 3.058s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.000, 0.011], mean action: 0.173 [-1.156, 1.127], mean observation: 0.146 [-10.365, 20.283], loss: 0.000120, mean_squared_error: 0.000240, mean_q: 0.292280\n",
      " 1329/2000: episode: 16, duration: 3.094s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.185 [-1.155, 1.096], mean observation: 0.146 [-10.420, 20.147], loss: 0.000059, mean_squared_error: 0.000119, mean_q: 0.294621\n",
      " 1412/2000: episode: 17, duration: 3.118s, episode steps: 83, steps per second: 27, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.173 [-1.130, 1.113], mean observation: 0.146 [-10.432, 20.250], loss: 0.000021, mean_squared_error: 0.000043, mean_q: 0.293797\n",
      " 1495/2000: episode: 18, duration: 3.102s, episode steps: 83, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.000, 0.011], mean action: 0.199 [-1.098, 1.191], mean observation: 0.146 [-10.369, 20.087], loss: 0.000018, mean_squared_error: 0.000035, mean_q: 0.297842\n",
      " 1579/2000: episode: 19, duration: 3.099s, episode steps: 84, steps per second: 27, episode reward: 0.460, mean reward: 0.005 [0.001, 0.011], mean action: 0.182 [-1.200, 1.191], mean observation: 0.147 [-10.555, 19.987], loss: 0.000017, mean_squared_error: 0.000035, mean_q: 0.292353\n",
      " 1662/2000: episode: 20, duration: 3.101s, episode steps: 83, steps per second: 27, episode reward: 0.459, mean reward: 0.006 [0.001, 0.011], mean action: 0.186 [-1.128, 1.163], mean observation: 0.146 [-10.369, 19.947], loss: 0.000124, mean_squared_error: 0.000249, mean_q: 0.300386\n",
      " 1745/2000: episode: 21, duration: 3.049s, episode steps: 83, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.001, 0.011], mean action: 0.188 [-1.187, 1.180], mean observation: 0.146 [-10.393, 20.094], loss: 0.000026, mean_squared_error: 0.000053, mean_q: 0.296099\n",
      " 1827/2000: episode: 22, duration: 3.018s, episode steps: 82, steps per second: 27, episode reward: 0.452, mean reward: 0.006 [0.000, 0.011], mean action: 0.206 [-1.099, 1.184], mean observation: 0.146 [-10.264, 20.276], loss: 0.000062, mean_squared_error: 0.000123, mean_q: 0.290830\n",
      " 1909/2000: episode: 23, duration: 3.048s, episode steps: 82, steps per second: 27, episode reward: 0.452, mean reward: 0.006 [0.000, 0.011], mean action: 0.203 [-1.120, 1.178], mean observation: 0.146 [-10.325, 20.252], loss: 0.000026, mean_squared_error: 0.000052, mean_q: 0.293704\n",
      " 1990/2000: episode: 24, duration: 3.030s, episode steps: 81, steps per second: 27, episode reward: 0.450, mean reward: 0.006 [0.000, 0.011], mean action: 0.220 [-1.119, 1.144], mean observation: 0.146 [-10.062, 20.279], loss: 0.000060, mean_squared_error: 0.000119, mean_q: 0.294146\n",
      "done, took 64.708 seconds\n",
      "\n",
      "\n",
      "iteration: 384\n",
      "Training for 2000 steps ...\n",
      "   81/2000: episode: 1, duration: 2.252s, episode steps: 81, steps per second: 36, episode reward: 0.448, mean reward: 0.006 [0.000, 0.011], mean action: 0.198 [-1.079, 1.223], mean observation: 0.146 [-10.003, 19.916], loss: --, mean_squared_error: --, mean_q: --\n",
      "  162/2000: episode: 2, duration: 2.239s, episode steps: 81, steps per second: 36, episode reward: 0.451, mean reward: 0.006 [0.000, 0.011], mean action: 0.181 [-1.112, 1.165], mean observation: 0.145 [-10.082, 20.043], loss: --, mean_squared_error: --, mean_q: --\n",
      "  243/2000: episode: 3, duration: 2.271s, episode steps: 81, steps per second: 36, episode reward: 0.451, mean reward: 0.006 [0.001, 0.011], mean action: 0.188 [-1.185, 1.157], mean observation: 0.145 [-9.962, 20.058], loss: --, mean_squared_error: --, mean_q: --\n",
      "  324/2000: episode: 4, duration: 2.215s, episode steps: 81, steps per second: 37, episode reward: 0.451, mean reward: 0.006 [0.000, 0.011], mean action: 0.176 [-1.161, 1.107], mean observation: 0.145 [-10.023, 20.033], loss: --, mean_squared_error: --, mean_q: --\n",
      "  405/2000: episode: 5, duration: 2.251s, episode steps: 81, steps per second: 36, episode reward: 0.451, mean reward: 0.006 [0.000, 0.011], mean action: 0.192 [-1.089, 1.100], mean observation: 0.145 [-10.032, 20.146], loss: --, mean_squared_error: --, mean_q: --\n",
      "  487/2000: episode: 6, duration: 2.352s, episode steps: 82, steps per second: 35, episode reward: 0.456, mean reward: 0.006 [0.001, 0.011], mean action: 0.197 [-1.080, 1.188], mean observation: 0.146 [-10.028, 19.992], loss: --, mean_squared_error: --, mean_q: --\n",
      "  568/2000: episode: 7, duration: 2.200s, episode steps: 81, steps per second: 37, episode reward: 0.447, mean reward: 0.006 [0.001, 0.011], mean action: 0.205 [-1.164, 1.132], mean observation: 0.146 [-9.990, 20.320], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 8, duration: 2.254s, episode steps: 81, steps per second: 36, episode reward: 0.451, mean reward: 0.006 [0.001, 0.011], mean action: 0.205 [-1.124, 1.176], mean observation: 0.145 [-9.987, 20.189], loss: --, mean_squared_error: --, mean_q: --\n",
      "  730/2000: episode: 9, duration: 2.321s, episode steps: 81, steps per second: 35, episode reward: 0.448, mean reward: 0.006 [0.000, 0.011], mean action: 0.200 [-1.150, 1.163], mean observation: 0.146 [-10.084, 20.012], loss: --, mean_squared_error: --, mean_q: --\n",
      "  811/2000: episode: 10, duration: 2.223s, episode steps: 81, steps per second: 36, episode reward: 0.449, mean reward: 0.006 [0.000, 0.011], mean action: 0.205 [-1.145, 1.187], mean observation: 0.146 [-9.991, 20.117], loss: --, mean_squared_error: --, mean_q: --\n",
      "  892/2000: episode: 11, duration: 2.261s, episode steps: 81, steps per second: 36, episode reward: 0.448, mean reward: 0.006 [0.000, 0.011], mean action: 0.194 [-1.163, 1.118], mean observation: 0.146 [-10.035, 20.009], loss: --, mean_squared_error: --, mean_q: --\n",
      "  973/2000: episode: 12, duration: 2.255s, episode steps: 81, steps per second: 36, episode reward: 0.450, mean reward: 0.006 [0.000, 0.011], mean action: 0.226 [-1.092, 1.235], mean observation: 0.146 [-10.035, 20.256], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1054/2000: episode: 13, duration: 2.805s, episode steps: 81, steps per second: 29, episode reward: 0.450, mean reward: 0.006 [0.000, 0.011], mean action: 0.201 [-1.141, 1.201], mean observation: 0.146 [-10.105, 20.006], loss: 0.000020, mean_squared_error: 0.000039, mean_q: 0.292230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1135/2000: episode: 14, duration: 3.079s, episode steps: 81, steps per second: 26, episode reward: 0.446, mean reward: 0.006 [0.000, 0.011], mean action: 0.192 [-1.129, 1.145], mean observation: 0.145 [-10.391, 20.102], loss: 0.000055, mean_squared_error: 0.000110, mean_q: 0.295050\n",
      " 1218/2000: episode: 15, duration: 3.119s, episode steps: 83, steps per second: 27, episode reward: 0.457, mean reward: 0.006 [0.001, 0.011], mean action: 0.192 [-1.071, 1.202], mean observation: 0.146 [-10.362, 20.033], loss: 0.000017, mean_squared_error: 0.000034, mean_q: 0.293906\n",
      " 1301/2000: episode: 16, duration: 3.063s, episode steps: 83, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.000, 0.011], mean action: 0.180 [-1.159, 1.150], mean observation: 0.146 [-10.287, 20.285], loss: 0.000081, mean_squared_error: 0.000161, mean_q: 0.286311\n",
      " 1384/2000: episode: 17, duration: 3.069s, episode steps: 83, steps per second: 27, episode reward: 0.461, mean reward: 0.006 [0.001, 0.011], mean action: 0.217 [-1.131, 1.161], mean observation: 0.146 [-10.282, 20.209], loss: 0.000049, mean_squared_error: 0.000098, mean_q: 0.297325\n",
      " 1467/2000: episode: 18, duration: 3.124s, episode steps: 83, steps per second: 27, episode reward: 0.460, mean reward: 0.006 [0.000, 0.011], mean action: 0.177 [-1.157, 1.101], mean observation: 0.145 [-10.323, 20.064], loss: 0.000034, mean_squared_error: 0.000069, mean_q: 0.294683\n",
      " 1549/2000: episode: 19, duration: 3.056s, episode steps: 82, steps per second: 27, episode reward: 0.443, mean reward: 0.005 [0.000, 0.011], mean action: 0.198 [-1.114, 1.145], mean observation: 0.143 [-13.835, 20.113], loss: 0.000043, mean_squared_error: 0.000086, mean_q: 0.289489\n",
      " 1633/2000: episode: 20, duration: 3.120s, episode steps: 84, steps per second: 27, episode reward: 0.453, mean reward: 0.005 [0.000, 0.010], mean action: 0.187 [-1.119, 1.145], mean observation: 0.143 [-13.517, 20.074], loss: 0.000018, mean_squared_error: 0.000036, mean_q: 0.291619\n",
      " 1716/2000: episode: 21, duration: 3.035s, episode steps: 83, steps per second: 27, episode reward: 0.446, mean reward: 0.005 [0.000, 0.011], mean action: 0.174 [-1.142, 1.145], mean observation: 0.144 [-13.638, 19.890], loss: 0.000048, mean_squared_error: 0.000095, mean_q: 0.290215\n",
      " 1800/2000: episode: 22, duration: 3.206s, episode steps: 84, steps per second: 26, episode reward: 0.449, mean reward: 0.005 [-0.000, 0.011], mean action: 0.168 [-1.101, 1.136], mean observation: 0.143 [-13.699, 20.103], loss: 0.000046, mean_squared_error: 0.000093, mean_q: 0.293326\n",
      " 1884/2000: episode: 23, duration: 3.212s, episode steps: 84, steps per second: 26, episode reward: 0.445, mean reward: 0.005 [0.000, 0.010], mean action: 0.165 [-1.153, 1.094], mean observation: 0.142 [-13.545, 20.150], loss: 0.000193, mean_squared_error: 0.000386, mean_q: 0.289448\n",
      " 1966/2000: episode: 24, duration: 3.085s, episode steps: 82, steps per second: 27, episode reward: 0.444, mean reward: 0.005 [-0.000, 0.011], mean action: 0.186 [-1.150, 1.134], mean observation: 0.143 [-13.918, 20.008], loss: 0.000047, mean_squared_error: 0.000095, mean_q: 0.286973\n",
      "done, took 65.794 seconds\n",
      "\n",
      "\n",
      "iteration: 385\n",
      "Training for 2000 steps ...\n",
      "   81/2000: episode: 1, duration: 2.311s, episode steps: 81, steps per second: 35, episode reward: 0.445, mean reward: 0.005 [0.000, 0.011], mean action: 0.189 [-1.120, 1.217], mean observation: 0.144 [-10.633, 20.014], loss: --, mean_squared_error: --, mean_q: --\n",
      "  163/2000: episode: 2, duration: 2.317s, episode steps: 82, steps per second: 35, episode reward: 0.451, mean reward: 0.005 [0.000, 0.011], mean action: 0.186 [-1.220, 1.117], mean observation: 0.144 [-10.680, 20.158], loss: --, mean_squared_error: --, mean_q: --\n",
      "  245/2000: episode: 3, duration: 2.303s, episode steps: 82, steps per second: 36, episode reward: 0.452, mean reward: 0.006 [0.000, 0.011], mean action: 0.199 [-1.073, 1.161], mean observation: 0.145 [-10.714, 20.075], loss: --, mean_squared_error: --, mean_q: --\n",
      "  327/2000: episode: 4, duration: 2.313s, episode steps: 82, steps per second: 35, episode reward: 0.452, mean reward: 0.006 [0.000, 0.011], mean action: 0.183 [-1.152, 1.117], mean observation: 0.144 [-10.678, 20.242], loss: --, mean_squared_error: --, mean_q: --\n",
      "  409/2000: episode: 5, duration: 2.249s, episode steps: 82, steps per second: 36, episode reward: 0.451, mean reward: 0.005 [0.000, 0.011], mean action: 0.191 [-1.138, 1.140], mean observation: 0.144 [-10.730, 20.003], loss: --, mean_squared_error: --, mean_q: --\n",
      "  491/2000: episode: 6, duration: 2.314s, episode steps: 82, steps per second: 35, episode reward: 0.454, mean reward: 0.006 [0.000, 0.011], mean action: 0.197 [-1.146, 1.279], mean observation: 0.144 [-10.638, 20.036], loss: --, mean_squared_error: --, mean_q: --\n",
      "  573/2000: episode: 7, duration: 2.246s, episode steps: 82, steps per second: 37, episode reward: 0.455, mean reward: 0.006 [0.000, 0.011], mean action: 0.190 [-1.149, 1.123], mean observation: 0.144 [-10.714, 20.076], loss: --, mean_squared_error: --, mean_q: --\n",
      "  655/2000: episode: 8, duration: 2.294s, episode steps: 82, steps per second: 36, episode reward: 0.453, mean reward: 0.006 [0.000, 0.011], mean action: 0.165 [-1.182, 1.125], mean observation: 0.144 [-10.671, 20.467], loss: --, mean_squared_error: --, mean_q: --\n",
      "  737/2000: episode: 9, duration: 2.337s, episode steps: 82, steps per second: 35, episode reward: 0.454, mean reward: 0.006 [0.000, 0.011], mean action: 0.192 [-1.100, 1.075], mean observation: 0.144 [-10.609, 20.157], loss: --, mean_squared_error: --, mean_q: --\n",
      "  819/2000: episode: 10, duration: 2.321s, episode steps: 82, steps per second: 35, episode reward: 0.453, mean reward: 0.006 [0.000, 0.011], mean action: 0.190 [-1.204, 1.273], mean observation: 0.144 [-10.724, 20.305], loss: --, mean_squared_error: --, mean_q: --\n",
      "  901/2000: episode: 11, duration: 2.303s, episode steps: 82, steps per second: 36, episode reward: 0.451, mean reward: 0.006 [0.000, 0.011], mean action: 0.172 [-1.107, 1.111], mean observation: 0.144 [-10.710, 20.208], loss: --, mean_squared_error: --, mean_q: --\n",
      "  983/2000: episode: 12, duration: 2.351s, episode steps: 82, steps per second: 35, episode reward: 0.453, mean reward: 0.006 [0.000, 0.011], mean action: 0.181 [-1.138, 1.156], mean observation: 0.144 [-10.608, 20.023], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1065/2000: episode: 13, duration: 2.966s, episode steps: 82, steps per second: 28, episode reward: 0.454, mean reward: 0.006 [0.000, 0.011], mean action: 0.197 [-1.091, 1.142], mean observation: 0.145 [-10.701, 20.131], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.282769\n",
      " 1148/2000: episode: 14, duration: 3.233s, episode steps: 83, steps per second: 26, episode reward: 0.454, mean reward: 0.005 [0.001, 0.010], mean action: 0.192 [-1.130, 1.248], mean observation: 0.145 [-10.690, 20.039], loss: 0.000019, mean_squared_error: 0.000037, mean_q: 0.290891\n",
      " 1236/2000: episode: 15, duration: 3.290s, episode steps: 88, steps per second: 27, episode reward: 0.469, mean reward: 0.005 [0.001, 0.011], mean action: 0.218 [-1.182, 1.302], mean observation: 0.146 [-9.819, 19.895], loss: 0.000026, mean_squared_error: 0.000053, mean_q: 0.281241\n",
      " 1327/2000: episode: 16, duration: 3.291s, episode steps: 91, steps per second: 28, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.196 [-1.096, 1.152], mean observation: 0.145 [-8.580, 19.950], loss: 0.000080, mean_squared_error: 0.000160, mean_q: 0.289433\n",
      " 1426/2000: episode: 17, duration: 3.636s, episode steps: 99, steps per second: 27, episode reward: 0.471, mean reward: 0.005 [0.000, 0.011], mean action: 0.167 [-1.222, 1.145], mean observation: 0.144 [-12.852, 21.039], loss: 0.000059, mean_squared_error: 0.000119, mean_q: 0.287558\n",
      " 1519/2000: episode: 18, duration: 3.412s, episode steps: 93, steps per second: 27, episode reward: 0.469, mean reward: 0.005 [0.001, 0.010], mean action: 0.202 [-1.103, 1.170], mean observation: 0.145 [-13.716, 20.913], loss: 0.000028, mean_squared_error: 0.000056, mean_q: 0.285880\n",
      " 1616/2000: episode: 19, duration: 3.576s, episode steps: 97, steps per second: 27, episode reward: 0.471, mean reward: 0.005 [0.001, 0.011], mean action: 0.182 [-1.195, 1.211], mean observation: 0.145 [-13.729, 21.051], loss: 0.000079, mean_squared_error: 0.000159, mean_q: 0.292455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1715/2000: episode: 20, duration: 3.662s, episode steps: 99, steps per second: 27, episode reward: 0.470, mean reward: 0.005 [0.000, 0.011], mean action: 0.190 [-1.141, 1.216], mean observation: 0.144 [-13.554, 20.860], loss: 0.000027, mean_squared_error: 0.000055, mean_q: 0.284888\n",
      " 1808/2000: episode: 21, duration: 3.395s, episode steps: 93, steps per second: 27, episode reward: 0.468, mean reward: 0.005 [0.001, 0.010], mean action: 0.181 [-1.151, 1.182], mean observation: 0.144 [-13.328, 20.942], loss: 0.000093, mean_squared_error: 0.000186, mean_q: 0.286440\n",
      " 1901/2000: episode: 22, duration: 3.404s, episode steps: 93, steps per second: 27, episode reward: 0.469, mean reward: 0.005 [0.001, 0.011], mean action: 0.220 [-1.085, 1.175], mean observation: 0.145 [-13.533, 20.931], loss: 0.000021, mean_squared_error: 0.000041, mean_q: 0.287751\n",
      " 1999/2000: episode: 23, duration: 3.624s, episode steps: 98, steps per second: 27, episode reward: 0.474, mean reward: 0.005 [0.001, 0.010], mean action: 0.187 [-1.085, 1.135], mean observation: 0.144 [-9.238, 20.936], loss: 0.000052, mean_squared_error: 0.000104, mean_q: 0.285814\n",
      "done, took 65.271 seconds\n",
      "\n",
      "\n",
      "iteration: 386\n",
      "Training for 2000 steps ...\n",
      "   93/2000: episode: 1, duration: 2.506s, episode steps: 93, steps per second: 37, episode reward: 0.469, mean reward: 0.005 [0.001, 0.011], mean action: 0.194 [-1.102, 1.103], mean observation: 0.145 [-13.785, 20.880], loss: --, mean_squared_error: --, mean_q: --\n",
      "  186/2000: episode: 2, duration: 2.499s, episode steps: 93, steps per second: 37, episode reward: 0.468, mean reward: 0.005 [0.001, 0.010], mean action: 0.227 [-1.153, 1.241], mean observation: 0.145 [-13.521, 21.116], loss: --, mean_squared_error: --, mean_q: --\n",
      "  280/2000: episode: 3, duration: 2.553s, episode steps: 94, steps per second: 37, episode reward: 0.474, mean reward: 0.005 [0.001, 0.011], mean action: 0.196 [-1.202, 1.181], mean observation: 0.145 [-13.432, 20.977], loss: --, mean_squared_error: --, mean_q: --\n",
      "  374/2000: episode: 4, duration: 2.539s, episode steps: 94, steps per second: 37, episode reward: 0.473, mean reward: 0.005 [0.001, 0.011], mean action: 0.190 [-1.174, 1.199], mean observation: 0.145 [-13.556, 21.030], loss: --, mean_squared_error: --, mean_q: --\n",
      "  467/2000: episode: 5, duration: 2.466s, episode steps: 93, steps per second: 38, episode reward: 0.465, mean reward: 0.005 [0.001, 0.010], mean action: 0.188 [-1.188, 1.155], mean observation: 0.145 [-13.600, 21.029], loss: --, mean_squared_error: --, mean_q: --\n",
      "  560/2000: episode: 6, duration: 2.471s, episode steps: 93, steps per second: 38, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.197 [-1.107, 1.173], mean observation: 0.145 [-13.742, 21.126], loss: --, mean_squared_error: --, mean_q: --\n",
      "  654/2000: episode: 7, duration: 2.494s, episode steps: 94, steps per second: 38, episode reward: 0.472, mean reward: 0.005 [0.001, 0.010], mean action: 0.176 [-1.197, 1.065], mean observation: 0.145 [-13.413, 20.958], loss: --, mean_squared_error: --, mean_q: --\n",
      "  747/2000: episode: 8, duration: 2.500s, episode steps: 93, steps per second: 37, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.225 [-1.094, 1.151], mean observation: 0.145 [-13.324, 20.952], loss: --, mean_squared_error: --, mean_q: --\n",
      "  841/2000: episode: 9, duration: 2.518s, episode steps: 94, steps per second: 37, episode reward: 0.472, mean reward: 0.005 [0.001, 0.010], mean action: 0.190 [-1.184, 1.232], mean observation: 0.145 [-13.367, 21.079], loss: --, mean_squared_error: --, mean_q: --\n",
      "  935/2000: episode: 10, duration: 2.517s, episode steps: 94, steps per second: 37, episode reward: 0.471, mean reward: 0.005 [0.001, 0.011], mean action: 0.198 [-1.108, 1.239], mean observation: 0.146 [-13.415, 20.865], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1029/2000: episode: 11, duration: 2.790s, episode steps: 94, steps per second: 34, episode reward: 0.475, mean reward: 0.005 [0.001, 0.011], mean action: 0.202 [-1.108, 1.185], mean observation: 0.145 [-13.018, 21.014], loss: 0.000135, mean_squared_error: 0.000271, mean_q: 0.284334\n",
      " 1122/2000: episode: 12, duration: 3.391s, episode steps: 93, steps per second: 27, episode reward: 0.469, mean reward: 0.005 [0.001, 0.011], mean action: 0.188 [-1.198, 1.128], mean observation: 0.145 [-13.781, 20.854], loss: 0.000128, mean_squared_error: 0.000257, mean_q: 0.279803\n",
      " 1215/2000: episode: 13, duration: 3.489s, episode steps: 93, steps per second: 27, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.174 [-1.182, 1.218], mean observation: 0.143 [-13.412, 20.931], loss: 0.000188, mean_squared_error: 0.000377, mean_q: 0.282137\n",
      " 1307/2000: episode: 14, duration: 3.504s, episode steps: 92, steps per second: 26, episode reward: 0.473, mean reward: 0.005 [0.001, 0.011], mean action: 0.088 [-1.192, 1.157], mean observation: 0.143 [-23.416, 20.930], loss: 0.000057, mean_squared_error: 0.000114, mean_q: 0.289842\n",
      " 1398/2000: episode: 15, duration: 3.431s, episode steps: 91, steps per second: 27, episode reward: 0.471, mean reward: 0.005 [0.001, 0.010], mean action: 0.130 [-1.106, 1.185], mean observation: 0.144 [-12.798, 20.932], loss: 0.000077, mean_squared_error: 0.000154, mean_q: 0.285736\n",
      " 1489/2000: episode: 16, duration: 3.437s, episode steps: 91, steps per second: 26, episode reward: 0.472, mean reward: 0.005 [0.001, 0.010], mean action: 0.103 [-1.105, 1.135], mean observation: 0.144 [-13.716, 20.929], loss: 0.000053, mean_squared_error: 0.000106, mean_q: 0.286952\n",
      " 1580/2000: episode: 17, duration: 3.428s, episode steps: 91, steps per second: 27, episode reward: 0.468, mean reward: 0.005 [0.001, 0.010], mean action: 0.117 [-1.095, 1.180], mean observation: 0.144 [-13.395, 21.043], loss: 0.000043, mean_squared_error: 0.000087, mean_q: 0.285678\n",
      " 1671/2000: episode: 18, duration: 3.409s, episode steps: 91, steps per second: 27, episode reward: 0.467, mean reward: 0.005 [0.001, 0.010], mean action: 0.108 [-1.232, 1.176], mean observation: 0.145 [-13.444, 21.145], loss: 0.000039, mean_squared_error: 0.000078, mean_q: 0.285305\n",
      " 1762/2000: episode: 19, duration: 3.406s, episode steps: 91, steps per second: 27, episode reward: 0.470, mean reward: 0.005 [0.001, 0.010], mean action: 0.111 [-1.071, 1.156], mean observation: 0.143 [-16.912, 20.910], loss: 0.000036, mean_squared_error: 0.000072, mean_q: 0.280682\n",
      " 1853/2000: episode: 20, duration: 3.423s, episode steps: 91, steps per second: 27, episode reward: 0.470, mean reward: 0.005 [0.001, 0.011], mean action: 0.101 [-1.135, 1.214], mean observation: 0.143 [-23.096, 20.838], loss: 0.000076, mean_squared_error: 0.000151, mean_q: 0.281751\n",
      " 1940/2000: episode: 21, duration: 3.188s, episode steps: 87, steps per second: 27, episode reward: 0.464, mean reward: 0.005 [0.001, 0.010], mean action: 0.079 [-1.178, 1.174], mean observation: 0.145 [-16.981, 19.946], loss: 0.000084, mean_squared_error: 0.000168, mean_q: 0.283581\n",
      "done, took 64.567 seconds\n",
      "\n",
      "\n",
      "iteration: 387\n",
      "Training for 2000 steps ...\n",
      "   91/2000: episode: 1, duration: 2.540s, episode steps: 91, steps per second: 36, episode reward: 0.470, mean reward: 0.005 [0.001, 0.010], mean action: 0.087 [-1.156, 1.102], mean observation: 0.142 [-22.143, 21.093], loss: --, mean_squared_error: --, mean_q: --\n",
      "  182/2000: episode: 2, duration: 2.506s, episode steps: 91, steps per second: 36, episode reward: 0.470, mean reward: 0.005 [0.001, 0.010], mean action: 0.101 [-1.096, 1.159], mean observation: 0.142 [-21.352, 21.139], loss: --, mean_squared_error: --, mean_q: --\n",
      "  273/2000: episode: 3, duration: 2.537s, episode steps: 91, steps per second: 36, episode reward: 0.465, mean reward: 0.005 [0.001, 0.010], mean action: 0.074 [-1.109, 1.194], mean observation: 0.142 [-21.656, 21.352], loss: --, mean_squared_error: --, mean_q: --\n",
      "  364/2000: episode: 4, duration: 2.514s, episode steps: 91, steps per second: 36, episode reward: 0.467, mean reward: 0.005 [0.001, 0.010], mean action: 0.086 [-1.252, 1.165], mean observation: 0.142 [-19.987, 21.330], loss: --, mean_squared_error: --, mean_q: --\n",
      "  455/2000: episode: 5, duration: 2.578s, episode steps: 91, steps per second: 35, episode reward: 0.473, mean reward: 0.005 [0.001, 0.011], mean action: 0.098 [-1.099, 1.133], mean observation: 0.142 [-21.754, 21.162], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  546/2000: episode: 6, duration: 2.554s, episode steps: 91, steps per second: 36, episode reward: 0.470, mean reward: 0.005 [0.001, 0.010], mean action: 0.079 [-1.164, 1.147], mean observation: 0.142 [-21.935, 21.388], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 7, duration: 2.549s, episode steps: 91, steps per second: 36, episode reward: 0.469, mean reward: 0.005 [0.001, 0.010], mean action: 0.089 [-1.236, 1.144], mean observation: 0.142 [-21.753, 21.351], loss: --, mean_squared_error: --, mean_q: --\n",
      "  728/2000: episode: 8, duration: 2.617s, episode steps: 91, steps per second: 35, episode reward: 0.465, mean reward: 0.005 [0.001, 0.010], mean action: 0.101 [-1.119, 1.155], mean observation: 0.142 [-21.970, 21.105], loss: --, mean_squared_error: --, mean_q: --\n",
      "  819/2000: episode: 9, duration: 2.583s, episode steps: 91, steps per second: 35, episode reward: 0.469, mean reward: 0.005 [0.001, 0.010], mean action: 0.092 [-1.115, 1.154], mean observation: 0.142 [-21.734, 21.143], loss: --, mean_squared_error: --, mean_q: --\n",
      "  910/2000: episode: 10, duration: 2.549s, episode steps: 91, steps per second: 36, episode reward: 0.471, mean reward: 0.005 [0.001, 0.010], mean action: 0.110 [-1.148, 1.181], mean observation: 0.143 [-21.187, 21.346], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1000/2000: episode: 11, duration: 2.538s, episode steps: 90, steps per second: 35, episode reward: 0.464, mean reward: 0.005 [0.001, 0.010], mean action: 0.110 [-1.168, 1.226], mean observation: 0.143 [-20.747, 21.338], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1091/2000: episode: 12, duration: 3.454s, episode steps: 91, steps per second: 26, episode reward: 0.467, mean reward: 0.005 [0.001, 0.010], mean action: 0.104 [-1.091, 1.153], mean observation: 0.142 [-21.512, 21.165], loss: 0.000035, mean_squared_error: 0.000070, mean_q: 0.285052\n",
      " 1182/2000: episode: 13, duration: 3.386s, episode steps: 91, steps per second: 27, episode reward: 0.467, mean reward: 0.005 [0.001, 0.010], mean action: 0.097 [-1.118, 1.110], mean observation: 0.143 [-22.852, 21.085], loss: 0.000018, mean_squared_error: 0.000036, mean_q: 0.283567\n",
      " 1273/2000: episode: 14, duration: 3.421s, episode steps: 91, steps per second: 27, episode reward: 0.472, mean reward: 0.005 [0.001, 0.010], mean action: 0.080 [-1.156, 1.109], mean observation: 0.142 [-21.982, 21.213], loss: 0.000055, mean_squared_error: 0.000111, mean_q: 0.284411\n",
      " 1366/2000: episode: 15, duration: 3.602s, episode steps: 93, steps per second: 26, episode reward: 0.466, mean reward: 0.005 [0.001, 0.010], mean action: 0.202 [-1.124, 1.187], mean observation: 0.148 [-25.104, 20.630], loss: 0.000025, mean_squared_error: 0.000049, mean_q: 0.282836\n",
      " 1460/2000: episode: 16, duration: 3.583s, episode steps: 94, steps per second: 26, episode reward: 0.474, mean reward: 0.005 [0.001, 0.011], mean action: 0.169 [-1.198, 1.146], mean observation: 0.145 [-22.190, 20.645], loss: 0.000043, mean_squared_error: 0.000086, mean_q: 0.282910\n",
      " 1554/2000: episode: 17, duration: 3.556s, episode steps: 94, steps per second: 26, episode reward: 0.468, mean reward: 0.005 [0.001, 0.010], mean action: 0.189 [-1.238, 1.241], mean observation: 0.144 [-23.308, 20.518], loss: 0.000062, mean_squared_error: 0.000124, mean_q: 0.283135\n",
      " 1648/2000: episode: 18, duration: 3.568s, episode steps: 94, steps per second: 26, episode reward: 0.477, mean reward: 0.005 [0.001, 0.011], mean action: 0.183 [-1.197, 1.136], mean observation: 0.147 [-24.031, 20.712], loss: 0.000038, mean_squared_error: 0.000076, mean_q: 0.283319\n",
      " 1740/2000: episode: 19, duration: 3.502s, episode steps: 92, steps per second: 26, episode reward: 0.467, mean reward: 0.005 [0.001, 0.010], mean action: 0.189 [-1.164, 1.160], mean observation: 0.141 [-28.427, 20.659], loss: 0.000027, mean_squared_error: 0.000054, mean_q: 0.284845\n",
      " 1832/2000: episode: 20, duration: 3.565s, episode steps: 92, steps per second: 26, episode reward: 0.470, mean reward: 0.005 [0.001, 0.010], mean action: 0.183 [-1.243, 1.144], mean observation: 0.145 [-23.131, 20.541], loss: 0.000059, mean_squared_error: 0.000118, mean_q: 0.280682\n",
      " 1923/2000: episode: 21, duration: 3.514s, episode steps: 91, steps per second: 26, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.183 [-1.156, 1.174], mean observation: 0.143 [-12.135, 16.818], loss: 0.000063, mean_squared_error: 0.000125, mean_q: 0.285355\n",
      "done, took 66.410 seconds\n",
      "\n",
      "\n",
      "iteration: 388\n",
      "Training for 2000 steps ...\n",
      "   89/2000: episode: 1, duration: 2.531s, episode steps: 89, steps per second: 35, episode reward: 0.464, mean reward: 0.005 [0.001, 0.010], mean action: 0.212 [-1.121, 1.158], mean observation: 0.145 [-18.452, 18.563], loss: --, mean_squared_error: --, mean_q: --\n",
      "  178/2000: episode: 2, duration: 2.582s, episode steps: 89, steps per second: 34, episode reward: 0.465, mean reward: 0.005 [0.001, 0.010], mean action: 0.202 [-1.214, 1.200], mean observation: 0.146 [-17.823, 18.532], loss: --, mean_squared_error: --, mean_q: --\n",
      "  268/2000: episode: 3, duration: 2.607s, episode steps: 90, steps per second: 35, episode reward: 0.475, mean reward: 0.005 [0.001, 0.011], mean action: 0.192 [-1.160, 1.194], mean observation: 0.145 [-18.563, 18.498], loss: --, mean_squared_error: --, mean_q: --\n",
      "  358/2000: episode: 4, duration: 2.638s, episode steps: 90, steps per second: 34, episode reward: 0.472, mean reward: 0.005 [0.001, 0.010], mean action: 0.202 [-1.132, 1.128], mean observation: 0.146 [-18.324, 18.624], loss: --, mean_squared_error: --, mean_q: --\n",
      "  448/2000: episode: 5, duration: 2.552s, episode steps: 90, steps per second: 35, episode reward: 0.476, mean reward: 0.005 [0.001, 0.011], mean action: 0.206 [-1.144, 1.159], mean observation: 0.146 [-18.049, 18.574], loss: --, mean_squared_error: --, mean_q: --\n",
      "  537/2000: episode: 6, duration: 2.527s, episode steps: 89, steps per second: 35, episode reward: 0.465, mean reward: 0.005 [0.001, 0.010], mean action: 0.202 [-1.232, 1.256], mean observation: 0.146 [-17.814, 18.504], loss: --, mean_squared_error: --, mean_q: --\n",
      "  626/2000: episode: 7, duration: 2.632s, episode steps: 89, steps per second: 34, episode reward: 0.469, mean reward: 0.005 [0.001, 0.011], mean action: 0.202 [-1.200, 1.303], mean observation: 0.145 [-17.118, 18.601], loss: --, mean_squared_error: --, mean_q: --\n",
      "  715/2000: episode: 8, duration: 2.546s, episode steps: 89, steps per second: 35, episode reward: 0.470, mean reward: 0.005 [0.001, 0.010], mean action: 0.212 [-1.115, 1.171], mean observation: 0.146 [-10.382, 18.727], loss: --, mean_squared_error: --, mean_q: --\n",
      "  805/2000: episode: 9, duration: 2.625s, episode steps: 90, steps per second: 34, episode reward: 0.476, mean reward: 0.005 [0.001, 0.011], mean action: 0.205 [-1.078, 1.190], mean observation: 0.146 [-18.835, 18.741], loss: --, mean_squared_error: --, mean_q: --\n",
      "  895/2000: episode: 10, duration: 2.615s, episode steps: 90, steps per second: 34, episode reward: 0.472, mean reward: 0.005 [0.001, 0.010], mean action: 0.222 [-1.092, 1.189], mean observation: 0.146 [-17.986, 18.566], loss: --, mean_squared_error: --, mean_q: --\n",
      "  985/2000: episode: 11, duration: 2.656s, episode steps: 90, steps per second: 34, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.217 [-1.141, 1.191], mean observation: 0.145 [-18.117, 18.555], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1074/2000: episode: 12, duration: 3.348s, episode steps: 89, steps per second: 27, episode reward: 0.463, mean reward: 0.005 [0.001, 0.010], mean action: 0.204 [-1.125, 1.176], mean observation: 0.146 [-18.443, 18.703], loss: 0.000064, mean_squared_error: 0.000129, mean_q: 0.277288\n",
      " 1164/2000: episode: 13, duration: 3.485s, episode steps: 90, steps per second: 26, episode reward: 0.472, mean reward: 0.005 [0.001, 0.011], mean action: 0.201 [-1.195, 1.172], mean observation: 0.147 [-11.566, 15.959], loss: 0.000079, mean_squared_error: 0.000157, mean_q: 0.276349\n",
      " 1255/2000: episode: 14, duration: 3.522s, episode steps: 91, steps per second: 26, episode reward: 0.474, mean reward: 0.005 [0.001, 0.010], mean action: 0.197 [-1.158, 1.181], mean observation: 0.146 [-10.836, 15.612], loss: 0.000032, mean_squared_error: 0.000064, mean_q: 0.276173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1347/2000: episode: 15, duration: 3.532s, episode steps: 92, steps per second: 26, episode reward: 0.478, mean reward: 0.005 [0.000, 0.011], mean action: 0.217 [-1.175, 1.166], mean observation: 0.147 [-14.029, 16.063], loss: 0.000041, mean_squared_error: 0.000082, mean_q: 0.278389\n",
      " 1438/2000: episode: 16, duration: 3.535s, episode steps: 91, steps per second: 26, episode reward: 0.470, mean reward: 0.005 [0.000, 0.010], mean action: 0.182 [-1.169, 1.140], mean observation: 0.147 [-14.804, 15.559], loss: 0.000030, mean_squared_error: 0.000061, mean_q: 0.282745\n",
      " 1525/2000: episode: 17, duration: 3.321s, episode steps: 87, steps per second: 26, episode reward: 0.473, mean reward: 0.005 [-0.000, 0.012], mean action: 0.196 [-1.143, 1.201], mean observation: 0.148 [-8.949, 17.185], loss: 0.000100, mean_squared_error: 0.000200, mean_q: 0.275157\n",
      " 1616/2000: episode: 18, duration: 3.502s, episode steps: 91, steps per second: 26, episode reward: 0.472, mean reward: 0.005 [0.000, 0.011], mean action: 0.182 [-1.197, 1.117], mean observation: 0.147 [-19.896, 16.982], loss: 0.000037, mean_squared_error: 0.000074, mean_q: 0.276412\n",
      " 1707/2000: episode: 19, duration: 3.473s, episode steps: 91, steps per second: 26, episode reward: 0.471, mean reward: 0.005 [0.000, 0.011], mean action: 0.201 [-1.183, 1.144], mean observation: 0.146 [-12.090, 14.992], loss: 0.000012, mean_squared_error: 0.000025, mean_q: 0.280457\n",
      " 1798/2000: episode: 20, duration: 3.471s, episode steps: 91, steps per second: 26, episode reward: 0.472, mean reward: 0.005 [0.000, 0.011], mean action: 0.199 [-1.176, 1.157], mean observation: 0.146 [-12.762, 15.559], loss: 0.000051, mean_squared_error: 0.000103, mean_q: 0.277189\n",
      " 1890/2000: episode: 21, duration: 3.549s, episode steps: 92, steps per second: 26, episode reward: 0.479, mean reward: 0.005 [0.000, 0.011], mean action: 0.197 [-1.083, 1.126], mean observation: 0.148 [-13.848, 14.601], loss: 0.000050, mean_squared_error: 0.000101, mean_q: 0.273190\n",
      " 1981/2000: episode: 22, duration: 3.522s, episode steps: 91, steps per second: 26, episode reward: 0.472, mean reward: 0.005 [0.000, 0.011], mean action: 0.158 [-1.156, 1.142], mean observation: 0.147 [-11.790, 14.753], loss: 0.000024, mean_squared_error: 0.000049, mean_q: 0.275608\n",
      "done, took 68.134 seconds\n",
      "\n",
      "\n",
      "iteration: 389\n",
      "Training for 2000 steps ...\n",
      "   91/2000: episode: 1, duration: 2.622s, episode steps: 91, steps per second: 35, episode reward: 0.477, mean reward: 0.005 [0.000, 0.011], mean action: 0.188 [-1.201, 1.150], mean observation: 0.148 [-11.448, 15.113], loss: --, mean_squared_error: --, mean_q: --\n",
      "  182/2000: episode: 2, duration: 2.591s, episode steps: 91, steps per second: 35, episode reward: 0.471, mean reward: 0.005 [0.000, 0.011], mean action: 0.197 [-1.162, 1.192], mean observation: 0.148 [-11.458, 15.083], loss: --, mean_squared_error: --, mean_q: --\n",
      "  273/2000: episode: 3, duration: 2.684s, episode steps: 91, steps per second: 34, episode reward: 0.473, mean reward: 0.005 [0.000, 0.011], mean action: 0.215 [-1.105, 1.204], mean observation: 0.148 [-11.582, 15.584], loss: --, mean_squared_error: --, mean_q: --\n",
      "  364/2000: episode: 4, duration: 2.620s, episode steps: 91, steps per second: 35, episode reward: 0.473, mean reward: 0.005 [0.000, 0.011], mean action: 0.211 [-1.190, 1.200], mean observation: 0.147 [-11.850, 15.561], loss: --, mean_squared_error: --, mean_q: --\n",
      "  455/2000: episode: 5, duration: 2.608s, episode steps: 91, steps per second: 35, episode reward: 0.474, mean reward: 0.005 [0.000, 0.011], mean action: 0.211 [-1.133, 1.192], mean observation: 0.147 [-11.444, 15.627], loss: --, mean_squared_error: --, mean_q: --\n",
      "  546/2000: episode: 6, duration: 2.675s, episode steps: 91, steps per second: 34, episode reward: 0.475, mean reward: 0.005 [0.000, 0.011], mean action: 0.204 [-1.229, 1.143], mean observation: 0.149 [-11.350, 15.468], loss: --, mean_squared_error: --, mean_q: --\n",
      "  637/2000: episode: 7, duration: 2.625s, episode steps: 91, steps per second: 35, episode reward: 0.472, mean reward: 0.005 [0.000, 0.011], mean action: 0.217 [-1.131, 1.182], mean observation: 0.148 [-11.491, 15.132], loss: --, mean_squared_error: --, mean_q: --\n",
      "  728/2000: episode: 8, duration: 2.622s, episode steps: 91, steps per second: 35, episode reward: 0.474, mean reward: 0.005 [0.000, 0.011], mean action: 0.194 [-1.204, 1.120], mean observation: 0.147 [-11.783, 15.570], loss: --, mean_squared_error: --, mean_q: --\n",
      "  819/2000: episode: 9, duration: 2.609s, episode steps: 91, steps per second: 35, episode reward: 0.475, mean reward: 0.005 [0.000, 0.011], mean action: 0.221 [-1.113, 1.208], mean observation: 0.147 [-11.782, 15.346], loss: --, mean_squared_error: --, mean_q: --\n",
      "  910/2000: episode: 10, duration: 2.656s, episode steps: 91, steps per second: 34, episode reward: 0.474, mean reward: 0.005 [0.000, 0.011], mean action: 0.202 [-1.113, 1.182], mean observation: 0.147 [-11.784, 15.364], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1001/2000: episode: 11, duration: 2.684s, episode steps: 91, steps per second: 34, episode reward: 0.474, mean reward: 0.005 [0.000, 0.011], mean action: 0.190 [-1.227, 1.166], mean observation: 0.148 [-11.728, 14.931], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1092/2000: episode: 12, duration: 3.537s, episode steps: 91, steps per second: 26, episode reward: 0.471, mean reward: 0.005 [0.000, 0.011], mean action: 0.193 [-1.150, 1.147], mean observation: 0.149 [-11.502, 15.031], loss: 0.000045, mean_squared_error: 0.000091, mean_q: 0.280618\n",
      " 1183/2000: episode: 13, duration: 3.531s, episode steps: 91, steps per second: 26, episode reward: 0.473, mean reward: 0.005 [0.000, 0.011], mean action: 0.180 [-1.224, 1.173], mean observation: 0.148 [-10.638, 14.551], loss: 0.000032, mean_squared_error: 0.000063, mean_q: 0.277768\n",
      " 1274/2000: episode: 14, duration: 3.490s, episode steps: 91, steps per second: 26, episode reward: 0.474, mean reward: 0.005 [0.000, 0.011], mean action: 0.191 [-1.157, 1.181], mean observation: 0.145 [-11.368, 13.856], loss: 0.000043, mean_squared_error: 0.000087, mean_q: 0.279680\n",
      " 1365/2000: episode: 15, duration: 3.580s, episode steps: 91, steps per second: 25, episode reward: 0.482, mean reward: 0.005 [0.000, 0.012], mean action: 0.215 [-1.054, 1.139], mean observation: 0.147 [-11.199, 14.311], loss: 0.000047, mean_squared_error: 0.000094, mean_q: 0.274044\n",
      " 1456/2000: episode: 16, duration: 3.491s, episode steps: 91, steps per second: 26, episode reward: 0.481, mean reward: 0.005 [0.000, 0.012], mean action: 0.194 [-1.108, 1.148], mean observation: 0.147 [-11.278, 14.331], loss: 0.000081, mean_squared_error: 0.000162, mean_q: 0.278892\n",
      " 1548/2000: episode: 17, duration: 3.569s, episode steps: 92, steps per second: 26, episode reward: 0.479, mean reward: 0.005 [-0.000, 0.012], mean action: 0.198 [-1.061, 1.146], mean observation: 0.148 [-11.309, 14.015], loss: 0.000105, mean_squared_error: 0.000210, mean_q: 0.273422\n",
      " 1640/2000: episode: 18, duration: 3.442s, episode steps: 92, steps per second: 27, episode reward: 0.484, mean reward: 0.005 [0.000, 0.011], mean action: 0.198 [-1.076, 1.153], mean observation: 0.147 [-11.590, 14.336], loss: 0.000018, mean_squared_error: 0.000037, mean_q: 0.273169\n",
      " 1732/2000: episode: 19, duration: 3.537s, episode steps: 92, steps per second: 26, episode reward: 0.481, mean reward: 0.005 [0.000, 0.011], mean action: 0.174 [-1.113, 1.167], mean observation: 0.146 [-10.857, 14.127], loss: 0.000023, mean_squared_error: 0.000045, mean_q: 0.277564\n",
      " 1823/2000: episode: 20, duration: 3.418s, episode steps: 91, steps per second: 27, episode reward: 0.477, mean reward: 0.005 [0.000, 0.011], mean action: 0.186 [-1.141, 1.200], mean observation: 0.149 [-11.273, 14.381], loss: 0.000044, mean_squared_error: 0.000087, mean_q: 0.276082\n",
      " 1914/2000: episode: 21, duration: 3.331s, episode steps: 91, steps per second: 27, episode reward: 0.473, mean reward: 0.005 [0.000, 0.011], mean action: 0.169 [-1.185, 1.117], mean observation: 0.149 [-10.360, 13.876], loss: 0.000077, mean_squared_error: 0.000153, mean_q: 0.278620\n",
      "done, took 67.168 seconds\n",
      "\n",
      "\n",
      "iteration: 390\n",
      "Training for 2000 steps ...\n",
      "   94/2000: episode: 1, duration: 2.602s, episode steps: 94, steps per second: 36, episode reward: 0.486, mean reward: 0.005 [-0.000, 0.013], mean action: 0.131 [-1.131, 1.158], mean observation: 0.146 [-14.504, 13.558], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  187/2000: episode: 2, duration: 2.565s, episode steps: 93, steps per second: 36, episode reward: 0.480, mean reward: 0.005 [-0.001, 0.013], mean action: 0.151 [-1.161, 1.347], mean observation: 0.149 [-11.335, 14.030], loss: --, mean_squared_error: --, mean_q: --\n",
      "  280/2000: episode: 3, duration: 2.570s, episode steps: 93, steps per second: 36, episode reward: 0.478, mean reward: 0.005 [-0.001, 0.013], mean action: 0.164 [-1.125, 1.201], mean observation: 0.147 [-12.677, 14.274], loss: --, mean_squared_error: --, mean_q: --\n",
      "  372/2000: episode: 4, duration: 2.636s, episode steps: 92, steps per second: 35, episode reward: 0.480, mean reward: 0.005 [0.000, 0.012], mean action: 0.156 [-1.126, 1.144], mean observation: 0.149 [-11.152, 14.306], loss: --, mean_squared_error: --, mean_q: --\n",
      "  463/2000: episode: 5, duration: 2.605s, episode steps: 91, steps per second: 35, episode reward: 0.481, mean reward: 0.005 [0.000, 0.012], mean action: 0.175 [-1.206, 1.212], mean observation: 0.148 [-11.153, 14.279], loss: --, mean_squared_error: --, mean_q: --\n",
      "  555/2000: episode: 6, duration: 2.592s, episode steps: 92, steps per second: 35, episode reward: 0.473, mean reward: 0.005 [-0.000, 0.013], mean action: 0.156 [-1.138, 1.141], mean observation: 0.147 [-11.380, 13.850], loss: --, mean_squared_error: --, mean_q: --\n",
      "  648/2000: episode: 7, duration: 2.591s, episode steps: 93, steps per second: 36, episode reward: 0.483, mean reward: 0.005 [-0.000, 0.012], mean action: 0.150 [-1.143, 1.118], mean observation: 0.149 [-11.209, 14.300], loss: --, mean_squared_error: --, mean_q: --\n",
      "  741/2000: episode: 8, duration: 2.442s, episode steps: 93, steps per second: 38, episode reward: 0.477, mean reward: 0.005 [-0.001, 0.013], mean action: 0.157 [-1.148, 1.183], mean observation: 0.149 [-10.545, 13.974], loss: --, mean_squared_error: --, mean_q: --\n",
      "  833/2000: episode: 9, duration: 2.671s, episode steps: 92, steps per second: 34, episode reward: 0.478, mean reward: 0.005 [-0.000, 0.013], mean action: 0.168 [-1.204, 1.196], mean observation: 0.147 [-11.028, 13.998], loss: --, mean_squared_error: --, mean_q: --\n",
      "  926/2000: episode: 10, duration: 2.600s, episode steps: 93, steps per second: 36, episode reward: 0.475, mean reward: 0.005 [-0.001, 0.012], mean action: 0.133 [-1.174, 1.138], mean observation: 0.148 [-11.820, 14.297], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1018/2000: episode: 11, duration: 2.837s, episode steps: 92, steps per second: 32, episode reward: 0.473, mean reward: 0.005 [-0.000, 0.012], mean action: 0.163 [-1.142, 1.154], mean observation: 0.146 [-12.251, 14.008], loss: 0.000015, mean_squared_error: 0.000030, mean_q: 0.276535\n",
      " 1111/2000: episode: 12, duration: 3.451s, episode steps: 93, steps per second: 27, episode reward: 0.476, mean reward: 0.005 [-0.001, 0.013], mean action: 0.153 [-1.146, 1.162], mean observation: 0.148 [-10.687, 14.131], loss: 0.000047, mean_squared_error: 0.000094, mean_q: 0.275545\n",
      " 1202/2000: episode: 13, duration: 3.353s, episode steps: 91, steps per second: 27, episode reward: 0.481, mean reward: 0.005 [0.000, 0.012], mean action: 0.163 [-1.114, 1.157], mean observation: 0.148 [-10.787, 14.105], loss: 0.000042, mean_squared_error: 0.000085, mean_q: 0.273031\n",
      " 1317/2000: episode: 14, duration: 4.004s, episode steps: 115, steps per second: 29, episode reward: 0.481, mean reward: 0.004 [-0.001, 0.011], mean action: 0.161 [-1.118, 1.253], mean observation: 0.143 [-14.267, 13.931], loss: 0.000036, mean_squared_error: 0.000072, mean_q: 0.277621\n",
      " 1422/2000: episode: 15, duration: 3.647s, episode steps: 105, steps per second: 29, episode reward: 0.490, mean reward: 0.005 [-0.000, 0.011], mean action: 0.116 [-1.130, 1.156], mean observation: 0.148 [-21.578, 14.242], loss: 0.000089, mean_squared_error: 0.000178, mean_q: 0.273887\n",
      " 1512/2000: episode: 16, duration: 3.240s, episode steps: 90, steps per second: 28, episode reward: 0.478, mean reward: 0.005 [-0.000, 0.012], mean action: 0.154 [-1.152, 1.187], mean observation: 0.150 [-10.466, 13.940], loss: 0.000044, mean_squared_error: 0.000087, mean_q: 0.274495\n",
      " 1603/2000: episode: 17, duration: 3.234s, episode steps: 91, steps per second: 28, episode reward: 0.484, mean reward: 0.005 [-0.000, 0.013], mean action: 0.165 [-1.068, 1.164], mean observation: 0.150 [-10.611, 14.037], loss: 0.000021, mean_squared_error: 0.000041, mean_q: 0.277409\n",
      " 1694/2000: episode: 18, duration: 3.357s, episode steps: 91, steps per second: 27, episode reward: 0.484, mean reward: 0.005 [-0.000, 0.012], mean action: 0.158 [-1.113, 1.159], mean observation: 0.150 [-11.183, 14.322], loss: 0.000044, mean_squared_error: 0.000087, mean_q: 0.270501\n",
      " 1788/2000: episode: 19, duration: 3.381s, episode steps: 94, steps per second: 28, episode reward: 0.479, mean reward: 0.005 [-0.001, 0.011], mean action: 0.176 [-1.182, 1.176], mean observation: 0.150 [-11.389, 14.426], loss: 0.000050, mean_squared_error: 0.000100, mean_q: 0.267013\n",
      "done, took 63.508 seconds\n",
      "\n",
      "\n",
      "iteration: 391\n",
      "Training for 2000 steps ...\n",
      "  143/2000: episode: 1, duration: 3.574s, episode steps: 143, steps per second: 40, episode reward: 0.478, mean reward: 0.003 [-0.001, 0.010], mean action: 0.155 [-1.183, 1.190], mean observation: 0.137 [-20.448, 14.184], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1143/2000: episode: 2, duration: 23.357s, episode steps: 1000, steps per second: 43, episode reward: 0.120, mean reward: 0.000 [-0.002, 0.011], mean action: 0.068 [-1.436, 1.347], mean observation: 0.117 [-17.984, 13.938], loss: 0.000044, mean_squared_error: 0.000088, mean_q: 0.272415\n",
      " 1257/2000: episode: 3, duration: 4.322s, episode steps: 114, steps per second: 26, episode reward: 0.482, mean reward: 0.004 [-0.002, 0.011], mean action: 0.146 [-1.197, 1.119], mean observation: 0.143 [-14.910, 14.416], loss: 0.000046, mean_squared_error: 0.000093, mean_q: 0.275152\n",
      " 1363/2000: episode: 4, duration: 3.725s, episode steps: 106, steps per second: 28, episode reward: 0.484, mean reward: 0.005 [-0.000, 0.011], mean action: 0.182 [-1.121, 1.157], mean observation: 0.147 [-15.674, 14.211], loss: 0.000105, mean_squared_error: 0.000210, mean_q: 0.273097\n",
      " 1494/2000: episode: 5, duration: 4.518s, episode steps: 131, steps per second: 29, episode reward: 0.488, mean reward: 0.004 [-0.000, 0.010], mean action: 0.179 [-1.138, 1.308], mean observation: 0.141 [-15.655, 14.205], loss: 0.000039, mean_squared_error: 0.000079, mean_q: 0.273283\n",
      " 1585/2000: episode: 6, duration: 3.409s, episode steps: 91, steps per second: 27, episode reward: 0.483, mean reward: 0.005 [-0.001, 0.012], mean action: 0.191 [-1.139, 1.323], mean observation: 0.151 [-10.747, 14.078], loss: 0.000015, mean_squared_error: 0.000030, mean_q: 0.274748\n",
      " 1730/2000: episode: 7, duration: 5.438s, episode steps: 145, steps per second: 27, episode reward: 0.464, mean reward: 0.003 [-0.002, 0.010], mean action: 0.139 [-1.220, 1.185], mean observation: 0.136 [-17.058, 14.189], loss: 0.000061, mean_squared_error: 0.000122, mean_q: 0.272808\n",
      " 1957/2000: episode: 8, duration: 7.595s, episode steps: 227, steps per second: 30, episode reward: 0.485, mean reward: 0.002 [-0.001, 0.011], mean action: 0.109 [-1.381, 1.231], mean observation: 0.131 [-18.017, 14.387], loss: 0.000083, mean_squared_error: 0.000166, mean_q: 0.271899\n",
      "done, took 58.026 seconds\n",
      "\n",
      "\n",
      "iteration: 392\n",
      "Training for 2000 steps ...\n",
      "   90/2000: episode: 1, duration: 2.521s, episode steps: 90, steps per second: 36, episode reward: 0.484, mean reward: 0.005 [-0.001, 0.012], mean action: 0.210 [-1.115, 1.175], mean observation: 0.151 [-10.234, 13.896], loss: --, mean_squared_error: --, mean_q: --\n",
      "  180/2000: episode: 2, duration: 2.506s, episode steps: 90, steps per second: 36, episode reward: 0.481, mean reward: 0.005 [-0.000, 0.012], mean action: 0.168 [-1.131, 1.159], mean observation: 0.149 [-11.185, 14.365], loss: --, mean_squared_error: --, mean_q: --\n",
      "  270/2000: episode: 3, duration: 2.506s, episode steps: 90, steps per second: 36, episode reward: 0.482, mean reward: 0.005 [-0.001, 0.012], mean action: 0.206 [-1.115, 1.163], mean observation: 0.152 [-11.161, 14.388], loss: --, mean_squared_error: --, mean_q: --\n",
      "  359/2000: episode: 4, duration: 2.504s, episode steps: 89, steps per second: 36, episode reward: 0.474, mean reward: 0.005 [-0.001, 0.012], mean action: 0.188 [-1.242, 1.217], mean observation: 0.150 [-10.676, 14.148], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  449/2000: episode: 5, duration: 2.533s, episode steps: 90, steps per second: 36, episode reward: 0.479, mean reward: 0.005 [-0.001, 0.012], mean action: 0.185 [-1.138, 1.171], mean observation: 0.150 [-10.600, 14.127], loss: --, mean_squared_error: --, mean_q: --\n",
      "  541/2000: episode: 6, duration: 2.662s, episode steps: 92, steps per second: 35, episode reward: 0.484, mean reward: 0.005 [-0.001, 0.012], mean action: 0.185 [-1.147, 1.228], mean observation: 0.152 [-11.228, 14.427], loss: --, mean_squared_error: --, mean_q: --\n",
      "  631/2000: episode: 7, duration: 2.561s, episode steps: 90, steps per second: 35, episode reward: 0.481, mean reward: 0.005 [-0.001, 0.013], mean action: 0.213 [-1.121, 1.176], mean observation: 0.152 [-10.877, 14.299], loss: --, mean_squared_error: --, mean_q: --\n",
      "  720/2000: episode: 8, duration: 2.473s, episode steps: 89, steps per second: 36, episode reward: 0.476, mean reward: 0.005 [-0.000, 0.012], mean action: 0.199 [-1.161, 1.149], mean observation: 0.150 [-10.608, 14.126], loss: --, mean_squared_error: --, mean_q: --\n",
      "  810/2000: episode: 9, duration: 2.530s, episode steps: 90, steps per second: 36, episode reward: 0.475, mean reward: 0.005 [-0.001, 0.012], mean action: 0.204 [-1.153, 1.207], mean observation: 0.151 [-10.519, 14.161], loss: --, mean_squared_error: --, mean_q: --\n",
      "  900/2000: episode: 10, duration: 2.578s, episode steps: 90, steps per second: 35, episode reward: 0.485, mean reward: 0.005 [-0.000, 0.012], mean action: 0.196 [-1.144, 1.120], mean observation: 0.150 [-10.352, 13.989], loss: --, mean_squared_error: --, mean_q: --\n",
      "  990/2000: episode: 11, duration: 2.632s, episode steps: 90, steps per second: 34, episode reward: 0.482, mean reward: 0.005 [-0.000, 0.012], mean action: 0.210 [-1.315, 1.143], mean observation: 0.149 [-11.312, 14.482], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1080/2000: episode: 12, duration: 3.400s, episode steps: 90, steps per second: 26, episode reward: 0.483, mean reward: 0.005 [-0.000, 0.012], mean action: 0.192 [-1.118, 1.153], mean observation: 0.149 [-11.604, 14.627], loss: 0.000019, mean_squared_error: 0.000038, mean_q: 0.266751\n",
      " 1170/2000: episode: 13, duration: 3.428s, episode steps: 90, steps per second: 26, episode reward: 0.477, mean reward: 0.005 [-0.001, 0.012], mean action: 0.190 [-1.157, 1.150], mean observation: 0.150 [-10.960, 14.267], loss: 0.000076, mean_squared_error: 0.000152, mean_q: 0.270022\n",
      " 1296/2000: episode: 14, duration: 5.072s, episode steps: 126, steps per second: 25, episode reward: 0.495, mean reward: 0.004 [-0.002, 0.012], mean action: 0.141 [-1.197, 1.172], mean observation: 0.143 [-11.089, 14.351], loss: 0.000050, mean_squared_error: 0.000101, mean_q: 0.275031\n",
      " 1412/2000: episode: 15, duration: 4.558s, episode steps: 116, steps per second: 25, episode reward: 0.471, mean reward: 0.004 [-0.004, 0.011], mean action: 0.133 [-1.268, 1.245], mean observation: 0.142 [-21.751, 14.461], loss: 0.000036, mean_squared_error: 0.000071, mean_q: 0.268264\n",
      "done, took 46.598 seconds\n",
      "\n",
      "\n",
      "iteration: 393\n",
      "Training for 2000 steps ...\n",
      "  131/2000: episode: 1, duration: 3.796s, episode steps: 131, steps per second: 35, episode reward: 0.480, mean reward: 0.004 [-0.002, 0.010], mean action: 0.189 [-1.160, 1.199], mean observation: 0.141 [-20.085, 14.172], loss: --, mean_squared_error: --, mean_q: --\n",
      "  260/2000: episode: 2, duration: 3.622s, episode steps: 129, steps per second: 36, episode reward: 0.488, mean reward: 0.004 [-0.002, 0.011], mean action: 0.181 [-1.164, 1.250], mean observation: 0.142 [-20.131, 14.367], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    }
   ],
   "source": [
    "for i in range(100): # Train in smaller batches to allow for interuption\n",
    "    print(\"\\n\\niteration:\",i)\n",
    "    agent.fit(nb_steps=2000, visualize=False, verbose=2)\n",
    "    ## Always save new weights\n",
    "    agent.save_weights( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(nb_episodes=5, visualize=True, nb_max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
