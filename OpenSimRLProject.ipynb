{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Sim RL Training\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Environment\n",
    "from osim.env import L2RunEnv as ENV # rename environment to be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.processors import WhiteningNormalizerProcessor\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class\n",
    "Reference: https://github.com/keras-rl/keras-rl/blob/master/examples/ddpg_mujoco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,env):\n",
    "        nb_actions = env.action_space.shape[0]\n",
    "        \n",
    "        self.env = env\n",
    "        self.actor = self.build_actor(env)\n",
    "        self.critic, action_input = self.build_critic(env)\n",
    "        self.loss = self.build_loss()\n",
    "\n",
    "        self.memory = SequentialMemory(limit=100000, window_length=1)\n",
    "        self.random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.1)\n",
    "        self.agent = DDPGAgent(   nb_actions=nb_actions, actor=self.actor, \n",
    "                                  critic=self.critic, critic_action_input=action_input,\n",
    "                                  memory=self.memory, nb_steps_warmup_critic=1000, \n",
    "                                  nb_steps_warmup_actor=1000,\n",
    "                                  random_process=self.random_process, \n",
    "                                  gamma=.99, target_model_update=1e-3,\n",
    "                                  processor=WhiteningNormalizerProcessor()  )\n",
    "        self.agent.compile([Adam(lr=1e-4), Adam(lr=1e-3)], metrics=self.loss)\n",
    "\n",
    "    def build_loss(self):\n",
    "        return ['mse']\n",
    "\n",
    "    def build_actor(self,env):\n",
    "        nb_actions = env.action_space.shape[0]\n",
    "        actor = Sequential()\n",
    "        actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "        actor.add(Dense(400))\n",
    "        actor.add(Activation('relu'))\n",
    "        actor.add(Dense(300))\n",
    "        actor.add(Activation('relu'))\n",
    "        actor.add(Dense(nb_actions,\n",
    "                        activation='tanh',\n",
    "                        kernel_constraint=  keras.constraints.min_max_norm(\n",
    "                                            min_value=0,\n",
    "                                            max_value=nb_actions,\n",
    "                                            axis=1) ) )\n",
    "        actor.summary()\n",
    "\n",
    "        inD = Input(shape=(1,) + env.observation_space.shape)\n",
    "        out = actor(inD)\n",
    "\n",
    "        return Model(inD,out)\n",
    "\n",
    "    def build_critic(self,env):\n",
    "        nb_actions = env.action_space.shape[0]\n",
    "        action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "        observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "        flattened_observation = Flatten()(observation_input)\n",
    "        x = Dense(400)(flattened_observation)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Concatenate()([x, action_input])\n",
    "        x = Dense(300)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('linear')(x)\n",
    "\n",
    "        critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "        critic.summary()\n",
    "\n",
    "        return critic, action_input\n",
    "    \n",
    "    def fit(self, **kwargs):\n",
    "        return self.agent.fit(self.env,**kwargs)\n",
    "    \n",
    "    def test(self, **kwargs):\n",
    "        return self.agent.test(self.env,**kwargs)\n",
    "    \n",
    "    def save_weights(self,filename='ddpg_{}_weights.h5f'):\n",
    "        self.agent.save_weights(filename.format(\"opensim\"), overwrite=True)\n",
    "        \n",
    "    def load_weights(self,filename='ddpg_{}_weights.h5f'):\n",
    "        self.agent.load_weights(filename.format(\"opensim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEnv(ENV):\n",
    "    pass\n",
    "# TODO: define virtual assistant forces on agent\n",
    "# TODO: define search through easier environments\n",
    "# TODO: make environment harder once the agent has trained for challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Simulation\n",
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quinn/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env = TrainEnv(visualize=False)\n",
    "observation = env.reset( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 41)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 400)               16800     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 18)                5418      \n",
      "=================================================================\n",
      "Total params: 142,518\n",
      "Trainable params: 142,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  (None, 1, 41)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 41)           0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 400)          16800       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 400)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 418)          0           activation_5[0][0]               \n",
      "                                                                 action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 300)          125700      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 300)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            301         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,801\n",
      "Trainable params: 142,801\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 steps ...\n",
      "   82/2000: episode: 1, duration: 8.452s, episode steps: 82, steps per second: 10, episode reward: 0.345, mean reward: 0.004 [0.001, 0.006], mean action: -0.100 [-0.895, 0.780], mean observation: -0.013 [-6.145, 5.816], loss: --, mean_squared_error: --, mean_q: --\n",
      "  220/2000: episode: 2, duration: 4.299s, episode steps: 138, steps per second: 32, episode reward: 0.528, mean reward: 0.004 [-0.000, 0.011], mean action: -0.056 [-0.888, 0.628], mean observation: 0.140 [-4.317, 6.370], loss: --, mean_squared_error: --, mean_q: --\n",
      "  349/2000: episode: 3, duration: 26.163s, episode steps: 129, steps per second: 5, episode reward: -0.539, mean reward: -0.004 [-0.011, 0.001], mean action: -0.036 [-0.817, 0.844], mean observation: 0.071 [-4.258, 4.407], loss: --, mean_squared_error: --, mean_q: --\n",
      "  444/2000: episode: 4, duration: 6.436s, episode steps: 95, steps per second: 15, episode reward: 0.330, mean reward: 0.003 [0.001, 0.007], mean action: -0.071 [-0.894, 0.625], mean observation: 0.016 [-6.046, 5.882], loss: --, mean_squared_error: --, mean_q: --\n",
      "  534/2000: episode: 5, duration: 6.775s, episode steps: 90, steps per second: 13, episode reward: 0.315, mean reward: 0.004 [0.001, 0.007], mean action: -0.072 [-0.846, 0.590], mean observation: 0.012 [-6.328, 6.036], loss: --, mean_squared_error: --, mean_q: --\n",
      "  617/2000: episode: 6, duration: 6.823s, episode steps: 83, steps per second: 12, episode reward: 0.273, mean reward: 0.003 [0.001, 0.007], mean action: -0.075 [-0.784, 0.605], mean observation: -0.005 [-6.020, 5.979], loss: --, mean_squared_error: --, mean_q: --\n",
      "  751/2000: episode: 7, duration: 3.517s, episode steps: 134, steps per second: 38, episode reward: 0.538, mean reward: 0.004 [0.001, 0.012], mean action: -0.017 [-1.092, 0.710], mean observation: 0.142 [-3.805, 5.314], loss: --, mean_squared_error: --, mean_q: --\n",
      "  889/2000: episode: 8, duration: 3.534s, episode steps: 138, steps per second: 39, episode reward: 0.551, mean reward: 0.004 [0.001, 0.015], mean action: -0.016 [-0.931, 0.695], mean observation: 0.141 [-3.745, 5.410], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1027/2000: episode: 9, duration: 7.662s, episode steps: 138, steps per second: 18, episode reward: 0.413, mean reward: 0.003 [-0.006, 0.017], mean action: -0.073 [-1.032, 0.920], mean observation: 0.167 [-3.413, 5.118], loss: 0.029525, mean_squared_error: 0.059049, mean_q: 0.282023\n",
      " 1231/2000: episode: 10, duration: 601.437s, episode steps: 204, steps per second: 0, episode reward: 0.627, mean reward: 0.003 [-0.007, 0.012], mean action: 0.281 [-1.143, 1.141], mean observation: 0.114 [-9.787, 9.025], loss: 0.004078, mean_squared_error: 0.008155, mean_q: 0.346760\n",
      " 1438/2000: episode: 11, duration: 7.867s, episode steps: 207, steps per second: 26, episode reward: -0.889, mean reward: -0.004 [-0.021, 0.008], mean action: 0.481 [-1.218, 1.155], mean observation: 0.035 [-8.948, 12.225], loss: 0.004762, mean_squared_error: 0.009524, mean_q: 0.465538\n",
      " 1556/2000: episode: 12, duration: 4.638s, episode steps: 118, steps per second: 25, episode reward: -0.762, mean reward: -0.006 [-0.021, 0.010], mean action: 0.344 [-1.110, 1.187], mean observation: 0.013 [-13.922, 14.319], loss: 0.005987, mean_squared_error: 0.011975, mean_q: 0.545556\n",
      " 1674/2000: episode: 13, duration: 4.598s, episode steps: 118, steps per second: 26, episode reward: -0.769, mean reward: -0.007 [-0.021, 0.010], mean action: 0.337 [-1.089, 1.190], mean observation: 0.011 [-13.964, 14.313], loss: 0.002592, mean_squared_error: 0.005184, mean_q: 0.587086\n",
      " 1797/2000: episode: 14, duration: 4.296s, episode steps: 123, steps per second: 29, episode reward: -0.780, mean reward: -0.006 [-0.020, 0.010], mean action: 0.196 [-1.219, 1.173], mean observation: 0.016 [-16.364, 15.061], loss: 0.004007, mean_squared_error: 0.008014, mean_q: 0.634049\n",
      " 1934/2000: episode: 15, duration: 4.650s, episode steps: 137, steps per second: 29, episode reward: -0.803, mean reward: -0.006 [-0.020, 0.011], mean action: 0.012 [-1.174, 1.193], mean observation: 0.021 [-9.059, 11.272], loss: 0.002184, mean_squared_error: 0.004367, mean_q: 0.669437\n",
      "done, took 705.155 seconds\n",
      "Training for 2000 steps ...\n",
      "  126/2000: episode: 1, duration: 5.894s, episode steps: 126, steps per second: 21, episode reward: -0.763, mean reward: -0.006 [-0.021, 0.012], mean action: 0.327 [-1.087, 1.219], mean observation: 0.020 [-9.343, 11.401], loss: --, mean_squared_error: --, mean_q: --\n",
      "  253/2000: episode: 2, duration: 5.944s, episode steps: 127, steps per second: 21, episode reward: -0.779, mean reward: -0.006 [-0.021, 0.013], mean action: 0.303 [-1.259, 1.099], mean observation: 0.018 [-12.696, 14.653], loss: --, mean_squared_error: --, mean_q: --\n",
      "  380/2000: episode: 3, duration: 6.058s, episode steps: 127, steps per second: 21, episode reward: -0.771, mean reward: -0.006 [-0.021, 0.013], mean action: 0.334 [-1.167, 1.191], mean observation: 0.019 [-9.291, 10.627], loss: --, mean_squared_error: --, mean_q: --\n",
      "  506/2000: episode: 4, duration: 5.706s, episode steps: 126, steps per second: 22, episode reward: -0.778, mean reward: -0.006 [-0.021, 0.012], mean action: 0.317 [-1.142, 1.157], mean observation: 0.020 [-13.195, 14.534], loss: --, mean_squared_error: --, mean_q: --\n",
      "  633/2000: episode: 5, duration: 6.152s, episode steps: 127, steps per second: 21, episode reward: -0.780, mean reward: -0.006 [-0.021, 0.013], mean action: 0.315 [-1.157, 1.161], mean observation: 0.018 [-12.734, 14.376], loss: --, mean_squared_error: --, mean_q: --\n",
      "  759/2000: episode: 6, duration: 6.276s, episode steps: 126, steps per second: 20, episode reward: -0.769, mean reward: -0.006 [-0.021, 0.013], mean action: 0.329 [-1.175, 1.177], mean observation: 0.018 [-12.399, 12.133], loss: --, mean_squared_error: --, mean_q: --\n",
      "  886/2000: episode: 7, duration: 6.355s, episode steps: 127, steps per second: 20, episode reward: -0.778, mean reward: -0.006 [-0.021, 0.013], mean action: 0.336 [-1.077, 1.177], mean observation: 0.018 [-13.258, 14.458], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1013/2000: episode: 8, duration: 7.380s, episode steps: 127, steps per second: 17, episode reward: -0.769, mean reward: -0.006 [-0.020, 0.013], mean action: 0.332 [-1.119, 1.189], mean observation: 0.018 [-14.455, 11.558], loss: 0.002811, mean_squared_error: 0.005622, mean_q: 0.688281\n",
      " 1142/2000: episode: 9, duration: 7.255s, episode steps: 129, steps per second: 18, episode reward: -0.769, mean reward: -0.006 [-0.020, 0.012], mean action: 0.277 [-1.226, 1.217], mean observation: 0.017 [-10.399, 12.711], loss: 0.004091, mean_squared_error: 0.008182, mean_q: 0.662536\n",
      " 1269/2000: episode: 10, duration: 6.545s, episode steps: 127, steps per second: 19, episode reward: -0.777, mean reward: -0.006 [-0.020, 0.011], mean action: 0.287 [-1.211, 1.182], mean observation: 0.019 [-9.928, 11.049], loss: 0.002007, mean_squared_error: 0.004015, mean_q: 0.683649\n",
      " 1397/2000: episode: 11, duration: 6.915s, episode steps: 128, steps per second: 19, episode reward: -0.777, mean reward: -0.006 [-0.021, 0.011], mean action: 0.276 [-1.205, 1.317], mean observation: 0.016 [-9.327, 10.502], loss: 0.001853, mean_squared_error: 0.003705, mean_q: 0.699201\n",
      " 1523/2000: episode: 12, duration: 6.325s, episode steps: 126, steps per second: 20, episode reward: -0.765, mean reward: -0.006 [-0.020, 0.012], mean action: 0.319 [-1.123, 1.185], mean observation: 0.018 [-8.929, 10.798], loss: 0.001847, mean_squared_error: 0.003693, mean_q: 0.717578\n",
      " 1649/2000: episode: 13, duration: 6.231s, episode steps: 126, steps per second: 20, episode reward: -0.766, mean reward: -0.006 [-0.021, 0.011], mean action: 0.276 [-1.211, 1.137], mean observation: 0.014 [-8.931, 10.591], loss: 0.001945, mean_squared_error: 0.003891, mean_q: 0.722749\n",
      " 1776/2000: episode: 14, duration: 6.223s, episode steps: 127, steps per second: 20, episode reward: -0.780, mean reward: -0.006 [-0.021, 0.012], mean action: 0.260 [-1.198, 1.217], mean observation: 0.014 [-13.164, 10.623], loss: 0.001453, mean_squared_error: 0.002907, mean_q: 0.731002\n",
      " 1903/2000: episode: 15, duration: 5.868s, episode steps: 127, steps per second: 22, episode reward: -0.800, mean reward: -0.006 [-0.021, 0.011], mean action: 0.262 [-1.109, 1.209], mean observation: 0.013 [-8.881, 10.647], loss: 0.001287, mean_squared_error: 0.002575, mean_q: 0.735981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 100.188 seconds\n",
      "Training for 2000 steps ...\n",
      "  120/2000: episode: 1, duration: 4.426s, episode steps: 120, steps per second: 27, episode reward: -0.760, mean reward: -0.006 [-0.021, 0.009], mean action: 0.261 [-1.084, 1.228], mean observation: 0.012 [-7.950, 9.813], loss: --, mean_squared_error: --, mean_q: --\n",
      "  240/2000: episode: 2, duration: 4.479s, episode steps: 120, steps per second: 27, episode reward: -0.753, mean reward: -0.006 [-0.021, 0.009], mean action: 0.230 [-1.149, 1.259], mean observation: 0.014 [-7.951, 9.848], loss: --, mean_squared_error: --, mean_q: --\n",
      "  360/2000: episode: 3, duration: 4.609s, episode steps: 120, steps per second: 26, episode reward: -0.757, mean reward: -0.006 [-0.021, 0.009], mean action: 0.232 [-1.140, 1.183], mean observation: 0.013 [-7.946, 9.957], loss: --, mean_squared_error: --, mean_q: --\n",
      "  480/2000: episode: 4, duration: 4.493s, episode steps: 120, steps per second: 27, episode reward: -0.759, mean reward: -0.006 [-0.021, 0.009], mean action: 0.216 [-1.148, 1.134], mean observation: 0.013 [-7.972, 9.840], loss: --, mean_squared_error: --, mean_q: --\n",
      "  600/2000: episode: 5, duration: 4.570s, episode steps: 120, steps per second: 26, episode reward: -0.757, mean reward: -0.006 [-0.021, 0.009], mean action: 0.225 [-1.235, 1.200], mean observation: 0.012 [-8.254, 9.955], loss: --, mean_squared_error: --, mean_q: --\n",
      "  721/2000: episode: 6, duration: 4.642s, episode steps: 121, steps per second: 26, episode reward: -0.776, mean reward: -0.006 [-0.021, 0.009], mean action: 0.217 [-1.177, 1.126], mean observation: 0.013 [-8.000, 9.807], loss: --, mean_squared_error: --, mean_q: --\n",
      "  841/2000: episode: 7, duration: 4.607s, episode steps: 120, steps per second: 26, episode reward: -0.753, mean reward: -0.006 [-0.021, 0.009], mean action: 0.246 [-1.185, 1.264], mean observation: 0.015 [-9.053, 10.956], loss: --, mean_squared_error: --, mean_q: --\n",
      "  962/2000: episode: 8, duration: 4.628s, episode steps: 121, steps per second: 26, episode reward: -0.771, mean reward: -0.006 [-0.021, 0.009], mean action: 0.240 [-1.125, 1.206], mean observation: 0.014 [-8.749, 10.673], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1082/2000: episode: 9, duration: 5.391s, episode steps: 120, steps per second: 22, episode reward: -0.747, mean reward: -0.006 [-0.020, 0.009], mean action: 0.245 [-1.217, 1.211], mean observation: 0.016 [-7.990, 9.912], loss: 0.001338, mean_squared_error: 0.002676, mean_q: 0.709501\n",
      " 1202/2000: episode: 10, duration: 6.120s, episode steps: 120, steps per second: 20, episode reward: -0.741, mean reward: -0.006 [-0.019, 0.008], mean action: 0.311 [-1.146, 1.126], mean observation: 0.035 [-15.066, 9.982], loss: 0.001030, mean_squared_error: 0.002060, mean_q: 0.712444\n",
      " 1320/2000: episode: 11, duration: 5.418s, episode steps: 118, steps per second: 22, episode reward: -0.738, mean reward: -0.006 [-0.020, 0.009], mean action: 0.378 [-1.124, 1.104], mean observation: 0.014 [-17.551, 10.475], loss: 0.001057, mean_squared_error: 0.002114, mean_q: 0.725248\n",
      " 1435/2000: episode: 12, duration: 5.402s, episode steps: 115, steps per second: 21, episode reward: -0.727, mean reward: -0.006 [-0.021, 0.009], mean action: 0.407 [-1.117, 1.160], mean observation: 0.022 [-8.225, 10.957], loss: 0.002273, mean_squared_error: 0.004545, mean_q: 0.719770\n",
      " 1551/2000: episode: 13, duration: 5.672s, episode steps: 116, steps per second: 20, episode reward: -0.737, mean reward: -0.006 [-0.020, 0.008], mean action: 0.414 [-1.104, 1.112], mean observation: 0.016 [-14.048, 10.794], loss: 0.003570, mean_squared_error: 0.007139, mean_q: 0.721590\n",
      " 1657/2000: episode: 14, duration: 4.751s, episode steps: 106, steps per second: 22, episode reward: -0.671, mean reward: -0.006 [-0.020, 0.007], mean action: 0.398 [-1.153, 1.202], mean observation: 0.020 [-10.644, 6.809], loss: 0.001980, mean_squared_error: 0.003961, mean_q: 0.715346\n",
      " 1768/2000: episode: 15, duration: 4.968s, episode steps: 111, steps per second: 22, episode reward: -0.698, mean reward: -0.006 [-0.020, 0.008], mean action: 0.456 [-1.142, 1.277], mean observation: 0.020 [-14.283, 8.335], loss: 0.001052, mean_squared_error: 0.002103, mean_q: 0.733168\n",
      " 1881/2000: episode: 16, duration: 5.438s, episode steps: 113, steps per second: 21, episode reward: -0.747, mean reward: -0.007 [-0.021, 0.009], mean action: 0.424 [-1.144, 1.168], mean observation: 0.015 [-14.048, 8.979], loss: 0.001163, mean_squared_error: 0.002327, mean_q: 0.728890\n",
      " 1997/2000: episode: 17, duration: 5.546s, episode steps: 116, steps per second: 21, episode reward: -0.730, mean reward: -0.006 [-0.020, 0.010], mean action: 0.429 [-1.145, 1.182], mean observation: 0.016 [-10.188, 9.847], loss: 0.000844, mean_squared_error: 0.001688, mean_q: 0.728291\n",
      "done, took 85.376 seconds\n",
      "Training for 2000 steps ...\n",
      "  114/2000: episode: 1, duration: 4.562s, episode steps: 114, steps per second: 25, episode reward: -0.732, mean reward: -0.006 [-0.020, 0.010], mean action: 0.470 [-1.103, 1.173], mean observation: 0.017 [-16.578, 9.840], loss: --, mean_squared_error: --, mean_q: --\n",
      "  227/2000: episode: 2, duration: 4.588s, episode steps: 113, steps per second: 25, episode reward: -0.715, mean reward: -0.006 [-0.020, 0.010], mean action: 0.478 [-1.130, 1.228], mean observation: 0.019 [-12.228, 9.867], loss: --, mean_squared_error: --, mean_q: --\n",
      "  340/2000: episode: 3, duration: 4.775s, episode steps: 113, steps per second: 24, episode reward: -0.716, mean reward: -0.006 [-0.020, 0.010], mean action: 0.472 [-1.118, 1.151], mean observation: 0.021 [-7.941, 9.834], loss: --, mean_squared_error: --, mean_q: --\n",
      "  453/2000: episode: 4, duration: 4.756s, episode steps: 113, steps per second: 24, episode reward: -0.713, mean reward: -0.006 [-0.020, 0.010], mean action: 0.479 [-1.149, 1.153], mean observation: 0.019 [-12.992, 9.811], loss: --, mean_squared_error: --, mean_q: --\n",
      "  566/2000: episode: 5, duration: 5.388s, episode steps: 113, steps per second: 21, episode reward: -0.714, mean reward: -0.006 [-0.020, 0.010], mean action: 0.460 [-1.151, 1.199], mean observation: 0.020 [-12.994, 9.935], loss: --, mean_squared_error: --, mean_q: --\n",
      "  680/2000: episode: 6, duration: 4.999s, episode steps: 114, steps per second: 23, episode reward: -0.728, mean reward: -0.006 [-0.020, 0.010], mean action: 0.484 [-1.123, 1.166], mean observation: 0.017 [-12.984, 9.774], loss: --, mean_squared_error: --, mean_q: --\n",
      "  794/2000: episode: 7, duration: 6.201s, episode steps: 114, steps per second: 18, episode reward: -0.722, mean reward: -0.006 [-0.020, 0.010], mean action: 0.483 [-1.070, 1.243], mean observation: 0.019 [-14.187, 9.751], loss: --, mean_squared_error: --, mean_q: --\n",
      "  908/2000: episode: 8, duration: 5.715s, episode steps: 114, steps per second: 20, episode reward: -0.729, mean reward: -0.006 [-0.020, 0.010], mean action: 0.485 [-1.093, 1.283], mean observation: 0.021 [-13.588, 9.856], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1021/2000: episode: 9, duration: 6.511s, episode steps: 113, steps per second: 17, episode reward: -0.712, mean reward: -0.006 [-0.020, 0.010], mean action: 0.466 [-1.203, 1.151], mean observation: 0.022 [-8.002, 9.890], loss: 0.002286, mean_squared_error: 0.004573, mean_q: 0.706576\n",
      " 1136/2000: episode: 10, duration: 6.175s, episode steps: 115, steps per second: 19, episode reward: -0.737, mean reward: -0.006 [-0.021, 0.009], mean action: 0.547 [-1.165, 1.145], mean observation: 0.012 [-16.109, 10.792], loss: 0.000384, mean_squared_error: 0.000769, mean_q: 0.689047\n",
      " 1253/2000: episode: 11, duration: 6.735s, episode steps: 117, steps per second: 17, episode reward: -0.739, mean reward: -0.006 [-0.021, 0.009], mean action: 0.454 [-1.116, 1.149], mean observation: 0.019 [-11.677, 13.132], loss: 0.001821, mean_squared_error: 0.003642, mean_q: 0.701008\n",
      " 1371/2000: episode: 12, duration: 6.542s, episode steps: 118, steps per second: 18, episode reward: -0.732, mean reward: -0.006 [-0.020, 0.009], mean action: 0.423 [-1.150, 1.149], mean observation: 0.018 [-25.483, 12.943], loss: 0.001676, mean_squared_error: 0.003351, mean_q: 0.706477\n",
      " 1486/2000: episode: 13, duration: 6.050s, episode steps: 115, steps per second: 19, episode reward: -0.733, mean reward: -0.006 [-0.021, 0.009], mean action: 0.437 [-1.205, 1.177], mean observation: 0.019 [-18.413, 12.424], loss: 0.000802, mean_squared_error: 0.001604, mean_q: 0.703462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1600/2000: episode: 14, duration: 6.467s, episode steps: 114, steps per second: 18, episode reward: -0.620, mean reward: -0.005 [-0.018, 0.009], mean action: 0.424 [-1.147, 1.262], mean observation: 0.029 [-13.629, 12.752], loss: 0.000730, mean_squared_error: 0.001460, mean_q: 0.699925\n",
      " 1690/2000: episode: 15, duration: 6.472s, episode steps: 90, steps per second: 14, episode reward: 0.004, mean reward: 0.000 [-0.005, 0.010], mean action: 0.378 [-1.259, 1.214], mean observation: 0.022 [-11.431, 12.595], loss: 0.002422, mean_squared_error: 0.004844, mean_q: 0.709731\n",
      " 1814/2000: episode: 16, duration: 6.807s, episode steps: 124, steps per second: 18, episode reward: -0.745, mean reward: -0.006 [-0.020, 0.010], mean action: 0.405 [-1.121, 1.148], mean observation: 0.018 [-22.600, 13.032], loss: 0.000661, mean_squared_error: 0.001321, mean_q: 0.708557\n",
      " 1886/2000: episode: 17, duration: 4.795s, episode steps: 72, steps per second: 15, episode reward: 0.126, mean reward: 0.002 [-0.002, 0.010], mean action: 0.384 [-1.203, 1.086], mean observation: -0.003 [-26.885, 12.658], loss: 0.001179, mean_squared_error: 0.002357, mean_q: 0.705257\n",
      " 1969/2000: episode: 18, duration: 6.374s, episode steps: 83, steps per second: 13, episode reward: 0.064, mean reward: 0.001 [-0.004, 0.010], mean action: 0.360 [-1.275, 1.137], mean observation: 0.033 [-25.237, 12.956], loss: 0.000672, mean_squared_error: 0.001344, mean_q: 0.712247\n",
      "done, took 106.405 seconds\n",
      "Training for 2000 steps ...\n",
      "  128/2000: episode: 1, duration: 5.408s, episode steps: 128, steps per second: 24, episode reward: -0.782, mean reward: -0.006 [-0.021, 0.010], mean action: 0.482 [-1.081, 1.231], mean observation: 0.015 [-20.993, 13.071], loss: --, mean_squared_error: --, mean_q: --\n",
      "  258/2000: episode: 2, duration: 5.344s, episode steps: 130, steps per second: 24, episode reward: -0.772, mean reward: -0.006 [-0.020, 0.010], mean action: 0.492 [-1.082, 1.252], mean observation: 0.013 [-25.190, 13.059], loss: --, mean_squared_error: --, mean_q: --\n",
      "  324/2000: episode: 3, duration: 4.420s, episode steps: 66, steps per second: 15, episode reward: 0.162, mean reward: 0.002 [-0.001, 0.010], mean action: 0.405 [-1.149, 1.215], mean observation: 0.021 [-22.621, 12.730], loss: --, mean_squared_error: --, mean_q: --\n",
      "  453/2000: episode: 4, duration: 5.356s, episode steps: 129, steps per second: 24, episode reward: -0.782, mean reward: -0.006 [-0.021, 0.010], mean action: 0.479 [-1.077, 1.150], mean observation: 0.017 [-26.300, 12.816], loss: --, mean_squared_error: --, mean_q: --\n",
      "  582/2000: episode: 5, duration: 5.419s, episode steps: 129, steps per second: 24, episode reward: -0.778, mean reward: -0.006 [-0.021, 0.010], mean action: 0.471 [-1.153, 1.288], mean observation: 0.014 [-25.958, 13.067], loss: --, mean_squared_error: --, mean_q: --\n",
      "  649/2000: episode: 6, duration: 4.469s, episode steps: 67, steps per second: 15, episode reward: 0.157, mean reward: 0.002 [-0.001, 0.010], mean action: 0.383 [-1.103, 1.142], mean observation: 0.023 [-25.157, 12.677], loss: --, mean_squared_error: --, mean_q: --\n",
      "  775/2000: episode: 7, duration: 5.489s, episode steps: 126, steps per second: 23, episode reward: -0.756, mean reward: -0.006 [-0.021, 0.010], mean action: 0.471 [-1.260, 1.231], mean observation: 0.020 [-17.426, 12.795], loss: --, mean_squared_error: --, mean_q: --\n",
      "  843/2000: episode: 8, duration: 4.479s, episode steps: 68, steps per second: 15, episode reward: 0.151, mean reward: 0.002 [-0.001, 0.010], mean action: 0.400 [-1.151, 1.347], mean observation: 0.019 [-22.524, 12.735], loss: --, mean_squared_error: --, mean_q: --\n",
      "  911/2000: episode: 9, duration: 5.090s, episode steps: 68, steps per second: 13, episode reward: 0.153, mean reward: 0.002 [-0.001, 0.010], mean action: 0.366 [-1.186, 1.084], mean observation: 0.024 [-22.348, 12.698], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1041/2000: episode: 10, duration: 6.618s, episode steps: 130, steps per second: 20, episode reward: -0.789, mean reward: -0.006 [-0.021, 0.010], mean action: 0.481 [-1.119, 1.187], mean observation: 0.015 [-26.875, 12.998], loss: 0.001428, mean_squared_error: 0.002855, mean_q: 0.716505\n",
      " 1165/2000: episode: 11, duration: 6.958s, episode steps: 124, steps per second: 18, episode reward: -0.773, mean reward: -0.006 [-0.020, 0.010], mean action: 0.425 [-1.283, 1.214], mean observation: 0.013 [-24.227, 12.941], loss: 0.000873, mean_squared_error: 0.001746, mean_q: 0.710626\n",
      " 1294/2000: episode: 12, duration: 6.709s, episode steps: 129, steps per second: 19, episode reward: -0.775, mean reward: -0.006 [-0.020, 0.011], mean action: 0.386 [-1.253, 1.146], mean observation: 0.015 [-28.100, 12.808], loss: 0.001397, mean_squared_error: 0.002795, mean_q: 0.718125\n",
      " 1422/2000: episode: 13, duration: 6.323s, episode steps: 128, steps per second: 20, episode reward: -0.780, mean reward: -0.006 [-0.021, 0.011], mean action: 0.358 [-1.167, 1.188], mean observation: 0.017 [-11.071, 12.996], loss: 0.001147, mean_squared_error: 0.002293, mean_q: 0.709332\n",
      " 1489/2000: episode: 14, duration: 5.416s, episode steps: 67, steps per second: 12, episode reward: 0.140, mean reward: 0.002 [-0.002, 0.012], mean action: 0.378 [-1.059, 1.116], mean observation: -0.010 [-25.188, 12.588], loss: 0.001665, mean_squared_error: 0.003330, mean_q: 0.729503\n",
      " 1613/2000: episode: 15, duration: 9.996s, episode steps: 124, steps per second: 12, episode reward: -0.758, mean reward: -0.006 [-0.019, 0.012], mean action: 0.324 [-1.187, 1.182], mean observation: 0.049 [-10.192, 12.917], loss: 0.001856, mean_squared_error: 0.003711, mean_q: 0.706421\n",
      " 1678/2000: episode: 16, duration: 10.785s, episode steps: 65, steps per second: 6, episode reward: 0.159, mean reward: 0.002 [-0.002, 0.012], mean action: 0.363 [-1.051, 1.139], mean observation: -0.010 [-9.741, 12.756], loss: 0.001266, mean_squared_error: 0.002532, mean_q: 0.706829\n",
      " 1747/2000: episode: 17, duration: 12.037s, episode steps: 69, steps per second: 6, episode reward: 0.146, mean reward: 0.002 [-0.002, 0.012], mean action: 0.351 [-1.131, 1.121], mean observation: -0.002 [-10.530, 12.991], loss: 0.001201, mean_squared_error: 0.002403, mean_q: 0.696437\n",
      " 1813/2000: episode: 18, duration: 9.631s, episode steps: 66, steps per second: 7, episode reward: 0.160, mean reward: 0.002 [-0.001, 0.011], mean action: 0.352 [-1.198, 1.200], mean observation: -0.012 [-24.173, 12.755], loss: 0.000696, mean_squared_error: 0.001392, mean_q: 0.708559\n",
      " 1877/2000: episode: 19, duration: 8.818s, episode steps: 64, steps per second: 7, episode reward: 0.164, mean reward: 0.003 [-0.001, 0.012], mean action: 0.323 [-1.142, 1.097], mean observation: -0.013 [-10.719, 13.049], loss: 0.000233, mean_squared_error: 0.000465, mean_q: 0.706444\n",
      " 1941/2000: episode: 20, duration: 8.887s, episode steps: 64, steps per second: 7, episode reward: 0.164, mean reward: 0.003 [-0.001, 0.012], mean action: 0.348 [-1.152, 1.168], mean observation: -0.010 [-9.874, 12.823], loss: 0.000229, mean_squared_error: 0.000457, mean_q: 0.713613\n",
      "done, took 143.370 seconds\n",
      "Training for 2000 steps ...\n",
      "  128/2000: episode: 1, duration: 7.980s, episode steps: 128, steps per second: 16, episode reward: -0.764, mean reward: -0.006 [-0.019, 0.011], mean action: 0.253 [-1.150, 1.233], mean observation: 0.052 [-9.452, 12.480], loss: --, mean_squared_error: --, mean_q: --\n",
      "  256/2000: episode: 2, duration: 6.174s, episode steps: 128, steps per second: 21, episode reward: -0.778, mean reward: -0.006 [-0.020, 0.011], mean action: 0.298 [-1.183, 1.274], mean observation: 0.016 [-9.407, 12.338], loss: --, mean_squared_error: --, mean_q: --\n",
      "  384/2000: episode: 3, duration: 6.477s, episode steps: 128, steps per second: 20, episode reward: -0.775, mean reward: -0.006 [-0.021, 0.011], mean action: 0.267 [-1.192, 1.140], mean observation: 0.016 [-10.010, 12.786], loss: --, mean_squared_error: --, mean_q: --\n",
      "  514/2000: episode: 4, duration: 6.291s, episode steps: 130, steps per second: 21, episode reward: -0.787, mean reward: -0.006 [-0.021, 0.011], mean action: 0.251 [-1.128, 1.164], mean observation: 0.015 [-9.571, 12.702], loss: --, mean_squared_error: --, mean_q: --\n",
      "  642/2000: episode: 5, duration: 6.193s, episode steps: 128, steps per second: 21, episode reward: -0.773, mean reward: -0.006 [-0.021, 0.011], mean action: 0.261 [-1.137, 1.107], mean observation: 0.016 [-9.644, 12.699], loss: --, mean_squared_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  771/2000: episode: 6, duration: 6.045s, episode steps: 129, steps per second: 21, episode reward: -0.786, mean reward: -0.006 [-0.021, 0.011], mean action: 0.278 [-1.067, 1.236], mean observation: 0.015 [-9.515, 12.662], loss: --, mean_squared_error: --, mean_q: --\n",
      "  900/2000: episode: 7, duration: 6.055s, episode steps: 129, steps per second: 21, episode reward: -0.781, mean reward: -0.006 [-0.021, 0.011], mean action: 0.303 [-1.114, 1.215], mean observation: 0.015 [-9.537, 12.662], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1028/2000: episode: 8, duration: 8.070s, episode steps: 128, steps per second: 16, episode reward: -0.760, mean reward: -0.006 [-0.019, 0.011], mean action: 0.247 [-1.107, 1.166], mean observation: 0.051 [-9.601, 12.587], loss: 0.001785, mean_squared_error: 0.003570, mean_q: 0.708173\n",
      " 1252/2000: episode: 9, duration: 13.561s, episode steps: 224, steps per second: 17, episode reward: -0.877, mean reward: -0.004 [-0.021, 0.011], mean action: 0.275 [-1.346, 1.214], mean observation: 0.034 [-19.023, 12.065], loss: 0.001214, mean_squared_error: 0.002428, mean_q: 0.719348\n",
      " 1383/2000: episode: 10, duration: 7.861s, episode steps: 131, steps per second: 17, episode reward: -0.791, mean reward: -0.006 [-0.020, 0.011], mean action: 0.341 [-1.117, 1.145], mean observation: 0.030 [-20.534, 12.075], loss: 0.000736, mean_squared_error: 0.001471, mean_q: 0.704233\n",
      " 1548/2000: episode: 11, duration: 9.831s, episode steps: 165, steps per second: 17, episode reward: -0.849, mean reward: -0.005 [-0.020, 0.012], mean action: 0.502 [-1.136, 1.250], mean observation: 0.031 [-10.322, 12.072], loss: 0.001068, mean_squared_error: 0.002136, mean_q: 0.705851\n",
      " 1670/2000: episode: 12, duration: 7.307s, episode steps: 122, steps per second: 17, episode reward: -0.766, mean reward: -0.006 [-0.020, 0.009], mean action: 0.406 [-1.134, 1.285], mean observation: 0.034 [-25.591, 13.806], loss: 0.000607, mean_squared_error: 0.001215, mean_q: 0.694595\n",
      " 1794/2000: episode: 13, duration: 7.219s, episode steps: 124, steps per second: 17, episode reward: -0.784, mean reward: -0.006 [-0.020, 0.009], mean action: 0.422 [-1.195, 1.261], mean observation: 0.027 [-22.686, 11.821], loss: 0.000466, mean_squared_error: 0.000932, mean_q: 0.709913\n",
      " 1864/2000: episode: 14, duration: 9.602s, episode steps: 70, steps per second: 7, episode reward: 0.092, mean reward: 0.001 [-0.002, 0.004], mean action: 0.300 [-1.175, 1.088], mean observation: 0.027 [-31.997, 11.851], loss: 0.000887, mean_squared_error: 0.001775, mean_q: 0.707758\n",
      " 1935/2000: episode: 15, duration: 9.532s, episode steps: 71, steps per second: 7, episode reward: 0.069, mean reward: 0.001 [-0.002, 0.005], mean action: 0.324 [-1.111, 1.185], mean observation: 0.024 [-25.548, 11.918], loss: 0.000769, mean_squared_error: 0.001539, mean_q: 0.709978\n",
      " 1995/2000: episode: 16, duration: 8.349s, episode steps: 60, steps per second: 7, episode reward: 0.091, mean reward: 0.002 [-0.006, 0.008], mean action: 0.307 [-1.135, 1.089], mean observation: -0.025 [-23.070, 31.287], loss: 0.001820, mean_squared_error: 0.003639, mean_q: 0.702082\n",
      "done, took 127.075 seconds\n",
      "Training for 2000 steps ...\n",
      "   59/2000: episode: 1, duration: 8.215s, episode steps: 59, steps per second: 7, episode reward: 0.068, mean reward: 0.001 [-0.007, 0.006], mean action: 0.318 [-1.066, 1.111], mean observation: -0.043 [-21.880, 29.812], loss: --, mean_squared_error: --, mean_q: --\n",
      "  117/2000: episode: 2, duration: 7.976s, episode steps: 58, steps per second: 7, episode reward: 0.048, mean reward: 0.001 [-0.008, 0.006], mean action: 0.324 [-1.155, 1.198], mean observation: -0.045 [-30.288, 31.935], loss: --, mean_squared_error: --, mean_q: --\n",
      "  177/2000: episode: 3, duration: 7.992s, episode steps: 60, steps per second: 8, episode reward: 0.094, mean reward: 0.002 [-0.006, 0.007], mean action: 0.306 [-1.098, 1.124], mean observation: -0.046 [-30.538, 30.512], loss: --, mean_squared_error: --, mean_q: --\n",
      "  235/2000: episode: 4, duration: 7.959s, episode steps: 58, steps per second: 7, episode reward: 0.071, mean reward: 0.001 [-0.007, 0.007], mean action: 0.323 [-1.129, 1.108], mean observation: -0.032 [-30.803, 30.628], loss: --, mean_squared_error: --, mean_q: --\n",
      "  295/2000: episode: 5, duration: 8.072s, episode steps: 60, steps per second: 7, episode reward: 0.077, mean reward: 0.001 [-0.007, 0.006], mean action: 0.317 [-1.131, 1.182], mean observation: -0.044 [-32.332, 28.273], loss: --, mean_squared_error: --, mean_q: --\n",
      "  349/2000: episode: 6, duration: 8.182s, episode steps: 54, steps per second: 7, episode reward: 0.045, mean reward: 0.001 [-0.007, 0.005], mean action: 0.329 [-1.193, 1.201], mean observation: -0.031 [-15.284, 32.634], loss: --, mean_squared_error: --, mean_q: --\n",
      "  410/2000: episode: 7, duration: 8.091s, episode steps: 61, steps per second: 8, episode reward: 0.084, mean reward: 0.001 [-0.008, 0.009], mean action: 0.309 [-1.216, 1.194], mean observation: -0.028 [-24.732, 30.876], loss: --, mean_squared_error: --, mean_q: --\n",
      "  470/2000: episode: 8, duration: 7.791s, episode steps: 60, steps per second: 8, episode reward: 0.088, mean reward: 0.001 [-0.007, 0.008], mean action: 0.329 [-1.087, 1.180], mean observation: -0.032 [-16.595, 30.734], loss: --, mean_squared_error: --, mean_q: --\n",
      "  532/2000: episode: 9, duration: 7.841s, episode steps: 62, steps per second: 8, episode reward: 0.097, mean reward: 0.002 [-0.008, 0.009], mean action: 0.309 [-1.179, 1.181], mean observation: -0.024 [-31.115, 30.365], loss: --, mean_squared_error: --, mean_q: --\n",
      "<built-in function Manager_integrate> returned a result with an error set\n"
     ]
    }
   ],
   "source": [
    "for i in range(200): # Train in smaller batches to allow for interuption\n",
    "    agent.fit(nb_steps=2000, visualize=False, verbose=2)\n",
    "    ## Always save new weights\n",
    "    agent.save_weights( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(nb_episodes=5, visualize=True, nb_max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
