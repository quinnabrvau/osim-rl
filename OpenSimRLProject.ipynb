{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Sim RL Training\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from random import uniform\n",
    "# Environment\n",
    "from TrainEnv import TrainEnv # rename environment to be used for training\n",
    "# Agent\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quinn Abrahams-Vaugh\\sim\\TrainEnv.py:30: UserWarning: new gravity value too small, setting gravity to 0.3G\n",
      "  warnings.warn('new gravity value too small, setting gravity to 0.3G')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 167)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               33600     \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 22)                2222      \n",
      "=================================================================\n",
      "Total params: 55,922\n",
      "Trainable params: 55,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observation_input (InputLayer)  (None, 1, 167)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 167)          0           observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          16800       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 22)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 122)          0           dense_4[0][0]                    \n",
      "                                                                 action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50)           6150        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            51          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,001\n",
      "Trainable params: 23,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = TrainEnv(visualize=False,integrator_accuracy = 5e-3)\n",
    "env.upd_grav(-9.8+5.0)\n",
    "env.upd_VA(2.0)\n",
    "\n",
    "observation = env.reset( )\n",
    "\n",
    "agent = Agent(env)\n",
    "GlobalAgent = agent\n",
    "T_steps = 5000\n",
    "W_steps = 1000\n",
    "\n",
    "OPEN = False\n",
    "data = np.array([])\n",
    "## resume from previous run\n",
    "if OPEN:\n",
    "    agent.load_weights( )\n",
    "    data = np.load('osim-rl/steps_plot.npz')['data']\n",
    "    h = agent.test_get_steps(nb_episodes=1, visualize=True, nb_max_episode_steps=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Simulation\n",
    "#### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration: 0\n",
      "history []\n",
      "-4.800000000000001 2.0\n",
      "Training for 5000 steps ...\n",
      "  108/5000: episode: 1, duration: 3.519s, episode steps: 108, steps per second: 31, episode reward: 10.572, mean reward: 0.098 [-0.016, 0.131], mean action: 0.516 [-0.205, 1.157], mean observation: -0.315 [-1245.360, 372.475], loss: --, mean_squared_error: --, mean_q: --\n",
      "  255/5000: episode: 2, duration: 3.918s, episode steps: 147, steps per second: 38, episode reward: 14.804, mean reward: 0.101 [-0.017, 0.138], mean action: 0.551 [-0.299, 1.370], mean observation: 0.070 [-597.582, 1559.085], loss: --, mean_squared_error: --, mean_q: --\n",
      "  334/5000: episode: 3, duration: 1.019s, episode steps: 79, steps per second: 78, episode reward: 8.901, mean reward: 0.113 [0.023, 0.141], mean action: 0.582 [-0.026, 1.293], mean observation: 0.205 [-440.791, 620.775], loss: --, mean_squared_error: --, mean_q: --\n",
      "  404/5000: episode: 4, duration: 1.161s, episode steps: 70, steps per second: 60, episode reward: 8.207, mean reward: 0.117 [0.030, 0.145], mean action: 0.482 [-0.285, 1.336], mean observation: 0.175 [-433.182, 1108.715], loss: --, mean_squared_error: --, mean_q: --\n",
      "  573/5000: episode: 5, duration: 3.600s, episode steps: 169, steps per second: 47, episode reward: 18.054, mean reward: 0.107 [-0.011, 0.133], mean action: 0.590 [-0.139, 1.815], mean observation: 0.049 [-457.258, 398.712], loss: --, mean_squared_error: --, mean_q: --\n",
      "  662/5000: episode: 6, duration: 1.810s, episode steps: 89, steps per second: 49, episode reward: 10.337, mean reward: 0.116 [0.027, 0.143], mean action: 0.470 [-0.307, 1.164], mean observation: -0.150 [-439.811, 423.391], loss: --, mean_squared_error: --, mean_q: --\n",
      "  736/5000: episode: 7, duration: 1.189s, episode steps: 74, steps per second: 62, episode reward: 8.448, mean reward: 0.114 [0.016, 0.139], mean action: 0.484 [-0.278, 1.035], mean observation: -0.010 [-451.875, 332.640], loss: --, mean_squared_error: --, mean_q: --\n",
      "  883/5000: episode: 8, duration: 4.015s, episode steps: 147, steps per second: 37, episode reward: 14.975, mean reward: 0.102 [-0.014, 0.138], mean action: 0.504 [-0.150, 1.385], mean observation: -0.289 [-528.706, 514.317], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1002/5000: episode: 9, duration: 3.599s, episode steps: 119, steps per second: 33, episode reward: 12.055, mean reward: 0.101 [-0.007, 0.136], mean action: 0.505 [-0.290, 1.101], mean observation: -0.115 [-1092.504, 1409.957], loss: 0.037770, mean_squared_error: 0.075540, mean_q: -0.350320\n",
      " 1119/5000: episode: 10, duration: 3.930s, episode steps: 117, steps per second: 30, episode reward: 11.551, mean reward: 0.099 [-0.015, 0.138], mean action: 0.476 [-0.396, 1.415], mean observation: 0.013 [-418.609, 1658.554], loss: 0.081789, mean_squared_error: 0.163578, mean_q: -0.138888\n",
      " 1283/5000: episode: 11, duration: 6.505s, episode steps: 164, steps per second: 25, episode reward: 16.911, mean reward: 0.103 [0.007, 0.122], mean action: 0.484 [-0.678, 1.492], mean observation: -0.051 [-470.944, 796.477], loss: 0.007029, mean_squared_error: 0.014058, mean_q: 0.012388\n",
      " 1381/5000: episode: 12, duration: 3.735s, episode steps: 98, steps per second: 26, episode reward: 11.957, mean reward: 0.122 [0.023, 0.144], mean action: 0.388 [-0.591, 1.530], mean observation: 0.241 [-4577.163, 2527.727], loss: 0.002847, mean_squared_error: 0.005694, mean_q: 0.148037\n",
      " 1484/5000: episode: 13, duration: 4.511s, episode steps: 103, steps per second: 23, episode reward: 12.515, mean reward: 0.122 [0.027, 0.137], mean action: 0.523 [-0.498, 1.908], mean observation: -3.073 [-78334.867, 1723.669], loss: 0.012375, mean_squared_error: 0.024750, mean_q: 0.202047\n",
      " 1555/5000: episode: 14, duration: 2.891s, episode steps: 71, steps per second: 25, episode reward: 8.440, mean reward: 0.119 [0.018, 0.134], mean action: 0.470 [-0.343, 1.429], mean observation: 0.418 [-3107.295, 6606.807], loss: 0.006115, mean_squared_error: 0.012230, mean_q: 0.275702\n",
      " 1761/5000: episode: 15, duration: 10.014s, episode steps: 206, steps per second: 21, episode reward: 20.177, mean reward: 0.098 [-0.020, 0.132], mean action: 0.586 [-0.462, 1.468], mean observation: 0.014 [-682.829, 1094.363], loss: 0.004728, mean_squared_error: 0.009457, mean_q: 0.354617\n",
      " 1923/5000: episode: 16, duration: 5.473s, episode steps: 162, steps per second: 30, episode reward: 16.726, mean reward: 0.103 [-0.021, 0.125], mean action: 0.560 [-0.653, 1.814], mean observation: -0.387 [-3198.066, 579.878], loss: 0.002685, mean_squared_error: 0.005371, mean_q: 0.436660\n",
      " 2085/5000: episode: 17, duration: 6.917s, episode steps: 162, steps per second: 23, episode reward: 15.611, mean reward: 0.096 [-0.019, 0.136], mean action: 0.655 [-0.583, 1.774], mean observation: 0.779 [-7184.451, 7256.572], loss: 0.007405, mean_squared_error: 0.014809, mean_q: 0.528705\n",
      " 2241/5000: episode: 18, duration: 6.166s, episode steps: 156, steps per second: 25, episode reward: 15.612, mean reward: 0.100 [-0.003, 0.118], mean action: 0.616 [-0.942, 1.712], mean observation: 0.044 [-707.570, 913.631], loss: 0.001912, mean_squared_error: 0.003825, mean_q: 0.582416\n",
      " 2358/5000: episode: 19, duration: 4.893s, episode steps: 117, steps per second: 24, episode reward: 11.325, mean reward: 0.097 [-0.017, 0.121], mean action: 0.729 [-0.587, 1.475], mean observation: 0.049 [-1748.740, 2405.078], loss: 0.001522, mean_squared_error: 0.003044, mean_q: 0.633547\n",
      " 2468/5000: episode: 20, duration: 4.812s, episode steps: 110, steps per second: 23, episode reward: 10.463, mean reward: 0.095 [-0.010, 0.122], mean action: 0.698 [-0.730, 1.530], mean observation: 0.198 [-1828.681, 2321.092], loss: 0.001596, mean_squared_error: 0.003192, mean_q: 0.672224\n",
      " 2585/5000: episode: 21, duration: 4.899s, episode steps: 117, steps per second: 24, episode reward: 11.406, mean reward: 0.097 [-0.008, 0.122], mean action: 0.735 [-0.607, 1.622], mean observation: 0.081 [-1599.128, 2050.642], loss: 0.001350, mean_squared_error: 0.002700, mean_q: 0.701544\n",
      " 2707/5000: episode: 22, duration: 5.383s, episode steps: 122, steps per second: 23, episode reward: 11.339, mean reward: 0.093 [-0.012, 0.124], mean action: 0.674 [-0.502, 1.676], mean observation: 0.411 [-2420.487, 4006.474], loss: 0.003706, mean_squared_error: 0.007411, mean_q: 0.742723\n",
      " 2817/5000: episode: 23, duration: 5.010s, episode steps: 110, steps per second: 22, episode reward: 10.274, mean reward: 0.093 [-0.012, 0.122], mean action: 0.798 [-0.271, 1.780], mean observation: -0.036 [-2330.521, 1411.982], loss: 0.010587, mean_squared_error: 0.021175, mean_q: 0.774485\n",
      " 2945/5000: episode: 24, duration: 5.270s, episode steps: 128, steps per second: 24, episode reward: 12.437, mean reward: 0.097 [-0.012, 0.122], mean action: 0.715 [-0.279, 1.701], mean observation: -0.152 [-1077.840, 1471.473], loss: 0.001548, mean_squared_error: 0.003096, mean_q: 0.777317\n",
      " 3086/5000: episode: 25, duration: 5.395s, episode steps: 141, steps per second: 26, episode reward: 13.366, mean reward: 0.095 [-0.031, 0.119], mean action: 0.763 [-0.595, 1.568], mean observation: 0.164 [-2366.553, 2836.325], loss: 0.001558, mean_squared_error: 0.003117, mean_q: 0.772955\n",
      " 3249/5000: episode: 26, duration: 5.988s, episode steps: 163, steps per second: 27, episode reward: 17.523, mean reward: 0.108 [-0.017, 0.127], mean action: 0.739 [-0.375, 1.607], mean observation: 0.349 [-2717.744, 3326.125], loss: 0.001820, mean_squared_error: 0.003640, mean_q: 0.809400\n",
      " 3353/5000: episode: 27, duration: 4.382s, episode steps: 104, steps per second: 24, episode reward: 10.001, mean reward: 0.096 [-0.007, 0.122], mean action: 0.763 [-0.270, 1.947], mean observation: 0.264 [-2454.755, 3189.067], loss: 0.003269, mean_squared_error: 0.006538, mean_q: 0.837077\n",
      " 3515/5000: episode: 28, duration: 6.616s, episode steps: 162, steps per second: 24, episode reward: 13.556, mean reward: 0.084 [-0.000, 0.131], mean action: 0.729 [-0.670, 1.834], mean observation: 0.217 [-3012.836, 4124.092], loss: 0.008061, mean_squared_error: 0.016121, mean_q: 0.860366\n",
      " 3595/5000: episode: 29, duration: 2.548s, episode steps: 80, steps per second: 31, episode reward: 8.820, mean reward: 0.110 [0.010, 0.131], mean action: 0.814 [-0.187, 1.581], mean observation: 0.027 [-1290.465, 1165.590], loss: 0.004881, mean_squared_error: 0.009761, mean_q: 0.873580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3663/5000: episode: 30, duration: 1.953s, episode steps: 68, steps per second: 35, episode reward: 8.091, mean reward: 0.119 [0.027, 0.135], mean action: 0.772 [-0.292, 1.432], mean observation: -0.260 [-2243.267, 1163.526], loss: 0.003145, mean_squared_error: 0.006289, mean_q: 0.910682\n",
      " 3797/5000: episode: 31, duration: 5.635s, episode steps: 134, steps per second: 24, episode reward: 13.453, mean reward: 0.100 [-0.002, 0.135], mean action: 0.804 [-0.822, 1.586], mean observation: 0.083 [-1847.725, 1945.513], loss: 0.002968, mean_squared_error: 0.005935, mean_q: 0.935717\n",
      " 3933/5000: episode: 32, duration: 5.241s, episode steps: 136, steps per second: 26, episode reward: 13.254, mean reward: 0.097 [-0.018, 0.133], mean action: 0.827 [-0.846, 1.663], mean observation: 0.078 [-2198.886, 2544.000], loss: 0.003306, mean_squared_error: 0.006612, mean_q: 0.978369\n",
      " 4057/5000: episode: 33, duration: 4.963s, episode steps: 124, steps per second: 25, episode reward: 11.824, mean reward: 0.095 [-0.012, 0.134], mean action: 0.883 [-0.319, 1.587], mean observation: 0.022 [-2548.915, 3152.205], loss: 0.004395, mean_squared_error: 0.008790, mean_q: 0.993001\n",
      " 4182/5000: episode: 34, duration: 4.877s, episode steps: 125, steps per second: 26, episode reward: 12.341, mean reward: 0.099 [-0.021, 0.129], mean action: 0.944 [-1.087, 1.523], mean observation: -0.198 [-2101.828, 1235.185], loss: 0.003378, mean_squared_error: 0.006755, mean_q: 1.026044\n",
      " 4308/5000: episode: 35, duration: 5.070s, episode steps: 126, steps per second: 25, episode reward: 11.943, mean reward: 0.095 [-0.011, 0.132], mean action: 0.886 [-0.577, 1.471], mean observation: 0.217 [-2223.438, 2631.198], loss: 0.047748, mean_squared_error: 0.095496, mean_q: 1.041385\n",
      " 4429/5000: episode: 36, duration: 4.536s, episode steps: 121, steps per second: 27, episode reward: 12.257, mean reward: 0.101 [-0.017, 0.128], mean action: 0.886 [-0.147, 1.636], mean observation: -0.070 [-911.530, 1212.934], loss: 0.009847, mean_squared_error: 0.019694, mean_q: 1.052765\n",
      " 4547/5000: episode: 37, duration: 4.857s, episode steps: 118, steps per second: 24, episode reward: 11.410, mean reward: 0.097 [-0.011, 0.128], mean action: 0.922 [-0.185, 1.726], mean observation: 0.119 [-1483.874, 1700.059], loss: 0.007400, mean_squared_error: 0.014799, mean_q: 1.064325\n",
      " 4660/5000: episode: 38, duration: 4.490s, episode steps: 113, steps per second: 25, episode reward: 10.941, mean reward: 0.097 [-0.012, 0.127], mean action: 0.950 [-0.579, 1.624], mean observation: -0.136 [-2065.151, 1336.828], loss: 0.010326, mean_squared_error: 0.020652, mean_q: 1.072019\n",
      " 4770/5000: episode: 39, duration: 4.293s, episode steps: 110, steps per second: 26, episode reward: 10.659, mean reward: 0.097 [-0.017, 0.126], mean action: 0.841 [-0.740, 1.440], mean observation: -0.075 [-2638.447, 1802.048], loss: 0.006965, mean_squared_error: 0.013930, mean_q: 1.117281\n",
      " 4884/5000: episode: 40, duration: 4.411s, episode steps: 114, steps per second: 26, episode reward: 11.090, mean reward: 0.097 [-0.019, 0.127], mean action: 0.922 [-0.437, 1.827], mean observation: -0.173 [-1934.925, 1330.215], loss: 0.015472, mean_squared_error: 0.030944, mean_q: 1.148832\n",
      " 4998/5000: episode: 41, duration: 4.506s, episode steps: 114, steps per second: 25, episode reward: 11.014, mean reward: 0.097 [-0.012, 0.128], mean action: 0.920 [-0.205, 1.679], mean observation: -0.213 [-933.822, 1081.621], loss: 0.004819, mean_squared_error: 0.009638, mean_q: 1.150015\n",
      "done, took 184.184 seconds\n",
      "\n",
      "\n",
      "do symetric loss back propigation\n",
      "\n",
      "\n",
      "testing\n",
      "gravity: -4.800000000000001 VA: 2.0\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 11.231, steps: 116\n",
      "\n",
      "\n",
      "iteration: 1\n",
      "history [116.]\n",
      "-4.800000000000001 2.0\n",
      "Training for 5000 steps ...\n",
      "  126/5000: episode: 1, duration: 3.769s, episode steps: 126, steps per second: 33, episode reward: 12.232, mean reward: 0.097 [-0.014, 0.127], mean action: 0.946 [-0.124, 1.751], mean observation: -0.053 [-1477.278, 1329.758], loss: --, mean_squared_error: --, mean_q: --\n",
      "  250/5000: episode: 2, duration: 3.629s, episode steps: 124, steps per second: 34, episode reward: 12.014, mean reward: 0.097 [-0.016, 0.124], mean action: 0.903 [-0.235, 1.904], mean observation: -0.230 [-2874.656, 1410.164], loss: --, mean_squared_error: --, mean_q: --\n",
      "  358/5000: episode: 3, duration: 3.337s, episode steps: 108, steps per second: 32, episode reward: 10.337, mean reward: 0.096 [-0.019, 0.127], mean action: 0.937 [-0.369, 1.616], mean observation: 0.158 [-1388.110, 1917.355], loss: --, mean_squared_error: --, mean_q: --\n",
      "  468/5000: episode: 4, duration: 3.331s, episode steps: 110, steps per second: 33, episode reward: 10.568, mean reward: 0.096 [-0.014, 0.126], mean action: 0.894 [-0.506, 1.467], mean observation: -0.073 [-1403.031, 1113.201], loss: --, mean_squared_error: --, mean_q: --\n",
      "  582/5000: episode: 5, duration: 3.395s, episode steps: 114, steps per second: 34, episode reward: 11.132, mean reward: 0.098 [-0.009, 0.126], mean action: 0.957 [-0.338, 1.573], mean observation: 0.285 [-2130.059, 3507.933], loss: --, mean_squared_error: --, mean_q: --\n",
      "  699/5000: episode: 6, duration: 3.378s, episode steps: 117, steps per second: 35, episode reward: 11.413, mean reward: 0.098 [-0.019, 0.128], mean action: 0.936 [-0.324, 1.676], mean observation: -0.290 [-1973.454, 1274.761], loss: --, mean_squared_error: --, mean_q: --\n",
      "  808/5000: episode: 7, duration: 3.195s, episode steps: 109, steps per second: 34, episode reward: 10.452, mean reward: 0.096 [-0.020, 0.126], mean action: 0.871 [-0.278, 1.695], mean observation: 0.147 [-1346.453, 1593.426], loss: --, mean_squared_error: --, mean_q: --\n",
      "  927/5000: episode: 8, duration: 3.429s, episode steps: 119, steps per second: 35, episode reward: 11.730, mean reward: 0.099 [-0.018, 0.127], mean action: 0.923 [-0.564, 1.662], mean observation: 0.078 [-1676.206, 1550.203], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1048/5000: episode: 9, duration: 4.043s, episode steps: 121, steps per second: 30, episode reward: 11.778, mean reward: 0.097 [-0.016, 0.128], mean action: 0.923 [-0.141, 1.673], mean observation: -0.247 [-2139.289, 1423.346], loss: 0.004983, mean_squared_error: 0.009967, mean_q: 1.135523\n",
      " 1157/5000: episode: 10, duration: 4.413s, episode steps: 109, steps per second: 25, episode reward: 10.501, mean reward: 0.096 [-0.012, 0.127], mean action: 0.899 [-0.225, 1.467], mean observation: 0.122 [-1923.691, 2114.134], loss: 0.004873, mean_squared_error: 0.009746, mean_q: 1.140032\n",
      " 1276/5000: episode: 11, duration: 4.965s, episode steps: 119, steps per second: 24, episode reward: 11.538, mean reward: 0.097 [-0.018, 0.129], mean action: 0.880 [-0.333, 1.722], mean observation: -0.060 [-1197.118, 1336.800], loss: 0.005855, mean_squared_error: 0.011710, mean_q: 1.156990\n",
      " 1391/5000: episode: 12, duration: 4.725s, episode steps: 115, steps per second: 24, episode reward: 11.185, mean reward: 0.097 [-0.017, 0.122], mean action: 0.935 [-0.439, 1.841], mean observation: -0.243 [-989.157, 1186.529], loss: 0.005557, mean_squared_error: 0.011114, mean_q: 1.193257\n",
      " 1505/5000: episode: 13, duration: 4.598s, episode steps: 114, steps per second: 25, episode reward: 11.003, mean reward: 0.097 [-0.018, 0.127], mean action: 0.903 [-0.561, 2.042], mean observation: 0.159 [-1434.814, 1396.349], loss: 0.007270, mean_squared_error: 0.014540, mean_q: 1.211798\n",
      " 1624/5000: episode: 14, duration: 4.691s, episode steps: 119, steps per second: 25, episode reward: 11.865, mean reward: 0.100 [-0.005, 0.128], mean action: 0.929 [-0.059, 1.780], mean observation: -0.257 [-2210.827, 1182.151], loss: 0.008098, mean_squared_error: 0.016196, mean_q: 1.240296\n",
      " 1730/5000: episode: 15, duration: 4.269s, episode steps: 106, steps per second: 25, episode reward: 10.095, mean reward: 0.095 [-0.019, 0.126], mean action: 0.892 [-0.177, 1.696], mean observation: -0.294 [-2403.913, 1633.162], loss: 0.008334, mean_squared_error: 0.016667, mean_q: 1.260128\n",
      " 1855/5000: episode: 16, duration: 5.172s, episode steps: 125, steps per second: 24, episode reward: 12.202, mean reward: 0.098 [-0.008, 0.125], mean action: 0.916 [-0.423, 1.728], mean observation: -0.260 [-2957.580, 1828.114], loss: 0.007061, mean_squared_error: 0.014122, mean_q: 1.305075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1956/5000: episode: 17, duration: 3.370s, episode steps: 101, steps per second: 30, episode reward: 10.547, mean reward: 0.104 [-0.012, 0.133], mean action: 0.933 [-0.416, 1.431], mean observation: -0.127 [-532.495, 470.870], loss: 0.010136, mean_squared_error: 0.020272, mean_q: 1.302930\n",
      " 2066/5000: episode: 18, duration: 4.118s, episode steps: 110, steps per second: 27, episode reward: 11.071, mean reward: 0.101 [-0.012, 0.130], mean action: 0.930 [-0.155, 1.853], mean observation: -0.104 [-756.230, 449.655], loss: 0.008174, mean_squared_error: 0.016347, mean_q: 1.334475\n",
      " 2174/5000: episode: 19, duration: 3.868s, episode steps: 108, steps per second: 28, episode reward: 10.678, mean reward: 0.099 [-0.019, 0.129], mean action: 1.003 [0.004, 2.124], mean observation: -0.245 [-514.941, 634.213], loss: 0.007807, mean_squared_error: 0.015613, mean_q: 1.342556\n",
      " 2283/5000: episode: 20, duration: 4.001s, episode steps: 109, steps per second: 27, episode reward: 10.926, mean reward: 0.100 [-0.014, 0.130], mean action: 0.936 [-0.295, 1.612], mean observation: -0.049 [-1476.394, 1984.334], loss: 0.007216, mean_squared_error: 0.014433, mean_q: 1.371562\n",
      " 2393/5000: episode: 21, duration: 4.121s, episode steps: 110, steps per second: 27, episode reward: 11.085, mean reward: 0.101 [-0.008, 0.128], mean action: 0.973 [0.011, 1.663], mean observation: -0.332 [-1500.708, 545.007], loss: 0.011455, mean_squared_error: 0.022909, mean_q: 1.392097\n",
      " 2498/5000: episode: 22, duration: 3.933s, episode steps: 105, steps per second: 27, episode reward: 10.270, mean reward: 0.098 [-0.017, 0.131], mean action: 0.981 [0.121, 1.748], mean observation: -0.313 [-901.408, 320.719], loss: 0.011370, mean_squared_error: 0.022741, mean_q: 1.409613\n",
      " 2577/5000: episode: 23, duration: 1.812s, episode steps: 79, steps per second: 44, episode reward: 8.899, mean reward: 0.113 [0.024, 0.133], mean action: 1.008 [0.394, 1.567], mean observation: 0.028 [-532.745, 350.578], loss: 0.012940, mean_squared_error: 0.025880, mean_q: 1.490604\n",
      " 2649/5000: episode: 24, duration: 1.729s, episode steps: 72, steps per second: 42, episode reward: 8.228, mean reward: 0.114 [0.023, 0.135], mean action: 0.972 [0.300, 1.579], mean observation: 0.138 [-537.471, 649.617], loss: 0.008569, mean_squared_error: 0.017138, mean_q: 1.447800\n",
      " 2724/5000: episode: 25, duration: 1.703s, episode steps: 75, steps per second: 44, episode reward: 8.539, mean reward: 0.114 [0.026, 0.135], mean action: 1.054 [0.418, 1.649], mean observation: 0.097 [-541.283, 767.287], loss: 0.010401, mean_squared_error: 0.020803, mean_q: 1.482785\n",
      " 2803/5000: episode: 26, duration: 1.757s, episode steps: 79, steps per second: 45, episode reward: 8.795, mean reward: 0.111 [0.017, 0.132], mean action: 0.997 [0.516, 1.550], mean observation: 0.040 [-535.977, 616.058], loss: 0.016000, mean_squared_error: 0.032000, mean_q: 1.469935\n",
      " 2885/5000: episode: 27, duration: 1.811s, episode steps: 82, steps per second: 45, episode reward: 9.157, mean reward: 0.112 [0.021, 0.132], mean action: 1.007 [0.538, 1.680], mean observation: 0.005 [-533.364, 421.914], loss: 0.015981, mean_squared_error: 0.031962, mean_q: 1.498719\n",
      " 2978/5000: episode: 28, duration: 2.367s, episode steps: 93, steps per second: 39, episode reward: 10.449, mean reward: 0.112 [0.016, 0.134], mean action: 0.971 [0.612, 1.294], mean observation: -0.089 [-525.463, 510.753], loss: 0.013011, mean_squared_error: 0.026023, mean_q: 1.536577\n",
      " 3063/5000: episode: 29, duration: 2.084s, episode steps: 85, steps per second: 41, episode reward: 9.523, mean reward: 0.112 [0.021, 0.132], mean action: 0.967 [0.472, 1.513], mean observation: 0.182 [-532.536, 349.653], loss: 0.009237, mean_squared_error: 0.018474, mean_q: 1.546250\n",
      " 3137/5000: episode: 30, duration: 1.713s, episode steps: 74, steps per second: 43, episode reward: 8.425, mean reward: 0.114 [0.022, 0.134], mean action: 0.980 [0.339, 1.590], mean observation: 0.374 [-533.488, 843.308], loss: 0.012747, mean_squared_error: 0.025495, mean_q: 1.579489\n",
      " 3223/5000: episode: 31, duration: 2.143s, episode steps: 86, steps per second: 40, episode reward: 9.760, mean reward: 0.113 [0.023, 0.134], mean action: 0.991 [0.372, 1.652], mean observation: -0.124 [-1617.058, 641.443], loss: 0.013133, mean_squared_error: 0.026266, mean_q: 1.578687\n",
      " 3309/5000: episode: 32, duration: 1.982s, episode steps: 86, steps per second: 43, episode reward: 9.733, mean reward: 0.113 [0.020, 0.134], mean action: 0.999 [0.499, 1.680], mean observation: -0.051 [-537.519, 455.842], loss: 0.013940, mean_squared_error: 0.027881, mean_q: 1.590726\n",
      " 3387/5000: episode: 33, duration: 1.750s, episode steps: 78, steps per second: 45, episode reward: 8.817, mean reward: 0.113 [0.025, 0.134], mean action: 0.986 [0.545, 1.400], mean observation: 0.125 [-523.666, 589.456], loss: 0.014367, mean_squared_error: 0.028734, mean_q: 1.608557\n",
      " 3459/5000: episode: 34, duration: 1.701s, episode steps: 72, steps per second: 42, episode reward: 8.409, mean reward: 0.117 [0.030, 0.136], mean action: 0.981 [0.203, 1.526], mean observation: 0.092 [-532.899, 431.419], loss: 0.013457, mean_squared_error: 0.026914, mean_q: 1.664567\n",
      " 3534/5000: episode: 35, duration: 1.678s, episode steps: 75, steps per second: 45, episode reward: 8.562, mean reward: 0.114 [0.024, 0.134], mean action: 1.016 [0.524, 1.570], mean observation: 0.215 [-532.716, 691.021], loss: 0.023089, mean_squared_error: 0.046177, mean_q: 1.665259\n",
      " 3625/5000: episode: 36, duration: 2.216s, episode steps: 91, steps per second: 41, episode reward: 10.013, mean reward: 0.110 [0.017, 0.134], mean action: 1.002 [0.521, 1.635], mean observation: 0.067 [-1605.619, 2043.748], loss: 0.017505, mean_squared_error: 0.035010, mean_q: 1.683089\n",
      " 3706/5000: episode: 37, duration: 1.819s, episode steps: 81, steps per second: 45, episode reward: 9.092, mean reward: 0.112 [0.020, 0.133], mean action: 0.988 [0.224, 1.656], mean observation: 0.150 [-537.179, 729.755], loss: 0.037555, mean_squared_error: 0.075111, mean_q: 1.703377\n",
      " 3822/5000: episode: 38, duration: 2.912s, episode steps: 116, steps per second: 40, episode reward: 12.805, mean reward: 0.110 [0.022, 0.131], mean action: 1.051 [0.542, 1.778], mean observation: 0.087 [-547.557, 640.250], loss: 0.013593, mean_squared_error: 0.027186, mean_q: 1.726627\n",
      " 3903/5000: episode: 39, duration: 1.853s, episode steps: 81, steps per second: 44, episode reward: 9.155, mean reward: 0.113 [0.024, 0.135], mean action: 1.047 [0.620, 1.581], mean observation: 0.051 [-534.954, 496.830], loss: 0.012934, mean_squared_error: 0.025869, mean_q: 1.694678\n",
      " 3975/5000: episode: 40, duration: 1.628s, episode steps: 72, steps per second: 44, episode reward: 8.311, mean reward: 0.115 [0.030, 0.136], mean action: 1.004 [0.496, 1.584], mean observation: 0.103 [-546.220, 536.939], loss: 0.013051, mean_squared_error: 0.026102, mean_q: 1.744575\n",
      " 4051/5000: episode: 41, duration: 1.706s, episode steps: 76, steps per second: 45, episode reward: 8.604, mean reward: 0.113 [0.019, 0.135], mean action: 0.939 [0.431, 1.431], mean observation: 0.269 [-535.242, 774.885], loss: 0.015163, mean_squared_error: 0.030326, mean_q: 1.782798\n",
      " 4136/5000: episode: 42, duration: 1.980s, episode steps: 85, steps per second: 43, episode reward: 9.629, mean reward: 0.113 [0.017, 0.134], mean action: 1.003 [0.259, 1.555], mean observation: 0.164 [-534.325, 661.722], loss: 0.018141, mean_squared_error: 0.036281, mean_q: 1.752854\n",
      " 4237/5000: episode: 43, duration: 2.546s, episode steps: 101, steps per second: 40, episode reward: 11.247, mean reward: 0.111 [0.023, 0.132], mean action: 1.013 [0.308, 1.723], mean observation: 0.173 [-533.672, 723.413], loss: 0.011012, mean_squared_error: 0.022025, mean_q: 1.835319\n",
      " 4313/5000: episode: 44, duration: 1.709s, episode steps: 76, steps per second: 44, episode reward: 8.696, mean reward: 0.114 [0.027, 0.133], mean action: 1.011 [0.431, 1.678], mean observation: 0.207 [-535.466, 567.578], loss: 0.017387, mean_squared_error: 0.034774, mean_q: 1.818803\n",
      " 4387/5000: episode: 45, duration: 1.667s, episode steps: 74, steps per second: 44, episode reward: 8.476, mean reward: 0.115 [0.028, 0.135], mean action: 1.028 [0.517, 1.506], mean observation: 0.042 [-522.377, 693.985], loss: 0.010417, mean_squared_error: 0.020835, mean_q: 1.837376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4463/5000: episode: 46, duration: 1.748s, episode steps: 76, steps per second: 43, episode reward: 8.679, mean reward: 0.114 [0.021, 0.137], mean action: 1.036 [0.554, 1.641], mean observation: 0.251 [-537.726, 625.027], loss: 0.020683, mean_squared_error: 0.041366, mean_q: 1.863481\n",
      " 4542/5000: episode: 47, duration: 1.759s, episode steps: 79, steps per second: 45, episode reward: 8.980, mean reward: 0.114 [0.025, 0.135], mean action: 0.990 [0.597, 1.475], mean observation: 0.127 [-522.343, 451.764], loss: 0.017425, mean_squared_error: 0.034850, mean_q: 1.857340\n",
      " 4636/5000: episode: 48, duration: 2.342s, episode steps: 94, steps per second: 40, episode reward: 10.542, mean reward: 0.112 [0.014, 0.132], mean action: 1.036 [0.298, 2.002], mean observation: -0.053 [-533.976, 677.040], loss: 0.013161, mean_squared_error: 0.026321, mean_q: 1.874941\n",
      " 4710/5000: episode: 49, duration: 1.648s, episode steps: 74, steps per second: 45, episode reward: 8.494, mean reward: 0.115 [0.028, 0.135], mean action: 0.974 [0.486, 1.549], mean observation: 0.082 [-527.809, 358.889], loss: 0.015002, mean_squared_error: 0.030004, mean_q: 1.863563\n",
      " 4785/5000: episode: 50, duration: 1.729s, episode steps: 75, steps per second: 43, episode reward: 8.597, mean reward: 0.115 [0.025, 0.136], mean action: 0.977 [0.379, 1.640], mean observation: 0.036 [-530.014, 359.168], loss: 0.013948, mean_squared_error: 0.027896, mean_q: 1.887444\n",
      " 4866/5000: episode: 51, duration: 1.882s, episode steps: 81, steps per second: 43, episode reward: 9.218, mean reward: 0.114 [0.026, 0.134], mean action: 1.042 [0.627, 1.576], mean observation: 0.252 [-530.871, 443.978], loss: 0.023122, mean_squared_error: 0.046244, mean_q: 1.931517\n",
      " 4942/5000: episode: 52, duration: 1.777s, episode steps: 76, steps per second: 43, episode reward: 8.607, mean reward: 0.113 [0.018, 0.133], mean action: 0.934 [0.460, 1.399], mean observation: 0.267 [-534.332, 711.133], loss: 0.015876, mean_squared_error: 0.031753, mean_q: 1.938030\n",
      "done, took 146.320 seconds\n",
      "\n",
      "\n",
      "do symetric loss back propigation\n",
      "\n",
      "\n",
      "testing\n",
      "gravity: -4.800000000000001 VA: 2.0\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 8.523, steps: 75\n",
      "\n",
      "\n",
      "iteration: 2\n",
      "history [116.  75.]\n",
      "-4.800000000000001 2.0\n",
      "Training for 5000 steps ...\n",
      "   80/5000: episode: 1, duration: 0.960s, episode steps: 80, steps per second: 83, episode reward: 8.971, mean reward: 0.112 [0.017, 0.132], mean action: 0.972 [0.161, 1.715], mean observation: 0.183 [-529.040, 534.410], loss: --, mean_squared_error: --, mean_q: --\n",
      "  161/5000: episode: 2, duration: 1.002s, episode steps: 81, steps per second: 81, episode reward: 9.205, mean reward: 0.114 [0.022, 0.134], mean action: 0.997 [0.398, 1.604], mean observation: 0.097 [-539.849, 543.304], loss: --, mean_squared_error: --, mean_q: --\n",
      "  257/5000: episode: 3, duration: 1.309s, episode steps: 96, steps per second: 73, episode reward: 10.983, mean reward: 0.114 [0.014, 0.135], mean action: 0.999 [0.456, 1.777], mean observation: 0.174 [-529.825, 612.628], loss: --, mean_squared_error: --, mean_q: --\n",
      "  333/5000: episode: 4, duration: 0.904s, episode steps: 76, steps per second: 84, episode reward: 8.686, mean reward: 0.114 [0.024, 0.135], mean action: 0.966 [0.484, 1.429], mean observation: 0.176 [-527.122, 570.524], loss: --, mean_squared_error: --, mean_q: --\n",
      "  410/5000: episode: 5, duration: 0.945s, episode steps: 77, steps per second: 82, episode reward: 8.739, mean reward: 0.113 [0.023, 0.136], mean action: 1.033 [0.570, 1.661], mean observation: 0.148 [-526.902, 580.531], loss: --, mean_squared_error: --, mean_q: --\n",
      "  490/5000: episode: 6, duration: 1.001s, episode steps: 80, steps per second: 80, episode reward: 9.077, mean reward: 0.113 [0.024, 0.135], mean action: 1.052 [0.585, 1.484], mean observation: -0.004 [-527.173, 433.239], loss: --, mean_squared_error: --, mean_q: --\n",
      "  592/5000: episode: 7, duration: 1.606s, episode steps: 102, steps per second: 64, episode reward: 11.227, mean reward: 0.110 [0.022, 0.132], mean action: 1.005 [0.314, 1.621], mean observation: 0.136 [-888.238, 638.645], loss: --, mean_squared_error: --, mean_q: --\n",
      "  663/5000: episode: 8, duration: 0.865s, episode steps: 71, steps per second: 82, episode reward: 8.240, mean reward: 0.116 [0.024, 0.136], mean action: 1.017 [0.519, 1.430], mean observation: 0.101 [-543.141, 622.405], loss: --, mean_squared_error: --, mean_q: --\n",
      "  738/5000: episode: 9, duration: 0.881s, episode steps: 75, steps per second: 85, episode reward: 8.553, mean reward: 0.114 [0.026, 0.135], mean action: 1.007 [0.293, 1.683], mean observation: 0.117 [-540.794, 585.558], loss: --, mean_squared_error: --, mean_q: --\n",
      "  823/5000: episode: 10, duration: 1.142s, episode steps: 85, steps per second: 74, episode reward: 9.540, mean reward: 0.112 [0.017, 0.133], mean action: 1.023 [0.545, 1.710], mean observation: 0.126 [-539.496, 743.877], loss: --, mean_squared_error: --, mean_q: --\n",
      "  926/5000: episode: 11, duration: 1.510s, episode steps: 103, steps per second: 68, episode reward: 11.498, mean reward: 0.112 [0.015, 0.133], mean action: 0.952 [0.383, 1.846], mean observation: 0.168 [-536.808, 583.609], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1002/5000: episode: 12, duration: 0.946s, episode steps: 76, steps per second: 80, episode reward: 8.603, mean reward: 0.113 [0.023, 0.134], mean action: 1.004 [0.526, 1.812], mean observation: 0.165 [-532.710, 641.119], loss: 0.001666, mean_squared_error: 0.003332, mean_q: 1.964074\n",
      " 1076/5000: episode: 13, duration: 1.683s, episode steps: 74, steps per second: 44, episode reward: 8.442, mean reward: 0.114 [0.023, 0.133], mean action: 0.994 [0.461, 1.489], mean observation: 0.188 [-535.034, 524.325], loss: 0.010301, mean_squared_error: 0.020601, mean_q: 2.006834\n",
      " 1153/5000: episode: 14, duration: 1.689s, episode steps: 77, steps per second: 46, episode reward: 8.758, mean reward: 0.114 [0.022, 0.135], mean action: 1.013 [0.564, 1.614], mean observation: 0.028 [-533.448, 378.436], loss: 0.017880, mean_squared_error: 0.035759, mean_q: 1.953261\n",
      " 1253/5000: episode: 15, duration: 2.511s, episode steps: 100, steps per second: 40, episode reward: 11.146, mean reward: 0.111 [0.018, 0.132], mean action: 1.062 [0.632, 2.016], mean observation: 0.070 [-539.573, 801.690], loss: 0.017479, mean_squared_error: 0.034957, mean_q: 1.971547\n",
      " 1334/5000: episode: 16, duration: 1.848s, episode steps: 81, steps per second: 44, episode reward: 9.081, mean reward: 0.112 [0.021, 0.133], mean action: 1.026 [0.491, 1.679], mean observation: 0.111 [-540.638, 500.224], loss: 0.024723, mean_squared_error: 0.049446, mean_q: 2.022529\n",
      " 1415/5000: episode: 17, duration: 1.827s, episode steps: 81, steps per second: 44, episode reward: 9.129, mean reward: 0.113 [0.019, 0.134], mean action: 1.012 [0.545, 1.658], mean observation: 0.099 [-542.603, 550.313], loss: 0.015825, mean_squared_error: 0.031649, mean_q: 2.021376\n",
      " 1512/5000: episode: 18, duration: 2.501s, episode steps: 97, steps per second: 39, episode reward: 11.092, mean reward: 0.114 [0.018, 0.134], mean action: 1.043 [0.203, 1.675], mean observation: 0.017 [-523.638, 390.169], loss: 0.014767, mean_squared_error: 0.029533, mean_q: 2.023901\n",
      " 1586/5000: episode: 19, duration: 1.653s, episode steps: 74, steps per second: 45, episode reward: 8.431, mean reward: 0.114 [0.020, 0.136], mean action: 0.963 [0.319, 1.562], mean observation: 0.243 [-528.712, 418.685], loss: 0.022667, mean_squared_error: 0.045333, mean_q: 2.009005\n",
      " 1661/5000: episode: 20, duration: 1.675s, episode steps: 75, steps per second: 45, episode reward: 8.526, mean reward: 0.114 [0.027, 0.134], mean action: 0.998 [0.622, 1.448], mean observation: -0.091 [-530.152, 440.764], loss: 0.019640, mean_squared_error: 0.039281, mean_q: 2.024184\n",
      " 1751/5000: episode: 21, duration: 2.188s, episode steps: 90, steps per second: 41, episode reward: 10.287, mean reward: 0.114 [0.020, 0.134], mean action: 1.064 [0.415, 1.707], mean observation: 0.088 [-537.559, 943.217], loss: 0.019849, mean_squared_error: 0.039697, mean_q: 2.059097\n",
      " 1827/5000: episode: 22, duration: 1.721s, episode steps: 76, steps per second: 44, episode reward: 8.633, mean reward: 0.114 [0.025, 0.133], mean action: 0.977 [0.490, 1.477], mean observation: 0.228 [-542.236, 607.486], loss: 0.029063, mean_squared_error: 0.058126, mean_q: 2.086808\n"
     ]
    }
   ],
   "source": [
    "for i in range(100): # Train in smaller batches to allow for interuption\n",
    "    print(\"\\n\\niteration:\",i)\n",
    "    print('history',data)\n",
    "    print(agent.env.get_grav(),agent.env.get_VA())\n",
    "    agent.fit(nb_steps=T_steps, visualize=False, verbose=2)\n",
    "    ## Always save new weights\n",
    "    agent.save_weights( )\n",
    "    \n",
    "    steps_ = agent.test_get_steps(nb_episodes=1, visualize=True, nb_max_episode_steps=W_steps)\n",
    "    data = np.append(data,[steps_])\n",
    "    np.savez('osim-rl/steps_plot.npz',data=data)\n",
    "    if steps_>(W_steps*7)//10:\n",
    "#        T_steps = (T_steps*5)//4\n",
    "#        W_steps = (W_steps*3)//2\n",
    "        agent.search_VA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data)\n",
    "plt.title('run steps')\n",
    "plt.xlabel('iterations')\n",
    "plt.savefig('osim-rl/steps_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000): # Train in smaller batches to allow for interuption\n",
    "    print(\"\\n\\niteration:\",i)\n",
    "    print('history',data)\n",
    "    print(agent.env.get_grav(),agent.env.get_VA())\n",
    "    agent.fit(nb_steps=T_steps, visualize=False, verbose=2)\n",
    "    ## Always save new weights\n",
    "    agent.save_weights( )\n",
    "    \n",
    "    steps_ = agent.test_get_steps(nb_episodes=1, visualize=True, nb_max_episode_steps=W_steps)\n",
    "    data = np.append(data,[steps_])\n",
    "    np.savez('osim-rl/steps_plot.npz',data=data)\n",
    "    if steps_>(W_steps*7)//10:\n",
    "#        T_steps = (T_steps*5)//4\n",
    "#        W_steps = (W_steps*3)//2\n",
    "        agent.search_VA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data)\n",
    "plt.title('run steps')\n",
    "plt.xlabel('iterations')\n",
    "plt.savefig('osim-rl/steps_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = agent.test(nb_episodes=10, visualize=True, nb_max_episode_steps=1000)\n",
    "print(h)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
